# Comparing `tmp/galaxy-data-22.1.1.tar.gz` & `tmp/galaxy-data-23.0.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "galaxy-data-22.1.1.tar", last modified: Mon Aug 22 19:45:45 2022, max compression
+gzip compressed data, was "/home/runner/work/galaxy/galaxy/packages/data/dist/.tmp-ukwe_ey1/galaxy-data-23.0.1.tar", last modified: Thu Jun  8 17:41:23 2023, max compression
```

## Comparing `galaxy-data-22.1.1.tar` & `galaxy-data-23.0.1.tar`

### file list

```diff
@@ -1,830 +1,362 @@
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.884919 galaxy-data-22.1.1/
--rw-r--r--   0 mvandenb   (501) staff       (20)      341 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/HISTORY.rst
--rw-r--r--   0 mvandenb   (501) staff       (20)    11109 2021-09-10 08:52:38.000000 galaxy-data-22.1.1/LICENSE
--rw-r--r--   0 mvandenb   (501) staff       (20)      249 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/MANIFEST.in
--rw-r--r--   0 mvandenb   (501) staff       (20)     2954 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/Makefile
--rw-r--r--   0 mvandenb   (501) staff       (20)     1706 2022-08-22 19:45:45.885302 galaxy-data-22.1.1/PKG-INFO
--rw-r--r--   0 mvandenb   (501) staff       (20)      333 2022-03-24 19:47:11.000000 galaxy-data-22.1.1/README.rst
--rw-r--r--   0 mvandenb   (501) staff       (20)       89 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/dev-requirements.txt
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.668128 galaxy-data-22.1.1/galaxy/
--rw-r--r--   0 mvandenb   (501) staff       (20)       91 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/__init__.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.682965 galaxy-data-22.1.1/galaxy/datatypes/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3960 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/_schema.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2749 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/annotation.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6586 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/anvio.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     8481 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/assembly.py
--rw-r--r--   0 mvandenb   (501) staff       (20)   143175 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/binary.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    21536 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/blast.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      443 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/checkers.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      425 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/chrominfo.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    32335 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/constructive_solid_geometry.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.710577 galaxy-data-22.1.1/galaxy/datatypes/converters/
--rw-r--r--   0 mvandenb   (501) staff       (20)      117 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/.tt_skip
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      693 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bam_to_bai.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1318 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bam_to_bigwig_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1930 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bcf_bcf_uncompressed_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1735 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bed_gff_or_vcf_to_bigwig_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      721 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_fli_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3259 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_gff_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      826 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_gff_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      871 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_interval_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      819 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bedgraph_to_bigwig_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1707 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bgzip.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1995 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/biom.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1238 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/bz2_to_uncompressed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     2290 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/cml_to_smi_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      514 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/cram_to_bam.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      838 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/cram_to_bam_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      758 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/csv_to_tabular.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)       71 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/dbkeys.loc.test
--rw-r--r--   0 mvandenb   (501) staff       (20)      781 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_2bit.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      978 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_bowtie_base_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1063 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_bowtie_color_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      723 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_fai.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1502 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_len.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      818 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_len.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1784 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_tabular_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      845 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_tabular_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1335 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fastq_to_fqtoc.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      779 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fastq_to_fqtoc.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1706 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      838 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3813 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      792 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1674 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_bed_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      766 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_bed_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      801 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_fli_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1043 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_interval_index_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      812 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_interval_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      833 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gro_to_pdb.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1029 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/gz_to_uncompressed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      700 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/hg17.len
--rw-r--r--   0 mvandenb   (501) staff       (20)     1073 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/inchi_to_mol_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1044 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed12_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      992 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed6_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     2189 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      968 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     8254 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1009 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     2118 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bgzip_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1714 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bigwig_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3394 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_fli.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1515 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_interval_index_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1026 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_interval_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1712 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_tabix_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2380 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_tabix_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      846 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/len_to_linecount.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3744 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_fped_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      751 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_fped_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3608 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_pbed_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      773 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_pbed_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1048 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_fasta_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      717 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_fasta_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1114 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_interval_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      774 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_interval_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     2822 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/mdconvert.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1066 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/mol2_to_mol_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)    11846 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/molecules_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      938 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/neostorezip_to_neostore_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      576 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/parquet_to_csv_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      844 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/parquet_to_csv_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     4160 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_ldreduced_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      782 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_ldreduced_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     2546 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_to_lped_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      802 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_to_lped_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      833 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pdb_to_gro.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1391 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      895 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      777 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      891 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      653 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      926 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1219 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/sam_to_bigwig_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      834 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/sam_to_unsorted_bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1068 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/smi_to_mol_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     2287 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/smi_to_smi_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1292 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/tabular_csv.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      836 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/tabular_to_csv.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1062 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/tabular_to_dbnsfp.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      828 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/tabular_to_dbnsfp.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      933 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/tar_to_directory.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      911 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/to_coordinate_sorted_bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      943 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/to_qname_sorted_bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      282 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/tool_data_table_conf.xml.test
--rw-r--r--   0 mvandenb   (501) staff       (20)     1537 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/uncompressed_to_gz.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      891 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_bgzip_to_tabix_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      892 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      968 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      547 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      916 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      923 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/wig_to_bigwig_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      902 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/wiggle_to_simple_converter.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      942 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/converters/wiggle_to_simple_converter.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1225 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/coverage.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    49825 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/data.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.713272 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/
--rw-r--r--   0 mvandenb   (501) staff       (20)      953 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    12051 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/base.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2415 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/chunk.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    13703 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    29936 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/dataset.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6595 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/decorators.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1186 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/exceptions.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5858 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/external.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5277 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/hierarchy.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     9440 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/dataproviders/line.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.714377 galaxy-data-22.1.1/galaxy/datatypes/display_applications/
--rw-r--r--   0 mvandenb   (501) staff       (20)      125 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    15332 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/application.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.661095 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.714744 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/biom/
--rw-r--r--   0 mvandenb   (501) staff       (20)      367 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/biom/biom_simple.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.715716 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1520 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3929 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_gff.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3992 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_interval_as_bed.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.716493 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1663 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_gff.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1863 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_interval_as_bed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1675 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_wig.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.716740 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/icn3d/
--rw-r--r--   0 mvandenb   (501) staff       (20)      441 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/icn3d/icn3d_simple.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.718789 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/
--rw-r--r--   0 mvandenb   (501) staff       (20)      767 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      720 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bb.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      794 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      843 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bedgraph.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1108 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bigwig.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      795 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/gtf.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1084 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/wig.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.721148 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/
--rw-r--r--   0 mvandenb   (501) staff       (20)     5123 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     3887 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/bigwig.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     4999 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/genbank.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     4747 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/genome_fasta.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     4937 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/gff.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     4965 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/interval_as_bed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     5400 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/vcf.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.721412 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/image/
--rw-r--r--   0 mvandenb   (501) staff       (20)      434 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/image/avivator.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.721665 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/intermine/
--rw-r--r--   0 mvandenb   (501) staff       (20)      420 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/intermine/intermine_simple.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.722228 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/iobio/
--rw-r--r--   0 mvandenb   (501) staff       (20)      442 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/iobio/bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)      488 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/iobio/vcf.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.722498 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/minerva/
--rw-r--r--   0 mvandenb   (501) staff       (20)      429 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/minerva/tabular.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.723027 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/rviewer/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1254 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/rviewer/bed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1236 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/rviewer/vcf.xml
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.724989 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1456 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/bam.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1295 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/bigbed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1295 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/bigwig.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1528 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/interval_as_bed.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1274 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/maf_customtrack.xml
--rwxr-xr-x   0 mvandenb   (501) staff       (20)      436 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/trackhub.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1432 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/vcf.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)    11842 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/parameters.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1189 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/util.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.725335 galaxy-data-22.1.1/galaxy/datatypes/display_applications/xsd/
--rw-r--r--   0 mvandenb   (501) staff       (20)    16714 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/display_applications/xsd/geda.xsd
--rw-r--r--   0 mvandenb   (501) staff       (20)     1435 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/flow.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    42735 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/genetics.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3615 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/gis.py
--rwxr-xr-x   0 mvandenb   (501) staff       (20)    20248 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/goldenpath.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5698 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/graph.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2475 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/hdf5.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    19615 2022-08-22 19:45:27.000000 galaxy-data-22.1.1/galaxy/datatypes/images.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    75502 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/interval.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    14623 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/isa.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6878 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/media.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1069 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/metacyto.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1117 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/metadata.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5530 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/microarrays.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    61477 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/molecules.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    36495 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/mothur.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     8963 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/msa.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5776 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/neo4j.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2567 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/ngsindex.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6088 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/phylip.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    10316 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/plant_tribes.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    33397 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/proteomics.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3734 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/qualityscore.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    61397 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/registry.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    58430 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/sequence.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      896 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/set_metadata_tool.xml
--rw-r--r--   0 mvandenb   (501) staff       (20)    33905 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/sniff.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6796 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/spaln.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3457 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/speech.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    67642 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/tabular.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.798644 galaxy-data-22.1.1/galaxy/datatypes/test/
--rw-r--r--   0 mvandenb   (501) staff       (20)        2 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/0.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)        1 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/0_nonewline
--rw-r--r--   0 mvandenb   (501) staff       (20)     2633 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.afg
--rw-r--r--   0 mvandenb   (501) staff       (20)     2890 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.arff
--rw-r--r--   0 mvandenb   (501) staff       (20)     3537 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.bam
--rw-r--r--   0 mvandenb   (501) staff       (20)     2370 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.bcf
--rw-r--r--   0 mvandenb   (501) staff       (20)     5777 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.bcf_uncompressed
--rw-r--r--   0 mvandenb   (501) staff       (20)     4202 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.bed
--rw-r--r--   0 mvandenb   (501) staff       (20)     1870 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.cmap
--rw-r--r--   0 mvandenb   (501) staff       (20)      178 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.dzi
--rw-r--r--   0 mvandenb   (501) staff       (20)    19456 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.excel.xls
--rw-r--r--   0 mvandenb   (501) staff       (20)     6105 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.excel.xlsx
--rw-r--r--   0 mvandenb   (501) staff       (20)     7012 2022-03-24 19:47:11.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fasta.gz
--rw-r--r--   0 mvandenb   (501) staff       (20)      419 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fastq
--rw-r--r--   0 mvandenb   (501) staff       (20)      180 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fastqcssanger
--rw-r--r--   0 mvandenb   (501) staff       (20)      178 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fastqsanger
--rw-r--r--   0 mvandenb   (501) staff       (20)      155 2021-09-10 08:52:40.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fastqsanger.bz2
--rw-r--r--   0 mvandenb   (501) staff       (20)      161 2021-09-10 08:52:40.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fastqsanger.gz
--rw-r--r--   0 mvandenb   (501) staff       (20)      419 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.fastqsolexa
--rw-r--r--   0 mvandenb   (501) staff       (20)     2249 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.genbank
--rw-r--r--   0 mvandenb   (501) staff       (20)     3838 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.genbank.gz
--rw-r--r--   0 mvandenb   (501) staff       (20)       68 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.gg
--rw-r--r--   0 mvandenb   (501) staff       (20)     6176 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.h5mlm
--rw-r--r--   0 mvandenb   (501) staff       (20)     3848 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.hdt
--rw-r--r--   0 mvandenb   (501) staff       (20)        4 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.json
--rw-r--r--   0 mvandenb   (501) staff       (20)      227 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.memepsp
--rw-r--r--   0 mvandenb   (501) staff       (20)     1064 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.mrc
--rw-r--r--   0 mvandenb   (501) staff       (20)     1248 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.mtx
--rw-r--r--   0 mvandenb   (501) staff       (20)     1091 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.obo
--rw-r--r--   0 mvandenb   (501) staff       (20)    29775 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.ome.tiff
--rw-r--r--   0 mvandenb   (501) staff       (20)    11606 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.owl
--rw-r--r--   0 mvandenb   (501) staff       (20)     1989 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.phyloxml
--rw-r--r--   0 mvandenb   (501) staff       (20)      457 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.ptkscmp
--rw-r--r--   0 mvandenb   (501) staff       (20)     4336 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.qname_sorted.bam
--rw-r--r--   0 mvandenb   (501) staff       (20)    11383 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.sam
--rw-r--r--   0 mvandenb   (501) staff       (20)      100 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.sff
--rw-r--r--   0 mvandenb   (501) staff       (20)      329 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.smat
--rw-r--r--   0 mvandenb   (501) staff       (20)      307 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.star
--rw-r--r--   0 mvandenb   (501) staff       (20)      673 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.stockholm
--rw-r--r--   0 mvandenb   (501) staff       (20)    19378 2022-03-24 19:47:11.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.tiff
--rw-r--r--   0 mvandenb   (501) staff       (20)      351 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.ttl
--rw-r--r--   0 mvandenb   (501) staff       (20)     1242 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.vcf
--rw-r--r--   0 mvandenb   (501) staff       (20)      422 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.vcf_bgzip
--rw-r--r--   0 mvandenb   (501) staff       (20)     1236 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.xmfa
--rw-r--r--   0 mvandenb   (501) staff       (20)     1442 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1.yaml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1651 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/10col.pileup
--rw-r--r--   0 mvandenb   (501) staff       (20)      730 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1_1119_2_22_001-1.par
--rw-r--r--   0 mvandenb   (501) staff       (20)      111 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1_1119_2_22_001.par
--rw-r--r--   0 mvandenb   (501) staff       (20)     1750 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1_1119_2_22_001.textgrid
--rw-r--r--   0 mvandenb   (501) staff       (20)    23929 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/1imzml
--rw-r--r--   0 mvandenb   (501) staff       (20)     1854 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.cmap
--rw-r--r--   0 mvandenb   (501) staff       (20)    54312 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.cram
--rw-r--r--   0 mvandenb   (501) staff       (20)      182 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.fastq
--rw-r--r--   0 mvandenb   (501) staff       (20)      182 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.fastqsolexa
--rw-r--r--   0 mvandenb   (501) staff       (20)      167 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.mtx
--rw-r--r--   0 mvandenb   (501) staff       (20)    59858 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.shuffled.unsorted.bam
--rw-r--r--   0 mvandenb   (501) staff       (20)       93 2022-03-24 19:47:11.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)        4 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/2.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)      304 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/3.fastq
--rw-r--r--   0 mvandenb   (501) staff       (20)      159 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/3.mtx
--rw-r--r--   0 mvandenb   (501) staff       (20)       24 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/3.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)     1666 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/3unsorted.bam
--rw-r--r--   0 mvandenb   (501) staff       (20)       61 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/4.bed
--rw-r--r--   0 mvandenb   (501) staff       (20)      292 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/4.fastqsanger
--rw-r--r--   0 mvandenb   (501) staff       (20)    10122 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/454Score.pdf
--rw-r--r--   0 mvandenb   (501) staff       (20)     2207 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/5e5z.gro
--rw-r--r--   0 mvandenb   (501) staff       (20)    28917 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/5e5z.pdb
--rw-r--r--   0 mvandenb   (501) staff       (20)     7115 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/5e5z.pqr
--rw-r--r--   0 mvandenb   (501) staff       (20)      704 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/6col.pileup
--rw-r--r--   0 mvandenb   (501) staff       (20)    72499 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/A-3105.paf
--rw-r--r--   0 mvandenb   (501) staff       (20)      356 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Acanium.snaphmm
--rw-r--r--   0 mvandenb   (501) staff       (20)    13603 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Accuri_C6_A01_H2O.fcs
--rw-r--r--   0 mvandenb   (501) staff       (20)    23517 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Human.colin.R.activations.label.gii
--rw-r--r--   0 mvandenb   (501) staff       (20)     2000 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/IIT2mean_top_2000bytes.trk
--rw-r--r--   0 mvandenb   (501) staff       (20)     2676 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/NuBBE_1_obabel_3D.pdbqt
--rw-r--r--   0 mvandenb   (501) staff       (20)     5286 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Si.cif
--rw-r--r--   0 mvandenb   (501) staff       (20)     1047 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Si.extxyz
--rw-r--r--   0 mvandenb   (501) staff       (20)      652 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Si.xyz
--rw-r--r--   0 mvandenb   (501) staff       (20)     1669 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Si_lowercase.cell
--rw-r--r--   0 mvandenb   (501) staff       (20)     1304 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Si_multi.xyz
--rw-r--r--   0 mvandenb   (501) staff       (20)     1669 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/Si_uppercase.cell
--rw-r--r--   0 mvandenb   (501) staff       (20)      350 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/T1_top_350bytes.nii1
--rw-r--r--   0 mvandenb   (501) staff       (20)    35304 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/adata_0_6_small.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)    58792 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/adata_0_6_small2.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)    52792 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/adata_0_7_4_small.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)    47720 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/adata_0_7_4_small2.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)    77344 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/adata_unk.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)    60640 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/adata_unk2.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)     1716 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/affy_v_3.cel
--rw-r--r--   0 mvandenb   (501) staff       (20)     1079 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/affy_v_4.cel
--rw-r--r--   0 mvandenb   (501) staff       (20)     2000 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/affy_v_agcc.cel
--rw-r--r--   0 mvandenb   (501) staff       (20)    24436 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/alignment.axt
--rw-r--r--   0 mvandenb   (501) staff       (20)     2757 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/alignment.lav
--rw-r--r--   0 mvandenb   (501) staff       (20)    16078 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/audio_1.wav
--rw-r--r--   0 mvandenb   (501) staff       (20)     1413 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/audio_2.mp3
--rw-r--r--   0 mvandenb   (501) staff       (20)      100 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/avg152T1_LR_nifti2_top_100bytes.nii2
--rw-r--r--   0 mvandenb   (501) staff       (20)      502 2021-09-10 08:52:40.000000 galaxy-data-22.1.1/galaxy/datatypes/test/bam_from_sam.bam
--rw-r--r--   0 mvandenb   (501) staff       (20)     2957 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/big.gfa1
--rw-r--r--   0 mvandenb   (501) staff       (20)    33800 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/biom2_sparse_otu_table_hdf5.biom2
--rw-r--r--   0 mvandenb   (501) staff       (20)    10240 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/brukerbaf.d.tar
--rw-r--r--   0 mvandenb   (501) staff       (20)     1300 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/chebi_57262.v3k.mol
--rw-r--r--   0 mvandenb   (501) staff       (20)      388 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/cntrl_hg19.scidx
--rw-r--r--   0 mvandenb   (501) staff       (20)      167 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/complete.bed
--rw-r--r--   0 mvandenb   (501) staff       (20)     2743 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/diamond.daa
--rw-r--r--   0 mvandenb   (501) staff       (20)     1447 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/diamond.rma6
--rw-r--r--   0 mvandenb   (501) staff       (20)      423 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/diamond_db.dmnd
--rw-r--r--   0 mvandenb   (501) staff       (20)    24309 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/dosimzml
--rw-r--r--   0 mvandenb   (501) staff       (20)    19144 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/drugbank_drugs.cml
--rw-r--r--   0 mvandenb   (501) staff       (20)      973 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/drugbank_drugs.inchi
--rw-r--r--   0 mvandenb   (501) staff       (20)    17032 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/drugbank_drugs.mol2
--rw-r--r--   0 mvandenb   (501) staff       (20)    18997 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/drugbank_drugs.sdf
--rw-r--r--   0 mvandenb   (501) staff       (20)      401 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/galaxy/datatypes/test/drugbank_drugs.smi
--rw-r--r--   0 mvandenb   (501) staff       (20)    10089 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/eg1.agp
--rw-r--r--   0 mvandenb   (501) staff       (20)     2557 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/test/eg2.agp
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/empty.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)    19626 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/example.iqtree
--rw-r--r--   0 mvandenb   (501) staff       (20)     3175 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/example.parquet
--rw-r--r--   0 mvandenb   (501) staff       (20)      225 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/fibers_sparse_top_6_lines.tck
--rw-r--r--   0 mvandenb   (501) staff       (20)     3611 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/file.html
--rw-r--r--   0 mvandenb   (501) staff       (20)     8530 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/gff.gff3
--rw-r--r--   0 mvandenb   (501) staff       (20)     1886 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/gff.gff3.gz
--rw-r--r--   0 mvandenb   (501) staff       (20)      852 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/gis.geojson
--rw-r--r--   0 mvandenb   (501) staff       (20)      589 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/github88.v3k.sdf
--rw-r--r--   0 mvandenb   (501) staff       (20)     5721 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/grch37.75.gtf
--rw-r--r--   0 mvandenb   (501) staff       (20)     9890 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/hello.wav
--rw-r--r--   0 mvandenb   (501) staff       (20)     2121 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/hexrd.eta_ome.npz
--rw-r--r--   0 mvandenb   (501) staff       (20)     4322 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/hexrd.images.npz
--rw-r--r--   0 mvandenb   (501) staff       (20)    10712 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/hexrd.materials.h5
--rw-r--r--   0 mvandenb   (501) staff       (20)   303654 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/imgt.json
--rw-r--r--   0 mvandenb   (501) staff       (20)    94812 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/import.loom.krumsiek11.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)   115956 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/infernal_model.cm
--rw-r--r--   0 mvandenb   (501) staff       (20)       44 2022-04-08 08:13:57.000000 galaxy-data-22.1.1/galaxy/datatypes/test/int-r3-version2.rds
--rw-r--r--   0 mvandenb   (501) staff       (20)       50 2022-04-08 08:13:57.000000 galaxy-data-22.1.1/galaxy/datatypes/test/int-r3.rds
--rw-r--r--   0 mvandenb   (501) staff       (20)       51 2022-04-08 08:13:57.000000 galaxy-data-22.1.1/galaxy/datatypes/test/int-r4.rds
--rw-r--r--   0 mvandenb   (501) staff       (20)      685 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/interv1.bed
--rw-r--r--   0 mvandenb   (501) staff       (20)      234 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/interval.interval
--rw-r--r--   0 mvandenb   (501) staff       (20)     1818 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/issue1818.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)     3774 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/linkstudies.allegro_fparam
--rw-r--r--   0 mvandenb   (501) staff       (20)     3600 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/linkstudies.alohomora_gts
--rw-r--r--   0 mvandenb   (501) staff       (20)    27227 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/linkstudies.linkage_datain
--rw-r--r--   0 mvandenb   (501) staff       (20)     2350 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/linkstudies.linkage_map
--rw-r--r--   0 mvandenb   (501) staff       (20)   408850 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/matrix.cool
--rw-r--r--   0 mvandenb   (501) staff       (20)   480088 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/matrix.mcool
--rw-r--r--   0 mvandenb   (501) staff       (20)      726 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mc_preprocess_summ.metacyto_summary.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)       10 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/md.cpt
--rw-r--r--   0 mvandenb   (501) staff       (20)       10 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/md.edr
--rw-r--r--   0 mvandenb   (501) staff       (20)       10 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/md.trr
--rw-r--r--   0 mvandenb   (501) staff       (20)       10 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/md.xtc
--rw-r--r--   0 mvandenb   (501) staff       (20)   176755 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/megablast_xml_parser_test1.blastxml
--rw-r--r--   0 mvandenb   (501) staff       (20)    39076 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.axes
--rw-r--r--   0 mvandenb   (501) staff       (20)    50007 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.filter
--rw-r--r--   0 mvandenb   (501) staff       (20)      288 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.freq
--rw-r--r--   0 mvandenb   (501) staff       (20)    31215 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.lower.dist
--rw-r--r--   0 mvandenb   (501) staff       (20)       39 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.map
--rw-r--r--   0 mvandenb   (501) staff       (20)      319 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.oligos
--rw-r--r--   0 mvandenb   (501) staff       (20)    25587 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.otu
--rw-r--r--   0 mvandenb   (501) staff       (20)      959 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.pair.dist
--rw-r--r--   0 mvandenb   (501) staff       (20)     1470 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.quan
--rw-r--r--   0 mvandenb   (501) staff       (20)     3690 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.ref.taxonomy
--rw-r--r--   0 mvandenb   (501) staff       (20)       72 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.sabund
--rw-r--r--   0 mvandenb   (501) staff       (20)      153 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.shared
--rw-r--r--   0 mvandenb   (501) staff       (20)    36453 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false.mothur.square.dist
--rw-r--r--   0 mvandenb   (501) staff       (20)       74 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_false_2.mothur.freq
--rw-r--r--   0 mvandenb   (501) staff       (20)    39078 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.axes
--rw-r--r--   0 mvandenb   (501) staff       (20)    50001 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.filter
--rw-r--r--   0 mvandenb   (501) staff       (20)      285 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.freq
--rw-r--r--   0 mvandenb   (501) staff       (20)    43470 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.lower.dist
--rw-r--r--   0 mvandenb   (501) staff       (20)       38 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.map
--rw-r--r--   0 mvandenb   (501) staff       (20)      320 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.oligos
--rw-r--r--   0 mvandenb   (501) staff       (20)    25580 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.otu
--rw-r--r--   0 mvandenb   (501) staff       (20)      963 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.pair.dist
--rw-r--r--   0 mvandenb   (501) staff       (20)     1465 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.quan
--rw-r--r--   0 mvandenb   (501) staff       (20)     3683 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.ref.taxonomy
--rw-r--r--   0 mvandenb   (501) staff       (20)       70 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.sabund
--rw-r--r--   0 mvandenb   (501) staff       (20)      151 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.shared
--rw-r--r--   0 mvandenb   (501) staff       (20)    87126 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/mothur_datatypetest_true.mothur.square.dist
--rw-r--r--   0 mvandenb   (501) staff       (20)       20 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/not_a_phylip_file.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)        6 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/not_a_xyz_file.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)    24037 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/oxli_countgraph.oxlicg
--rw-r--r--   0 mvandenb   (501) staff       (20)      178 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/oxli_graphlabels.oxligl
--rw-r--r--   0 mvandenb   (501) staff       (20)       28 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/oxli_nodegraph.oxling
--rw-r--r--   0 mvandenb   (501) staff       (20)     1178 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/oxli_stoptags.oxlist
--rw-r--r--   0 mvandenb   (501) staff       (20)     2394 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/oxli_subset.oxliss
--rw-r--r--   0 mvandenb   (501) staff       (20)     1078 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/oxli_tagset.oxlits
--rw-r--r--   0 mvandenb   (501) staff       (20)     4656 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/pbmc3k_tiny.h5ad
--rw-r--r--   0 mvandenb   (501) staff       (20)     9154 2022-03-24 19:46:44.000000 galaxy-data-22.1.1/galaxy/datatypes/test/postgresql_fake.tar.bz2
--rw-r--r--   0 mvandenb   (501) staff       (20)      365 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/q.fps
--rw-r--r--   0 mvandenb   (501) staff       (20)    65025 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/read-db.meryldb
--rw-r--r--   0 mvandenb   (501) staff       (20)    12671 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/ref.bref3
--rw-r--r--   0 mvandenb   (501) staff       (20)      786 2022-03-24 19:46:45.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sam_with_header.sam
--rw-r--r--   0 mvandenb   (501) staff       (20)      290 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sample.gfa2
--rw-r--r--   0 mvandenb   (501) staff       (20)   565520 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sample.pretext
--rw-r--r--   0 mvandenb   (501) staff       (20)      705 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sequence.csfasta
--rw-r--r--   0 mvandenb   (501) staff       (20)    21625 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sequence.fasta
--rw-r--r--   0 mvandenb   (501) staff       (20)      238 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sequence.fastg
--rw-r--r--   0 mvandenb   (501) staff       (20)      531 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sequence.maf
--rw-r--r--   0 mvandenb   (501) staff       (20)     1675 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sequence.qual454
--rw-r--r--   0 mvandenb   (501) staff       (20)      601 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/sequence.qualsolid
--rw-r--r--   0 mvandenb   (501) staff       (20)     2048 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/some.wiff.tar
--rw-r--r--   0 mvandenb   (501) staff       (20)    34630 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/tblastn_four_human_vs_rhodopsin.blastxml
--rw-------   0 mvandenb   (501) staff       (20)        8 2022-02-11 10:59:52.000000 galaxy-data-22.1.1/galaxy/datatypes/test/temp.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)    77824 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.blib
--rw-r--r--   0 mvandenb   (501) staff       (20)    35840 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.dlib
--rw-r--r--   0 mvandenb   (501) staff       (20)      108 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.dta2d
--rw-r--r--   0 mvandenb   (501) staff       (20)      117 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.edta
--rw-r--r--   0 mvandenb   (501) staff       (20)    35840 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.elib
--rw-r--r--   0 mvandenb   (501) staff       (20)   122880 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.fast5.tar
--rw-r--r--   0 mvandenb   (501) staff       (20)    56257 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.fast5.tar.bz2
--rw-r--r--   0 mvandenb   (501) staff       (20)    60443 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.fast5.tar.gz
--rw-r--r--   0 mvandenb   (501) staff       (20)     1877 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.gal
--rw-r--r--   0 mvandenb   (501) staff       (20)     1525 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.gff
--rw-r--r--   0 mvandenb   (501) staff       (20)   120661 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.gmsh.msh
--rw-r--r--   0 mvandenb   (501) staff       (20)     2546 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.gpr
--rw-r--r--   0 mvandenb   (501) staff       (20)     1440 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.grib
--rw-r--r--   0 mvandenb   (501) staff       (20)   103056 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.gtf
--rw-r--r--   0 mvandenb   (501) staff       (20)    55296 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.idpdb
--rw-r--r--   0 mvandenb   (501) staff       (20)    16298 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.jp2
--rw-r--r--   0 mvandenb   (501) staff       (20)     3207 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.kroenik
--rw-r--r--   0 mvandenb   (501) staff       (20)    14147 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.loom
--rw-r--r--   0 mvandenb   (501) staff       (20)   173408 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.mz5
--rw-r--r--   0 mvandenb   (501) staff       (20)     2165 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.mztab
--rw-r--r--   0 mvandenb   (501) staff       (20)     8318 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.mztab2
--rw-r--r--   0 mvandenb   (501) staff       (20)   102400 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.ncbitaxonomy.sqlite
--rw-r--r--   0 mvandenb   (501) staff       (20)    23796 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.neper.tesr
--rw-r--r--   0 mvandenb   (501) staff       (20)    18136 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.neper.tess
--rw-r--r--   0 mvandenb   (501) staff       (20)   102400 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.osw
--rw-r--r--   0 mvandenb   (501) staff       (20)      238 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.peff
--rw-r--r--   0 mvandenb   (501) staff       (20)     2126 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.peplist
--rw-r--r--   0 mvandenb   (501) staff       (20)     3836 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.plyascii
--rw-r--r--   0 mvandenb   (501) staff       (20)    65536 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.pqp
--rw-r--r--   0 mvandenb   (501) staff       (20)    63814 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.psms
--rw-r--r--   0 mvandenb   (501) staff       (20)       71 2022-04-08 08:13:57.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.rdata
--rw-r--r--   0 mvandenb   (501) staff       (20)    90112 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.sqmass
--rw-r--r--   0 mvandenb   (501) staff       (20)    83977 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.vtkascii
--rw-r--r--   0 mvandenb   (501) staff       (20)    16275 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test.vtkbinary
--rw-r--r--   0 mvandenb   (501) staff       (20)    15876 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_binary_loom_spec_version.loom
--rwxr-xr-x   0 mvandenb   (501) staff       (20)   517716 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_charmm.vel
--rw-r--r--   0 mvandenb   (501) staff       (20)      511 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_ensembl.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)    93956 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_glucose_vacuum.dcd
--rw-r--r--   0 mvandenb   (501) staff       (20)     1654 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_relaxed_interleaved.phylip
--rw-r--r--   0 mvandenb   (501) staff       (20)      800 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_space.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)       89 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_strict_interleaved.phylip
--rw-r--r--   0 mvandenb   (501) staff       (20)      118 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_tab.bed
--rw-r--r--   0 mvandenb   (501) staff       (20)     1020 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_tab1.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)       12 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/datatypes/test/test_tab2.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)      337 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/too_many_comments_gff3.tabular
--rw-r--r--   0 mvandenb   (501) staff       (20)      265 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/ucsc.customtrack
--rw-r--r--   0 mvandenb   (501) staff       (20)      392 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/vcf_gzipped.vcf.gz
--rw-r--r--   0 mvandenb   (501) staff       (20)   112419 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/video_1.mp4
--rw-r--r--   0 mvandenb   (501) staff       (20)      274 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/datatypes/test/wiggle.wig
--rw-r--r--   0 mvandenb   (501) staff       (20)    36804 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/text.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1846 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/tracks.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5718 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/triples.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4276 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/upload_util.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.799718 galaxy-data-22.1.1/galaxy/datatypes/util/
--rw-r--r--   0 mvandenb   (501) staff       (20)       40 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/datatypes/util/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      480 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/util/generic_util.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    18176 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/util/gff_util.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    30283 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/util/maf_utilities.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     9217 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/datatypes/xml.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.804207 galaxy-data-22.1.1/galaxy/model/
--rw-r--r--   0 mvandenb   (501) staff       (20)   376914 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4754 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/base.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    12254 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/custom_types.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3719 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/database_heartbeat.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4275 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/database_utils.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.805914 galaxy-data-22.1.1/galaxy/model/dataset_collections/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:10.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5652 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/builder.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4333 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/matching.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      892 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/registry.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     7231 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/structure.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1094 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/subcollections.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6358 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/type_description.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.806709 galaxy-data-22.1.1/galaxy/model/dataset_collections/types/
--rw-r--r--   0 mvandenb   (501) staff       (20)      629 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/types/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      545 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/types/list.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1418 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/dataset_collections/types/paired.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     8321 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/item_attrs.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3292 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/mapping.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    27727 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/metadata.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.807404 galaxy-data-22.1.1/galaxy/model/migrate/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     8069 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/check.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      989 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/migrate.cfg
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.808074 galaxy-data-22.1.1/galaxy/model/migrate/triggers/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/triggers/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5624 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/triggers/history_update_time_field.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6144 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/triggers/update_audit_table.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.856177 galaxy-data-22.1.1/galaxy/model/migrate/versions/
--rw-r--r--   0 mvandenb   (501) staff       (20)     9726 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0001_initial_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1132 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0002_metadata_file_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    29478 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0003_security_and_libraries.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1450 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0004_indexes_and_defaults.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      135 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0005_cleanup_datasets_fix.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1073 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0006_change_qual_datatype.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1417 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0007_sharing_histories.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5883 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0008_galaxy_forms.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1122 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0009_request_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2284 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0010_hda_display_at_authz_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1078 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0011_v0010_mysql_index_fix.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2859 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0012_user_address.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3818 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0013_change_lib_item_templates_to_forms.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2095 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0014_pages.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3210 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0015_tagging.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1224 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0016_v0015_mysql_index_fix.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1074 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0017_library_item_indexes.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5008 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0018_ordered_tags_and_page_tags.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1195 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0019_request_library_folder.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1929 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0020_library_upload_job.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      973 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0021_user_prefs.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2158 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0022_visualization_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      930 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0023_page_published_and_deleted_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      865 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0024_page_slug_unique_constraint.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      794 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0025_user_info.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     7922 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0026_cloud_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1670 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0027_request_events.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      874 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0028_external_metadata_file_override.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1314 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0029_user_actions.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      866 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0030_history_slug_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4900 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0031_community_and_workflow_tags.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      917 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0032_stored_workflow_slug_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2657 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0033_published_cols_for_histories_and_workflows.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1194 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0034_page_user_share_association.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3497 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0035_item_annotations_and_workflow_step_tags.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2729 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0036_add_deleted_column_to_library_template_assoc_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2722 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0037_samples_library.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3652 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0038_add_inheritable_column_to_library_template_assoc_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      923 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0039_add_synopsis_column_to_library_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1034 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0040_page_annotations.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1932 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0041_workflow_invocation.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2054 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0042_workflow_invocation_fix.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4716 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0043_visualization_sharing_tagging_annotating.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      630 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0044_add_notify_column_to_request_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1496 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0045_request_type_permissions_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1148 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0046_post_job_actions.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1753 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0047_job_table_user_id_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1022 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0048_dataset_instance_state_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1115 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0049_api_keys_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     7946 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0050_drop_cloud_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      968 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0051_imported_col_for_jobs_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3045 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0052_sample_dataset_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4302 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0053_item_ratings.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2039 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0054_visualization_dbkey.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1000 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0055_add_pja_assoc_for_jobs.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      953 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0056_workflow_outputs.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1557 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0057_request_notify.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1648 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0058_history_import_export.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1679 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0059_sample_dataset_file_path.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2371 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0060_history_archive_import.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1579 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0061_tasks.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1794 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0062_user_openid_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1755 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0063_sequencer_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2099 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0064_add_run_and_sample_run_association_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5160 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2113 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0066_deferred_job_and_transfer_job_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    12212 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0067_populate_sequencer_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6999 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0068_rename_sequencer_to_external_services.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      887 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0069_rename_sequencer_form_type.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      917 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0070_add_info_column_to_deferred_job_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      967 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0071_add_history_and_workflow_to_sample.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1088 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0072_add_pid_and_socket_columns_to_transfer_job_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      901 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0073_add_ldda_to_implicit_conversion_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2004 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0074_add_purged_column_to_library_dataset_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      973 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0075_add_subindex_column_to_run_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4182 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0076_fix_form_values_data_corruption.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1527 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0077_create_tool_tag_association_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1804 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0078_add_columns_for_disk_usage_accounting.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1376 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0079_input_library_to_job_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4077 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0080_quota_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1301 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0081_add_tool_version_to_hda_ldda.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1598 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0082_add_tool_shed_repository_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1181 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0083_add_prepare_files_to_task.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      868 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0084_add_ldda_id_to_implicit_conversion_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      918 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0085_add_task_info.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2004 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0086_add_tool_shed_repository_table_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1266 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0087_tool_id_guid_map_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2038 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0088_add_installed_changeset_revison_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1138 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0089_add_object_store_id_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1663 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0090_add_tool_shed_repository_table_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4090 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0091_add_tool_version_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1114 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0092_add_migrate_tools_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1157 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0093_add_job_params_col.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      710 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0094_add_job_handler_col.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1935 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0095_hda_subsets.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1363 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0096_openid_provider.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      711 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0097_add_ctx_rev_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1665 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0098_genome_index_tool_data_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1577 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0099_add_tool_dependency_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1831 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0100_alter_tool_dependency_table_version_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      759 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0101_drop_installed_changeset_revision_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1584 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0102_add_tool_dependency_status_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2671 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0103_add_tool_shed_repository_status_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      183 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0104_update_genome_downloader_job_parameters.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     7503 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0105_add_cleanup_event_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4279 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0106_add_missing_indexes.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1825 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0107_add_exit_code_to_job_and_task.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2059 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0108_add_extended_metadata.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2049 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0109_add_repository_dependency_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1114 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0110_add_dataset_uuid.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1389 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0111_add_job_destinations.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1899 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0112_add_data_manager_history_association_and_data_manager_job_association_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      895 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0113_update_migrate_tools_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      903 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0114_update_migrate_tools_table_again.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      509 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0115_longer_user_password_field.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1550 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0116_drop_update_available_col_add_tool_shed_status_col.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1856 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0117_add_user_activation.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1272 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0118_add_hda_extended_metadata.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2066 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0119_job_metrics.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    10944 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0120_dataset_collections.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1241 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0121_workflow_uuids.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1326 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0122_grow_mysql_blobs.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4246 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0123_add_workflow_request_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1195 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0124_job_state_history.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      880 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0125_workflow_step_tracking.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      918 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0126_password_reset.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1859 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0127_output_collection_adjustments.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      646 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0128_session_timeout.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      811 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0129_job_external_output_metadata_validity.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      572 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0130_change_pref_datatype.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4135 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0131_subworkflow_and_input_parameter_modules.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      514 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0132_add_lastpasswordchange_to_user.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1162 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0133_add_dependency_column_to_job.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      128 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0134_hda_set_deleted_if_purged.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1564 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0135_add_library_tags.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6320 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0136_collection_and_workflow_state.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1189 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0137_add_copied_from_job_id_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1193 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0138_add_hda_version.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1370 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0139_add_history_dataset_association_history_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1320 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0140_add_dataset_version_to_job_to_input_dataset_association_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2268 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0141_add_oidc_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      823 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0142_change_numeric_metric_precision.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1223 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0143_add_cloudauthz_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1084 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0144_add_cleanup_event_user_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5665 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0145_add_workflow_step_input.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      613 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0146_workflow_paths.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2051 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0147_job_messages.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1649 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0148_add_checksum_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1747 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0149_dynamic_tools.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      766 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0150_add_create_time_field_for_cloudauthz.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      951 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0151_add_worker_process.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      690 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0152_add_metadata_file_uuid.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1217 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0153_add_custos_authnz_token_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      729 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0154_created_from_basename.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      722 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0155_job_galaxy_version.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2200 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0156_add_interactivetools.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2180 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0157_rework_dataset_validation.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      714 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0158_workflow_reports.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      614 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0159_add_job_external_id_index.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      581 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0160_hda_set_deleted_if_purged_again.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1288 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0161_add_workflow_invocation_output_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      938 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0162_job_only_pjas.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      652 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0163_worker_process_pid.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      772 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0164_page_format.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1617 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0165_add_content_update_time.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      817 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0166_job_state_summary_view.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1185 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0167_add_job_to_input_dataset_collection_element_association.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      660 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0168_stored_workflow_hidden_col.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2928 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0169_add_missing_indexes.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1711 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0170_add_more_missing_indexes.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1164 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0171_schemaorg_metadata.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      790 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0172_it_entrypoint_requires_domain.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      720 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0173_add_job_id_to_dataset.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      611 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0174_readd_update_time_triggers.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2314 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0175_history_audit.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1390 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0176_add_indexes_on_update_time.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      706 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0177_update_job_state_summary.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1630 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0178_drop_deferredjob_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1639 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0179_drop_transferjob_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1080 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/0180_add_vault_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     7189 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/migrate/versions/util.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      878 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/none_like.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.857109 galaxy-data-22.1.1/galaxy/model/orm/
--rw-r--r--   0 mvandenb   (501) staff       (20)       53 2022-02-02 10:06:09.000000 galaxy-data-22.1.1/galaxy/model/orm/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4440 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/orm/engine_factory.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      512 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/orm/now.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5917 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/orm/scripts.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      554 2022-08-18 15:49:02.000000 galaxy-data-22.1.1/galaxy/model/scoped_session.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    23177 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/search.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    78933 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/security.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.857948 galaxy-data-22.1.1/galaxy/model/store/
--rw-r--r--   0 mvandenb   (501) staff       (20)    76905 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/store/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4697 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/store/build_objects.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    38875 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/store/discover.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    18323 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tags.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.858581 galaxy-data-22.1.1/galaxy/model/tool_shed_install/
--rw-r--r--   0 mvandenb   (501) staff       (20)    32140 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      769 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/mapping.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.859248 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4231 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/check.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      998 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/migrate.cfg
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.864104 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1598 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0001_add_tool_shed_repository_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2004 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0002_add_tool_shed_repository_table_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1266 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0003_tool_id_guid_map_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2038 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0004_add_installed_changeset_revison_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1663 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0005_add_tool_shed_repository_table_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4090 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0006_add_tool_version_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1114 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0007_add_migrate_tools_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      711 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0008_add_ctx_rev_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1577 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0009_add_tool_dependency_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1831 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0010_alter_tool_dependency_table_version_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      759 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0011_drop_installed_changeset_revision_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1584 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0012_add_tool_dependency_status_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2671 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0013_add_tool_shed_repository_status_columns.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2049 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0014_add_repository_dependency_tables.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      895 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0015_update_migrate_tools_table.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      903 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0016_update_migrate_tools_table_again.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1550 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/0017_drop_update_available_col_add_tool_shed_status_col.py
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/tool_shed_install/migrate/versions/__init__.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.864628 galaxy-data-22.1.1/galaxy/model/unittest_utils/
--rw-r--r--   0 mvandenb   (501) staff       (20)      230 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/unittest_utils/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3509 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/unittest_utils/data_app.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.865166 galaxy-data-22.1.1/galaxy/model/view/
--rw-r--r--   0 mvandenb   (501) staff       (20)     2606 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/view/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2694 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/model/view/utils.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      432 2022-08-22 19:45:29.000000 galaxy-data-22.1.1/galaxy/project_galaxy_data.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.865700 galaxy-data-22.1.1/galaxy/quota/
--rw-r--r--   0 mvandenb   (501) staff       (20)     8864 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/quota/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     7417 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/quota/_schema.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.867128 galaxy-data-22.1.1/galaxy/schema/
--rw-r--r--   0 mvandenb   (501) staff       (20)     2550 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/schema/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3372 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/schema/fields.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2661 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/schema/remote_files.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    89052 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/schema/schema.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      201 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/schema/types.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.869016 galaxy-data-22.1.1/galaxy/security/
--rw-r--r--   0 mvandenb   (501) staff       (20)     5780 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     5334 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/idencoding.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    18327 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/object_wrapper.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2290 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/passwords.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      763 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/ssh_util.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4583 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/validate_user_input.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     9694 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/galaxy/security/vault.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.870683 galaxy-data-22.1.1/galaxy_data.egg-info/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1706 2022-08-22 19:45:44.000000 galaxy-data-22.1.1/galaxy_data.egg-info/PKG-INFO
--rw-r--r--   0 mvandenb   (501) staff       (20)    37325 2022-08-22 19:45:45.000000 galaxy-data-22.1.1/galaxy_data.egg-info/SOURCES.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)        1 2022-08-22 19:45:44.000000 galaxy-data-22.1.1/galaxy_data.egg-info/dependency_links.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)      154 2022-08-22 19:45:44.000000 galaxy-data-22.1.1/galaxy_data.egg-info/entry_points.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)        1 2022-08-22 19:45:44.000000 galaxy-data-22.1.1/galaxy_data.egg-info/not-zip-safe
--rw-r--r--   0 mvandenb   (501) staff       (20)      347 2022-08-22 19:45:44.000000 galaxy-data-22.1.1/galaxy_data.egg-info/requires.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)        7 2022-08-22 19:45:44.000000 galaxy-data-22.1.1/galaxy_data.egg-info/top_level.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)      387 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/requirements.txt
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.871454 galaxy-data-22.1.1/scripts/
--rw-r--r--   0 mvandenb   (501) staff       (20)     1442 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/scripts/commit_version.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2166 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/scripts/new_version.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      804 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/scripts/print_version_for_release.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    33060 2022-08-22 19:45:45.892680 galaxy-data-22.1.1/setup.cfg
--rw-r--r--   0 mvandenb   (501) staff       (20)     3498 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/setup.py
--rw-r--r--   0 mvandenb   (501) staff       (20)        7 2022-03-24 19:47:11.000000 galaxy-data-22.1.1/test-requirements.txt
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.871672 galaxy-data-22.1.1/tests/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:11.000000 galaxy-data-22.1.1/tests/__init__.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.873479 galaxy-data-22.1.1/tests/data/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:13.000000 galaxy-data-22.1.1/tests/data/__init__.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.873870 galaxy-data-22.1.1/tests/data/dataset_collections/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/tests/data/dataset_collections/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4052 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/dataset_collections/test_matching.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.876885 galaxy-data-22.1.1/tests/data/datatypes/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:13.000000 galaxy-data-22.1.1/tests/data/datatypes/__init__.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.877275 galaxy-data-22.1.1/tests/data/datatypes/converters/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:13.000000 galaxy-data-22.1.1/tests/data/datatypes/converters/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      498 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/converters/test_interval_to_tabix.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.878009 galaxy-data-22.1.1/tests/data/datatypes/dataproviders/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-03-24 19:47:13.000000 galaxy-data-22.1.1/tests/data/datatypes/dataproviders/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    14932 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/dataproviders/test_base_dataproviders.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    11567 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/dataproviders/test_line_dataproviders.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2195 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_bam.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      692 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_bcf.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1569 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_check_required.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      692 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_cram.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1117 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_data.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6318 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_datatypes_registry.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3130 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_sniff.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      447 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_tabular.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1804 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_validation.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1170 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/test_vcf.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1835 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/datatypes/util.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.880453 galaxy-data-22.1.1/tests/data/model/
--rw-r--r--   0 mvandenb   (501) staff       (20)      208 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/tests/data/model/1.txt
--rw-r--r--   0 mvandenb   (501) staff       (20)     4272 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/tests/data/model/2.bed
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/tests/data/model/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1664 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/common.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      389 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/conftest.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.882640 galaxy-data-22.1.1/tests/data/model/mapping/
--rw-r--r--   0 mvandenb   (501) staff       (20)        0 2022-08-18 15:49:03.000000 galaxy-data-22.1.1/tests/data/model/mapping/__init__.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     4414 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/mapping/common.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      835 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/mapping/conftest.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     8200 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/mapping/test_common.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    12526 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/mapping/test_install_model_mapping.py
--rw-r--r--   0 mvandenb   (501) staff       (20)   292991 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/mapping/test_model_mapping.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2450 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/test_database_utils.py
--rw-r--r--   0 mvandenb   (501) staff       (20)      236 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/test_model.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     8673 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/test_model_discovery.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    20430 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/test_model_store.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2663 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/model/test_views.py
-drwxr-xr-x   0 mvandenb   (501) staff       (20)        0 2022-08-22 19:45:45.884545 galaxy-data-22.1.1/tests/data/security/
--rw-r--r--   0 mvandenb   (501) staff       (20)     4760 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/security/test_id_encode_decode.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     2422 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/security/test_passwords.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1172 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/security/test_validate_user_input.py
--rw-r--r--   0 mvandenb   (501) staff       (20)    47700 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/test_galaxy_mapping.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1144 2022-04-08 08:13:57.000000 galaxy-data-22.1.1/tests/data/test_metadata_limit.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     6474 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/test_model_copy.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3139 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/test_mutable_json_column.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     3979 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/test_quota.py
--rw-r--r--   0 mvandenb   (501) staff       (20)     1159 2022-08-22 19:45:28.000000 galaxy-data-22.1.1/tests/data/test_sniff_file_sources.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/
+-rw-r--r--   0 runner    (1001) docker     (123)     2632 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/HISTORY.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    12875 2023-06-08 17:37:03.000000 galaxy-data-23.0.1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      427 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     4013 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/README.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       89 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/dev-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/
+-rw-r--r--   0 runner    (1001) docker     (123)       91 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4483 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/_schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2997 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/annotation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6836 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/anvio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9026 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/assembly.py
+-rw-r--r--   0 runner    (1001) docker     (123)   156175 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/binary.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22547 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/blast.py
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/checkers.py
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/chrominfo.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33391 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/constructive_solid_geometry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      701 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bam_to_bai.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1318 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bam_to_bigwig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1937 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bcf_bcf_uncompressed_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1735 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bed_gff_or_vcf_to_bigwig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      721 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_fli_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3552 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_gff_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      826 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_gff_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      871 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_interval_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      819 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bedgraph_to_bigwig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3692 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bigwig_to_wig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1995 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/biom.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1238 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/bz2_to_uncompressed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/cml_to_smi_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      890 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/cram_to_bam_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      758 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/csv_to_tabular.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      781 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_2bit.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      978 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_bowtie_base_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1063 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_bowtie_color_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      723 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_fai.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1501 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_len.py
+-rw-r--r--   0 runner    (1001) docker     (123)      818 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_len.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1784 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_tabular_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      845 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_tabular_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1357 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fastq_to_fqtoc.py
+-rw-r--r--   0 runner    (1001) docker     (123)      779 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fastq_to_fqtoc.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1706 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      838 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3963 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      792 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1709 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_bed_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      766 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_bed_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      801 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_fli_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1083 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_interval_index_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      812 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_interval_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      841 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gro_to_pdb.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1029 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/gz_to_uncompressed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1081 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/inchi_to_mol_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1044 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed12_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      992 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed6_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     2419 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1009 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4348 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bgzip_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1714 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bigwig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3252 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_fli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_interval_index_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1026 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_interval_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4590 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_tabix_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      846 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/len_to_linecount.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3737 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_fped_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      751 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_fped_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3568 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_pbed_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      773 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_pbed_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1201 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_fasta_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_fasta_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1387 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_interval_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      774 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_interval_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     2816 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/mdconvert.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1074 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/mol2_to_mol_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)    11846 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/molecules_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      938 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/neostorezip_to_neostore_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      577 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/parquet_to_csv_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      844 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/parquet_to_csv_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4269 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_ldreduced_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      782 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_ldreduced_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_to_lped_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      802 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_to_lped_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      841 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pdb_to_gro.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1424 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      895 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      776 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      891 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      653 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      926 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/sam_to_bigwig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      834 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/sam_to_unsorted_bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1076 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/smi_to_mol_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     2287 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/smi_to_smi_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_csv.py
+-rw-r--r--   0 runner    (1001) docker     (123)      836 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_to_csv.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_to_dbnsfp.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      933 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/tar_to_directory.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      911 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/to_coordinate_sorted_bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      943 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/to_qname_sorted_bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1957 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/uncompressed_to_gz.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1181 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_bgzip_to_tabix_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      892 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      991 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      923 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/wig_to_bigwig_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      933 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/wiggle_to_simple_converter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      950 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/converters/wiggle_to_simple_converter.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      906 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/coverage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53221 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/data.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/
+-rw-r--r--   0 runner    (1001) docker     (123)      954 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12033 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2392 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/chunk.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13755 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/column.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29793 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6614 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/decorators.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1186 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5862 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/external.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5233 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/hierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9450 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/dataproviders/line.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/
+-rw-r--r--   0 runner    (1001) docker     (123)      125 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15624 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/application.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/biom/
+-rw-r--r--   0 runner    (1001) docker     (123)      367 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/biom/biom_simple.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3929 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_gff.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3992 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_interval_as_bed.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/
+-rw-r--r--   0 runner    (1001) docker     (123)     1663 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_gff.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1863 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_interval_as_bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1675 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_wig.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/icn3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      441 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/icn3d/icn3d_simple.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/
+-rw-r--r--   0 runner    (1001) docker     (123)      767 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      720 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bb.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      794 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      843 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bedgraph.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1108 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bigwig.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      795 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/gtf.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/wig.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/
+-rw-r--r--   0 runner    (1001) docker     (123)     5123 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     3887 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/bigwig.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4999 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/genbank.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4747 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/genome_fasta.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/gff.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     4965 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/interval_as_bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     5383 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/vcf.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/image/
+-rw-r--r--   0 runner    (1001) docker     (123)      434 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/image/avivator.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/intermine/
+-rw-r--r--   0 runner    (1001) docker     (123)      420 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/intermine/intermine_simple.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/iobio/
+-rw-r--r--   0 runner    (1001) docker     (123)      442 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/iobio/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)      488 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/iobio/vcf.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/minerva/
+-rw-r--r--   0 runner    (1001) docker     (123)      429 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/minerva/tabular.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/qiime/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/qiime/qiime2/
+-rw-r--r--   0 runner    (1001) docker     (123)      368 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/qiime/qiime2/q2view.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/rviewer/
+-rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/rviewer/bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1236 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/rviewer/vcf.xml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/
+-rw-r--r--   0 runner    (1001) docker     (123)     1456 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/bam.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1295 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/bigbed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1295 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/bigwig.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1528 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/interval_as_bed.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1274 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/maf_customtrack.xml
+-rwxr-xr-x   0 runner    (1001) docker     (123)      436 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/trackhub.xml
+-rw-r--r--   0 runner    (1001) docker     (123)     1432 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/vcf.xml
+-rw-r--r--   0 runner    (1001) docker     (123)    12315 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/parameters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1189 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/xsd/
+-rw-r--r--   0 runner    (1001) docker     (123)    16714 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/display_applications/xsd/geda.xsd
+-rw-r--r--   0 runner    (1001) docker     (123)     1601 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/flow.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44260 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/genetics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4230 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/gis.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21230 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/goldenpath.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6120 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2728 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/hdf5.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15642 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/images.py
+-rw-r--r--   0 runner    (1001) docker     (123)    83879 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/interval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15596 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/isa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8435 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/media.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/metacyto.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1117 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5984 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/microarrays.py
+-rw-r--r--   0 runner    (1001) docker     (123)    63504 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/molecules.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37941 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/mothur.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9849 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/msa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5725 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/neo4j.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3055 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/ngsindex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6406 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/phylip.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5344 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/plant_tribes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36677 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/proteomics.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10668 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/qiime2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3915 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/qualityscore.py
+-rw-r--r--   0 runner    (1001) docker     (123)    50280 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)    61062 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/sequence.py
+-rw-r--r--   0 runner    (1001) docker     (123)      896 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/set_metadata_tool.xml
+-rw-r--r--   0 runner    (1001) docker     (123)    35552 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/sniff.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7336 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/spaln.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3949 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/speech.py
+-rw-r--r--   0 runner    (1001) docker     (123)    77900 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/tabular.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49814 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/text.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2111 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/tracks.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6171 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/triples.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4422 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/upload_util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/datatypes/util/
+-rw-r--r--   0 runner    (1001) docker     (123)       40 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/util/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      480 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/util/generic_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19188 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/util/gff_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30760 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/util/maf_utilities.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9613 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/datatypes/xml.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/
+-rw-r--r--   0 runner    (1001) docker     (123)   401837 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4786 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12138 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/custom_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3832 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/database_heartbeat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4749 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/database_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5787 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4617 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/matching.py
+-rw-r--r--   0 runner    (1001) docker     (123)      892 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7632 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/structure.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1088 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/subcollections.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6601 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/type_description.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/types/
+-rw-r--r--   0 runner    (1001) docker     (123)      626 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/types/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      534 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/types/list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/dataset_collections/types/paired.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12186 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/deferred.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1822 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/index_filter_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8282 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/item_attrs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28077 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/migrations/
+-rw-r--r--   0 runner    (1001) docker     (123)      295 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)    20123 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/
+-rw-r--r--   0 runner    (1001) docker     (123)     4527 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/env.py
+-rw-r--r--   0 runner    (1001) docker     (123)      494 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/script.py.mako
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/
+-rw-r--r--   0 runner    (1001) docker     (123)      673 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/186d4835587b_drop_job_state_history_update_time_.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1398 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/3100452fa030_add_workflow_invocation_message_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)      654 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/3a2914d703ca_add_index_wf_r_s_s__workflow_invocation_.py
+-rw-r--r--   0 runner    (1001) docker     (123)      742 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/518c8438a91b_add_when_expression_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)      887 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/59e024ceaca1_add_export_association_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)      781 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/6a67bf27e6a6_deferred_data_tables.py
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/b182f655505f_add_workflow_source_metadata_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3542 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/c39f1de47a04_add_skipped_state_to_collection_job_.py
+-rw-r--r--   0 runner    (1001) docker     (123)      670 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/caa7742f7bca_add_index_wf_r_i_p__workflow_invocation_.py
+-rw-r--r--   0 runner    (1001) docker     (123)      728 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/e0e3bb173ee6_add_column_deleted_to_api_keys.py
+-rw-r--r--   0 runner    (1001) docker     (123)      289 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/e7b6dcb09efd_create_gxy_branch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_tsi/
+-rw-r--r--   0 runner    (1001) docker     (123)      289 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_tsi/d4a650f47a3c_create_tsi_branch.py
+-rw-r--r--   0 runner    (1001) docker     (123)      695 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/alembic.ini
+-rw-r--r--   0 runner    (1001) docker     (123)     9550 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/dbscript.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10225 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/scripts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1750 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/migrations/util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      879 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/none_like.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/orm/
+-rw-r--r--   0 runner    (1001) docker     (123)       53 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/orm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4859 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/orm/engine_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)      511 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/orm/now.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6154 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/orm/scripts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1240 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/orm/util.py
+-rw-r--r--   0 runner    (1001) docker     (123)      554 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/scoped_session.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22810 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/search.py
+-rw-r--r--   0 runner    (1001) docker     (123)    80831 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/security.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/store/
+-rw-r--r--   0 runner    (1001) docker     (123)   135431 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/store/_bco_convert_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4698 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/store/build_objects.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41133 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/store/discover.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2426 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/store/load_objects.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17773 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/store/ro_crate_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17823 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/tags.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/tool_shed_install/
+-rw-r--r--   0 runner    (1001) docker     (123)    32646 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/tool_shed_install/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      708 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/tool_shed_install/mapping.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/triggers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/triggers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5558 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/triggers/history_update_time_field.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6135 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/triggers/update_audit_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)      272 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/triggers/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3728 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/data_app.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29112 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/gxy_model_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1923 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/mapping_testing_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7774 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/model_testing_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10637 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/store_fixtures.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1752 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/unittest_utils/tsi_model_fixtures.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/model/view/
+-rw-r--r--   0 runner    (1001) docker     (123)     2724 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/view/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2598 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/model/view/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/quota/
+-rw-r--r--   0 runner    (1001) docker     (123)     8962 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/quota/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7384 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/quota/_schema.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/schema/
+-rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/schema/bco/
+-rw-r--r--   0 runner    (1001) docker     (123)     2427 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4976 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/description_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/error_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4571 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/execution_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3326 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/io_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1362 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/parametric_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6690 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/provenance_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)      602 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/usability_domain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3100 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/bco/util.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/schema/drs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1931 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/AccessMethod.py
+-rw-r--r--   0 runner    (1001) docker     (123)      780 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/AccessURL.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/Checksum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1719 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/ContentsObject.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7950 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/DrsObject.py
+-rw-r--r--   0 runner    (1001) docker     (123)      480 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/DrsService.py
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/Error.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4005 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/Service.py
+-rw-r--r--   0 runner    (1001) docker     (123)      477 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/drs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6915 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/fetch_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3388 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/fields.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9256 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/invocation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2730 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/remote_files.py
+-rw-r--r--   0 runner    (1001) docker     (123)   104342 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3042 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/tasks.py
+-rw-r--r--   0 runner    (1001) docker     (123)      546 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/schema/types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy/security/
+-rw-r--r--   0 runner    (1001) docker     (123)     6309 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5620 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/idencoding.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18691 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/object_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2291 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/passwords.py
+-rw-r--r--   0 runner    (1001) docker     (123)      763 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/ssh_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5988 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/validate_user_input.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9851 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/galaxy/security/vault.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy_data.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     4013 2023-06-08 17:41:22.000000 galaxy-data-23.0.1/galaxy_data.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    14488 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/galaxy_data.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-08 17:41:22.000000 galaxy-data-23.0.1/galaxy_data.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-06-08 17:41:22.000000 galaxy-data-23.0.1/galaxy_data.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      325 2023-06-08 17:41:22.000000 galaxy-data-23.0.1/galaxy_data.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-06-08 17:41:22.000000 galaxy-data-23.0.1/galaxy_data.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       81 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-06-08 17:41:23.000000 galaxy-data-23.0.1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-06-08 17:37:04.000000 galaxy-data-23.0.1/test-requirements.txt
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive
+POSIX tar archive (GNU)
```

### Comparing `galaxy-data-22.1.1/LICENSE` & `galaxy-data-23.0.1/LICENSE`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,61 @@
-Copyright (c) 2005-2016 Galaxy Contributors (see CONTRIBUTORS.md)
+Copyright (c) 2005-2022 Galaxy Contributors (see CONTRIBUTORS.md)
+
+Work contributed from 2021-04-07 onwards is licensed under the MIT License.
+Work contributed before this date is licensed under the Academic Free License
+version 3.0.
+See https://github.com/galaxyproject/galaxy/ for the contribution history.
+See below for the full text of both licenses.
+
+
+Some icons found in Galaxy are from the Silk Icons set, available under
+the Creative Commons Attribution 2.5 License, from:
+
+http://www.famfamfam.com/lab/icons/silk/
+
+
+Other images and documentation are licensed under the Creative Commons
+Attribution 3.0 (CC BY 3.0) License. See:
+
+http://creativecommons.org/licenses/by/3.0/
+
+
+--------------------------------------------------------------------------------
+
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
+
+
+--------------------------------------------------------------------------------
+
+
+Academic Free License ("AFL") v. 3.0
+
+This Academic Free License (the "License") applies to any original work of
+authorship (the "Original Work") whose owner (the "Licensor") has placed the
+following licensing notice adjacent to the copyright notice for the Original
+Work:
 
 Licensed under the Academic Free License version 3.0
 
  1) Grant of Copyright License. Licensor grants You a worldwide, royalty-free, 
     non-exclusive, sublicensable license, for the duration of the copyright, to 
     do the following:
 
@@ -169,18 +222,7 @@
     replace the notice specified in the first paragraph above with the 
     notice "Licensed under <insert your license name here>" or with a notice 
     of your own that is not confusingly similar to the notice in this 
     License; and (iii) You may not claim that your original works are open 
     source software unless your Modified License has been approved by Open 
     Source Initiative (OSI) and You comply with its license review and 
     certification process.
-
-
-Some icons found in Galaxy are from the Silk Icons set, available under
-the Creative Commons Attribution 2.5 License, from:
-
-http://www.famfamfam.com/lab/icons/silk/
-
-
-Other images and documentation are licensed under the Creative Commons Attribution 3.0 (CC BY 3.0) License.   See 
-
-http://creativecommons.org/licenses/by/3.0/
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/_schema.py` & `galaxy-data-23.0.1/galaxy/datatypes/_schema.py`

 * *Files 21% similar despite different names*

```diff
@@ -3,79 +3,53 @@
     List,
     Optional,
 )
 
 from pydantic import (
     BaseModel,
     Field,
-    HttpUrl
+    HttpUrl,
 )
 
 
 class CompositeFileInfo(BaseModel):
-    name: str = Field(
-        ...,  # Mark this field as required
-        title="Name",
-        description="The name of this composite file"
-    )
-    optional: bool = Field(
-        title="Optional",
-        description=""  # TODO add description
-    )
-    mimetype: Optional[str] = Field(
-        title="MIME type",
-        description="The MIME type of this file"
-    )
+    name: str = Field(..., title="Name", description="The name of this composite file")  # Mark this field as required
+    optional: bool = Field(title="Optional", description="")  # TODO add description
+    mimetype: Optional[str] = Field(title="MIME type", description="The MIME type of this file")
     description: Optional[str] = Field(
-        title="Description",
-        description="Summary description of the purpouse of this file"
+        title="Description", description="Summary description of the purpouse of this file"
     )
     substitute_name_with_metadata: Optional[str] = Field(
-        title="Substitute name with metadata",
-        description=""  # TODO add description
-    )
-    is_binary: bool = Field(
-        title="Is binary",
-        description="Whether this file is a binary file"
-    )
-    to_posix_lines: bool = Field(
-        title="To posix lines",
-        description=""  # TODO add description
-    )
-    space_to_tab: bool = Field(
-        title="Spaces to tabulation",
-        description=""  # TODO add description
+        title="Substitute name with metadata", description=""  # TODO add description
     )
+    is_binary: bool = Field(title="Is binary", description="Whether this file is a binary file")
+    to_posix_lines: bool = Field(title="To posix lines", description="")  # TODO add description
+    space_to_tab: bool = Field(title="Spaces to tabulation", description="")  # TODO add description
 
 
 class DatatypeDetails(BaseModel):
     extension: str = Field(
         ...,  # Mark this field as required
         title="Extension",
         description="The data type’s Dataset file extension",
-        example="bed"
-    )
-    description: Optional[str] = Field(
-        title="Description",
-        description="A summary description for this data type"
+        example="bed",
     )
+    description: Optional[str] = Field(title="Description", description="A summary description for this data type")
     description_url: Optional[HttpUrl] = Field(
         title="Description URL",
         description="The URL to a detailed description for this datatype",
-        example="https://wiki.galaxyproject.org/Learn/Datatypes#Bed"
+        example="https://wiki.galaxyproject.org/Learn/Datatypes#Bed",
     )
     display_in_upload: bool = Field(
         default=False,
         title="Display in upload",
-        description="If True, the associated file extension will be displayed in the `File Format` select list in the `Upload File from your computer` tool in the `Get Data` tool section of the tool panel"
+        description="If True, the associated file extension will be displayed in the `File Format` select list in the `Upload File from your computer` tool in the `Get Data` tool section of the tool panel",
     )
     composite_files: Optional[List[CompositeFileInfo]] = Field(
-        default=None,
-        title="Composite files",
-        description="A collection of files composing this data type"
+        default=None, title="Composite files", description="A collection of files composing this data type"
     )
 
 
 class DatatypesMap(BaseModel):
     ext_to_class_name: Dict[str, str] = Field(
         ...,  # Mark this field as required
         title="Extension Map",
@@ -119,11 +93,34 @@
         title="Tool identifier",
         description="The converter tool identifier",
         example="CONVERTER_Bam_Bai_0",
     )
 
 
 class DatatypeConverterList(BaseModel):
-    __root__: List[DatatypeConverter] = Field(
-        title='List of data type converters',
-        default=[]
+    __root__: List[DatatypeConverter] = Field(title="List of data type converters", default=[])
+
+
+class DatatypeEDAMDetails(BaseModel):
+    prefix_IRI: str = Field(
+        ...,  # Mark this field as required
+        title="Prefix IRI",
+        description="The EDAM prefixed Resource Identifier",
+        example="format_1782",
+    )
+    label: Optional[str] = Field(
+        title="Label",
+        description="The EDAM label",
+        example="NCBI gene report format",
+    )
+    definition: Optional[str] = Field(
+        title="Definition",
+        description="The EDAM definition",
+        example="Entry (gene) format of the NCBI database.",
+    )
+
+
+class DatatypesEDAMDetailsDict(BaseModel):
+    __root__: Dict[str, DatatypeEDAMDetails] = Field(
+        title="Dict of EDAM details for formats",
+        default={},
     )
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/annotation.py` & `galaxy-data-23.0.1/galaxy/datatypes/annotation.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,84 +1,94 @@
 import logging
 import tarfile
+from typing import TYPE_CHECKING
 
 from galaxy.datatypes.binary import CompressedArchive
-from galaxy.datatypes.data import get_file_peek, Text
+from galaxy.datatypes.data import (
+    get_file_peek,
+    Text,
+)
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.util import nice_size
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class SnapHmm(Text):
     file_ext = "snaphmm"
     edam_data = "data_1364"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "SNAP HMM model"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"SNAP HMM model ({nice_size(dataset.get_size())})"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         SNAP model files start with zoeHMM
         """
-        return file_prefix.startswith('zoeHMM')
+        return file_prefix.startswith("zoeHMM")
 
 
 class Augustus(CompressedArchive):
     """
-        Class describing an Augustus prediction model
+    Class describing an Augustus prediction model
     """
+
     file_ext = "augustus"
     edam_data = "data_0950"
     compressed = True
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Augustus model"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Augustus model ({nice_size(dataset.get_size())})"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Augustus archives always contain the same files
         """
         if filename and tarfile.is_tarfile(filename):
-            with tarfile.open(filename, 'r') as temptar:
+            with tarfile.open(filename, "r") as temptar:
                 for f in temptar:
                     if not f.isfile():
                         continue
-                    if f.name.endswith('_exon_probs.pbl') \
-                            or f.name.endswith('_igenic_probs.pbl') \
-                            or f.name.endswith('_intron_probs.pbl') \
-                            or f.name.endswith('_metapars.cfg') \
-                            or f.name.endswith('_metapars.utr.cfg') \
-                            or f.name.endswith('_parameters.cfg') \
-                            or f.name.endswith('_parameters.cgp.cfg') \
-                            or f.name.endswith('_utr_probs.pbl') \
-                            or f.name.endswith('_weightmatrix.txt'):
+                    if (
+                        f.name.endswith("_exon_probs.pbl")
+                        or f.name.endswith("_igenic_probs.pbl")
+                        or f.name.endswith("_intron_probs.pbl")
+                        or f.name.endswith("_metapars.cfg")
+                        or f.name.endswith("_metapars.utr.cfg")
+                        or f.name.endswith("_parameters.cfg")
+                        or f.name.endswith("_parameters.cgp.cfg")
+                        or f.name.endswith("_utr_probs.pbl")
+                        or f.name.endswith("_weightmatrix.txt")
+                    ):
                         return True
                     else:
                         return False
         return False
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/anvio.py` & `galaxy-data-23.0.1/galaxy/datatypes/anvio.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,165 +1,176 @@
 """
 Datatypes for Anvi'o
 https://github.com/merenlab/anvio
 """
 import glob
 import logging
 import os
-import sys
-from typing import Optional
+from typing import (
+    Optional,
+    TYPE_CHECKING,
+)
 
+from galaxy.datatypes.data import GeneratePrimaryFileDataset
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.text import Html
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 class AnvioComposite(Html):
     """
     Base class to use for Anvi'o composite datatypes.
     Generally consist of a sqlite database, plus optional additional files
     """
+
     file_ext = "anvio_composite"
-    composite_type = 'auto_primary_file'
+    composite_type = "auto_primary_file"
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """
         This is called only at upload to write the html file
         cannot rename the datasets here - they come with the default unfortunately
         """
         defined_files = self.get_composite_files(dataset=dataset).items()
         rval = [f"<html><head><title>Files for Anvi'o Composite Dataset ({self.file_ext})</title></head>"]
         if defined_files:
             rval.append("<p/>This composite dataset is composed of the following defined files:<p/><ul>")
             for composite_name, composite_file in defined_files:
-                opt_text = ''
+                opt_text = ""
                 if composite_file.optional:
-                    opt_text = ' (optional)'
-                missing_text = ''
+                    opt_text = " (optional)"
+                missing_text = ""
                 if not os.path.exists(os.path.join(dataset.extra_files_path, composite_name)):
-                    missing_text = ' (missing)'
+                    missing_text = " (missing)"
                 rval.append(f'<li><a href="{composite_name}">{composite_name}</a>{opt_text}{missing_text}</li>')
             rval.append("</ul>")
         defined_files = map(lambda x: x[0], defined_files)
         extra_files = []
         for dirpath, _dirnames, filenames in os.walk(dataset.extra_files_path, followlinks=True):
             for filename in filenames:
                 rel_path = os.path.relpath(os.path.join(dirpath, filename), dataset.extra_files_path)
                 if rel_path not in defined_files:
                     extra_files.append(rel_path)
         if extra_files:
             rval.append("<p/>This composite dataset contains these undefined files:<p/><ul>")
             for rel_path in extra_files:
                 rval.append(f'<li><a href="{rel_path}">{rel_path}</a></li>')
-            rval.append('</ul>')
+            rval.append("</ul>")
         if not (defined_files or extra_files):
             rval.append("<p/>This composite dataset does not contain any files!<p/><ul>")
-        rval.append('</html>')
+        rval.append("</html>")
         return "\n".join(rval)
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/html'
+        return "text/html"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
-            dataset.peek = 'Anvio database (multiple files)'
-            dataset.blurb = 'Anvio database (multiple files)'
+            dataset.peek = "Anvio database (multiple files)"
+            dataset.blurb = "Anvio database (multiple files)"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "Anvio database (multiple files)"
 
 
 class AnvioDB(AnvioComposite):
     """Class for AnvioDB database files."""
+
     _anvio_basename: Optional[str] = None
     MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_db'
+    file_ext = "anvio_db"
 
     def __init__(self, *args, **kwd):
         super().__init__(*args, **kwd)
         if self._anvio_basename is not None:
             self.add_composite_file(self._anvio_basename, is_binary=True, optional=False)
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the anvio_basename based upon actual extra_files_path contents.
         """
-        super().set_meta(dataset, **kwd)
-        if dataset.metadata.anvio_basename is not None and os.path.exists(os.path.join(dataset.extra_files_path, dataset.metadata.anvio_basename)):
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
+        if dataset.metadata.anvio_basename is not None and os.path.exists(
+            os.path.join(dataset.extra_files_path, dataset.metadata.anvio_basename)
+        ):
             return
         found = False
         for basename in [dataset.metadata.anvio_basename, self._anvio_basename]:
             if found:
                 break
             if basename is not None and not os.path.exists(os.path.join(dataset.extra_files_path, basename)):
                 for name in glob.glob(os.path.join(dataset.extra_files_path, f"*{basename}")):
                     dataset.metadata.anvio_basename = os.path.basename(name)
                     found = True
                     break
 
 
 class AnvioStructureDB(AnvioDB):
     """Class for Anvio Structure DB database files."""
-    _anvio_basename = 'STRUCTURE.db'
+
+    _anvio_basename = "STRUCTURE.db"
     MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_structure_db'
+    file_ext = "anvio_structure_db"
 
 
 class AnvioGenomesDB(AnvioDB):
     """Class for Anvio Genomes DB database files."""
-    _anvio_basename = '-GENOMES.db'
+
+    _anvio_basename = "-GENOMES.db"
     MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_genomes_db'
+    file_ext = "anvio_genomes_db"
 
 
 class AnvioContigsDB(AnvioDB):
     """Class for Anvio Contigs DB database files."""
-    _anvio_basename = 'CONTIGS.db'
+
+    _anvio_basename = "CONTIGS.db"
     MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_contigs_db'
+    file_ext = "anvio_contigs_db"
 
     def __init__(self, *args, **kwd):
         super().__init__(*args, **kwd)
-        self.add_composite_file('CONTIGS.h5', is_binary=True, optional=True)
+        self.add_composite_file("CONTIGS.h5", is_binary=True, optional=True)
 
 
 class AnvioProfileDB(AnvioDB):
     """Class for Anvio Profile DB database files."""
-    _anvio_basename = 'PROFILE.db'
+
+    _anvio_basename = "PROFILE.db"
     MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_profile_db'
+    file_ext = "anvio_profile_db"
 
     def __init__(self, *args, **kwd):
         super().__init__(*args, **kwd)
-        self.add_composite_file('RUNINFO.cp', is_binary=True, optional=True)
-        self.add_composite_file('RUNINFO.mcp', is_binary=True, optional=True)
-        self.add_composite_file('AUXILIARY_DATA.db', is_binary=True, optional=True)
-        self.add_composite_file('RUNLOG.txt', is_binary=False, optional=True)
+        self.add_composite_file("RUNINFO.cp", is_binary=True, optional=True)
+        self.add_composite_file("RUNINFO.mcp", is_binary=True, optional=True)
+        self.add_composite_file("AUXILIARY_DATA.db", is_binary=True, optional=True)
+        self.add_composite_file("RUNLOG.txt", is_binary=False, optional=True)
 
 
 class AnvioPanDB(AnvioDB):
     """Class for Anvio Pan DB database files."""
-    _anvio_basename = 'PAN.db'
+
+    _anvio_basename = "PAN.db"
     MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_pan_db'
+    file_ext = "anvio_pan_db"
 
 
 class AnvioSamplesDB(AnvioDB):
     """Class for Anvio Samples DB database files."""
-    _anvio_basename = 'SAMPLES.db'
-    MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
-    file_ext = 'anvio_samples_db'
-
 
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
+    _anvio_basename = "SAMPLES.db"
+    MetadataElement(name="anvio_basename", default=_anvio_basename, desc="Basename", readonly=True)
+    file_ext = "anvio_samples_db"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/assembly.py` & `galaxy-data-23.0.1/galaxy/datatypes/assembly.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,36 +3,43 @@
 James E Johnson - University of Minnesota
 for velvet assembler tool in galaxy
 """
 
 import logging
 import os
 import re
-import sys
+from typing import TYPE_CHECKING
 
-from galaxy.datatypes import data
-from galaxy.datatypes import sequence
+from galaxy.datatypes import (
+    data,
+    sequence,
+)
+from galaxy.datatypes.data import GeneratePrimaryFileDataset
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.datatypes.text import Html
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class Amos(data.Text):
-    """Class describing the AMOS assembly file """
+    """Class describing the AMOS assembly file"""
+
     edam_data = "data_0925"
     edam_format = "format_3582"
-    file_ext = 'afg'
+    file_ext = "afg"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is an amos assembly file format
         Example::
 
           {CTG
           iid:1
           eid:1
@@ -53,163 +60,189 @@
           }
         """
         for line in file_prefix.line_iterator():
             if not line:
                 break  # EOF
             line = line.strip()
             if line:  # first non-empty line
-                if line.startswith('{'):
-                    if re.match(r'{(RED|CTG|TLE)$', line):
+                if line.startswith("{"):
+                    if re.match(r"{(RED|CTG|TLE)$", line):
                         return True
         return False
 
 
 @build_sniff_from_prefix
 class Sequences(sequence.Fasta):
-    """Class describing the Sequences file generated by velveth """
+    """Class describing the Sequences file generated by velveth"""
+
     edam_data = "data_0925"
-    file_ext = 'sequences'
+    file_ext = "sequences"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a velveth produced  fasta format
         The id line has 3 fields separated by tabs: sequence_name  sequence_index category::
 
           >SEQUENCE_0_length_35   1       1
           GGATATAGGGCCAACCCAACTCAACGGCCTGTCTT
           >SEQUENCE_1_length_35   2       1
           CGACGAATGACAGGTCACGAATTTGGCGGGGATTA
         """
         fh = file_prefix.string_io()
         for line in fh:
             line = line.strip()
             if line:  # first non-empty line
-                if line.startswith('>'):
-                    if not re.match(r'>[^\t]+\t\d+\t\d+$', line):
+                if line.startswith(">"):
+                    if not re.match(r">[^\t]+\t\d+\t\d+$", line):
                         return False
                     # The next line.strip() must not be '', nor startwith '>'
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         return False
                     return True
                 else:
                     return False
         return False
 
 
 @build_sniff_from_prefix
 class Roadmaps(data.Text):
-    """Class describing the Sequences file generated by velveth """
+    """Class describing the Sequences file generated by velveth"""
+
     edam_format = "format_2561"
-    file_ext = 'roadmaps'
+    file_ext = "roadmaps"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a velveth produced RoadMap::
           142858  21      1
           ROADMAP 1
           ROADMAP 2
           ...
         """
 
         fh = file_prefix.string_io()
         for line in fh:
             line = line.strip()
             if line:  # first non-empty line
-                if not re.match(r'\d+\t\d+\t\d+$', line):
+                if not re.match(r"\d+\t\d+\t\d+$", line):
                     return False
                 # The next line.strip() should be 'ROADMAP 1'
                 line = fh.readline().strip()
-                return bool(re.match(r'ROADMAP \d+$', line))
+                return bool(re.match(r"ROADMAP \d+$", line))
             else:
                 return False  # we found a non-empty line, but it's not a fasta header
         return False
 
 
 class Velvet(Html):
-    MetadataElement(name="base_name", desc="base name for velveth dataset", default="velvet", readonly=True, set_in_upload=True)
-    MetadataElement(name="paired_end_reads", desc="has paired-end reads", default="False", readonly=False, set_in_upload=True)
+    MetadataElement(
+        name="base_name", desc="base name for velveth dataset", default="velvet", readonly=True, set_in_upload=True
+    )
+    MetadataElement(
+        name="paired_end_reads", desc="has paired-end reads", default="False", readonly=False, set_in_upload=True
+    )
     MetadataElement(name="long_reads", desc="has long reads", default="False", readonly=False, set_in_upload=True)
-    MetadataElement(name="short2_reads", desc="has 2nd short reads", default="False", readonly=False, set_in_upload=True)
-    composite_type = 'auto_primary_file'
-    file_ext = 'velvet'
+    MetadataElement(
+        name="short2_reads", desc="has 2nd short reads", default="False", readonly=False, set_in_upload=True
+    )
+    composite_type = "auto_primary_file"
+    file_ext = "velvet"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('Sequences', mimetype='text/html', description='Sequences', substitute_name_with_metadata=None, is_binary=False)
-        self.add_composite_file('Roadmaps', mimetype='text/html', description='Roadmaps', substitute_name_with_metadata=None, is_binary=False)
-        self.add_composite_file('Log', mimetype='text/html', description='Log', optional='True', substitute_name_with_metadata=None, is_binary=False)
+        self.add_composite_file(
+            "Sequences",
+            mimetype="text/html",
+            description="Sequences",
+            substitute_name_with_metadata=None,
+            is_binary=False,
+        )
+        self.add_composite_file(
+            "Roadmaps",
+            mimetype="text/html",
+            description="Roadmaps",
+            substitute_name_with_metadata=None,
+            is_binary=False,
+        )
+        self.add_composite_file(
+            "Log",
+            mimetype="text/html",
+            description="Log",
+            optional="True",
+            substitute_name_with_metadata=None,
+            is_binary=False,
+        )
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         log.debug(f"Velvet log info  JJ generate_primary_file {dataset}")
-        rval = ['<html><head><title>Velvet Galaxy Composite Dataset </title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+        rval = ["<html><head><title>Velvet Galaxy Composite Dataset </title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
             log.debug(f"Velvet log info  JJ generate_primary_file {fn} {composite_file}")
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         """
         cannot do this until we are setting metadata
         """
         log.debug(f"Velvet log info  {'JJ regenerate_primary_file'}")
-        gen_msg = ''
+        gen_msg = ""
         try:
             efp = dataset.extra_files_path
-            log_path = os.path.join(efp, 'Log')
+            log_path = os.path.join(efp, "Log")
             with open(log_path) as f:
                 log_content = f.read(1000)
-            log_msg = re.sub(r'/\S*/', '', log_content)
+            log_msg = re.sub(r"/\S*/", "", log_content)
             log.debug(f"Velveth log info  {log_msg}")
-            paired_end_reads = re.search(r'-(short|long)Paired', log_msg) is not None
+            paired_end_reads = re.search(r"-(short|long)Paired", log_msg) is not None
             dataset.metadata.paired_end_reads = paired_end_reads
-            long_reads = re.search(r'-long', log_msg) is not None
+            long_reads = re.search(r"-long", log_msg) is not None
             dataset.metadata.long_reads = long_reads
-            short2_reads = re.search(r'-short(Paired)?2', log_msg) is not None
+            short2_reads = re.search(r"-short(Paired)?2", log_msg) is not None
             dataset.metadata.short2_reads = short2_reads
-            dataset.info = re.sub(r'.*velveth \S+', 'hash_length', re.sub(r'\n', ' ', log_msg))
+            dataset.info = re.sub(r".*velveth \S+", "hash_length", re.sub(r"\n", " ", log_msg))
             if paired_end_reads:
                 gen_msg = f"{gen_msg} Paired-End Reads"
             if long_reads:
                 gen_msg = f"{gen_msg} Long Reads"
             if len(gen_msg) > 0:
                 gen_msg = f"Uses: {gen_msg}"
         except Exception:
             log.debug(f"Velveth could not read Log file in {efp}")
         log.debug(f"Velveth log info  {gen_msg}")
-        rval = ['<html><head><title>Velvet Galaxy Composite Dataset </title></head><p/>']
+        rval = ["<html><head><title>Velvet Galaxy Composite Dataset </title></head><p/>"]
         # rval.append('<div>Generated:<p/><code> %s </code></div>' %(re.sub('\n','<br>',log_msg)))
-        rval.append(f'<div>Generated:<p/> {gen_msg} </div>')
-        rval.append('<div>Velveth dataset:<p/><ul>')
+        rval.append(f"<div>Generated:<p/> {gen_msg} </div>")
+        rval.append("<div>Velveth dataset:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
             log.debug(f"Velvet log info  JJ regenerate_primary_file {fn} {composite_file}")
-            if re.search('Log', fn) is None:
-                opt_text = ''
+            if re.search("Log", fn) is None:
+                opt_text = ""
                 if composite_file.optional:
-                    opt_text = ' (optional)'
-                if composite_file.get('description'):
-                    rval.append(f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                    opt_text = " (optional)"
+                if composite_file.get("description"):
+                    rval.append(
+                        f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                    )
                 else:
                     rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
-        with open(dataset.file_name, 'w') as f:
+        rval.append("</ul></div></html>")
+        with open(dataset.file_name, "w") as f:
             f.write("\n".join(rval))
-            f.write('\n')
+            f.write("\n")
 
-    def set_meta(self, dataset, **kwd):
-        Html.set_meta(self, dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        Html.set_meta(self, dataset, overwrite=overwrite, **kwd)
         self.regenerate_primary_file(dataset)
-
-
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/binary.py` & `galaxy-data-23.0.1/galaxy/datatypes/binary.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,133 +5,182 @@
 import io
 import json
 import logging
 import os
 import shutil
 import struct
 import subprocess
-import sys
 import tarfile
 import tempfile
 import zipfile
 from json import dumps
-from typing import Optional
+from typing import (
+    Any,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    Tuple,
+    TYPE_CHECKING,
+)
 
 import h5py
 import numpy as np
 import pysam
 from bx.seq.twobit import (
     TWOBIT_MAGIC_NUMBER,
     TWOBIT_MAGIC_NUMBER_SWAP,
 )
 
 from galaxy import util
 from galaxy.datatypes import metadata
 from galaxy.datatypes.data import (
     Data,
     DatatypeValidation,
+    GeneratePrimaryFileDataset,
     get_file_peek,
 )
+from galaxy.datatypes.dataproviders.column import (
+    ColumnarDataProvider,
+    DictDataProvider,
+)
+from galaxy.datatypes.dataproviders.dataset import (
+    DatasetDataProvider,
+    SamtoolsDataProvider,
+    SQliteDataDictProvider,
+    SQliteDataProvider,
+    SQliteDataTableProvider,
+)
+from galaxy.datatypes.dataproviders.line import (
+    FilteredLineDataProvider,
+    RegexLineDataProvider,
+)
 from galaxy.datatypes.metadata import (
     DictParameter,
     FileParameter,
     ListParameter,
     MetadataElement,
     MetadataParameter,
 )
-from galaxy.datatypes.sniff import build_sniff_from_prefix, FilePrefix
+from galaxy.datatypes.sniff import (
+    build_sniff_from_prefix,
+    FilePrefix,
+)
 from galaxy.datatypes.text import Html
-from galaxy.util import compression_utils, nice_size, sqlite
-from galaxy.util.checkers import is_bz2, is_gzip
-from . import data, dataproviders
+from galaxy.util import (
+    compression_utils,
+    nice_size,
+    sqlite,
+)
+from galaxy.util.checkers import (
+    is_bz2,
+    is_gzip,
+)
+from . import (
+    data,
+    dataproviders,
+)
+
+if TYPE_CHECKING:
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+    from galaxy.util.compression_utils import FileObjType
 
 log = logging.getLogger(__name__)
 # pysam 0.16.0.1 emits logs containing the word 'Error', this can confuse the stdout/stderr checkers.
 # Can be be removed once https://github.com/pysam-developers/pysam/issues/939 is resolved.
 pysam.set_verbosity(0)
 
 # Currently these supported binary data types must be manually set on upload
 
 
 class Binary(data.Data):
     """Binary data"""
+
     edam_format = "format_2333"
     file_ext = "binary"
 
     @staticmethod
     def register_sniffable_binary_format(data_type, ext, type_class):
         """Deprecated method."""
 
     @staticmethod
     def register_unsniffable_binary_ext(ext):
         """Deprecated method."""
 
-    def set_peek(self, dataset, **kwd):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
-            dataset.peek = 'binary data'
+            dataset.peek = "binary data"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/octet-stream'
+        return "application/octet-stream"
 
 
 class Ab1(Binary):
     """Class describing an ab1 binary sequence file"""
+
     file_ext = "ab1"
     edam_format = "format_3000"
     edam_data = "data_0924"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary ab1 sequence file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary ab1 sequence file ({nice_size(dataset.get_size())})"
 
 
 class Idat(Binary):
     """Binary data in idat format"""
+
     file_ext = "idat"
     edam_format = "format_2058"
     edam_data = "data_2603"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         try:
-            header = open(filename, 'rb').read(4)
-            if header == b'IDAT':
+            header = open(filename, "rb").read(4)
+            if header == b"IDAT":
                 return True
             return False
         except Exception:
             return False
 
 
 class Cel(Binary):
-    """ Cel File format described at:
-            http://media.affymetrix.com/support/developer/powertools/changelog/gcos-agcc/cel.html
+    """Cel File format described at:
+    http://media.affymetrix.com/support/developer/powertools/changelog/gcos-agcc/cel.html
     """
 
+    # cel 3 is a text format
+    is_binary = "maybe"  # type: ignore[assignment]  # https://github.com/python/mypy/issues/8796
     file_ext = "cel"
     edam_format = "format_1638"
     edam_data = "data_3110"
-    MetadataElement(name="version", default="3", desc="Version", readonly=True, visible=True,
-                    optional=True, no_value="3")
+    MetadataElement(
+        name="version", default="3", desc="Version", readonly=True, visible=True, optional=True, no_value="3"
+    )
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Try to guess if the file is a Cel file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('affy_v_agcc.cel')
         >>> Cel().sniff(fname)
         True
@@ -141,153 +190,160 @@
         >>> fname = get_test_fname('affy_v_4.cel')
         >>> Cel().sniff(fname)
         True
         >>> fname = get_test_fname('test.gal')
         >>> Cel().sniff(fname)
         False
         """
-        with open(filename, 'rb') as handle:
+        with open(filename, "rb") as handle:
             header_bytes = handle.read(8)
         found_cel_4 = False
         found_cel_3 = False
         found_cel_agcc = False
         if struct.unpack("<ii", header_bytes[:9]) == (64, 4):
             found_cel_4 = True
         elif struct.unpack(">bb", header_bytes[:2]) == (59, 1):
             found_cel_agcc = True
-        elif header_bytes.decode("utf8", errors="ignore").startswith('[CEL]'):
+        elif header_bytes.decode("utf8", errors="ignore").startswith("[CEL]"):
             found_cel_3 = True
         return found_cel_3 or found_cel_4 or found_cel_agcc
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set metadata for Cel file.
         """
-        with open(dataset.file_name, 'rb') as handle:
+        with open(dataset.file_name, "rb") as handle:
             header_bytes = handle.read(8)
         if struct.unpack("<ii", header_bytes[:9]) == (64, 4):
             dataset.metadata.version = "4"
         elif struct.unpack(">bb", header_bytes[:2]) == (59, 1):
             dataset.metadata.version = "agcc"
-        elif header_bytes.decode("utf8", errors="ignore").startswith('[CEL]'):
+        elif header_bytes.decode("utf8", errors="ignore").startswith("[CEL]"):
             dataset.metadata.version = "3"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.blurb = f"Cel version: {dataset.metadata.version}"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class MashSketch(Binary):
     """
-        Mash Sketch file.
-        Sketches are used by the MinHash algorithm to allow fast distance estimations
-        with low storage and memory requirements. To make a sketch, each k-mer in a sequence
-        is hashed, which creates a pseudo-random identifier. By sorting these identifiers (hashes),
-        a small subset from the top of the sorted list can represent the entire sequence (these are min-hashes).
-        The more similar another sequence is, the more min-hashes it is likely to share.
+    Mash Sketch file.
+    Sketches are used by the MinHash algorithm to allow fast distance estimations
+    with low storage and memory requirements. To make a sketch, each k-mer in a sequence
+    is hashed, which creates a pseudo-random identifier. By sorting these identifiers (hashes),
+    a small subset from the top of the sorted list can represent the entire sequence (these are min-hashes).
+    The more similar another sequence is, the more min-hashes it is likely to share.
     """
+
     file_ext = "msh"
+    # example data is actually text, maybe text would be a better base
+    is_binary = "maybe"  # type: ignore[assignment]  # https://github.com/python/mypy/issues/8796
 
 
 class CompressedArchive(Binary):
     """
-        Class describing an compressed binary file
-        This class can be sublass'ed to implement archive filetypes that will not be unpacked by upload.py.
+    Class describing an compressed binary file
+    This class can be sublass'ed to implement archive filetypes that will not be unpacked by upload.py.
     """
+
     file_ext = "compressed_archive"
     compressed = True
+    is_binary = "maybe"  # type: ignore[assignment]  # https://github.com/python/mypy/issues/8796
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Compressed binary file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Compressed binary file ({nice_size(dataset.get_size())})"
 
 
 class Meryldb(CompressedArchive):
     """MerylDB is a tar.gz archive, with 128 files. 64 data files and 64 index files."""
+
     file_ext = "meryldb"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Try to guess if the file is a Cel file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('affy_v_agcc.cel')
         >>> Meryldb().sniff(fname)
         False
         >>> fname = get_test_fname('read-db.meryldb')
         >>> Meryldb().sniff(fname)
         True
         """
         try:
             if filename and tarfile.is_tarfile(filename):
-                with tarfile.open(filename, 'r') as temptar:
+                with tarfile.open(filename, "r") as temptar:
                     _tar_content = temptar.getnames()
                     # 64 data files ad 64 indices + 2 folders
                     if len(_tar_content) == 130:
-                        if len([_ for _ in _tar_content if _.endswith('.merylIndex')]) == 64:
+                        if len([_ for _ in _tar_content if _.endswith(".merylIndex")]) == 64:
                             return True
         except Exception as e:
-            log.warning('%s, sniff Exception: %s', self, e)
+            log.warning("%s, sniff Exception: %s", self, e)
         return False
 
 
 class Bref3(Binary):
     """Bref3 format is a binary format for storing phased, non-missing genotypes for a list of samples."""
 
     file_ext = "bref3"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = binascii.unhexlify("7a8874f400156272")
 
-    def sniff_prefix(self, sniff_prefix):
-        return sniff_prefix.startswith_bytes(self._magic)
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith_bytes(self._magic)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary bref3 file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary bref3 file ({nice_size(dataset.get_size())})"
 
 
 class DynamicCompressedArchive(CompressedArchive):
-
     compressed_format: str
     uncompressed_datatype_instance: Data
 
-    def matches_any(self, target_datatypes) -> bool:
-        """Treat two aspects of compressed datatypes separately.
-        """
+    def matches_any(self, target_datatypes: List[Any]) -> bool:
+        """Treat two aspects of compressed datatypes separately."""
         compressed_target_datatypes = []
         uncompressed_target_datatypes = []
 
         for target_datatype in target_datatypes:
-            if hasattr(target_datatype, "uncompressed_datatype_instance") and target_datatype.compressed_format == self.compressed_format:
+            if (
+                hasattr(target_datatype, "uncompressed_datatype_instance")
+                and target_datatype.compressed_format == self.compressed_format
+            ):
                 uncompressed_target_datatypes.append(target_datatype.uncompressed_datatype_instance)
             else:
                 compressed_target_datatypes.append(target_datatype)
 
         # TODO: Add gz and bz2 as proper datatypes and use those instances instead of
         # CompressedArchive() in the following check.
         if not hasattr(self, "uncompressed_datatype_instance"):
@@ -304,215 +360,304 @@
 
 class Bz2DynamicCompressedArchive(DynamicCompressedArchive):
     compressed_format = "bz2"
 
 
 class CompressedZipArchive(CompressedArchive):
     """
-        Class describing an compressed binary file
-        This class can be sublass'ed to implement archive filetypes that will not be unpacked by upload.py.
+    Class describing an compressed binary file
+    This class can be sublass'ed to implement archive filetypes that will not be unpacked by upload.py.
     """
+
     file_ext = "zip"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Compressed zip file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Compressed zip file ({nice_size(dataset.get_size())})"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         with zipfile.ZipFile(filename) as zf:
             zf_files = zf.infolist()
             count = 0
             for f in zf_files:
-                if f.file_size > 0 and not f.filename.startswith('__MACOSX/') and not f.filename.endswith('.DS_Store'):
+                if f.file_size > 0 and not f.filename.startswith("__MACOSX/") and not f.filename.endswith(".DS_Store"):
                     count += 1
                 if count > 1:
                     return True
+        return False
 
 
 class GenericAsn1Binary(Binary):
     """Class for generic ASN.1 binary format"""
+
     file_ext = "asn1-binary"
     edam_format = "format_1966"
     edam_data = "data_0849"
 
 
 class _BamOrSam:
     """
     Helper class to set the metadata common to sam and bam files
     """
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         try:
-            bam_file = pysam.AlignmentFile(dataset.file_name, mode='rb')
+            bam_file = pysam.AlignmentFile(dataset.file_name, mode="rb")
             # TODO: Reference names, lengths, read_groups and headers can become very large, truncate when necessary
             dataset.metadata.reference_names = list(bam_file.references)
             dataset.metadata.reference_lengths = list(bam_file.lengths)
-            dataset.metadata.bam_header = dict(bam_file.header.items())
-            dataset.metadata.read_groups = [read_group['ID'] for read_group in dataset.metadata.bam_header.get('RG', []) if 'ID' in read_group]
-            dataset.metadata.sort_order = dataset.metadata.bam_header.get('HD', {}).get('SO', None)
-            dataset.metadata.bam_version = dataset.metadata.bam_header.get('HD', {}).get('VN', None)
+            dataset.metadata.bam_header = dict(bam_file.header.items())  # type: ignore [attr-defined]
+            dataset.metadata.read_groups = [
+                read_group["ID"] for read_group in dataset.metadata.bam_header.get("RG", []) if "ID" in read_group
+            ]
+            dataset.metadata.sort_order = dataset.metadata.bam_header.get("HD", {}).get("SO", None)
+            dataset.metadata.bam_version = dataset.metadata.bam_header.get("HD", {}).get("VN", None)
         except Exception:
             # Per Dan, don't log here because doing so will cause datasets that
             # fail metadata to end in the error state
             pass
 
 
 class BamNative(CompressedArchive, _BamOrSam):
     """Class describing a BAM binary file that is not necessarily sorted"""
+
     edam_format = "format_2572"
     edam_data = "data_0863"
     file_ext = "unsorted.bam"
     sort_flag: Optional[str] = None
 
     MetadataElement(name="columns", default=12, desc="Number of columns", readonly=True, visible=False, no_value=0)
-    MetadataElement(name="column_types", default=['str', 'int', 'str', 'int', 'int', 'str', 'str', 'int', 'int', 'str', 'str', 'str'], desc="Column types", param=metadata.ColumnTypesParameter, readonly=True, visible=False, no_value=[])
-    MetadataElement(name="column_names", default=['QNAME', 'FLAG', 'RNAME', 'POS', 'MAPQ', 'CIGAR', 'MRNM', 'MPOS', 'ISIZE', 'SEQ', 'QUAL', 'OPT'], desc="Column names", readonly=True, visible=False, optional=True, no_value=[])
-
-    MetadataElement(name="bam_version", default=None, desc="BAM Version", param=MetadataParameter, readonly=True, visible=False, optional=True)
-    MetadataElement(name="sort_order", default=None, desc="Sort Order", param=MetadataParameter, readonly=True, visible=False, optional=True)
-    MetadataElement(name="read_groups", default=[], desc="Read Groups", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="reference_names", default=[], desc="Chromosome Names", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="reference_lengths", default=[], desc="Chromosome Lengths", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="bam_header", default={}, desc="Dictionary of BAM Headers", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value={})
+    MetadataElement(
+        name="column_types",
+        default=["str", "int", "str", "int", "int", "str", "str", "int", "int", "str", "str", "str"],
+        desc="Column types",
+        param=metadata.ColumnTypesParameter,
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
+    MetadataElement(
+        name="column_names",
+        default=["QNAME", "FLAG", "RNAME", "POS", "MAPQ", "CIGAR", "MRNM", "MPOS", "ISIZE", "SEQ", "QUAL", "OPT"],
+        desc="Column names",
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+
+    MetadataElement(
+        name="bam_version",
+        default=None,
+        desc="BAM Version",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+    MetadataElement(
+        name="sort_order",
+        default=None,
+        desc="Sort Order",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+    MetadataElement(
+        name="read_groups",
+        default=[],
+        desc="Read Groups",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="reference_names",
+        default=[],
+        desc="Chromosome Names",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="reference_lengths",
+        default=[],
+        desc="Chromosome Lengths",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="bam_header",
+        default={},
+        desc="Dictionary of BAM Headers",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value={},
+    )
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
-        _BamOrSam().set_meta(dataset)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        _BamOrSam().set_meta(dataset, overwrite=overwrite, **kwd)
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """
         Merges BAM files
 
         :param split_files: List of bam file paths to merge
         :param output_file: Write merged bam file to this location
         """
-        pysam.merge('-O', 'BAM', output_file, *split_files)
+        pysam.merge("-O", "BAM", output_file, *split_files)  # type: ignore[attr-defined]
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         Binary.init_meta(self, dataset, copy_from=copy_from)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         return BamNative.is_bam(filename)
 
     @classmethod
-    def is_bam(cls, filename):
+    def is_bam(cls, filename: str) -> bool:
         # BAM is compressed in the BGZF format, and must not be uncompressed in Galaxy.
         # The first 4 bytes of any bam file is 'BAM\1', and the file is binary.
         try:
             header = gzip.open(filename).read(4)
-            if header == b'BAM\1':
+            if header == b"BAM\1":
                 return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary bam alignments file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary bam alignments file ({nice_size(dataset.get_size())})"
 
-    def to_archive(self, dataset, name=""):
+    def to_archive(self, dataset: "DatasetInstance", name: str = "") -> Iterable:
         rel_paths = []
         file_paths = []
         rel_paths.append(f"{name or dataset.file_name}.{dataset.extension}")
         file_paths.append(dataset.file_name)
         # We may or may not have a bam index file (BamNative doesn't have it, but also index generation may have failed)
         if dataset.metadata.bam_index:
             rel_paths.append(f"{name or dataset.file_name}.{dataset.extension}.bai")
             file_paths.append(dataset.metadata.bam_index.file_name)
         return zip(file_paths, rel_paths)
 
-    def groom_dataset_content(self, file_name):
+    def groom_dataset_content(self, file_name: str) -> None:
         """
         Ensures that the BAM file contents are coordinate-sorted.  This function is called
         on an output dataset after the content is initially generated.
         """
         # Use pysam to sort the BAM file
         # This command may also creates temporary files <out.prefix>.%d.bam when the
         # whole alignment cannot fit into memory.
         # do this in a unique temp directory, because of possible <out.prefix>.%d.bam temp files
         if not self.dataset_content_needs_grooming(file_name):
             # Don't re-sort if already sorted
             return
         tmp_dir = tempfile.mkdtemp()
-        tmp_sorted_dataset_file_name_prefix = os.path.join(tmp_dir, 'sorted')
+        tmp_sorted_dataset_file_name_prefix = os.path.join(tmp_dir, "sorted")
         sorted_file_name = f"{tmp_sorted_dataset_file_name_prefix}.bam"
-        slots = os.environ.get('GALAXY_SLOTS', 1)
+        slots = os.environ.get("GALAXY_SLOTS", 1)
         sort_args = []
         if self.sort_flag:
             sort_args = [self.sort_flag]
-        sort_args.extend([f"-@{slots}", file_name, '-T', tmp_sorted_dataset_file_name_prefix, '-O', 'BAM', '-o', sorted_file_name])
+        sort_args.extend(
+            [f"-@{slots}", file_name, "-T", tmp_sorted_dataset_file_name_prefix, "-O", "BAM", "-o", sorted_file_name]
+        )
         try:
-            pysam.sort(*sort_args)
+            pysam.sort(*sort_args)  # type: ignore[attr-defined]
         except Exception:
             shutil.rmtree(tmp_dir, ignore_errors=True)
             raise
         # Move samtools_created_sorted_file_name to our output dataset location
         shutil.move(sorted_file_name, file_name)
         # Remove temp file and empty temporary directory
         os.rmdir(tmp_dir)
 
-    def get_chunk(self, trans, dataset, offset=0, ck_size=None):
+    def get_chunk(self, trans, dataset: "DatasetInstance", offset: int = 0, ck_size: Optional[int] = None) -> str:
         if not offset == -1:
             try:
                 with pysam.AlignmentFile(dataset.file_name, "rb", check_sq=False) as bamfile:
                     ck_size = 300  # 300 lines
-                    ck_data = ""
-                    header_line_count = 0
                     if offset == 0:
-                        ck_data = bamfile.text.replace('\t', ' ')
-                        header_line_count = bamfile.text.count('\n')
+                        offset = bamfile.tell()
+                        ck_lines = bamfile.text.strip().replace("\t", " ").splitlines()  # type: ignore[attr-defined]
                     else:
                         bamfile.seek(offset)
-                    for line_number, alignment in enumerate(bamfile):
+                        ck_lines = []
+                    for line_number, alignment in enumerate(bamfile, len(ck_lines)):
                         # return only Header lines if 'header_line_count' exceeds 'ck_size'
                         # FIXME: Can be problematic if bam has million lines of header
-                        offset = bamfile.tell()
-                        if (line_number + header_line_count) > ck_size:
+                        if line_number > ck_size:
                             break
-                        else:
-                            bamline = alignment.tostring(bamfile)
-                            # Galaxy display each tag as separate column because 'tostring()' funcition put tabs in between each tag of tags column.
-                            # Below code will remove spaces between each tag.
-                            bamline_modified = ('\t').join(bamline.split()[:11] + [(' ').join(bamline.split()[11:])])
-                            ck_data = f"{ck_data}\n{bamline_modified}"
+
+                        offset = bamfile.tell()
+                        bamline = alignment.tostring(bamfile)
+                        # With multiple tags, Galaxy would display each as a separate column
+                        # because the 'tostring()' function uses tabs also between tags.
+                        # Below code will turn these extra tabs into spaces.
+                        n_tabs = bamline.count("\t")
+                        if n_tabs > 11:
+                            bamline, *extra_tags = bamline.rsplit("\t", maxsplit=n_tabs - 11)
+                            bamline = f"{bamline} {' '.join(extra_tags)}"
+                        ck_lines.append(bamline)
                     else:
                         # Nothing to enumerate; we've either offset to the end
                         # of the bamfile, or there is no data. (possible with
                         # header-only bams)
                         offset = -1
+                    ck_data = "\n".join(ck_lines)
             except Exception as e:
                 offset = -1
                 ck_data = f"Could not display BAM file, error was:\n{e}"
         else:
-            ck_data = ''
+            ck_data = ""
             offset = -1
-        return dumps({'ck_data': util.unicodify(ck_data),
-                      'offset': offset})
+        return dumps({"ck_data": util.unicodify(ck_data), "offset": offset})
 
-    def display_data(self, trans, dataset, preview=False, filename=None, to_ext=None, offset=None, ck_size=None, **kwd):
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        offset: Optional[int] = None,
+        ck_size: Optional[int] = None,
+        **kwd,
+    ):
         headers = kwd.get("headers", {})
         preview = util.string_as_bool(preview)
         if offset is not None:
             return self.get_chunk(trans, dataset, offset, ck_size), headers
         elif to_ext or not preview:
             return super().display_data(trans, dataset, preview, filename, to_ext, **kwd)
         else:
@@ -521,285 +666,343 @@
                 column_names = []
             column_types = dataset.metadata.column_types
             if not column_types:
                 column_types = []
             column_number = dataset.metadata.columns
             if column_number is None:
                 column_number = 1
-            return trans.fill_template("/dataset/tabular_chunked.mako",
-                                       dataset=dataset,
-                                       chunk=self.get_chunk(trans, dataset, 0),
-                                       column_number=column_number,
-                                       column_names=column_names,
-                                       column_types=column_types), headers
+            return (
+                trans.fill_template(
+                    "/dataset/tabular_chunked.mako",
+                    dataset=dataset,
+                    chunk=self.get_chunk(trans, dataset, 0),
+                    column_number=column_number,
+                    column_names=column_names,
+                    column_types=column_types,
+                ),
+                headers,
+            )
 
-    def validate(self, dataset, **kwd):
+    def validate(self, dataset: "DatasetInstance", **kwd) -> DatatypeValidation:
         if not BamNative.is_bam(dataset.file_name):
             return DatatypeValidation.invalid("This dataset does not appear to a BAM file.")
         elif self.dataset_content_needs_grooming(dataset.file_name):
-            return DatatypeValidation.invalid("This BAM file does not appear to have the correct sorting for declared datatype.")
+            return DatatypeValidation.invalid(
+                "This BAM file does not appear to have the correct sorting for declared datatype."
+            )
         return DatatypeValidation.validated()
 
 
 @dataproviders.decorators.has_dataproviders
 class Bam(BamNative):
     """Class describing a BAM binary file"""
+
     edam_format = "format_2572"
     edam_data = "data_0863"
     file_ext = "bam"
     track_type = "ReadTrack"
     data_sources = {"data": "bai", "index": "bigwig"}
 
-    MetadataElement(name="bam_index", desc="BAM Index File", param=metadata.FileParameter, file_ext="bai", readonly=True, visible=False, optional=True)
-    MetadataElement(name="bam_csi_index", desc="BAM CSI Index File", param=metadata.FileParameter, file_ext="bam.csi", readonly=True, visible=False, optional=True)
+    MetadataElement(
+        name="bam_index",
+        desc="BAM Index File",
+        param=metadata.FileParameter,
+        file_ext="bai",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+    MetadataElement(
+        name="bam_csi_index",
+        desc="BAM CSI Index File",
+        param=metadata.FileParameter,
+        file_ext="bam.csi",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
 
-    def get_index_flag(self, file_name):
+    def get_index_flag(self, file_name: str) -> str:
         """
         Return pysam flag for bai index (default) or csi index (contig size > (2**29 - 1) )
         """
-        index_flag = '-b'  # bai index
+        index_flag = "-b"  # bai index
         try:
             with pysam.AlignmentFile(file_name) as alignment_file:
-                if max(alignment_file.header.lengths) > (2 ** 29) - 1:
-                    index_flag = '-c'  # csi index
+                if max(alignment_file.header.lengths) > (2**29) - 1:
+                    index_flag = "-c"  # csi index
         except Exception:
             # File may not have a header, that's OK
             pass
         return index_flag
 
-    def dataset_content_needs_grooming(self, file_name):
+    def dataset_content_needs_grooming(self, file_name: str) -> bool:
         """
         Check if file_name is a coordinate-sorted BAM file
         """
         # The best way to ensure that BAM files are coordinate-sorted and indexable
         # is to actually index them.
         index_flag = self.get_index_flag(file_name)
         index_name = tempfile.NamedTemporaryFile(prefix="bam_index").name
         try:
             # If pysam fails to index a file it will write to stderr,
             # and this causes the set_meta script to fail. So instead
             # we start another process and discard stderr.
-            if index_flag == '-b':
+            if index_flag == "-b":
                 # IOError: No such file or directory: '-b' if index_flag is set to -b (pysam 0.15.4)
-                cmd = ['python', '-c', f"import pysam; pysam.set_verbosity(0); pysam.index('{file_name}', '{index_name}')"]
+                cmd = [
+                    "python",
+                    "-c",
+                    f"import pysam; pysam.set_verbosity(0); pysam.index('-o', '{index_name}', '{file_name}')",
+                ]
             else:
-                cmd = ['python', '-c', f"import pysam; pysam.set_verbosity(0); pysam.index('{index_flag}', '{file_name}', '{index_name}')"]
-            with open(os.devnull, 'w') as devnull:
+                cmd = [
+                    "python",
+                    "-c",
+                    f"import pysam; pysam.set_verbosity(0); pysam.index('{index_flag}', '-o', '{index_name}', '{file_name}')",
+                ]
+            with open(os.devnull, "w") as devnull:
                 subprocess.check_call(cmd, stderr=devnull, shell=False)
             needs_sorting = False
         except subprocess.CalledProcessError:
             needs_sorting = True
         try:
             os.unlink(index_name)
         except Exception:
             pass
         return needs_sorting
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
         # These metadata values are not accessible by users, always overwrite
         super().set_meta(dataset=dataset, overwrite=overwrite, **kwd)
         index_flag = self.get_index_flag(dataset.file_name)
-        if index_flag == '-b':
-            spec_key = 'bam_index'
+        if index_flag == "-b":
+            spec_key = "bam_index"
             index_file = dataset.metadata.bam_index
         else:
-            spec_key = 'bam_csi_index'
+            spec_key = "bam_csi_index"
             index_file = dataset.metadata.bam_csi_index
         if not index_file:
-            index_file = dataset.metadata.spec[spec_key].param.new_file(dataset=dataset)
-        if index_flag == '-b':
+            index_file = dataset.metadata.spec[spec_key].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
+        if index_flag == "-b":
             # IOError: No such file or directory: '-b' if index_flag is set to -b (pysam 0.15.4)
-            pysam.index(dataset.file_name, index_file.file_name)
+            pysam.index("-o", index_file.file_name, dataset.file_name)  # type: ignore [attr-defined]
         else:
-            pysam.index(index_flag, dataset.file_name, index_file.file_name)
+            pysam.index(index_flag, "-o", index_file.file_name, dataset.file_name)  # type: ignore [attr-defined]
         dataset.metadata.bam_index = index_file
 
-    def sniff(self, file_name):
-        return super().sniff(file_name) and not self.dataset_content_needs_grooming(file_name)
+    def sniff(self, filename: str) -> bool:
+        return super().sniff(filename) and not self.dataset_content_needs_grooming(filename)
 
     # ------------- Dataproviders
     # pipe through samtools view
     # ALSO: (as Sam)
     # bam does not use '#' to indicate comments/headers - we need to strip out those headers from the std. providers
     # TODO:?? seems like there should be an easier way to do/inherit this - metadata.comment_char?
     # TODO: incorporate samtools options to control output: regions first, then flags, etc.
-    @dataproviders.decorators.dataprovider_factory('line', dataproviders.line.FilteredLineDataProvider.settings)
-    def line_dataprovider(self, dataset, **settings):
-        samtools_source = dataproviders.dataset.SamtoolsDataProvider(dataset)
-        settings['comment_char'] = '@'
-        return dataproviders.line.FilteredLineDataProvider(samtools_source, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('regex-line', dataproviders.line.RegexLineDataProvider.settings)
-    def regex_line_dataprovider(self, dataset, **settings):
-        samtools_source = dataproviders.dataset.SamtoolsDataProvider(dataset)
-        settings['comment_char'] = '@'
-        return dataproviders.line.RegexLineDataProvider(samtools_source, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('column', dataproviders.column.ColumnarDataProvider.settings)
-    def column_dataprovider(self, dataset, **settings):
-        samtools_source = dataproviders.dataset.SamtoolsDataProvider(dataset)
-        settings['comment_char'] = '@'
-        return dataproviders.column.ColumnarDataProvider(samtools_source, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('dict', dataproviders.column.DictDataProvider.settings)
-    def dict_dataprovider(self, dataset, **settings):
-        samtools_source = dataproviders.dataset.SamtoolsDataProvider(dataset)
-        settings['comment_char'] = '@'
-        return dataproviders.column.DictDataProvider(samtools_source, **settings)
+    @dataproviders.decorators.dataprovider_factory("line", FilteredLineDataProvider.settings)
+    def line_dataprovider(self, dataset: "DatasetInstance", **settings) -> FilteredLineDataProvider:
+        samtools_source = SamtoolsDataProvider(dataset)
+        settings["comment_char"] = "@"
+        return FilteredLineDataProvider(samtools_source, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("regex-line", RegexLineDataProvider.settings)
+    def regex_line_dataprovider(self, dataset: "DatasetInstance", **settings) -> RegexLineDataProvider:
+        samtools_source = SamtoolsDataProvider(dataset)
+        settings["comment_char"] = "@"
+        return RegexLineDataProvider(samtools_source, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("column", ColumnarDataProvider.settings)
+    def column_dataprovider(self, dataset: "DatasetInstance", **settings) -> ColumnarDataProvider:
+        samtools_source = SamtoolsDataProvider(dataset)
+        settings["comment_char"] = "@"
+        return ColumnarDataProvider(samtools_source, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("dict", DictDataProvider.settings)
+    def dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> DictDataProvider:
+        samtools_source = SamtoolsDataProvider(dataset)
+        settings["comment_char"] = "@"
+        return DictDataProvider(samtools_source, **settings)
 
     # these can't be used directly - may need BamColumn, BamDict (Bam metadata -> column/dict)
     # OR - see genomic_region_dataprovider
     # @dataproviders.decorators.dataprovider_factory('dataset-column', dataproviders.column.ColumnarDataProvider.settings)
     # def dataset_column_dataprovider(self, dataset, **settings):
     #    settings['comment_char'] = '@'
     #    return super().dataset_column_dataprovider(dataset, **settings)
 
     # @dataproviders.decorators.dataprovider_factory('dataset-dict', dataproviders.column.DictDataProvider.settings)
     # def dataset_dict_dataprovider(self, dataset, **settings):
     #    settings['comment_char'] = '@'
     #    return super().dataset_dict_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('header', dataproviders.line.RegexLineDataProvider.settings)
-    def header_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("header", RegexLineDataProvider.settings)
+    def header_dataprovider(self, dataset: "DatasetInstance", **settings) -> RegexLineDataProvider:
         # in this case we can use an option of samtools view to provide just what we need (w/o regex)
-        samtools_source = dataproviders.dataset.SamtoolsDataProvider(dataset, '-H')
-        return dataproviders.line.RegexLineDataProvider(samtools_source, **settings)
+        samtools_source = SamtoolsDataProvider(dataset, "-H")
+        return RegexLineDataProvider(samtools_source, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('id-seq-qual', dataproviders.column.DictDataProvider.settings)
-    def id_seq_qual_dataprovider(self, dataset, **settings):
-        settings['indeces'] = [0, 9, 10]
-        settings['column_types'] = ['str', 'str', 'str']
-        settings['column_names'] = ['id', 'seq', 'qual']
+    @dataproviders.decorators.dataprovider_factory("id-seq-qual", DictDataProvider.settings)
+    def id_seq_qual_dataprovider(self, dataset: "DatasetInstance", **settings) -> DictDataProvider:
+        settings["indeces"] = [0, 9, 10]
+        settings["column_types"] = ["str", "str", "str"]
+        settings["column_names"] = ["id", "seq", "qual"]
         return self.dict_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('genomic-region', dataproviders.column.ColumnarDataProvider.settings)
-    def genomic_region_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("genomic-region", ColumnarDataProvider.settings)
+    def genomic_region_dataprovider(self, dataset: "DatasetInstance", **settings) -> ColumnarDataProvider:
         # GenomicRegionDataProvider currently requires a dataset as source - may not be necc.
         # TODO:?? consider (at least) the possible use of a kwarg: metadata_source (def. to source.dataset),
         #   or remove altogether...
         # samtools_source = dataproviders.dataset.SamtoolsDataProvider(dataset)
         # return dataproviders.dataset.GenomicRegionDataProvider(samtools_source, metadata_source=dataset,
         #                                                        2, 3, 3, **settings)
 
         # instead, set manually and use in-class column gen
-        settings['indeces'] = [2, 3, 3]
-        settings['column_types'] = ['str', 'int', 'int']
+        settings["indeces"] = [2, 3, 3]
+        settings["column_types"] = ["str", "int", "int"]
         return self.column_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('genomic-region-dict', dataproviders.column.DictDataProvider.settings)
-    def genomic_region_dict_dataprovider(self, dataset, **settings):
-        settings['indeces'] = [2, 3, 3]
-        settings['column_types'] = ['str', 'int', 'int']
-        settings['column_names'] = ['chrom', 'start', 'end']
+    @dataproviders.decorators.dataprovider_factory("genomic-region-dict", DictDataProvider.settings)
+    def genomic_region_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> DictDataProvider:
+        settings["indeces"] = [2, 3, 3]
+        settings["column_types"] = ["str", "int", "int"]
+        settings["column_names"] = ["chrom", "start", "end"]
         return self.dict_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('samtools')
-    def samtools_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("samtools")
+    def samtools_dataprovider(self, dataset: "DatasetInstance", **settings) -> SamtoolsDataProvider:
         """Generic samtools interface - all options available through settings."""
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        return dataproviders.dataset.SamtoolsDataProvider(dataset_source, **settings)
+        dataset_source = DatasetDataProvider(dataset)
+        return SamtoolsDataProvider(dataset_source, **settings)
 
 
 class ProBam(Bam):
     """Class describing a BAM binary file - extended for proteomics data"""
+
     edam_format = "format_3826"
     edam_data = "data_0863"
     file_ext = "probam"
 
 
 class BamInputSorted(BamNative):
     """
     A class for BAM files that can formally be unsorted or queryname sorted.
     Alignments are either ordered based on the order with which the queries appear when producing the alignment,
     or ordered by their queryname.
     This notaby keeps alignments produced by paired end sequencing adjacent.
     """
-    sort_flag = '-n'
-    file_ext = 'qname_input_sorted.bam'
 
-    def sniff(self, file_name):
+    sort_flag = "-n"
+    file_ext = "qname_input_sorted.bam"
+
+    def sniff(self, filename: str) -> bool:
         # We never want to sniff to this datatype
         return False
 
-    def dataset_content_needs_grooming(self, file_name):
+    def dataset_content_needs_grooming(self, file_name: str) -> bool:
         """
         Groom if the file is coordinate sorted
         """
         # The best way to ensure that BAM files are coordinate-sorted and indexable
         # is to actually index them.
         with pysam.AlignmentFile(filename=file_name) as f:
             # The only sure thing we know here is that the sort order can't be coordinate
-            return f.header.get('HD', {}).get('SO') == 'coordinate'
+            return f.header.get("HD", {}).get("SO") == "coordinate"  # type: ignore[attr-defined]
 
 
 class BamQuerynameSorted(BamInputSorted):
     """A class for queryname sorted BAM files."""
 
-    sort_flag = '-n'
+    sort_flag = "-n"
     file_ext = "qname_sorted.bam"
 
-    def sniff(self, file_name):
-        return BamNative().sniff(file_name) and not self.dataset_content_needs_grooming(file_name)
+    def sniff(self, filename: str) -> bool:
+        return BamNative().sniff(filename) and not self.dataset_content_needs_grooming(filename)
 
-    def dataset_content_needs_grooming(self, file_name):
+    def dataset_content_needs_grooming(self, file_name: str) -> bool:
         """
         Check if file_name is a queryname-sorted BAM file
         """
         # The best way to ensure that BAM files are coordinate-sorted and indexable
         # is to actually index them.
         with pysam.AlignmentFile(filename=file_name) as f:
-            return f.header.get('HD', {}).get('SO') != 'queryname'
+            return f.header.get("HD", {}).get("SO") != "queryname"  # type: ignore[attr-defined]
 
 
 class CRAM(Binary):
     file_ext = "cram"
     edam_format = "format_3462"
     edam_data = "data_0863"
 
-    MetadataElement(name="cram_version", default=None, desc="CRAM Version", param=MetadataParameter, readonly=True, visible=False, optional=False)
-    MetadataElement(name="cram_index", desc="CRAM Index File", param=metadata.FileParameter, file_ext="crai", readonly=True, visible=False, optional=True)
-
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    MetadataElement(
+        name="cram_version",
+        default=None,
+        desc="CRAM Version",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=False,
+    )
+    MetadataElement(
+        name="cram_index",
+        desc="CRAM Index File",
+        param=metadata.FileParameter,
+        file_ext="crai",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
         major_version, minor_version = self.get_cram_version(dataset.file_name)
         if major_version != -1:
             dataset.metadata.cram_version = f"{str(major_version)}.{str(minor_version)}"
 
         if not dataset.metadata.cram_index:
-            index_file = dataset.metadata.spec['cram_index'].param.new_file(dataset=dataset)
+            index_file = dataset.metadata.spec["cram_index"].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
             if self.set_index_file(dataset, index_file):
                 dataset.metadata.cram_index = index_file
 
-    def get_cram_version(self, filename):
+    def get_cram_version(self, filename: str) -> Tuple[int, int]:
         try:
             with open(filename, "rb") as fh:
                 header = bytearray(fh.read(6))
             return header[4], header[5]
         except Exception as exc:
-            log.warning('%s, get_cram_version Exception: %s', self, exc)
+            log.warning("%s, get_cram_version Exception: %s", self, exc)
             return -1, -1
 
-    def set_index_file(self, dataset, index_file):
+    def set_index_file(self, dataset: "DatasetInstance", index_file) -> bool:
         try:
-            pysam.index(dataset.file_name, index_file.file_name)
+            pysam.index("-o", index_file.file_name, dataset.file_name)  # type: ignore [attr-defined]
             return True
         except Exception as exc:
-            log.warning('%s, set_index_file Exception: %s', self, exc)
+            log.warning("%s, set_index_file Exception: %s", self, exc)
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = 'CRAM binary alignment file'
-            dataset.blurb = 'binary data'
+            dataset.peek = "CRAM binary alignment file"
+            dataset.blurb = "binary data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         try:
-            header = open(filename, 'rb').read(4)
+            header = open(filename, "rb").read(4)
             if header == b"CRAM":
                 return True
             return False
         except Exception:
             return False
 
 
@@ -809,45 +1012,60 @@
 
 
 class Bcf(BaseBcf):
     """
     Class describing a (BGZF-compressed) BCF file
 
     """
+
     file_ext = "bcf"
 
-    MetadataElement(name="bcf_index", desc="BCF Index File", param=metadata.FileParameter, file_ext="csi", readonly=True, visible=False, optional=True)
+    MetadataElement(
+        name="bcf_index",
+        desc="BCF Index File",
+        param=metadata.FileParameter,
+        file_ext="csi",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # BCF is compressed in the BGZF format, and must not be uncompressed in Galaxy.
         try:
             header = gzip.open(filename).read(3)
             # The first 3 bytes of any BCF file are 'BCF', and the file is binary.
-            if header == b'BCF':
+            if header == b"BCF":
                 return True
             return False
         except Exception:
             return False
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
-        """ Creates the index for the BCF file. """
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
+        """Creates the index for the BCF file."""
         # These metadata values are not accessible by users, always overwrite
         index_file = dataset.metadata.bcf_index
         if not index_file:
-            index_file = dataset.metadata.spec['bcf_index'].param.new_file(dataset=dataset)
+            index_file = dataset.metadata.spec["bcf_index"].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
         # Create the bcf index
-        dataset_symlink = os.path.join(os.path.dirname(index_file.file_name),
-                                       '__dataset_%d_%s' % (dataset.id, os.path.basename(index_file.file_name)))
+        dataset_symlink = os.path.join(
+            os.path.dirname(index_file.file_name),
+            "__dataset_%d_%s" % (dataset.id, os.path.basename(index_file.file_name)),
+        )
         os.symlink(dataset.file_name, dataset_symlink)
         try:
-            cmd = ['python', '-c', f"import pysam.bcftools; pysam.bcftools.index('{dataset_symlink}')"]
+            cmd = ["python", "-c", f"import pysam.bcftools; pysam.bcftools.index('{dataset_symlink}')"]
             subprocess.check_call(cmd)
             shutil.move(f"{dataset_symlink}.csi", index_file.file_name)
         except Exception as e:
-            raise Exception(f'Error setting BCF metadata: {util.unicodify(e)}')
+            raise Exception(f"Error setting BCF metadata: {util.unicodify(e)}")
         finally:
             # Remove temp file and symlink
             os.remove(dataset_symlink)
         dataset.metadata.bcf_index = index_file
 
 
 class BcfUncompressed(BaseBcf):
@@ -858,21 +1076,23 @@
     >>> fname = get_test_fname('1.bcf_uncompressed')
     >>> BcfUncompressed().sniff(fname)
     True
     >>> fname = get_test_fname('1.bcf')
     >>> BcfUncompressed().sniff(fname)
     False
     """
+
     file_ext = "bcf_uncompressed"
+    compressed = False
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         try:
-            header = open(filename, mode='rb').read(3)
+            header = open(filename, mode="rb").read(3)
             # The first 3 bytes of any BCF file are 'BCF', and the file is binary.
-            if header == b'BCF':
+            if header == b"BCF":
                 return True
             return False
         except Exception:
             return False
 
 
 class H5(Binary):
@@ -883,40 +1103,41 @@
     >>> fname = get_test_fname('test.mz5')
     >>> H5().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> H5().sniff(fname)
     False
     """
+
     file_ext = "h5"
     edam_format = "format_3590"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = binascii.unhexlify("894844460d0a1a0a")
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # The first 8 bytes of any hdf5 file are 0x894844460d0a1a0a
         try:
-            header = open(filename, 'rb').read(8)
+            header = open(filename, "rb").read(8)
             if header == self._magic:
                 return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary HDF5 file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary HDF5 file ({nice_size(dataset.get_size())})"
 
 
 class Loom(H5):
@@ -927,105 +1148,149 @@
     >>> fname = get_test_fname('test.loom')
     >>> Loom().sniff(fname)
     True
     >>> fname = get_test_fname('test.mz5')
     >>> Loom().sniff(fname)
     False
     """
+
     file_ext = "loom"
     edam_format = "format_3590"
 
     MetadataElement(name="title", default="", desc="title", readonly=True, visible=True, no_value="")
     MetadataElement(name="description", default="", desc="description", readonly=True, visible=True, no_value="")
     MetadataElement(name="url", default="", desc="url", readonly=True, visible=True, no_value="")
     MetadataElement(name="doi", default="", desc="doi", readonly=True, visible=True, no_value="")
-    MetadataElement(name="loom_spec_version", default="", desc="loom_spec_version", readonly=True, visible=True, no_value="")
+    MetadataElement(
+        name="loom_spec_version", default="", desc="loom_spec_version", readonly=True, visible=True, no_value=""
+    )
     MetadataElement(name="creation_date", default=None, desc="creation_date", readonly=True, visible=True)
-    MetadataElement(name="shape", default=(), desc="shape", param=metadata.ListParameter, readonly=True, visible=True, no_value=())
+    MetadataElement(
+        name="shape", default=(), desc="shape", param=metadata.ListParameter, readonly=True, visible=True, no_value=()
+    )
     MetadataElement(name="layers_count", default=0, desc="layers_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="layers_names", desc="layers_names", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="layers_names",
+        desc="layers_names",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
     MetadataElement(name="row_attrs_count", default=0, desc="row_attrs_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="row_attrs_names", desc="row_attrs_names", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="row_attrs_names",
+        desc="row_attrs_names",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
     MetadataElement(name="col_attrs_count", default=0, desc="col_attrs_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="col_attrs_names", desc="col_attrs_names", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
-    MetadataElement(name="col_graphs_count", default=0, desc="col_graphs_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="col_graphs_names", desc="col_graphs_names", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
-    MetadataElement(name="row_graphs_count", default=0, desc="row_graphs_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="row_graphs_names", desc="row_graphs_names", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="col_attrs_names",
+        desc="col_attrs_names",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
+    MetadataElement(
+        name="col_graphs_count", default=0, desc="col_graphs_count", readonly=True, visible=True, no_value=0
+    )
+    MetadataElement(
+        name="col_graphs_names",
+        desc="col_graphs_names",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
+    MetadataElement(
+        name="row_graphs_count", default=0, desc="row_graphs_count", readonly=True, visible=True, no_value=0
+    )
+    MetadataElement(
+        name="row_graphs_names",
+        desc="row_graphs_names",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            with h5py.File(filename, 'r') as loom_file:
+            with h5py.File(filename, "r") as loom_file:
                 # Check the optional but distinctive LOOM_SPEC_VERSION attribute
-                if bool(loom_file.attrs.get('LOOM_SPEC_VERSION')):
+                if bool(loom_file.attrs.get("LOOM_SPEC_VERSION")):
                     return True
                 # Check some mandatory H5 datasets and groups
-                for el in ('matrix', 'row_attrs', 'col_attrs'):
+                for el in ("matrix", "row_attrs", "col_attrs"):
                     if loom_file.get(el) is None:
                         return False
                 else:
                     return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary Loom file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary Loom file ({nice_size(dataset.get_size())})"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
-            with h5py.File(dataset.file_name, 'r') as loom_file:
-                dataset.metadata.title = loom_file.attrs.get('title')
-                dataset.metadata.description = loom_file.attrs.get('description')
-                dataset.metadata.url = loom_file.attrs.get('url')
-                dataset.metadata.doi = loom_file.attrs.get('doi')
-                loom_spec_version = loom_file.attrs.get('LOOM_SPEC_VERSION')
+            with h5py.File(dataset.file_name, "r") as loom_file:
+                dataset.metadata.title = loom_file.attrs.get("title")
+                dataset.metadata.description = loom_file.attrs.get("description")
+                dataset.metadata.url = loom_file.attrs.get("url")
+                dataset.metadata.doi = loom_file.attrs.get("doi")
+                loom_spec_version = loom_file.attrs.get("LOOM_SPEC_VERSION")
                 if isinstance(loom_spec_version, np.ndarray):
                     loom_spec_version = loom_spec_version[0]
                     if isinstance(loom_spec_version, bytes):
                         loom_spec_version = loom_spec_version.decode()
                 dataset.metadata.loom_spec_version = loom_spec_version
-                dataset.creation_date = loom_file.attrs.get('creation_date')
-                dataset.metadata.shape = tuple(loom_file['matrix'].shape)
+                dataset.metadata.creation_date = loom_file.attrs.get("creation_date")
+                dataset.metadata.shape = tuple(loom_file["matrix"].shape)
 
-                tmp = list(loom_file.get('layers', {}).keys())
+                tmp = list(loom_file.get("layers", {}).keys())
                 dataset.metadata.layers_count = len(tmp)
                 dataset.metadata.layers_names = tmp
 
-                tmp = list(loom_file['row_attrs'].keys())
+                tmp = list(loom_file["row_attrs"].keys())
                 dataset.metadata.row_attrs_count = len(tmp)
                 dataset.metadata.row_attrs_names = tmp
 
-                tmp = list(loom_file['col_attrs'].keys())
+                tmp = list(loom_file["col_attrs"].keys())
                 dataset.metadata.col_attrs_count = len(tmp)
                 dataset.metadata.col_attrs_names = tmp
 
                 # According to the Loom file format specification, col_graphs
                 # and row_graphs are mandatory groups, but files created by
                 # Bioconductor LoomExperiment do not always have them:
                 # https://github.com/Bioconductor/LoomExperiment/issues/7
-                tmp = list(loom_file.get('col_graphs', {}).keys())
+                tmp = list(loom_file.get("col_graphs", {}).keys())
                 dataset.metadata.col_graphs_count = len(tmp)
                 dataset.metadata.col_graphs_names = tmp
 
-                tmp = list(loom_file.get('row_graphs', {}).keys())
+                tmp = list(loom_file.get("row_graphs", {}).keys())
                 dataset.metadata.row_graphs_count = len(tmp)
                 dataset.metadata.row_graphs_names = tmp
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
 
 class Anndata(H5):
     """
     Class describing an HDF5 anndata files: http://anndata.rtfd.io
 
     >>> from galaxy.datatypes.sniff import get_test_fname
@@ -1044,93 +1309,128 @@
     >>> Anndata().sniff(get_test_fname('adata_0_7_4_small.h5ad'))
     True
     >>> Anndata().sniff(get_test_fname('adata_unk2.h5ad'))
     True
     >>> Anndata().sniff(get_test_fname('adata_unk.h5ad'))
     True
     """
-    file_ext = 'h5ad'
+
+    file_ext = "h5ad"
 
     MetadataElement(name="title", default="", desc="title", readonly=True, visible=True, no_value="")
     MetadataElement(name="description", default="", desc="description", readonly=True, visible=True, no_value="")
     MetadataElement(name="url", default="", desc="url", readonly=True, visible=True, no_value="")
     MetadataElement(name="doi", default="", desc="doi", readonly=True, visible=True, no_value="")
-    MetadataElement(name="anndata_spec_version", default="", desc="anndata_spec_version", readonly=True, visible=True, no_value="")
+    MetadataElement(
+        name="anndata_spec_version", default="", desc="anndata_spec_version", readonly=True, visible=True, no_value=""
+    )
     MetadataElement(name="creation_date", default=None, desc="creation_date", readonly=True, visible=True)
     MetadataElement(name="layers_count", default=0, desc="layers_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="layers_names", desc="layers_names", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="layers_names",
+        desc="layers_names",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
     MetadataElement(name="row_attrs_count", default=0, desc="row_attrs_count", readonly=True, visible=True, no_value=0)
     # obs_names: Cell1, Cell2, Cell3,...
     # obs_layers: louvain, leidein, isBcell
     # obs_count: number of obs_layers
     # obs_size: number of obs_names
     MetadataElement(name="obs_names", desc="obs_names", default=[], multiple=True, readonly=True)
-    MetadataElement(name="obs_layers", desc="obs_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="obs_layers", desc="obs_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
     MetadataElement(name="obs_count", default=0, desc="obs_count", readonly=True, visible=True, no_value=0)
     MetadataElement(name="obs_size", default=-1, desc="obs_size", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="obsm_layers", desc="obsm_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="obsm_layers", desc="obsm_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
     MetadataElement(name="obsm_count", default=0, desc="obsm_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="raw_var_layers", desc="raw_var_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="raw_var_layers",
+        desc="raw_var_layers",
+        default=[],
+        param=metadata.SelectParameter,
+        multiple=True,
+        readonly=True,
+    )
     MetadataElement(name="raw_var_count", default=0, desc="raw_var_count", readonly=True, visible=True, no_value=0)
     MetadataElement(name="raw_var_size", default=0, desc="raw_var_size", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="var_layers", desc="var_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="var_layers", desc="var_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
     MetadataElement(name="var_count", default=0, desc="var_count", readonly=True, visible=True, no_value=0)
     MetadataElement(name="var_size", default=-1, desc="var_size", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="varm_layers", desc="varm_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="varm_layers", desc="varm_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
     MetadataElement(name="varm_count", default=0, desc="varm_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="uns_layers", desc="uns_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="uns_layers", desc="uns_layers", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
     MetadataElement(name="uns_count", default=0, desc="uns_count", readonly=True, visible=True, no_value=0)
-    MetadataElement(name="shape", default=(-1, -1), desc="shape", param=metadata.ListParameter, readonly=True, visible=True, no_value=(0, 0))
+    MetadataElement(
+        name="shape",
+        default=(-1, -1),
+        desc="shape",
+        param=metadata.ListParameter,
+        readonly=True,
+        visible=True,
+        no_value=(0, 0),
+    )
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
             try:
-                with h5py.File(filename, 'r') as f:
-                    return all(attr in f for attr in ['X', 'obs', 'var'])
+                with h5py.File(filename, "r") as f:
+                    return all(attr in f for attr in ["X", "obs", "var"])
             except Exception:
                 return False
         return False
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
-        with h5py.File(dataset.file_name, 'r') as anndata_file:
-            dataset.metadata.title = anndata_file.attrs.get('title')
-            dataset.metadata.description = anndata_file.attrs.get('description')
-            dataset.metadata.url = anndata_file.attrs.get('url')
-            dataset.metadata.doi = anndata_file.attrs.get('doi')
-            dataset.creation_date = anndata_file.attrs.get('creation_date')
-            dataset.metadata.shape = anndata_file.attrs.get('shape', dataset.metadata.shape)
+        with h5py.File(dataset.file_name, "r") as anndata_file:
+            dataset.metadata.title = anndata_file.attrs.get("title")
+            dataset.metadata.description = anndata_file.attrs.get("description")
+            dataset.metadata.url = anndata_file.attrs.get("url")
+            dataset.metadata.doi = anndata_file.attrs.get("doi")
+            dataset.metadata.creation_date = anndata_file.attrs.get("creation_date")
+            dataset.metadata.shape = anndata_file.attrs.get("shape", dataset.metadata.shape)
             # none of the above appear to work in any dataset tested, but could be useful for
             # future AnnData datasets
             dataset.metadata.layers_count = len(anndata_file)
             dataset.metadata.layers_names = list(anndata_file.keys())
 
             def _layercountsize(tmp, lennames=0):
                 "From TMP and LENNAMES, return layers, their number, and the length of one of the layers (all equal)."
-                if hasattr(tmp, 'dtype'):
+                if hasattr(tmp, "dtype"):
                     layers = list(tmp.dtype.names)
                     count = len(tmp.dtype)
                     size = int(tmp.size)
                 else:
                     layers = list(tmp.keys())
                     count = len(layers)
                     size = lennames
                 return (layers, count, size)
 
-            if 'obs' in dataset.metadata.layers_names:
+            if "obs" in dataset.metadata.layers_names:
                 tmp = anndata_file["obs"]
                 obs_index = None
                 if "index" in tmp:
                     obs_index = "index"
                 elif "_index" in tmp:
                     obs_index = "_index"
                 # Determine cell labels
                 if obs_index:
                     dataset.metadata.obs_names = list(tmp[obs_index])
-                elif hasattr(tmp, 'dtype'):
+                elif hasattr(tmp, "dtype"):
                     if "index" in tmp.dtype.names:
                         # Yes, we call tmp["index"], and not tmp.dtype["index"]
                         # here, despite the above tests.
                         dataset.metadata.obs_names = list(tmp["index"])
                     elif "_index" in tmp.dtype.names:
                         dataset.metadata.obs_names = list(tmp["_index"])
                     else:
@@ -1139,28 +1439,28 @@
                     log.warning("Could not determine observation index for %s", self)
 
                 x, y, z = _layercountsize(tmp, len(dataset.metadata.obs_names))
                 dataset.metadata.obs_layers = x
                 dataset.metadata.obs_count = y
                 dataset.metadata.obs_size = z
 
-            if 'obsm' in dataset.metadata.layers_names:
+            if "obsm" in dataset.metadata.layers_names:
                 tmp = anndata_file["obsm"]
                 dataset.metadata.obsm_layers, dataset.metadata.obsm_count, _ = _layercountsize(tmp)
 
-            if 'raw.var' in dataset.metadata.layers_names:
+            if "raw.var" in dataset.metadata.layers_names:
                 tmp = anndata_file["raw.var"]
                 # full set of genes would never need to be previewed
                 # dataset.metadata.raw_var_names = tmp["index"]
                 x, y, z = _layercountsize(tmp, len(tmp["index"]))
                 dataset.metadata.raw_var_layers = x
                 dataset.metadata.raw_var_count = y
                 dataset.metadata.raw_var_size = z
 
-            if 'var' in dataset.metadata.layers_names:
+            if "var" in dataset.metadata.layers_names:
                 tmp = anndata_file["var"]
                 var_index = None
                 if "index" in tmp:
                     var_index = "index"
                 elif "_index" in tmp:
                     var_index = "_index"
                 # We never use var_names
@@ -1172,63 +1472,63 @@
                     # that the dataset is empty
                     x, y, z = _layercountsize(tmp)
 
                 dataset.metadata.var_layers = x
                 dataset.metadata.var_count = y
                 dataset.metadata.var_size = z
 
-            if 'varm' in dataset.metadata.layers_names:
+            if "varm" in dataset.metadata.layers_names:
                 tmp = anndata_file["varm"]
                 dataset.metadata.varm_layers, dataset.metadata.varm_count, _ = _layercountsize(tmp)
 
-            if 'uns' in dataset.metadata.layers_names:
+            if "uns" in dataset.metadata.layers_names:
                 tmp = anndata_file["uns"]
                 dataset.metadata.uns_layers, dataset.metadata.uns_count, _ = _layercountsize(tmp)
 
             # Resolving the problematic shape parameter
-            if 'X' in dataset.metadata.layers_names:
+            if "X" in dataset.metadata.layers_names:
                 # Shape we determine here due to the non-standard representation of 'X' dimensions
-                shape = anndata_file['X'].attrs.get("shape")
+                shape = anndata_file["X"].attrs.get("shape")
                 if shape is not None:
                     dataset.metadata.shape = tuple(shape)
-                elif hasattr(anndata_file['X'], 'shape'):
-                    dataset.metadata.shape = tuple(anndata_file['X'].shape)
+                elif hasattr(anndata_file["X"], "shape"):
+                    dataset.metadata.shape = tuple(anndata_file["X"].shape)
 
             if dataset.metadata.shape is None:
                 dataset.metadata.shape = (int(dataset.metadata.obs_size), int(dataset.metadata.var_size))
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             tmp = dataset.metadata
 
             def _makelayerstrings(layer, count, names):
                 "Format the layers."
                 if layer in tmp.layers_names:
                     return "\n[%s]: %d %s\n    %s" % (
                         layer,
                         count,
                         "layer" if count == 1 else "layers",
-                        ', '.join(sorted(names))
+                        ", ".join(sorted(names)),
                     )
                 return ""
 
             peekstr = "[n_obs x n_vars]\n    %d x %d" % tuple(tmp.shape)
             peekstr += _makelayerstrings("obs", tmp.obs_count, tmp.obs_layers)
             peekstr += _makelayerstrings("var", tmp.var_count, tmp.var_layers)
             peekstr += _makelayerstrings("obsm", tmp.obsm_count, tmp.obsm_layers)
             peekstr += _makelayerstrings("varm", tmp.varm_count, tmp.varm_layers)
             peekstr += _makelayerstrings("uns", tmp.uns_count, tmp.uns_layers)
 
             dataset.peek = peekstr
             dataset.blurb = f"Anndata file ({nice_size(dataset.get_size())})"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary Anndata file ({nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
@@ -1240,60 +1540,63 @@
     >>> fname = get_test_fname('test.grib')
     >>> Grib().sniff_prefix(FilePrefix(fname))
     True
     >>> fname = FilePrefix(get_test_fname('interval.interval'))
     >>> Grib().sniff_prefix(fname)
     False
     """
+
     file_ext = "grib"
     # GRIB not yet in EDAM (work in progress). For now, so set to binary
     edam_format = "format_2333"
-    MetadataElement(name="grib_edition", default=1, desc="GRIB edition", readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="grib_edition", default=1, desc="GRIB edition", readonly=True, visible=True, optional=True, no_value=0
+    )
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self._magic = b'GRIB'
+        self._magic = b"GRIB"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # The first 4 bytes of any GRIB file are GRIB
         try:
             if file_prefix.startswith_bytes(self._magic):
                 tmp = file_prefix.contents_header_bytes[4:8]
                 _uint8struct = struct.Struct(b">B")
                 edition = _uint8struct.unpack_from(tmp, 3)[0]
                 if edition == 1 or edition == 2:
                     return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary GRIB file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary GRIB file ({nice_size(dataset.get_size())})"
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the GRIB edition.
         """
         dataset.metadata.grib_edition = self._get_grib_edition(dataset.file_name)
 
-    def _get_grib_edition(self, filename):
+    def _get_grib_edition(self, filename: str) -> int:
         _uint8struct = struct.Struct(b">B")
         edition = 0
-        with open(filename, 'rb') as f:
+        with open(filename, "rb") as f:
             f.seek(4)
             tmp = f.read(4)
             edition = _uint8struct.unpack_from(tmp, 3)[0]
         return edition
 
 
 @build_sniff_from_prefix
@@ -1301,27 +1604,27 @@
     """
     Base class for GROMACS binary files - xtc, trr, cpt
     """
 
     magic_number: Optional[int] = None  # variables to be overwritten in the child class
     file_ext = ""
 
-    def sniff_prefix(self, sniff_prefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # The first 4 bytes of any GROMACS binary file containing the magic number
-        return sniff_prefix.magic_header('>1i') == self.magic_number
+        return file_prefix.magic_header(">1i") == self.magic_number
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = f"Binary GROMACS {self.file_ext} file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary GROMACS {self.file_ext} trajectory file ({nice_size(dataset.get_size())})"
 
 
 class Trr(GmxBinary):
@@ -1385,105 +1688,124 @@
     True
     >>> fname = get_test_fname('md.trr')
     >>> Edr().sniff(fname)
     False
     """
 
     file_ext = "edr"
-    magic_number = -55555  # reference: https://github.com/gromacs/gromacs/blob/cec211b2c835ba6e8ea849fb1bf67d7fc19693a4/src/gromacs/fileio/enxio.cpp
+    magic_number = (
+        -55555
+    )  # reference: https://github.com/gromacs/gromacs/blob/cec211b2c835ba6e8ea849fb1bf67d7fc19693a4/src/gromacs/fileio/enxio.cpp
 
 
 class Biom2(H5):
     """
     Class describing a biom2 file (http://biom-format.org/documentation/biom_format.html)
     """
+
     MetadataElement(name="id", default=None, desc="table id", readonly=True, visible=True)
     MetadataElement(name="format_url", default=None, desc="format-url", readonly=True, visible=True)
-    MetadataElement(name="format_version", default=None, desc="format-version (equal to format)", readonly=True, visible=True)
+    MetadataElement(
+        name="format_version", default=None, desc="format-version (equal to format)", readonly=True, visible=True
+    )
     MetadataElement(name="format", default=None, desc="format (equal to format=version)", readonly=True, visible=True)
     MetadataElement(name="type", default=None, desc="table type", readonly=True, visible=True)
     MetadataElement(name="generated_by", default=None, desc="generated by", readonly=True, visible=True)
     MetadataElement(name="creation_date", default=None, desc="creation date", readonly=True, visible=True)
-    MetadataElement(name="nnz", default=-1, desc="nnz: The number of non-zero elements in the table", readonly=True, visible=True, no_value=-1)
-    MetadataElement(name="shape", default=(), desc="shape: The number of rows and columns in the dataset", readonly=True, visible=True, no_value=())
+    MetadataElement(
+        name="nnz",
+        default=-1,
+        desc="nnz: The number of non-zero elements in the table",
+        readonly=True,
+        visible=True,
+        no_value=-1,
+    )
+    MetadataElement(
+        name="shape",
+        default=(),
+        desc="shape: The number of rows and columns in the dataset",
+        readonly=True,
+        visible=True,
+        no_value=(),
+    )
 
     file_ext = "biom2"
     edam_format = "format_3746"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('biom2_sparse_otu_table_hdf5.biom2')
         >>> Biom2().sniff(fname)
         True
         >>> fname = get_test_fname('test.mz5')
         >>> Biom2().sniff(fname)
         False
         >>> fname = get_test_fname('wiggle.wig')
         >>> Biom2().sniff(fname)
         False
         """
         if super().sniff(filename):
-            with h5py.File(filename, 'r') as f:
-                required_fields = {'id', 'format-url', 'type', 'generated-by', 'creation-date', 'nnz', 'shape'}
+            with h5py.File(filename, "r") as f:
+                required_fields = {"id", "format-url", "type", "generated-by", "creation-date", "nnz", "shape"}
                 return required_fields.issubset(f.attrs.keys())
         return False
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
-            with h5py.File(dataset.file_name, 'r') as f:
+            with h5py.File(dataset.file_name, "r") as f:
                 attributes = f.attrs
 
-                dataset.metadata.id = util.unicodify(attributes['id'])
-                dataset.metadata.format_url = util.unicodify(attributes['format-url'])
-                if 'format-version' in attributes:  # biom 2.1
-                    dataset.metadata.format_version = '.'.join(str(_) for _ in attributes['format-version'])
+                dataset.metadata.id = util.unicodify(attributes["id"])
+                dataset.metadata.format_url = util.unicodify(attributes["format-url"])
+                if "format-version" in attributes:  # biom 2.1
+                    dataset.metadata.format_version = ".".join(str(_) for _ in attributes["format-version"])
                     dataset.metadata.format = dataset.metadata.format_version
-                elif 'format' in attributes:  # biom 2.0
-                    dataset.metadata.format = util.unicodify(attributes['format'])
+                elif "format" in attributes:  # biom 2.0
+                    dataset.metadata.format = util.unicodify(attributes["format"])
                     dataset.metadata.format_version = dataset.metadata.format
-                dataset.metadata.type = util.unicodify(attributes['type'])
-                dataset.metadata.shape = tuple(int(_) for _ in attributes['shape'])
-                dataset.metadata.generated_by = util.unicodify(attributes['generated-by'])
-                dataset.metadata.creation_date = util.unicodify(attributes['creation-date'])
-                dataset.metadata.nnz = int(attributes['nnz'])
+                dataset.metadata.type = util.unicodify(attributes["type"])
+                dataset.metadata.shape = tuple(int(_) for _ in attributes["shape"])
+                dataset.metadata.generated_by = util.unicodify(attributes["generated-by"])
+                dataset.metadata.creation_date = util.unicodify(attributes["creation-date"])
+                dataset.metadata.nnz = int(attributes["nnz"])
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, util.unicodify(e))
+            log.warning("%s, set_meta Exception: %s", self, util.unicodify(e))
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            lines = ['Biom2 (HDF5) file']
+            lines = ["Biom2 (HDF5) file"]
             try:
                 with h5py.File(dataset.file_name) as f:
                     for k, v in f.attrs.items():
-                        lines.append(f'{k}:  {util.unicodify(v)}')
+                        lines.append(f"{k}:  {util.unicodify(v)}")
             except Exception as e:
-                log.warning('%s, set_peek Exception: %s', self, util.unicodify(e))
-            dataset.peek = '\n'.join(lines)
+                log.warning("%s, set_peek Exception: %s", self, util.unicodify(e))
+            dataset.peek = "\n".join(lines)
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Biom2 (HDF5) file ({nice_size(dataset.get_size())})"
 
 
 class Cool(H5):
     """
     Class describing the cool format (https://github.com/mirnylab/cooler)
     """
 
     file_ext = "cool"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('matrix.cool')
         >>> Cool().sniff(fname)
         True
         >>> fname = get_test_fname('test.mz5')
         >>> Cool().sniff(fname)
@@ -1496,47 +1818,47 @@
         False
         """
 
         MAGIC = "HDF5::Cooler"
         URL = "https://github.com/mirnylab/cooler"
 
         if super().sniff(filename):
-            keys = ['chroms', 'bins', 'pixels', 'indexes']
-            with h5py.File(filename, 'r') as handle:
-                fmt = util.unicodify(handle.attrs.get('format'))
-                url = util.unicodify(handle.attrs.get('format-url'))
+            keys = ["chroms", "bins", "pixels", "indexes"]
+            with h5py.File(filename, "r") as handle:
+                fmt = util.unicodify(handle.attrs.get("format"))
+                url = util.unicodify(handle.attrs.get("format-url"))
                 if fmt == MAGIC or url == URL:
                     if not all(name in handle.keys() for name in keys):
                         return False
                     return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Cool (HDF5) file for storing genomic interaction data."
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Cool (HDF5) file ({nice_size(dataset.get_size())})."
 
 
 class MCool(H5):
     """
     Class describing the multi-resolution cool format (https://github.com/mirnylab/cooler)
     """
 
     file_ext = "mcool"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('matrix.mcool')
         >>> MCool().sniff(fname)
         True
         >>> fname = get_test_fname('matrix.cool')
         >>> MCool().sniff(fname)
@@ -1552,168 +1874,214 @@
         False
         """
 
         MAGIC = "HDF5::Cooler"
         URL = "https://github.com/mirnylab/cooler"
 
         if super().sniff(filename):
-            keys0 = ['resolutions']
-            with h5py.File(filename, 'r') as handle:
+            keys0 = ["resolutions"]
+            with h5py.File(filename, "r") as handle:
                 if not all(name in handle.keys() for name in keys0):
                     return False
-                res0 = next(iter(handle['resolutions'].keys()))
-                keys = ['chroms', 'bins', 'pixels', 'indexes']
-                fmt = util.unicodify(handle['resolutions'][res0].attrs.get('format'))
-                url = util.unicodify(handle['resolutions'][res0].attrs.get('format-url'))
+                res0 = next(iter(handle["resolutions"].keys()))
+                keys = ["chroms", "bins", "pixels", "indexes"]
+                fmt = util.unicodify(handle["resolutions"][res0].attrs.get("format"))
+                url = util.unicodify(handle["resolutions"][res0].attrs.get("format-url"))
                 if fmt == MAGIC or url == URL:
-                    if not all(name in handle['resolutions'][res0].keys() for name in keys):
+                    if not all(name in handle["resolutions"][res0].keys() for name in keys):
                         return False
                     return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Multi-resolution Cool (HDF5) file for storing genomic interaction data."
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"MCool (HDF5) file ({nice_size(dataset.get_size())})."
 
 
 class H5MLM(H5):
     """
     Machine learning model generated by Galaxy-ML.
     """
-    file_ext = "h5mlm"
-    URL = "https://github.com/goeckslab/Galaxy-ML"
 
-    max_peek_size = 1000         # 1 KB
-    max_preview_size = 1000000   # 1 MB
+    file_ext = "h5mlm"
+    TARGET_URL = "https://github.com/goeckslab/Galaxy-ML"
 
-    MetadataElement(name="hyper_params", desc="Hyperparameter File", param=FileParameter, file_ext="tabular", readonly=True, visible=False, optional=True)
+    max_peek_size = 1000  # 1 KB
+    max_preview_size = 1000000  # 1 MB
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    # reserved keys
+    CONFIG = "-model_config-"
+    HTTP_REPR = "-http_repr-"
+    HYPERPARAMETER = "-model_hyperparameters-"
+    REPR = "-repr-"
+    URL = "-URL-"
+
+    MetadataElement(
+        name="hyper_params",
+        desc="Hyperparameter File",
+        param=FileParameter,
+        file_ext="tabular",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
         try:
             spec_key = "hyper_params"
             params_file = dataset.metadata.hyper_params
             if not params_file:
-                params_file = dataset.metadata.spec[spec_key].param.new_file(dataset=dataset)
+                params_file = dataset.metadata.spec[spec_key].param.new_file(
+                    dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+                )
             with h5py.File(dataset.file_name, "r") as handle:
-                hyper_params = handle["-model_hyperparameters-"][()]
+                hyper_params = handle[self.HYPERPARAMETER][()]
             hyper_params = json.loads(util.unicodify(hyper_params))
             with open(params_file.file_name, "w") as f:
                 f.write("\tParameter\tValue\n")
                 for p in hyper_params:
                     f.write("\t".join(p) + "\n")
             dataset.metadata.hyper_params = params_file
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            keys = ["-model_config-"]
+            keys = [self.CONFIG]
             with h5py.File(filename, "r") as handle:
                 if not all(name in handle.keys() for name in keys):
                     return False
-                url = util.unicodify(handle.attrs.get("-URL-"))
-            if url == self.URL:
+                url = util.unicodify(handle.attrs.get(self.URL))
+            if url == self.TARGET_URL:
                 return True
         return False
 
-    def get_repr(self, filename):
+    def get_attribute(self, filename: str, attr_key: str) -> str:
         try:
             with h5py.File(filename, "r") as handle:
-                repr_ = util.unicodify(handle.attrs.get("-repr-"))
-            return repr_
+                attr = util.unicodify(handle.attrs.get(attr_key))
+            return attr
         except Exception as e:
-            log.warning('%s, get_repr Except: %s', self, e)
+            log.warning("%s, get_attribute Except: %s", self, e)
             return ""
 
-    def get_config_string(self, filename):
+    def get_repr(self, filename: str) -> str:
+        repr = self.get_attribute(filename, self.REPR)
+        if len(repr) <= self.max_preview_size:
+            return repr
+        else:
+            return "<p><strong>The model representation is too big to be displayed!</strong></p>"
+
+    def get_html_repr(self, filename: str) -> str:
+        repr = self.get_attribute(filename, self.HTTP_REPR)
+        if len(repr) <= self.max_preview_size:
+            return repr
+        else:
+            return "<p><strong>The model diagram is too big to be displayed!</strong></p>"
+
+    def get_config_string(self, filename: str) -> str:
         try:
             with h5py.File(filename, "r") as handle:
-                config = util.unicodify(handle["-model_config-"][()])
+                config = util.unicodify(handle[self.CONFIG][()])
             return config
         except Exception as e:
-            log.warning('%s, get model configuration Except: %s', self, e)
+            log.warning("%s, get model configuration Except: %s", self, e)
             return ""
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            repr_ = self.get_repr(dataset.file_name)
-            dataset.peek = repr_[:self.max_peek_size]
+            repr = self.get_repr(dataset.file_name)
+            dataset.peek = repr[: self.max_peek_size]
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "HDF5 Model (%s)" % (nice_size(dataset.get_size()))
 
-    def display_data(self, trans, dataset, preview=False, filename=None, to_ext=None, **kwd):
-        headers = kwd.get("headers", {})
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        **kwd,
+    ):
+        headers = kwd.pop("headers", {})
         preview = util.string_as_bool(preview)
 
         if to_ext or not preview:
             to_ext = to_ext or dataset.extension
             return self._serve_raw(dataset, to_ext, headers, **kwd)
 
-        rval = {}
+        out_dict: Dict = {}
         try:
             with h5py.File(dataset.file_name, "r") as handle:
-                rval['Attributes'] = {}
+                out_dict["Attributes"] = {}
                 attributes = handle.attrs
-                for k in (set(attributes.keys()) - {'-URL-', '-repr-'}):
-                    rval['Attributes'][k] = util.unicodify(attributes.get(k))
+                for k in set(attributes.keys()) - {self.HTTP_REPR, self.REPR, self.URL}:
+                    out_dict["Attributes"][k] = util.unicodify(attributes.get(k))
         except Exception as e:
             log.warning(e)
 
         config = self.get_config_string(dataset.file_name)
-        rval['Config'] = json.loads(config) if config else ''
-        rval = json.dumps(rval, sort_keys=True, indent=2)
-        rval = rval[:self.max_preview_size]
+        out_dict["Config"] = json.loads(config) if config else ""
+        out = json.dumps(out_dict, sort_keys=True, indent=2)
+        out = out[: self.max_preview_size]
 
-        repr_ = self.get_repr(dataset.file_name)
+        repr = self.get_repr(dataset.file_name)
+        html_repr = self.get_html_repr(dataset.file_name)
 
-        return f"<pre>{repr_}</pre><pre>{rval}</pre>", headers
+        return f"<div>{html_repr}</div><div><pre>{repr}</pre></div><div><pre>{out}</pre></div>", headers
 
 
 class LudwigModel(Html):
     """
     Composite datatype that encloses multiple files for a Ludwig trained model.
     """
-    composite_type = 'auto_primary_file'
+
+    composite_type = "auto_primary_file"
     file_ext = "ludwig_model"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-        self.add_composite_file('model_hyperparameters.json', description='Model hyperparameters', is_binary=False)
-        self.add_composite_file('model_weights', description='Model weights', is_binary=True)
-        self.add_composite_file('training_set_metadata.json', description='Training set metadata', is_binary=False)
-        self.add_composite_file('training_progress.json', description='Training progress', is_binary=False, optional=True)
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>Ludwig Model Composite Dataset.</title></head><p/>']
-        rval.append('<div>This model dataset is composed of the following items:<p/><ul>')
+        self.add_composite_file("model_hyperparameters.json", description="Model hyperparameters", is_binary=False)
+        self.add_composite_file("model_weights", description="Model weights", is_binary=True)
+        self.add_composite_file("training_set_metadata.json", description="Training set metadata", is_binary=False)
+        self.add_composite_file(
+            "training_progress.json", description="Training progress", is_binary=False, optional=True
+        )
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Ludwig Model Composite Dataset.</title></head><p/>"]
+        rval.append("<div>This model dataset is composed of the following items:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
-            description = composite_file.get('description')
-            link_text = f'{composite_name} ({description})' if description else composite_name
-            opt_text = ' (optional)' if composite_file.optional else ''
+            description = composite_file.get("description")
+            link_text = f"{composite_name} ({description})" if description else composite_name
+            opt_text = " (optional)" if composite_file.optional else ""
             rval.append(f'<li><a href="{composite_name}" type="text/plain">{link_text}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
 
 
 class HexrdMaterials(H5):
     """
     Class describing a Hexrd Materials file: https://github.com/HEXRD/hexrd
 
@@ -1721,422 +2089,507 @@
     >>> fname = get_test_fname('hexrd.materials.h5')
     >>> HexrdMaterials().sniff(fname)
     True
     >>> fname = get_test_fname('test.loom')
     >>> HexrdMaterials().sniff(fname)
     False
     """
+
     file_ext = "hexrd.materials.h5"
     edam_format = "format_3590"
 
-    MetadataElement(name="materials", desc="materials", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
-    MetadataElement(name="SpaceGroupNumber", default={}, param=DictParameter, desc="SpaceGroupNumber", readonly=True, visible=True, no_value={})
-    MetadataElement(name="LatticeParameters", default={}, param=DictParameter, desc="LatticeParameters", readonly=True, visible=True, no_value={})
+    MetadataElement(
+        name="materials", desc="materials", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
+    MetadataElement(
+        name="SpaceGroupNumber",
+        default={},
+        param=DictParameter,
+        desc="SpaceGroupNumber",
+        readonly=True,
+        visible=True,
+        no_value={},
+    )
+    MetadataElement(
+        name="LatticeParameters",
+        default={},
+        param=DictParameter,
+        desc="LatticeParameters",
+        readonly=True,
+        visible=True,
+        no_value={},
+    )
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            req = {'AtomData', 'Atomtypes', 'CrystalSystem', 'LatticeParameters'}
-            with h5py.File(filename, 'r') as mat_file:
+            req = {"AtomData", "Atomtypes", "CrystalSystem", "LatticeParameters"}
+            with h5py.File(filename, "r") as mat_file:
                 for k in mat_file.keys():
                     if isinstance(mat_file[k], h5py._hl.group.Group) and set(mat_file[k].keys()) >= req:
                         return True
         return False
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
-            with h5py.File(dataset.file_name, 'r') as mat_file:
+            with h5py.File(dataset.file_name, "r") as mat_file:
                 dataset.metadata.materials = list(mat_file.keys())
                 sgn = dict()
                 lp = dict()
                 for m in mat_file.keys():
-                    if 'SpaceGroupNumber' in mat_file[m] and len(mat_file[m]['SpaceGroupNumber']) > 0:
-                        sgn[m] = mat_file[m]['SpaceGroupNumber'][0].item()
-                    if 'LatticeParameters' in mat_file[m]:
-                        lp[m] = mat_file[m]['LatticeParameters'][0:].tolist()
+                    if "SpaceGroupNumber" in mat_file[m] and len(mat_file[m]["SpaceGroupNumber"]) > 0:
+                        sgn[m] = mat_file[m]["SpaceGroupNumber"][0].item()
+                    if "LatticeParameters" in mat_file[m]:
+                        lp[m] = mat_file[m]["LatticeParameters"][0:].tolist()
                 dataset.metadata.SpaceGroupNumber = sgn
                 dataset.metadata.LatticeParameters = lp
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            lines = ['Material SpaceGroup Lattice']
+            lines = ["Material SpaceGroup Lattice"]
             if dataset.metadata.materials:
                 for m in dataset.metadata.materials:
                     try:
-                        lines.append(f'{m} {dataset.metadata.SpaceGroupNumber[m]} {dataset.metadata.LatticeParameters[m]}')
+                        lines.append(
+                            f"{m} {dataset.metadata.SpaceGroupNumber[m]} {dataset.metadata.LatticeParameters[m]}"
+                        )
                     except Exception:
                         continue
-            dataset.peek = '\n'.join(lines)
+            dataset.peek = "\n".join(lines)
             dataset.blurb = f"Materials: {' '.join(dataset.metadata.materials)}"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class Scf(Binary):
     """Class describing an scf binary sequence file"""
+
     edam_format = "format_1632"
     edam_data = "data_0924"
     file_ext = "scf"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary scf sequence file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary scf sequence file ({nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
 class Sff(Binary):
-    """ Standard Flowgram Format (SFF) """
+    """Standard Flowgram Format (SFF)"""
+
     edam_format = "format_3284"
     edam_data = "data_0924"
     file_ext = "sff"
 
-    def sniff_prefix(self, sniff_prefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # The first 4 bytes of any sff file is '.sff', and the file is binary. For details
         # about the format, see http://www.ncbi.nlm.nih.gov/Traces/trace.cgi?cmd=show&f=formats&m=doc&s=format
-        return sniff_prefix.startswith_bytes(b'.sff')
+        return file_prefix.startswith_bytes(b".sff")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary sff file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary sff file ({nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
 class BigWig(Binary):
     """
     Accessing binary BigWig files from UCSC.
     The supplemental info in the paper has the binary details:
     http://bioinformatics.oxfordjournals.org/cgi/content/abstract/btq351v1
     """
+
     edam_format = "format_3006"
     edam_data = "data_3002"
     file_ext = "bigwig"
     track_type = "LineTrack"
     data_sources = {"data_standalone": "bigwig"}
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = 0x888FFC26
         self._name = "BigWig"
 
-    def sniff_prefix(self, sniff_prefix):
-        return sniff_prefix.magic_header("I") == self._magic
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.magic_header("I") == self._magic
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = f"Binary UCSC {self._name} file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary UCSC {self._name} file ({nice_size(dataset.get_size())})"
 
 
 class BigBed(BigWig):
     """BigBed support from UCSC."""
+
     edam_format = "format_3004"
     edam_data = "data_3002"
     file_ext = "bigbed"
     data_sources = {"data_standalone": "bigbed"}
 
     def __init__(self, **kwd):
         Binary.__init__(self, **kwd)
         self._magic = 0x8789F2EB
         self._name = "BigBed"
 
 
 @build_sniff_from_prefix
 class TwoBit(Binary):
     """Class describing a TwoBit format nucleotide file"""
+
     edam_format = "format_3009"
     edam_data = "data_0848"
     file_ext = "twobit"
 
-    def sniff_prefix(self, sniff_prefix):
-        magic = sniff_prefix.magic_header(">L")
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        magic = file_prefix.magic_header(">L")
         return magic == TWOBIT_MAGIC_NUMBER or magic == TWOBIT_MAGIC_NUMBER_SWAP
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary TwoBit format nucleotide file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
             return super().set_peek(dataset)
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary TwoBit format nucleotide file ({nice_size(dataset.get_size())})"
 
 
 @dataproviders.decorators.has_dataproviders
 class SQlite(Binary):
-    """Class describing a Sqlite database """
-    MetadataElement(name="tables", default=[], param=ListParameter, desc="Database Tables", readonly=True, visible=True, no_value=[])
-    MetadataElement(name="table_columns", default={}, param=DictParameter, desc="Database Table Columns", readonly=True, visible=True, no_value={})
-    MetadataElement(name="table_row_count", default={}, param=DictParameter, desc="Database Table Row Count", readonly=True, visible=True, no_value={})
+    """Class describing a Sqlite database"""
+
+    MetadataElement(
+        name="tables", default=[], param=ListParameter, desc="Database Tables", readonly=True, visible=True, no_value=[]
+    )
+    MetadataElement(
+        name="table_columns",
+        default={},
+        param=DictParameter,
+        desc="Database Table Columns",
+        readonly=True,
+        visible=True,
+        no_value={},
+    )
+    MetadataElement(
+        name="table_row_count",
+        default={},
+        param=DictParameter,
+        desc="Database Table Row Count",
+        readonly=True,
+        visible=True,
+        no_value={},
+    )
     file_ext = "sqlite"
     edam_format = "format_3621"
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         Binary.init_meta(self, dataset, copy_from=copy_from)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         try:
             tables = []
             columns = dict()
             rowcounts = dict()
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
             tables_query = "SELECT name,sql FROM sqlite_master WHERE type='table' ORDER BY name"
             rslt = c.execute(tables_query).fetchall()
             for table, _ in rslt:
                 tables.append(table)
                 try:
-                    col_query = f'SELECT * FROM {table} LIMIT 0'
+                    col_query = f"SELECT * FROM {table} LIMIT 0"
                     cur = conn.cursor().execute(col_query)
                     cols = [col[0] for col in cur.description]
                     columns[table] = cols
                 except Exception as exc:
-                    log.warning('%s, set_meta Exception: %s', self, exc)
+                    log.warning("%s, set_meta Exception: %s", self, exc)
             for table in tables:
                 try:
                     row_query = f"SELECT count(*) FROM {table}"
                     rowcounts[table] = c.execute(row_query).fetchone()[0]
                 except Exception as exc:
-                    log.warning('%s, set_meta Exception: %s', self, exc)
+                    log.warning("%s, set_meta Exception: %s", self, exc)
             dataset.metadata.tables = tables
             dataset.metadata.table_columns = columns
             dataset.metadata.table_row_count = rowcounts
         except Exception as exc:
-            log.warning('%s, set_meta Exception: %s', self, exc)
+            log.warning("%s, set_meta Exception: %s", self, exc)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # The first 16 bytes of any SQLite3 database file is 'SQLite format 3\0', and the file is binary. For details
         # about the format, see http://www.sqlite.org/fileformat.html
         try:
-            header = open(filename, 'rb').read(16)
-            if header == b'SQLite format 3\0':
+            header = open(filename, "rb").read(16)
+            if header == b"SQLite format 3\0":
                 return True
             return False
         except Exception:
             return False
 
-    def sniff_table_names(self, filename, table_names):
+    def sniff_table_names(self, filename: str, table_names: Iterable) -> bool:
         # All table names should be in the schema
         try:
             conn = sqlite.connect(filename)
             c = conn.cursor()
             tables_query = "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name"
             result = c.execute(tables_query).fetchall()
             result = [_[0] for _ in result]
             for table_name in table_names:
                 if table_name not in result:
                     return False
             return True
         except Exception as e:
-            log.warning('%s, sniff Exception: %s', self, e)
+            log.warning("%s, sniff Exception: %s", self, e)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "SQLite Database"
-            lines = ['SQLite Database']
+            lines = ["SQLite Database"]
             if dataset.metadata.tables:
                 for table in dataset.metadata.tables:
                     try:
-                        lines.append(f'{table} [{dataset.metadata.table_row_count[table]}]')
+                        lines.append(f"{table} [{dataset.metadata.table_row_count[table]}]")
                     except Exception:
                         continue
-            dataset.peek = '\n'.join(lines)
+            dataset.peek = "\n".join(lines)
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"SQLite Database ({nice_size(dataset.get_size())})"
 
-    @dataproviders.decorators.dataprovider_factory('sqlite', dataproviders.dataset.SQliteDataProvider.settings)
-    def sqlite_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        return dataproviders.dataset.SQliteDataProvider(dataset_source, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('sqlite-table', dataproviders.dataset.SQliteDataTableProvider.settings)
-    def sqlite_datatableprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        return dataproviders.dataset.SQliteDataTableProvider(dataset_source, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('sqlite-dict', dataproviders.dataset.SQliteDataDictProvider.settings)
-    def sqlite_datadictprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        return dataproviders.dataset.SQliteDataDictProvider(dataset_source, **settings)
+    @dataproviders.decorators.dataprovider_factory("sqlite", SQliteDataProvider.settings)
+    def sqlite_dataprovider(self, dataset: "DatasetInstance", **settings) -> SQliteDataProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        return SQliteDataProvider(dataset_source, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("sqlite-table", SQliteDataTableProvider.settings)
+    def sqlite_datatableprovider(self, dataset: "DatasetInstance", **settings) -> SQliteDataTableProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        return SQliteDataTableProvider(dataset_source, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("sqlite-dict", SQliteDataDictProvider.settings)
+    def sqlite_datadictprovider(self, dataset: "DatasetInstance", **settings) -> SQliteDataDictProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        return SQliteDataDictProvider(dataset_source, **settings)
 
 
 class GeminiSQLite(SQlite):
-    """Class describing a Gemini Sqlite database """
-    MetadataElement(name="gemini_version", default='0.10.0', param=MetadataParameter, desc="Gemini Version",
-                    readonly=True, visible=True, no_value='0.10.0')
+    """Class describing a Gemini Sqlite database"""
+
+    MetadataElement(
+        name="gemini_version",
+        default="0.10.0",
+        param=MetadataParameter,
+        desc="Gemini Version",
+        readonly=True,
+        visible=True,
+        no_value="0.10.0",
+    )
     file_ext = "gemini.sqlite"
     edam_format = "format_3622"
     edam_data = "data_3498"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
             tables_query = "SELECT version FROM version"
             result = c.execute(tables_query).fetchall()
-            for version, in result:
+            for (version,) in result:
                 dataset.metadata.gemini_version = version
             # TODO: Can/should we detect even more attributes, such as use of PED file, what was input annotation type, etc.
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = ["gene_detailed", "gene_summary", "resources", "sample_genotype_counts",
-                           "sample_genotypes", "samples", "variant_impacts", "variants", "version"]
+            table_names = [
+                "gene_detailed",
+                "gene_summary",
+                "resources",
+                "sample_genotype_counts",
+                "sample_genotypes",
+                "samples",
+                "variant_impacts",
+                "variants",
+                "version",
+            ]
             return self.sniff_table_names(filename, table_names)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = "Gemini SQLite Database, version %s" % (dataset.metadata.gemini_version or 'unknown')
+            dataset.peek = "Gemini SQLite Database, version %s" % (dataset.metadata.gemini_version or "unknown")
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
-            return "Gemini SQLite Database, version %s" % (dataset.metadata.gemini_version or 'unknown')
+            return "Gemini SQLite Database, version %s" % (dataset.metadata.gemini_version or "unknown")
 
 
 class ChiraSQLite(SQlite):
-    """Class describing a ChiRAViz Sqlite database """
+    """Class describing a ChiRAViz Sqlite database"""
+
     file_ext = "chira.sqlite"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            self.sniff_table_names(filename, ['Chimeras'])
+            self.sniff_table_names(filename, ["Chimeras"])
         return False
 
 
 class CuffDiffSQlite(SQlite):
-    """Class describing a CuffDiff SQLite database """
-    MetadataElement(name="cuffdiff_version", default='2.2.1', param=MetadataParameter, desc="CuffDiff Version",
-                    readonly=True, visible=True, no_value='2.2.1')
-    MetadataElement(name="genes", default=[], param=MetadataParameter, desc="Genes",
-                    readonly=True, visible=True, no_value=[])
-    MetadataElement(name="samples", default=[], param=MetadataParameter, desc="Samples",
-                    readonly=True, visible=True, no_value=[])
+    """Class describing a CuffDiff SQLite database"""
+
+    MetadataElement(
+        name="cuffdiff_version",
+        default="2.2.1",
+        param=MetadataParameter,
+        desc="CuffDiff Version",
+        readonly=True,
+        visible=True,
+        no_value="2.2.1",
+    )
+    MetadataElement(
+        name="genes", default=[], param=MetadataParameter, desc="Genes", readonly=True, visible=True, no_value=[]
+    )
+    MetadataElement(
+        name="samples", default=[], param=MetadataParameter, desc="Samples", readonly=True, visible=True, no_value=[]
+    )
     file_ext = "cuffdiff.sqlite"
     # TODO: Update this when/if there is a specific EDAM format for CuffDiff SQLite data.
     edam_format = "format_3621"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             genes = []
             samples = []
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
             tables_query = "SELECT value FROM runInfo where param = 'version'"
             result = c.execute(tables_query).fetchall()
-            for version, in result:
+            for (version,) in result:
                 dataset.metadata.cuffdiff_version = version
-            genes_query = 'SELECT gene_id, gene_short_name FROM genes ORDER BY gene_short_name'
+            genes_query = "SELECT gene_id, gene_short_name FROM genes ORDER BY gene_short_name"
             result = c.execute(genes_query).fetchall()
             for gene_id, gene_name in result:
                 if gene_name is None:
                     continue
-                gene = f'{gene_id}: {gene_name}'
+                gene = f"{gene_id}: {gene_name}"
                 if gene not in genes:
                     genes.append(gene)
-            samples_query = 'SELECT DISTINCT(sample_name) as sample_name FROM samples ORDER BY sample_name'
+            samples_query = "SELECT DISTINCT(sample_name) as sample_name FROM samples ORDER BY sample_name"
             result = c.execute(samples_query).fetchall()
-            for sample_name, in result:
+            for (sample_name,) in result:
                 if sample_name not in samples:
                     samples.append(sample_name)
             dataset.metadata.genes = genes
             dataset.metadata.samples = samples
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
             # These tables should be in any CuffDiff SQLite output.
-            table_names = ['CDS', 'genes', 'isoforms', 'replicates', 'runInfo', 'samples', 'TSS']
+            table_names = ["CDS", "genes", "isoforms", "replicates", "runInfo", "samples", "TSS"]
             return self.sniff_table_names(filename, table_names)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = "CuffDiff SQLite Database, version %s" % (dataset.metadata.cuffdiff_version or 'unknown')
+            dataset.peek = "CuffDiff SQLite Database, version %s" % (dataset.metadata.cuffdiff_version or "unknown")
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
-            return "CuffDiff SQLite Database, version %s" % (dataset.metadata.cuffdiff_version or 'unknown')
+            return "CuffDiff SQLite Database, version %s" % (dataset.metadata.cuffdiff_version or "unknown")
 
 
 class MzSQlite(SQlite):
-    """Class describing a Proteomics Sqlite database """
+    """Class describing a Proteomics Sqlite database"""
+
     file_ext = "mz.sqlite"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = ["DBSequence", "Modification", "Peaks", "Peptide", "PeptideEvidence",
-                           "Score", "SearchDatabase", "Source", "SpectraData", "Spectrum", "SpectrumIdentification"]
+            table_names = [
+                "DBSequence",
+                "Modification",
+                "Peaks",
+                "Peptide",
+                "PeptideEvidence",
+                "Score",
+                "SearchDatabase",
+                "Source",
+                "SpectraData",
+                "Spectrum",
+                "SpectrumIdentification",
+            ]
             return self.sniff_table_names(filename, table_names)
         return False
 
 
 class PQP(SQlite):
     """
     Class describing a Peptide query parameters file
@@ -2145,31 +2598,41 @@
     >>> fname = get_test_fname('test.pqp')
     >>> PQP().sniff(fname)
     True
     >>> fname = get_test_fname('test.osw')
     >>> PQP().sniff(fname)
     False
     """
+
     file_ext = "pqp"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         table definition according to https://github.com/grosenberger/OpenMS/blob/develop/src/openms/source/ANALYSIS/OPENSWATH/TransitionPQPFile.cpp#L264
         for now VERSION GENE PEPTIDE_GENE_MAPPING are excluded, since
         there is test data wo these tables, see also here https://github.com/OpenMS/OpenMS/issues/4365
         """
         if not super().sniff(filename):
             return False
-        table_names = ['COMPOUND', 'PEPTIDE', 'PEPTIDE_PROTEIN_MAPPING', 'PRECURSOR',
-                       'PRECURSOR_COMPOUND_MAPPING', 'PRECURSOR_PEPTIDE_MAPPING', 'PROTEIN',
-                       'TRANSITION', 'TRANSITION_PEPTIDE_MAPPING', 'TRANSITION_PRECURSOR_MAPPING']
-        osw_table_names = ['FEATURE', 'FEATURE_MS1', 'FEATURE_MS2', 'FEATURE_TRANSITION', 'RUN']
+        table_names = [
+            "COMPOUND",
+            "PEPTIDE",
+            "PEPTIDE_PROTEIN_MAPPING",
+            "PRECURSOR",
+            "PRECURSOR_COMPOUND_MAPPING",
+            "PRECURSOR_PEPTIDE_MAPPING",
+            "PROTEIN",
+            "TRANSITION",
+            "TRANSITION_PEPTIDE_MAPPING",
+            "TRANSITION_PRECURSOR_MAPPING",
+        ]
+        osw_table_names = ["FEATURE", "FEATURE_MS1", "FEATURE_MS2", "FEATURE_TRANSITION", "RUN"]
         return self.sniff_table_names(filename, table_names) and not self.sniff_table_names(filename, osw_table_names)
 
 
 class OSW(SQlite):
     """
     Class describing OpenSwath output
 
@@ -2177,28 +2640,42 @@
     >>> fname = get_test_fname('test.osw')
     >>> OSW().sniff(fname)
     True
     >>> fname = get_test_fname('test.sqmass')
     >>> OSW().sniff(fname)
     False
     """
+
     file_ext = "osw"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # osw seems to be an extension of pqp (few tables are added)
         # see also here https://github.com/OpenMS/OpenMS/issues/4365
         if not super().sniff(filename):
             return False
-        table_names = ['COMPOUND', 'PEPTIDE', 'PEPTIDE_PROTEIN_MAPPING', 'PRECURSOR',
-                       'PRECURSOR_COMPOUND_MAPPING', 'PRECURSOR_PEPTIDE_MAPPING', 'PROTEIN',
-                       'TRANSITION', 'TRANSITION_PEPTIDE_MAPPING', 'TRANSITION_PRECURSOR_MAPPING',
-                       'FEATURE', 'FEATURE_MS1', 'FEATURE_MS2', 'FEATURE_TRANSITION', 'RUN']
+        table_names = [
+            "COMPOUND",
+            "PEPTIDE",
+            "PEPTIDE_PROTEIN_MAPPING",
+            "PRECURSOR",
+            "PRECURSOR_COMPOUND_MAPPING",
+            "PRECURSOR_PEPTIDE_MAPPING",
+            "PROTEIN",
+            "TRANSITION",
+            "TRANSITION_PEPTIDE_MAPPING",
+            "TRANSITION_PRECURSOR_MAPPING",
+            "FEATURE",
+            "FEATURE_MS1",
+            "FEATURE_MS2",
+            "FEATURE_TRANSITION",
+            "RUN",
+        ]
         return self.sniff_table_names(filename, table_names)
 
 
 class SQmass(SQlite):
     """
     Class describing a Sqmass database
 
@@ -2206,47 +2683,64 @@
     >>> fname = get_test_fname('test.sqmass')
     >>> SQmass().sniff(fname)
     True
     >>> fname = get_test_fname('test.pqp')
     >>> SQmass().sniff(fname)
     False
     """
+
     file_ext = "sqmass"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
             table_names = ["CHROMATOGRAM", "PRECURSOR", "RUN", "SPECTRUM", "DATA", "PRODUCT", "RUN_EXTRA"]
             return self.sniff_table_names(filename, table_names)
         return False
 
 
 class BlibSQlite(SQlite):
-    """Class describing a Proteomics Spectral Library Sqlite database """
-    MetadataElement(name="blib_version", default='1.8', param=MetadataParameter, desc="Blib Version",
-                    readonly=True, visible=True, no_value='1.8')
+    """Class describing a Proteomics Spectral Library Sqlite database"""
+
+    MetadataElement(
+        name="blib_version",
+        default="1.8",
+        param=MetadataParameter,
+        desc="Blib Version",
+        readonly=True,
+        visible=True,
+        no_value="1.8",
+    )
     file_ext = "blib"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
             tables_query = "SELECT majorVersion,minorVersion FROM LibInfo"
             (majorVersion, minorVersion) = c.execute(tables_query).fetchall()[0]
-            dataset.metadata.blib_version = f'{majorVersion}.{minorVersion}'
+            dataset.metadata.blib_version = f"{majorVersion}.{minorVersion}"
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = ['IonMobilityTypes', 'LibInfo', 'Modifications', 'RefSpectra',
-                           'RefSpectraPeakAnnotations', 'RefSpectraPeaks', 'ScoreTypes', 'SpectrumSourceFiles']
+            table_names = [
+                "IonMobilityTypes",
+                "LibInfo",
+                "Modifications",
+                "RefSpectra",
+                "RefSpectraPeakAnnotations",
+                "RefSpectraPeaks",
+                "ScoreTypes",
+                "SpectrumSourceFiles",
+            ]
             return self.sniff_table_names(filename, table_names)
         return False
 
 
 class DlibSQlite(SQlite):
     """
     Class describing a Proteomics Spectral Library Sqlite database
@@ -2257,32 +2751,40 @@
     >>> fname = get_test_fname('test.dlib')
     >>> DlibSQlite().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> DlibSQlite().sniff(fname)
     False
     """
-    MetadataElement(name="dlib_version", default='1.8', param=MetadataParameter, desc="Dlib Version",
-                    readonly=True, visible=True, no_value='1.8')
+
+    MetadataElement(
+        name="dlib_version",
+        default="1.8",
+        param=MetadataParameter,
+        desc="Dlib Version",
+        readonly=True,
+        visible=True,
+        no_value="1.8",
+    )
     file_ext = "dlib"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
             tables_query = "SELECT Value FROM metadata WHERE Key = 'version'"
             version = c.execute(tables_query).fetchall()[0]
-            dataset.metadata.dlib_version = f'{version}'
+            dataset.metadata.dlib_version = f"{version}"
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = ['entries', 'metadata', 'peptidetoprotein']
+            table_names = ["entries", "metadata", "peptidetoprotein"]
             return self.sniff_table_names(filename, table_names)
         return False
 
 
 class ElibSQlite(SQlite):
     """
     Class describing a Proteomics Chromatagram Library Sqlite database
@@ -2293,42 +2795,59 @@
     >>> fname = get_test_fname('test.elib')
     >>> ElibSQlite().sniff(fname)
     True
     >>> fname = get_test_fname('test.dlib')
     >>> ElibSQlite().sniff(fname)
     False
     """
-    MetadataElement(name="version", default='0.1.14', param=MetadataParameter, desc="Elib Version",
-                    readonly=True, visible=True, no_value='0.1.14')
+
+    MetadataElement(
+        name="version",
+        default="0.1.14",
+        param=MetadataParameter,
+        desc="Elib Version",
+        readonly=True,
+        visible=True,
+        no_value="0.1.14",
+    )
     file_ext = "elib"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
             tables_query = "SELECT Value FROM metadata WHERE Key = 'version'"
             version = c.execute(tables_query).fetchall()[0]
-            dataset.metadata.dlib_version = f'{version}'
+            dataset.metadata.dlib_version = f"{version}"
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = ['entries', 'fragmentquants', 'metadata', 'peptidelocalizations', 'peptidequants',
-                           'peptidescores', 'peptidetoprotein', 'proteinscores', 'retentiontimes']
+            table_names = [
+                "entries",
+                "fragmentquants",
+                "metadata",
+                "peptidelocalizations",
+                "peptidequants",
+                "peptidescores",
+                "peptidetoprotein",
+                "proteinscores",
+                "retentiontimes",
+            ]
             if self.sniff_table_names(filename, table_names):
                 try:
                     conn = sqlite.connect(filename)
                     c = conn.cursor()
                     row_query = "SELECT count(*) FROM peptidescores"
                     count = c.execute(row_query).fetchone()[0]
                     return int(count) > 0
                 except Exception as e:
-                    log.warning('%s, sniff Exception: %s', self, e)
+                    log.warning("%s, sniff Exception: %s", self, e)
         return False
 
 
 class IdpDB(SQlite):
     """
     Class describing an IDPicker 3 idpDB (sqlite) database
 
@@ -2336,192 +2855,219 @@
     >>> fname = get_test_fname('test.idpdb')
     >>> IdpDB().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> IdpDB().sniff(fname)
     False
     """
+
     file_ext = "idpdb"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = ["About", "Analysis", "AnalysisParameter", "PeptideSpectrumMatch",
-                           "Spectrum", "SpectrumSource"]
+            table_names = [
+                "About",
+                "Analysis",
+                "AnalysisParameter",
+                "PeptideSpectrumMatch",
+                "Spectrum",
+                "SpectrumSource",
+            ]
             return self.sniff_table_names(filename, table_names)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "IDPickerDB SQLite file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"IDPickerDB SQLite file ({nice_size(dataset.get_size())})"
 
 
 class GAFASQLite(SQlite):
     """Class describing a GAFA SQLite database"""
-    MetadataElement(name='gafa_schema_version', default='0.3.0', param=MetadataParameter, desc='GAFA schema version',
-                    readonly=True, visible=True, no_value='0.3.0')
-    file_ext = 'gafa.sqlite'
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    MetadataElement(
+        name="gafa_schema_version",
+        default="0.3.0",
+        param=MetadataParameter,
+        desc="GAFA schema version",
+        readonly=True,
+        visible=True,
+        no_value="0.3.0",
+    )
+    file_ext = "gafa.sqlite"
+
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
-            version_query = 'SELECT version FROM meta'
+            version_query = "SELECT version FROM meta"
             results = c.execute(version_query).fetchall()
             if len(results) == 0:
-                raise Exception('version not found in meta table')
+                raise Exception("version not found in meta table")
             elif len(results) > 1:
-                raise Exception('Multiple versions found in meta table')
+                raise Exception("Multiple versions found in meta table")
             dataset.metadata.gafa_schema_version = results[0][0]
         except Exception as e:
             log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = frozenset({'gene', 'gene_family', 'gene_family_member', 'meta', 'transcript'})
+            table_names = frozenset({"gene", "gene_family", "gene_family_member", "meta", "transcript"})
             return self.sniff_table_names(filename, table_names)
         return False
 
 
 class NcbiTaxonomySQlite(SQlite):
     """Class describing the NCBI Taxonomy database stored in SQLite as done by rust-ncbitaxonomy"""
-    MetadataElement(name='ncbitaxonomy_schema_version', default='20200501095116', param=MetadataParameter, desc='ncbitaxonomy schema version',
-                    readonly=True, visible=True, no_value='20200501095116')
-    MetadataElement(name="taxon_count", default=[], param=MetadataParameter, desc="Count of taxa in the taxonomy",
-                    readonly=True, visible=True, no_value=[])
 
-    file_ext = 'ncbitaxonomy.sqlite'
+    MetadataElement(
+        name="ncbitaxonomy_schema_version",
+        default="20200501095116",
+        param=MetadataParameter,
+        desc="ncbitaxonomy schema version",
+        readonly=True,
+        visible=True,
+        no_value="20200501095116",
+    )
+    MetadataElement(
+        name="taxon_count",
+        default=[],
+        param=MetadataParameter,
+        desc="Count of taxa in the taxonomy",
+        readonly=True,
+        visible=True,
+        no_value=[],
+    )
+
+    file_ext = "ncbitaxonomy.sqlite"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             conn = sqlite.connect(dataset.file_name)
             c = conn.cursor()
-            version_query = 'SELECT version FROM __diesel_schema_migrations ORDER BY run_on DESC LIMIT 1'
+            version_query = "SELECT version FROM __diesel_schema_migrations ORDER BY run_on DESC LIMIT 1"
             results = c.execute(version_query).fetchall()
             if len(results) == 0:
-                raise Exception('version not found in __diesel_schema_migrations table')
+                raise Exception("version not found in __diesel_schema_migrations table")
             dataset.metadata.ncbitaxonomy_schema_version = results[0][0]
-            taxons_query = 'SELECT count(name) FROM taxonomy'
+            taxons_query = "SELECT count(name) FROM taxonomy"
             results = c.execute(taxons_query).fetchall()
             if len(results) == 0:
-                raise Exception('could not count size of taxonomy table')
+                raise Exception("could not count size of taxonomy table")
             dataset.metadata.taxon_count = results[0][0]
         except Exception as e:
             log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
-            table_names = frozenset({'__diesel_schema_migrations', 'taxonomy'})
+            table_names = frozenset({"__diesel_schema_migrations", "taxonomy"})
             return self.sniff_table_names(filename, table_names)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "NCBI Taxonomy SQLite Database, version {} ({} taxons)".format(
                 getattr(dataset.metadata, "ncbitaxonomy_schema_version", "unknown"),
-                getattr(dataset.metadata, "taxon_count", "unknown")
+                getattr(dataset.metadata, "taxon_count", "unknown"),
             )
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "NCBI Taxonomy SQLite Database, version {} ({} taxons)".format(
                 getattr(dataset.metadata, "ncbitaxonomy_schema_version", "unknown"),
-                getattr(dataset.metadata, "taxon_count", "unknown")
+                getattr(dataset.metadata, "taxon_count", "unknown"),
             )
 
 
+@build_sniff_from_prefix
 class Xlsx(Binary):
     """Class for Excel 2007 (xlsx) files"""
+
     file_ext = "xlsx"
     compressed = True
 
-    def sniff(self, filename):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # Xlsx is compressed in zip format and must not be uncompressed in Galaxy.
-        try:
-            if zipfile.is_zipfile(filename):
-                tempzip = zipfile.ZipFile(filename)
-                if "[Content_Types].xml" in tempzip.namelist() and tempzip.read("[Content_Types].xml").find(b'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet.main+xml') != -1:
-                    return True
-            return False
-        except Exception:
-            return False
+        return file_prefix.compressed_mime_type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
 
 
+@build_sniff_from_prefix
 class ExcelXls(Binary):
     """Class describing an Excel (xls) file"""
+
     file_ext = "excel.xls"
     edam_format = "format_3468"
 
-    def sniff(self, filename):
-        mime_type = subprocess.check_output(['file', '--mime-type', filename])
-        return b"application/vnd.ms-excel" in mime_type
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.mime_type == self.get_mime()
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/vnd.ms-excel'
+        return "application/vnd.ms-excel"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Microsoft Excel XLS file"
             dataset.blurb = data.nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Microsoft Excel XLS file ({data.nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
 class Sra(Binary):
-    """ Sequence Read Archive (SRA) datatype originally from mdshw5/sra-tools-galaxy"""
-    file_ext = 'sra'
+    """Sequence Read Archive (SRA) datatype originally from mdshw5/sra-tools-galaxy"""
+
+    file_ext = "sra"
 
-    def sniff_prefix(self, sniff_prefix):
-        """ The first 8 bytes of any NCBI sra file is 'NCBI.sra', and the file is binary.
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """The first 8 bytes of any NCBI sra file is 'NCBI.sra', and the file is binary.
         For details about the format, see http://www.ncbi.nlm.nih.gov/books/n/helpsra/SRA_Overview_BK/#SRA_Overview_BK.4_SRA_Data_Structure
         """
-        return sniff_prefix.startswith_bytes(b'NCBI.sra')
+        return file_prefix.startswith_bytes(b"NCBI.sra")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = 'Binary sra file'
+            dataset.peek = "Binary sra file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
-            return f'Binary sra file ({nice_size(dataset.get_size())})'
+            return f"Binary sra file ({nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
 class RData(CompressedArchive):
     """Generic R Data file datatype implementation, i.e. files generated with R's save or save.img function
     see https://www.loc.gov/preservation/digital/formats/fdd/fdd000470.shtml
     and https://cran.r-project.org/doc/manuals/r-patched/R-ints.html#Serialization-Formats
@@ -2535,37 +3081,43 @@
     >>> dataset.metadata = Bunch
     >>> dataset.file_name = fname
     >>> dataset.has_data = lambda: True
     >>> RData().set_meta(dataset)
     >>> dataset.metadata.version
     '3'
     """
-    VERSION_2_PREFIX = b'RDX2\nX\n'
-    VERSION_3_PREFIX = b'RDX3\nX\n'
-    file_ext = 'rdata'
-    check_required_metadata = True
-
-    # Tools may in the past have output rdata files that are actually RDS files, and so parsing the version fails,
-    # that is why we have to keep optional="True".
-    MetadataElement(name="version", default=None, desc="serialisation version", param=MetadataParameter, readonly=True, visible=False, optional=True)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    VERSION_2_PREFIX = b"RDX2\nX\n"
+    VERSION_3_PREFIX = b"RDX3\nX\n"
+    file_ext = "rdata"
+
+    MetadataElement(
+        name="version",
+        default=None,
+        desc="serialisation version",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=False,
+    )
+
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         _, fh = compression_utils.get_fileobj_raw(dataset.file_name, "rb")
         try:
             dataset.metadata.version = self._parse_rdata_header(fh)
         except Exception:
             pass
         finally:
             fh.close()
 
-    def sniff_prefix(self, sniff_prefix):
-        return sniff_prefix.startswith_bytes((self.VERSION_2_PREFIX, self.VERSION_3_PREFIX))
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith_bytes((self.VERSION_2_PREFIX, self.VERSION_3_PREFIX))
 
-    def _parse_rdata_header(self, fh):
+    def _parse_rdata_header(self, fh: "FileObjType") -> str:
         header = fh.read(7)
         if header == self.VERSION_2_PREFIX:
             return "2"
         elif header == self.VERSION_3_PREFIX:
             return "3"
         else:
             raise ValueError()
@@ -2596,51 +3148,80 @@
     >>> dataset.metadata.version
     '3'
     >>> dataset.metadata.rversion
     '4.1.1'
     >>> dataset.metadata.minrversion
     '3.5.0'
     """
-    file_ext = 'rds'
-    check_required_metadata = True
 
-    MetadataElement(name="version", default=None, desc="serialisation version", param=MetadataParameter, readonly=True, visible=False, optional=False)
-    MetadataElement(name="rversion", default=None, desc="R version", param=MetadataParameter, readonly=True, visible=False, optional=False)
-    MetadataElement(name="minrversion", default=None, desc="minimum R version", param=MetadataParameter, readonly=False, visible=True, optional=False)
+    file_ext = "rds"
+
+    MetadataElement(
+        name="version",
+        default=None,
+        desc="serialisation version",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=False,
+    )
+    MetadataElement(
+        name="rversion",
+        default=None,
+        desc="R version",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=False,
+    )
+    MetadataElement(
+        name="minrversion",
+        default=None,
+        desc="minimum R version",
+        param=MetadataParameter,
+        readonly=False,
+        visible=True,
+        optional=False,
+    )
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         _, fh = compression_utils.get_fileobj_raw(dataset.file_name, "rb")
         try:
-            _, dataset.metadata.version, dataset.metadata.rversion, dataset.metadata.minrversion = self._parse_rds_header(fh.read(14))
+            (
+                _,
+                dataset.metadata.version,
+                dataset.metadata.rversion,
+                dataset.metadata.minrversion,
+            ) = self._parse_rds_header(fh.read(14))
         except Exception:
             pass
         finally:
             fh.close()
 
-    def sniff_prefix(self, sniff_prefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         try:
-            self._parse_rds_header(sniff_prefix.contents_header_bytes[:14])
+            self._parse_rds_header(file_prefix.contents_header_bytes[:14])
         except Exception:
             return False
         return True
 
     def _parse_rds_header(self, header_bytes):
         """
         get the header info from a rds file
         - starts with b'X\n' or 'A\n'
         - then 3 integers (each 4bytes) encoded with base 10, e.g. b"\x00\x03\x06\x03" for version "3.6.3"
           - the serialization version (2/3)
           - the r version used to generate the file
           - the minimum r version needed to read the file
         """
         header = header_bytes[:2]
-        if header == b'X\n':
+        if header == b"X\n":
             mode = "X"
-        elif header == b'A\n':
+        elif header == b"A\n":
             mode = "A"
         else:
             raise Exception()
         version = header_bytes[2:6]
         rversion = header_bytes[6:10]
         minrversion = header_bytes[10:14]
         version = int("".join(str(_) for _ in version))
@@ -2651,21 +3232,20 @@
         version = ".".join(str(version))
         rversion = ".".join(str(rversion))
         minrversion = ".".join(str(minrversion))
         return mode, version, rversion, minrversion
 
 
 class OxliBinary(Binary):
-
     @staticmethod
-    def _sniff(filename, oxlitype):
+    def _sniff(filename: str, oxlitype: bytes) -> bool:
         try:
-            with open(filename, 'rb') as fileobj:
+            with open(filename, "rb") as fileobj:
                 header = fileobj.read(4)
-                if header == b'OXLI':
+                if header == b"OXLI":
                     fileobj.read(1)  # skip the version number
                     ftype = fileobj.read(1)
                     if binascii.hexlify(ftype) == oxlitype:
                         return True
             return False
         except OSError:
             return False
@@ -2686,17 +3266,18 @@
     >>> fname = get_test_fname('sequence.csfasta')
     >>> OxliCountGraph().sniff(fname)
     False
     >>> fname = get_test_fname("oxli_countgraph.oxlicg")
     >>> OxliCountGraph().sniff(fname)
     True
     """
-    file_ext = 'oxlicg'
 
-    def sniff(self, filename):
+    file_ext = "oxlicg"
+
+    def sniff(self, filename: str) -> bool:
         return OxliBinary._sniff(filename, b"01")
 
 
 class OxliNodeGraph(OxliBinary):
     """
     OxliNodeGraph starts with "OXLI" + one byte version number +
     8-bit binary '2'
@@ -2711,17 +3292,18 @@
     >>> fname = get_test_fname('sequence.csfasta')
     >>> OxliNodeGraph().sniff(fname)
     False
     >>> fname = get_test_fname("oxli_nodegraph.oxling")
     >>> OxliNodeGraph().sniff(fname)
     True
     """
-    file_ext = 'oxling'
 
-    def sniff(self, filename):
+    file_ext = "oxling"
+
+    def sniff(self, filename: str) -> bool:
         return OxliBinary._sniff(filename, b"02")
 
 
 class OxliTagSet(OxliBinary):
     """
     OxliTagSet starts with "OXLI" + one byte version number +
     8-bit binary '3'
@@ -2737,17 +3319,18 @@
     >>> fname = get_test_fname('sequence.csfasta')
     >>> OxliTagSet().sniff(fname)
     False
     >>> fname = get_test_fname("oxli_tagset.oxlits")
     >>> OxliTagSet().sniff(fname)
     True
     """
-    file_ext = 'oxlits'
 
-    def sniff(self, filename):
+    file_ext = "oxlits"
+
+    def sniff(self, filename: str) -> bool:
         return OxliBinary._sniff(filename, b"03")
 
 
 class OxliStopTags(OxliBinary):
     """
     OxliStopTags starts with "OXLI" + one byte version number +
     8-bit binary '4'
@@ -2758,17 +3341,18 @@
     >>> fname = get_test_fname('sequence.csfasta')
     >>> OxliStopTags().sniff(fname)
     False
     >>> fname = get_test_fname("oxli_stoptags.oxlist")
     >>> OxliStopTags().sniff(fname)
     True
     """
-    file_ext = 'oxlist'
 
-    def sniff(self, filename):
+    file_ext = "oxlist"
+
+    def sniff(self, filename: str) -> bool:
         return OxliBinary._sniff(filename, b"04")
 
 
 class OxliSubset(OxliBinary):
     """
     OxliSubset starts with "OXLI" + one byte version number +
     8-bit binary '5'
@@ -2784,17 +3368,18 @@
     >>> fname = get_test_fname('sequence.csfasta')
     >>> OxliSubset().sniff(fname)
     False
     >>> fname = get_test_fname("oxli_subset.oxliss")
     >>> OxliSubset().sniff(fname)
     True
     """
-    file_ext = 'oxliss'
 
-    def sniff(self, filename):
+    file_ext = "oxliss"
+
+    def sniff(self, filename: str) -> bool:
         return OxliBinary._sniff(filename, b"05")
 
 
 class OxliGraphLabels(OxliBinary):
     """
     OxliGraphLabels starts with "OXLI" + one byte version number +
     8-bit binary '6'
@@ -2811,17 +3396,18 @@
     >>> fname = get_test_fname('sequence.csfasta')
     >>> OxliGraphLabels().sniff(fname)
     False
     >>> fname = get_test_fname("oxli_graphlabels.oxligl")
     >>> OxliGraphLabels().sniff(fname)
     True
     """
-    file_ext = 'oxligl'
 
-    def sniff(self, filename):
+    file_ext = "oxligl"
+
+    def sniff(self, filename: str) -> bool:
         return OxliBinary._sniff(filename, b"06")
 
 
 class PostgresqlArchive(CompressedArchive):
     """
     Class describing a Postgresql database packed into a tar archive
 
@@ -2829,43 +3415,52 @@
     >>> fname = get_test_fname('postgresql_fake.tar.bz2')
     >>> PostgresqlArchive().sniff(fname)
     True
     >>> fname = get_test_fname('test.fast5.tar')
     >>> PostgresqlArchive().sniff(fname)
     False
     """
-    MetadataElement(name="version", default=None, param=MetadataParameter, desc="PostgreSQL database version",
-                    readonly=True, visible=True)
+
+    MetadataElement(
+        name="version",
+        default=None,
+        param=MetadataParameter,
+        desc="PostgreSQL database version",
+        readonly=True,
+        visible=True,
+    )
     file_ext = "postgresql"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             if dataset and tarfile.is_tarfile(dataset.file_name):
-                with tarfile.open(dataset.file_name, 'r') as temptar:
-                    pg_version_file = temptar.extractfile('postgresql/db/PG_VERSION')
+                with tarfile.open(dataset.file_name, "r") as temptar:
+                    pg_version_file = temptar.extractfile("postgresql/db/PG_VERSION")
+                    if not pg_version_file:
+                        raise Exception("Error setting PostgresqlArchive metadata: PG_VERSION file not found")
                     dataset.metadata.version = util.unicodify(pg_version_file.read()).strip()
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, util.unicodify(e))
+            log.warning("%s, set_meta Exception: %s", self, util.unicodify(e))
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if filename and tarfile.is_tarfile(filename):
-            with tarfile.open(filename, 'r') as temptar:
-                return 'postgresql/db/PG_VERSION' in temptar.getnames()
+            with tarfile.open(filename, "r") as temptar:
+                return "postgresql/db/PG_VERSION" in temptar.getnames()
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = f"PostgreSQL Archive ({nice_size(dataset.get_size())})"
-            dataset.blurb = "PostgreSQL version %s" % (dataset.metadata.version or 'unknown')
+            dataset.blurb = "PostgreSQL version %s" % (dataset.metadata.version or "unknown")
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"PostgreSQL Archive ({nice_size(dataset.get_size())})"
 
 
 class Fast5Archive(CompressedArchive):
@@ -2873,53 +3468,53 @@
     Class describing a FAST5 archive
 
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('test.fast5.tar')
     >>> Fast5Archive().sniff(fname)
     True
     """
-    MetadataElement(name="fast5_count", default='0', param=MetadataParameter, desc="Read Count",
-                    readonly=True, visible=True)
+
+    MetadataElement(
+        name="fast5_count", default="0", param=MetadataParameter, desc="Read Count", readonly=True, visible=True
+    )
     file_ext = "fast5.tar"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             if dataset and tarfile.is_tarfile(dataset.file_name):
-                with tarfile.open(dataset.file_name, 'r') as temptar:
-                    dataset.metadata.fast5_count = sum(
-                        1 for f in temptar if f.name.endswith('.fast5')
-                    )
+                with tarfile.open(dataset.file_name, "r") as temptar:
+                    dataset.metadata.fast5_count = sum(1 for f in temptar if f.name.endswith(".fast5"))
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         try:
             if filename and tarfile.is_tarfile(filename):
-                with tarfile.open(filename, 'r') as temptar:
+                with tarfile.open(filename, "r") as temptar:
                     for f in temptar:
                         if not f.isfile():
                             continue
-                        if f.name.endswith('.fast5'):
+                        if f.name.endswith(".fast5"):
                             return True
                         else:
                             return False
         except Exception as e:
-            log.warning('%s, sniff Exception: %s', self, e)
+            log.warning("%s, sniff Exception: %s", self, e)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = f"FAST5 Archive ({nice_size(dataset.get_size())})"
-            dataset.blurb = "%s sequences" % (dataset.metadata.fast5_count or 'unknown')
+            dataset.blurb = "%s sequences" % (dataset.metadata.fast5_count or "unknown")
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"FAST5 Archive ({nice_size(dataset.get_size())})"
 
 
 class Fast5ArchiveGz(Fast5Archive):
@@ -2933,17 +3528,18 @@
     >>> fname = get_test_fname('test.fast5.tar.bz2')
     >>> Fast5ArchiveGz().sniff(fname)
     False
     >>> fname = get_test_fname('test.fast5.tar')
     >>> Fast5ArchiveGz().sniff(fname)
     False
     """
+
     file_ext = "fast5.tar.gz"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if not is_gzip(filename):
             return False
         return Fast5Archive.sniff(self, filename)
 
 
 class Fast5ArchiveBz2(Fast5Archive):
     """
@@ -2956,141 +3552,157 @@
     >>> fname = get_test_fname('test.fast5.tar.gz')
     >>> Fast5ArchiveBz2().sniff(fname)
     False
     >>> fname = get_test_fname('test.fast5.tar')
     >>> Fast5ArchiveBz2().sniff(fname)
     False
     """
+
     file_ext = "fast5.tar.bz2"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if not is_bz2(filename):
             return False
         return Fast5Archive.sniff(self, filename)
 
 
 class SearchGuiArchive(CompressedArchive):
-    """Class describing a SearchGUI archive """
-    MetadataElement(name="searchgui_version", default='1.28.0', param=MetadataParameter, desc="SearchGui Version",
-                    readonly=True, visible=True)
-    MetadataElement(name="searchgui_major_version", default='1', param=MetadataParameter, desc="SearchGui Major Version",
-                    readonly=True, visible=True)
+    """Class describing a SearchGUI archive"""
+
+    MetadataElement(
+        name="searchgui_version",
+        default="1.28.0",
+        param=MetadataParameter,
+        desc="SearchGui Version",
+        readonly=True,
+        visible=True,
+    )
+    MetadataElement(
+        name="searchgui_major_version",
+        default="1",
+        param=MetadataParameter,
+        desc="SearchGui Major Version",
+        readonly=True,
+        visible=True,
+    )
     file_ext = "searchgui_archive"
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             if dataset and zipfile.is_zipfile(dataset.file_name):
                 with zipfile.ZipFile(dataset.file_name) as tempzip:
-                    if 'searchgui.properties' in tempzip.namelist():
-                        with tempzip.open('searchgui.properties') as fh:
+                    if "searchgui.properties" in tempzip.namelist():
+                        with tempzip.open("searchgui.properties") as fh:
                             for line in io.TextIOWrapper(fh):
-                                if line.startswith('searchgui.version'):
-                                    version = line.split('=')[1].strip()
+                                if line.startswith("searchgui.version"):
+                                    version = line.split("=")[1].strip()
                                     dataset.metadata.searchgui_version = version
-                                    dataset.metadata.searchgui_major_version = version.split('.')[0]
+                                    dataset.metadata.searchgui_major_version = version.split(".")[0]
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         try:
             if filename and zipfile.is_zipfile(filename):
-                with zipfile.ZipFile(filename, 'r') as tempzip:
-                    is_searchgui = 'searchgui.properties' in tempzip.namelist()
+                with zipfile.ZipFile(filename, "r") as tempzip:
+                    is_searchgui = "searchgui.properties" in tempzip.namelist()
                 return is_searchgui
         except Exception as e:
-            log.warning('%s, sniff Exception: %s', self, e)
+            log.warning("%s, sniff Exception: %s", self, e)
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = "SearchGUI Archive, version %s" % (dataset.metadata.searchgui_version or 'unknown')
+            dataset.peek = "SearchGUI Archive, version %s" % (dataset.metadata.searchgui_version or "unknown")
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
-            return "SearchGUI Archive, version %s" % (dataset.metadata.searchgui_version or 'unknown')
+            return "SearchGUI Archive, version %s" % (dataset.metadata.searchgui_version or "unknown")
 
 
 @build_sniff_from_prefix
 class NetCDF(Binary):
     """Binary data in netCDF format"""
+
     file_ext = "netcdf"
     edam_format = "format_3650"
     edam_data = "data_0943"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary netCDF file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary netCDF file ({nice_size(dataset.get_size())})"
 
-    def sniff_prefix(self, sniff_prefix):
-        return sniff_prefix.startswith_bytes(b'CDF')
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith_bytes(b"CDF")
 
 
 class Dcd(Binary):
     """
     Class describing a dcd file from the CHARMM molecular simulation program
 
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('test_glucose_vacuum.dcd')
     >>> Dcd().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> Dcd().sniff(fname)
     False
     """
+
     file_ext = "dcd"
     edam_data = "data_3842"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self._magic_number = b'CORD'
+        self._magic_number = b"CORD"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # Match the keyword 'CORD' at position 4 or 8 - intsize dependent
         # Not checking for endianness
         try:
-            with open(filename, 'rb') as header:
+            with open(filename, "rb") as header:
                 intsize = 4
                 header.seek(intsize)
                 if header.read(intsize) == self._magic_number:
                     return True
                 else:
                     intsize = 8
                     header.seek(intsize)
                     if header.read(intsize) == self._magic_number:
                         return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary CHARMM/NAMD dcd file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary CHARMM/NAMD dcd file ({nice_size(dataset.get_size())})"
 
 
 class Vel(Binary):
@@ -3101,47 +3713,48 @@
     >>> fname = get_test_fname('test_charmm.vel')
     >>> Vel().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> Vel().sniff(fname)
     False
     """
+
     file_ext = "vel"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self._magic_number = b'VELD'
+        self._magic_number = b"VELD"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # Match the keyword 'VELD' at position 4 or 8 - intsize dependent
         # Not checking for endianness
         try:
-            with open(filename, 'rb') as header:
+            with open(filename, "rb") as header:
                 intsize = 4
                 header.seek(intsize)
                 if header.read(intsize) == self._magic_number:
                     return True
                 else:
                     intsize = 8
                     header.seek(intsize)
                     if header.read(intsize) == self._magic_number:
                         return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary CHARMM velocity file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Binary CHARMM velocity file ({nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
@@ -3153,23 +3766,24 @@
     >>> fname = get_test_fname('diamond.daa')
     >>> DAA().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> DAA().sniff(fname)
     False
     """
+
     file_ext = "daa"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = binascii.unhexlify("6be33e6d47530e3c")
 
-    def sniff_prefix(self, sniff_prefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # The first 8 bytes of any daa file are 0x3c0e53476d3ee36b
-        return sniff_prefix.startswith_bytes(self._magic)
+        return file_prefix.startswith_bytes(self._magic)
 
 
 @build_sniff_from_prefix
 class RMA6(Binary):
     """
     Class describing an RMA6 (MEGAN6 read-match archive) file
 
@@ -3177,22 +3791,23 @@
     >>> fname = get_test_fname('diamond.rma6')
     >>> RMA6().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> RMA6().sniff(fname)
     False
     """
+
     file_ext = "rma6"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = binascii.unhexlify("000003f600000006")
 
-    def sniff_prefix(self, sniff_prefix):
-        return sniff_prefix.startswith_bytes(self._magic)
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith_bytes(self._magic)
 
 
 @build_sniff_from_prefix
 class DMND(Binary):
     """
     Class describing an DMND file
 
@@ -3200,43 +3815,51 @@
     >>> fname = get_test_fname('diamond_db.dmnd')
     >>> DMND().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> DMND().sniff(fname)
     False
     """
+
     file_ext = "dmnd"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = binascii.unhexlify("6d18ee15a4f84a02")
 
-    def sniff_prefix(self, sniff_prefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # The first 8 bytes of any dmnd file are 0x24af8a415ee186d
-        return sniff_prefix.startswith_bytes(self._magic)
+        return file_prefix.startswith_bytes(self._magic)
 
 
 class ICM(Binary):
     """
     Class describing an ICM (interpolated context model) file, used by Glimmer
     """
+
     file_ext = "icm"
     edam_data = "data_0950"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary ICM (interpolated context model) file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, dataset):
-        line = open(dataset).read(100)
-        if '>ver = ' in line and 'len = ' in line and 'depth = ' in line and 'periodicity =' in line and 'nodes = ' in line:
+    def sniff(self, filename: str) -> bool:
+        line = open(filename).read(100)
+        if (
+            ">ver = " in line
+            and "len = " in line
+            and "depth = " in line
+            and "periodicity =" in line
+            and "nodes = " in line
+        ):
             return True
 
         return False
 
 
 @build_sniff_from_prefix
 class Parquet(Binary):
@@ -3247,108 +3870,114 @@
     >>> fname = get_test_fname('example.parquet')
     >>> Parquet().sniff(fname)
     True
     >>> fname = get_test_fname('test.mz5')
     >>> Parquet().sniff(fname)
     False
     """
+
     file_ext = "parquet"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = b"PAR1"  # Defined at https://parquet.apache.org/documentation/latest/
 
-    def sniff_prefix(self, sniff_prefix):
-        return sniff_prefix.startswith_bytes(self._magic)
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith_bytes(self._magic)
 
 
 class BafTar(CompressedArchive):
     """
     Base class for common behavior of tar files of directory-based raw file formats
 
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('brukerbaf.d.tar')
     >>> BafTar().sniff(fname)
     True
     >>> fname = get_test_fname('test.fast5.tar')
     >>> BafTar().sniff(fname)
     False
     """
+
     edam_data = "data_2536"  # mass spectrometry data
     edam_format = "format_3712"  # TODO: add more raw formats to EDAM?
     file_ext = "brukerbaf.d.tar"
 
-    def get_signature_file(self):
+    def get_signature_file(self) -> str:
         return "analysis.baf"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if tarfile.is_tarfile(filename):
             with tarfile.open(filename) as rawtar:
                 return self.get_signature_file() in [os.path.basename(f).lower() for f in rawtar.getnames()]
         return False
 
-    def get_type(self):
+    def get_type(self) -> str:
         return "Bruker BAF directory archive"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = self.get_type()
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"{self.get_type()} ({nice_size(dataset.get_size())})"
 
 
 class YepTar(BafTar):
-    """ A tar'd up .d directory containing Agilent/Bruker YEP format data """
+    """A tar'd up .d directory containing Agilent/Bruker YEP format data"""
+
     file_ext = "agilentbrukeryep.d.tar"
 
-    def get_signature_file(self):
+    def get_signature_file(self) -> str:
         return "analysis.yep"
 
-    def get_type(self):
+    def get_type(self) -> str:
         return "Agilent/Bruker YEP directory archive"
 
 
 class TdfTar(BafTar):
-    """ A tar'd up .d directory containing Bruker TDF format data """
+    """A tar'd up .d directory containing Bruker TDF format data"""
+
     file_ext = "brukertdf.d.tar"
 
-    def get_signature_file(self):
+    def get_signature_file(self) -> str:
         return "analysis.tdf"
 
-    def get_type(self):
+    def get_type(self) -> str:
         return "Bruker TDF directory archive"
 
 
 class MassHunterTar(BafTar):
-    """ A tar'd up .d directory containing Agilent MassHunter format data """
+    """A tar'd up .d directory containing Agilent MassHunter format data"""
+
     file_ext = "agilentmasshunter.d.tar"
 
-    def get_signature_file(self):
+    def get_signature_file(self) -> str:
         return "msscan.bin"
 
-    def get_type(self):
+    def get_type(self) -> str:
         return "Agilent MassHunter directory archive"
 
 
 class MassLynxTar(BafTar):
-    """ A tar'd up .d directory containing Waters MassLynx format data """
+    """A tar'd up .d directory containing Waters MassLynx format data"""
+
     file_ext = "watersmasslynx.raw.tar"
 
-    def get_signature_file(self):
+    def get_signature_file(self) -> str:
         return "_func001.dat"
 
-    def get_type(self):
+    def get_type(self) -> str:
         return "Waters MassLynx RAW directory archive"
 
 
 class WiffTar(BafTar):
     """
     A tar'd up .wiff/.scan pair containing Sciex WIFF format data
 
@@ -3359,53 +3988,83 @@
     >>> fname = get_test_fname('brukerbaf.d.tar')
     >>> WiffTar().sniff(fname)
     False
     >>> fname = get_test_fname('test.fast5.tar')
     >>> WiffTar().sniff(fname)
     False
     """
+
     file_ext = "wiff.tar"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if tarfile.is_tarfile(filename):
             with tarfile.open(filename) as rawtar:
                 return ".wiff" in [os.path.splitext(os.path.basename(f).lower())[1] for f in rawtar.getnames()]
         return False
 
-    def get_type(self):
+    def get_type(self) -> str:
         return "Sciex WIFF/SCAN archive"
 
 
+class Wiff2Tar(BafTar):
+    """
+    A tar'd up .wiff2/.scan pair containing Sciex WIFF format data
+
+    >>> from galaxy.datatypes.sniff import get_test_fname
+    >>> fname = get_test_fname('some.wiff2.tar')
+    >>> Wiff2Tar().sniff(fname)
+    True
+    >>> fname = get_test_fname('brukerbaf.d.tar')
+    >>> Wiff2Tar().sniff(fname)
+    False
+    >>> fname = get_test_fname('test.fast5.tar')
+    >>> Wiff2Tar().sniff(fname)
+    False
+    """
+
+    file_ext = "wiff2.tar"
+
+    def sniff(self, filename: str) -> bool:
+        if tarfile.is_tarfile(filename):
+            with tarfile.open(filename) as rawtar:
+                return ".wiff2" in [os.path.splitext(os.path.basename(f).lower())[1] for f in rawtar.getnames()]
+        return False
+
+    def get_type(self) -> str:
+        return "Sciex WIFF2/SCAN archive"
+
+
 @build_sniff_from_prefix
 class Pretext(Binary):
     """
     PretextMap contact map file
     Try to guess if the file is a Pretext file.
 
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('sample.pretext')
     >>> Pretext().sniff(fname)
     True
     """
+
     file_ext = "pretext"
 
-    def sniff_prefix(self, sniff_prefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # The first 4 bytes of any pretext file is 'pstm', and the rest of the
         # file contains binary data.
-        return sniff_prefix.startswith_bytes(b'pstm')
+        return file_prefix.startswith_bytes(b"pstm")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary pretext file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Binary pretext file (%s)" % (nice_size(dataset.get_size()))
 
 
 class JP2(Binary):
@@ -3416,39 +4075,40 @@
     >>> fname = get_test_fname('test.jp2')
     >>> JP2().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> JP2().sniff(fname)
     False
     """
+
     file_ext = "jp2"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self._magic = binascii.unhexlify("0000000C6A5020200D0A870A")
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # The first 12 bytes of any jp2 file are 0000000C6A5020200D0A870A
         try:
-            header = open(filename, 'rb').read(12)
+            header = open(filename, "rb").read(12)
             if header == self._magic:
                 return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary JPEG 2000 file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Binary JPEG 2000 file (%s)" % (nice_size(dataset.get_size()))
 
 
 class Npz(CompressedArchive):
@@ -3459,51 +4119,52 @@
     >>> fname = get_test_fname('hexrd.images.npz')
     >>> Npz().sniff(fname)
     True
     >>> fname = get_test_fname('interval.interval')
     >>> Npz().sniff(fname)
     False
     """
+
     file_ext = "npz"
     # edam_format = "format_4003"
 
     MetadataElement(name="nfiles", default=0, desc="nfiles", readonly=True, visible=True, no_value=0)
     MetadataElement(name="files", default=[], desc="files", readonly=True, visible=True, no_value=[])
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         try:
             npz = np.load(filename)
             if isinstance(npz, np.lib.npyio.NpzFile):
                 for f in npz.files:
                     if isinstance(npz[f], np.ndarray):
                         return True
         except Exception:
             return False
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         try:
             with np.load(dataset.file_name) as npz:
                 dataset.metadata.nfiles = len(npz.files)
                 dataset.metadata.files = npz.files
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = f"Binary Numpy npz {dataset.metadata.nfiles} files ({nice_size(dataset.get_size())})"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Binary Numpy npz file (%s)" % (nice_size(dataset.get_size()))
 
 
 class HexrdImagesNpz(Npz):
@@ -3514,59 +4175,73 @@
     >>> fname = get_test_fname('hexrd.images.npz')
     >>> HexrdImagesNpz().sniff(fname)
     True
     >>> fname = get_test_fname('eta_ome.npz')
     >>> HexrdImagesNpz().sniff(fname)
     False
     """
+
     file_ext = "hexrd.images.npz"
 
-    MetadataElement(name="panel_id", default='', desc="Detector Panel ID", param=MetadataParameter, readonly=True, visible=True, optional=True, no_value='')
-    MetadataElement(name="shape", default=(), desc="shape", param=metadata.ListParameter, readonly=True, visible=True, no_value=())
+    MetadataElement(
+        name="panel_id",
+        default="",
+        desc="Detector Panel ID",
+        param=MetadataParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="shape", default=(), desc="shape", param=metadata.ListParameter, readonly=True, visible=True, no_value=()
+    )
     MetadataElement(name="nframes", default=0, desc="nframes", readonly=True, visible=True, no_value=0)
     MetadataElement(name="omegas", desc="has omegas", default="False", visible=False)
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
             try:
-                req_files = {'0_row', '0_col', '0_data', 'shape', 'nframes', 'dtype'}
+                req_files = {"0_row", "0_col", "0_data", "shape", "nframes", "dtype"}
                 with np.load(filename) as npz:
                     return set(npz.files) >= req_files
             except Exception as e:
-                log.warning('%s, sniff Exception: %s', self, e)
+                log.warning("%s, sniff Exception: %s", self, e)
                 return False
         return False
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             with np.load(dataset.file_name) as npz:
-                if 'panel_id' in npz.files:
-                    dataset.metadata.panel_id = str(npz['panel_id'])
-                if 'omega' in npz.files:
+                if "panel_id" in npz.files:
+                    dataset.metadata.panel_id = str(npz["panel_id"])
+                if "omega" in npz.files:
                     dataset.metadata.omegas = "True"
-                dataset.metadata.shape = npz['shape'].tolist()
-                dataset.metadata.nframes = npz['nframes'].tolist()
+                dataset.metadata.shape = npz["shape"].tolist()
+                dataset.metadata.nframes = npz["nframes"].tolist()
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            lines = [f"Binary Hexrd Image npz {dataset.metadata.nfiles} files ({nice_size(dataset.get_size())})",
-                     f"Panel: {dataset.metadata.panel_id} Frames: {dataset.metadata.nframes} Shape: {dataset.metadata.shape}"]
-            dataset.peek = '\n'.join(lines)
+            lines = [
+                f"Binary Hexrd Image npz {dataset.metadata.nfiles} files ({nice_size(dataset.get_size())})",
+                f"Panel: {dataset.metadata.panel_id} Frames: {dataset.metadata.nframes} Shape: {dataset.metadata.shape}",
+            ]
+            dataset.peek = "\n".join(lines)
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Binary Numpy npz file (%s)" % (nice_size(dataset.get_size()))
 
 
 class HexrdEtaOmeNpz(Npz):
@@ -3577,55 +4252,55 @@
     >>> fname = get_test_fname('hexrd.eta_ome.npz')
     >>> HexrdEtaOmeNpz().sniff(fname)
     True
     >>> fname = get_test_fname('hexrd.images.npz')
     >>> HexrdEtaOmeNpz().sniff(fname)
     False
     """
+
     file_ext = "hexrd.eta_ome.npz"
 
-    MetadataElement(name="HKLs", default=(), desc="HKLs", param=metadata.ListParameter, readonly=True, visible=True, no_value=())
+    MetadataElement(
+        name="HKLs", default=(), desc="HKLs", param=metadata.ListParameter, readonly=True, visible=True, no_value=()
+    )
     MetadataElement(name="nframes", default=0, desc="nframes", readonly=True, visible=True, no_value=0)
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if super().sniff(filename):
             try:
-                req_files = {'dataStore', 'etas', 'etaEdges', 'iHKLList', 'omegas', 'omeEdges', 'planeData_hkls'}
+                req_files = {"dataStore", "etas", "etaEdges", "iHKLList", "omegas", "omeEdges", "planeData_hkls"}
                 with np.load(filename) as npz:
                     return set(npz.files) >= req_files
             except Exception as e:
-                log.warning('%s, sniff Exception: %s', self, e)
+                log.warning("%s, sniff Exception: %s", self, e)
                 return False
         return False
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             with np.load(dataset.file_name) as npz:
-                dataset.metadata.HKLs = npz['iHKLList'].tolist()
-                dataset.metadata.nframes = len(npz['omegas'])
+                dataset.metadata.HKLs = npz["iHKLList"].tolist()
+                dataset.metadata.nframes = len(npz["omegas"])
         except Exception as e:
-            log.warning('%s, set_meta Exception: %s', self, e)
+            log.warning("%s, set_meta Exception: %s", self, e)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            lines = [f"Binary Hexrd Eta-Ome npz {dataset.metadata.nfiles} files ({nice_size(dataset.get_size())})",
-                     f"Eta-Ome HKLs: {dataset.metadata.HKLs} Frames: {dataset.metadata.nframes}"]
-            dataset.peek = '\n'.join(lines)
+            lines = [
+                f"Binary Hexrd Eta-Ome npz {dataset.metadata.nfiles} files ({nice_size(dataset.get_size())})",
+                f"Eta-Ome HKLs: {dataset.metadata.HKLs} Frames: {dataset.metadata.nframes}",
+            ]
+            dataset.peek = "\n".join(lines)
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Binary Numpy npz file (%s)" % (nice_size(dataset.get_size()))
-
-
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/blast.py` & `galaxy-data-23.0.1/galaxy/datatypes/blast.py`

 * *Files 7% similar despite different names*

```diff
@@ -29,47 +29,61 @@
 """NCBI BLAST datatypes.
 
 Covers the ``blastxml`` format and the BLAST databases.
 """
 import logging
 import os
 from time import sleep
+from typing import (
+    Callable,
+    Dict,
+    List,
+    Optional,
+    TYPE_CHECKING,
+)
 
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.util import smart_str
 from .data import (
     Data,
     get_file_peek,
-    Text
+    Text,
 )
 from .xml import GenericXml
 
+if TYPE_CHECKING:
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class BlastXml(GenericXml):
     """NCBI Blast XML Output data"""
+
     file_ext = "blastxml"
     edam_format = "format_3331"
     edam_data = "data_0857"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
-            dataset.blurb = 'NCBI Blast XML data'
+            dataset.blurb = "NCBI Blast XML data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """Determines whether the file is blastxml
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('megablast_xml_parser_test1.blastxml')
         >>> BlastXml().sniff(fname)
         True
         >>> fname = get_test_fname('tblastn_four_human_vs_rhodopsin.blastxml')
@@ -80,31 +94,32 @@
         False
         """
         handle = file_prefix.string_io()
         line = handle.readline()
         if line.strip() != '<?xml version="1.0"?>':
             return False
         line = handle.readline()
-        if line.strip() not in ['<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "http://www.ncbi.nlm.nih.gov/dtd/NCBI_BlastOutput.dtd">',
-                                '<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "NCBI_BlastOutput.dtd">']:
+        if line.strip() not in [
+            '<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "http://www.ncbi.nlm.nih.gov/dtd/NCBI_BlastOutput.dtd">',
+            '<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "NCBI_BlastOutput.dtd">',
+        ]:
             return False
         line = handle.readline()
-        if line.strip() != '<BlastOutput>':
+        if line.strip() != "<BlastOutput>":
             return False
         return True
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """Merging multiple XML files is non-trivial and must be done in subclasses."""
         if len(split_files) == 1:
             # For one file only, use base class method (move/copy)
             return Text.merge(split_files, output_file)
         if not split_files:
-            raise ValueError("Given no BLAST XML files, %r, to merge into %s"
-                             % (split_files, output_file))
+            raise ValueError(f"Given no BLAST XML files, {split_files!r}, to merge into {output_file}")
         with open(output_file, "w") as out:
             h = None
             old_header = None
             for f in split_files:
                 if not os.path.isfile(f):
                     log.warning(f"BLAST XML file {f} missing, retry in 1s...")
                     sleep(1)
@@ -125,16 +140,18 @@
                         raise ValueError(f"BLAST XML file {f} was empty")
                 if header.strip() != '<?xml version="1.0"?>':
                     out.write(header)  # for diagnosis
                     h.close()
                     raise ValueError(f"{f} is not an XML file!")
                 line = h.readline()
                 header += line
-                if line.strip() not in ['<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "http://www.ncbi.nlm.nih.gov/dtd/NCBI_BlastOutput.dtd">',
-                                        '<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "NCBI_BlastOutput.dtd">']:
+                if line.strip() not in [
+                    '<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "http://www.ncbi.nlm.nih.gov/dtd/NCBI_BlastOutput.dtd">',
+                    '<!DOCTYPE BlastOutput PUBLIC "-//NCBI//NCBI BlastOutput/EN" "NCBI_BlastOutput.dtd">',
+                ]:
                     out.write(header)  # for diagnosis
                     h.close()
                     raise ValueError(f"{f} is not a BLAST XML file!")
                 while True:
                     line = h.readline()
                     if not line:
                         out.write(header)  # for diagnosis
@@ -154,16 +171,18 @@
                     raise ValueError(f"{f} is not a BLAST XML file:\n{header}\n...")
                 if f == split_files[0]:
                     out.write(header)
                     old_header = header
                 elif old_header is not None and old_header[:300] != header[:300]:
                     # Enough to check <BlastOutput_program> and <BlastOutput_version> match
                     h.close()
-                    raise ValueError("BLAST XML headers don't match for %s and %s - have:\n%s\n...\n\nAnd:\n%s\n...\n"
-                                     % (split_files[0], f, old_header[:300], header[:300]))
+                    raise ValueError(
+                        "BLAST XML headers don't match for %s and %s - have:\n%s\n...\n\nAnd:\n%s\n...\n"
+                        % (split_files[0], f, old_header[:300], header[:300])
+                    )
                 else:
                     out.write("    <Iteration>\n")
                 for line in h:
                     if "</BlastOutput_iterations>" in line:
                         break
                     # TODO - Increment <Iteration_iter-num> and if required automatic query names
                     # like <Iteration_query-ID>Query_3</Iteration_query-ID> to be increasing?
@@ -172,239 +191,303 @@
             out.write("  </BlastOutput_iterations>\n")
             out.write("</BlastOutput>\n")
 
 
 class _BlastDb(Data):
     """Base class for BLAST database datatype."""
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text."""
         if not dataset.dataset.purged:
             dataset.peek = "BLAST database (multiple files)"
             dataset.blurb = "BLAST database (multiple files)"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "BLAST database (multiple files)"
 
-    def display_data(self, trans, data, preview=False, filename=None,
-                     to_ext=None, size=None, offset=None, **kwd):
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        offset: Optional[int] = None,
+        ck_size: Optional[int] = None,
+        **kwd,
+    ):
         """
         If preview is `True` allows us to format the data shown in the central pane via the "eye" icon.
         If preview is `False` triggers download.
         """
         headers = kwd.get("headers", {})
         if not preview:
-            return super().display_data(trans,
-                                        data=data,
-                                        preview=preview,
-                                        filename=filename,
-                                        to_ext=to_ext,
-                                        size=size,
-                                        offset=offset,
-                                        **kwd)
+            return super().display_data(
+                trans,
+                dataset=dataset,
+                preview=preview,
+                filename=filename,
+                to_ext=to_ext,
+                offset=offset,
+                ck_size=ck_size,
+                **kwd,
+            )
         if self.file_ext == "blastdbn":
             title = "This is a nucleotide BLAST database"
         elif self.file_ext == "blastdbp":
             title = "This is a protein BLAST database"
         elif self.file_ext == "blastdbd":
             title = "This is a domain BLAST database"
         else:
             # Error?
             title = "This is a BLAST database."
         msg = ""
         try:
             # Try to use any text recorded in the dummy index file:
-            with open(data.file_name, encoding='utf-8') as handle:
+            with open(dataset.file_name, encoding="utf-8") as handle:
                 msg = handle.read().strip()
         except Exception:
             pass
         if not msg:
             msg = title
         # Galaxy assumes HTML for the display of composite datatypes,
         return smart_str(f"<html><head><title>{title}</title></head><body><pre>{msg}</pre></body></html>"), headers
 
-    def merge(split_files, output_file):
+    @staticmethod
+    def merge(split_files: List[str], output_file: str) -> None:
         """Merge BLAST databases (not implemented for now)."""
         raise NotImplementedError("Merging BLAST databases is non-trivial (do this via makeblastdb?)")
 
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    @classmethod
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """Split a BLAST database (not implemented for now)."""
         if split_params is None:
             return None
         raise NotImplementedError("Can't split BLAST databases")
 
 
 class BlastNucDb(_BlastDb):
     """Class for nucleotide BLAST database files."""
-    file_ext = 'blastdbn'
-    composite_type = 'basic'
+
+    file_ext = "blastdbn"
+    composite_type = "basic"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('blastdb.nhr', is_binary=True)  # sequence headers
-        self.add_composite_file('blastdb.nin', is_binary=True)  # index file
-        self.add_composite_file('blastdb.nsq', is_binary=True)  # nucleotide sequences
-        self.add_composite_file('blastdb.nal', is_binary=False, optional=True)  # alias ( -gi_mask option of makeblastdb)
-        self.add_composite_file('blastdb.nhd', is_binary=True, optional=True)  # sorted sequence hash values ( -hash_index option of makeblastdb)
-        self.add_composite_file('blastdb.nhi', is_binary=True, optional=True)  # index of sequence hash values ( -hash_index option of makeblastdb)
-        self.add_composite_file('blastdb.nnd', is_binary=True, optional=True)  # sorted GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
-        self.add_composite_file('blastdb.nni', is_binary=True, optional=True)  # index of GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
-        self.add_composite_file('blastdb.nog', is_binary=True, optional=True)  # OID->GI lookup file ( -hash_index or -parse_seqids option of makeblastdb)
-        self.add_composite_file('blastdb.nsd', is_binary=True, optional=True)  # sorted sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
-        self.add_composite_file('blastdb.nsi', is_binary=True, optional=True)  # index of sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
-#        self.add_composite_file('blastdb.00.idx', is_binary=True, optional=True)  # first volume of the MegaBLAST index generated by makembindex
-# The previous line should be repeated for each index volume, with filename extensions like '.01.idx', '.02.idx', etc.
-        self.add_composite_file('blastdb.shd', is_binary=True, optional=True)  # MegaBLAST index superheader (-old_style_index false option of makembindex)
+        self.add_composite_file("blastdb.nhr", is_binary=True)  # sequence headers
+        self.add_composite_file("blastdb.nin", is_binary=True)  # index file
+        self.add_composite_file("blastdb.nsq", is_binary=True)  # nucleotide sequences
+        self.add_composite_file(
+            "blastdb.nal", is_binary=False, optional=True
+        )  # alias ( -gi_mask option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nhd", is_binary=True, optional=True
+        )  # sorted sequence hash values ( -hash_index option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nhi", is_binary=True, optional=True
+        )  # index of sequence hash values ( -hash_index option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nnd", is_binary=True, optional=True
+        )  # sorted GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
+        self.add_composite_file(
+            "blastdb.nni", is_binary=True, optional=True
+        )  # index of GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
+        self.add_composite_file(
+            "blastdb.nog", is_binary=True, optional=True
+        )  # OID->GI lookup file ( -hash_index or -parse_seqids option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nsd", is_binary=True, optional=True
+        )  # sorted sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nsi", is_binary=True, optional=True
+        )  # index of sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
+        #        self.add_composite_file('blastdb.00.idx', is_binary=True, optional=True)  # first volume of the MegaBLAST index generated by makembindex
+        # The previous line should be repeated for each index volume, with filename extensions like '.01.idx', '.02.idx', etc.
+        self.add_composite_file(
+            "blastdb.shd", is_binary=True, optional=True
+        )  # MegaBLAST index superheader (-old_style_index false option of makembindex)
+
+
 #        self.add_composite_file('blastdb.naa', is_binary=True, optional=True)  # index of a WriteDB column for e.g. mask data
 #        self.add_composite_file('blastdb.nab', is_binary=True, optional=True)  # data of a WriteDB column
 #        self.add_composite_file('blastdb.nac', is_binary=True, optional=True)  # multiple byte order for a WriteDB column
 # The previous 3 lines should be repeated for each WriteDB column, with filename extensions like ('.nba', '.nbb', '.nbc'), ('.nca', '.ncb', '.ncc'), etc.
 
 
 class BlastProtDb(_BlastDb):
     """Class for protein BLAST database files."""
-    file_ext = 'blastdbp'
-    composite_type = 'basic'
+
+    file_ext = "blastdbp"
+    composite_type = "basic"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-# Component file comments are as in BlastNucDb except where noted
-        self.add_composite_file('blastdb.phr', is_binary=True)
-        self.add_composite_file('blastdb.pin', is_binary=True)
-        self.add_composite_file('blastdb.psq', is_binary=True)  # protein sequences
-        self.add_composite_file('blastdb.phd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.phi', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.pnd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.pni', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.pog', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psi', is_binary=True, optional=True)
+        # Component file comments are as in BlastNucDb except where noted
+        self.add_composite_file("blastdb.phr", is_binary=True)
+        self.add_composite_file("blastdb.pin", is_binary=True)
+        self.add_composite_file("blastdb.psq", is_binary=True)  # protein sequences
+        self.add_composite_file("blastdb.phd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.phi", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.pnd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.pni", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.pog", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psi", is_binary=True, optional=True)
+
+
 #        self.add_composite_file('blastdb.paa', is_binary=True, optional=True)
 #        self.add_composite_file('blastdb.pab', is_binary=True, optional=True)
 #        self.add_composite_file('blastdb.pac', is_binary=True, optional=True)
 # The last 3 lines should be repeated for each WriteDB column, with filename extensions like ('.pba', '.pbb', '.pbc'), ('.pca', '.pcb', '.pcc'), etc.
 
 
 class BlastDomainDb(_BlastDb):
     """Class for domain BLAST database files."""
-    file_ext = 'blastdbd'
-    composite_type = 'basic'
+
+    file_ext = "blastdbd"
+    composite_type = "basic"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('blastdb.phr', is_binary=True)
-        self.add_composite_file('blastdb.pin', is_binary=True)
-        self.add_composite_file('blastdb.psq', is_binary=True)
-        self.add_composite_file('blastdb.freq', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.loo', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psi', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.rps', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.aux', is_binary=True, optional=True)
+        self.add_composite_file("blastdb.phr", is_binary=True)
+        self.add_composite_file("blastdb.pin", is_binary=True)
+        self.add_composite_file("blastdb.psq", is_binary=True)
+        self.add_composite_file("blastdb.freq", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.loo", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psi", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.rps", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.aux", is_binary=True, optional=True)
 
 
 class LastDb(Data):
     """Class for LAST database files."""
-    file_ext = 'lastdb'
-    composite_type = 'basic'
 
-    def set_peek(self, dataset):
+    file_ext = "lastdb"
+    composite_type = "basic"
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text."""
         if not dataset.dataset.purged:
             dataset.peek = "LAST database (multiple files)"
             dataset.blurb = "LAST database (multiple files)"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "LAST database (multiple files)"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('lastdb.bck', is_binary=True)
-        self.add_composite_file('lastdb.des', description="Description file", is_binary=False)
-        self.add_composite_file('lastdb.prj', description="Project resume file", is_binary=False)
-        self.add_composite_file('lastdb.sds', is_binary=True)
-        self.add_composite_file('lastdb.ssp', is_binary=True)
-        self.add_composite_file('lastdb.suf', is_binary=True)
-        self.add_composite_file('lastdb.tis', is_binary=True)
+        self.add_composite_file("lastdb.bck", is_binary=True)
+        self.add_composite_file("lastdb.des", description="Description file", is_binary=False)
+        self.add_composite_file("lastdb.prj", description="Project resume file", is_binary=False)
+        self.add_composite_file("lastdb.sds", is_binary=True)
+        self.add_composite_file("lastdb.ssp", is_binary=True)
+        self.add_composite_file("lastdb.suf", is_binary=True)
+        self.add_composite_file("lastdb.tis", is_binary=True)
 
 
 class BlastNucDb5(_BlastDb):
     """Class for nucleotide BLAST database files."""
-    file_ext = 'blastdbn5'
-    composite_type = 'basic'
+
+    file_ext = "blastdbn5"
+    composite_type = "basic"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('blastdb.nhr', is_binary=True)  # sequence headers
-        self.add_composite_file('blastdb.nin', is_binary=True)  # index file
-        self.add_composite_file('blastdb.nsq', is_binary=True)  # nucleotide sequences
-        self.add_composite_file('blastdb.nal', is_binary=False, optional=True)  # alias ( -gi_mask option of makeblastdb)
-        self.add_composite_file('blastdb.nhd', is_binary=True, optional=True)  # sorted sequence hash values ( -hash_index option of makeblastdb)
-        self.add_composite_file('blastdb.nhi', is_binary=True, optional=True)  # index of sequence hash values ( -hash_index option of makeblastdb)
-        self.add_composite_file('blastdb.nnd', is_binary=True, optional=True)  # sorted GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
-        self.add_composite_file('blastdb.nni', is_binary=True, optional=True)  # index of GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
-        self.add_composite_file('blastdb.nog', is_binary=True, optional=True)  # OID->GI lookup file ( -hash_index or -parse_seqids option of makeblastdb)
-        self.add_composite_file('blastdb.nsd', is_binary=True, optional=True)  # sorted sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
-        self.add_composite_file('blastdb.nsi', is_binary=True, optional=True)  # index of sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
-#        self.add_composite_file('blastdb.00.idx', is_binary=True, optional=True)  # first volume of the MegaBLAST index generated by makembindex
-# The previous line should be repeated for each index volume, with filename extensions like '.01.idx', '.02.idx', etc.
-        self.add_composite_file('blastdb.shd', is_binary=True, optional=True)  # MegaBLAST index superheader (-old_style_index false option of makembindex)
+        self.add_composite_file("blastdb.nhr", is_binary=True)  # sequence headers
+        self.add_composite_file("blastdb.nin", is_binary=True)  # index file
+        self.add_composite_file("blastdb.nsq", is_binary=True)  # nucleotide sequences
+        self.add_composite_file(
+            "blastdb.nal", is_binary=False, optional=True
+        )  # alias ( -gi_mask option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nhd", is_binary=True, optional=True
+        )  # sorted sequence hash values ( -hash_index option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nhi", is_binary=True, optional=True
+        )  # index of sequence hash values ( -hash_index option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nnd", is_binary=True, optional=True
+        )  # sorted GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
+        self.add_composite_file(
+            "blastdb.nni", is_binary=True, optional=True
+        )  # index of GI values ( -parse_seqids option of makeblastdb and gi present in the description lines)
+        self.add_composite_file(
+            "blastdb.nog", is_binary=True, optional=True
+        )  # OID->GI lookup file ( -hash_index or -parse_seqids option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nsd", is_binary=True, optional=True
+        )  # sorted sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
+        self.add_composite_file(
+            "blastdb.nsi", is_binary=True, optional=True
+        )  # index of sequence accession values ( -hash_index or -parse_seqids option of makeblastdb)
+        #        self.add_composite_file('blastdb.00.idx', is_binary=True, optional=True)  # first volume of the MegaBLAST index generated by makembindex
+        # The previous line should be repeated for each index volume, with filename extensions like '.01.idx', '.02.idx', etc.
+        self.add_composite_file(
+            "blastdb.shd", is_binary=True, optional=True
+        )  # MegaBLAST index superheader (-old_style_index false option of makembindex)
+
+
 #        self.add_composite_file('blastdb.naa', is_binary=True, optional=True)  # index of a WriteDB column for e.g. mask data
 #        self.add_composite_file('blastdb.nab', is_binary=True, optional=True)  # data of a WriteDB column
 #        self.add_composite_file('blastdb.nac', is_binary=True, optional=True)  # multiple byte order for a WriteDB column
 # The previous 3 lines should be repeated for each WriteDB column, with filename extensions like ('.nba', '.nbb', '.nbc'), ('.nca', '.ncb', '.ncc'), etc.
 
 
 class BlastProtDb5(_BlastDb):
     """Class for protein BLAST database files."""
-    file_ext = 'blastdbp5'
-    composite_type = 'basic'
+
+    file_ext = "blastdbp5"
+    composite_type = "basic"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-# Component file comments are as in BlastNucDb except where noted
-        self.add_composite_file('blastdb.phr', is_binary=True)
-        self.add_composite_file('blastdb.pin', is_binary=True)
-        self.add_composite_file('blastdb.psq', is_binary=True)  # protein sequences
-        self.add_composite_file('blastdb.phd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.phi', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.pnd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.pni', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.pog', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psi', is_binary=True, optional=True)
+        # Component file comments are as in BlastNucDb except where noted
+        self.add_composite_file("blastdb.phr", is_binary=True)
+        self.add_composite_file("blastdb.pin", is_binary=True)
+        self.add_composite_file("blastdb.psq", is_binary=True)  # protein sequences
+        self.add_composite_file("blastdb.phd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.phi", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.pnd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.pni", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.pog", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psi", is_binary=True, optional=True)
+
+
 #        self.add_composite_file('blastdb.paa', is_binary=True, optional=True)
 #        self.add_composite_file('blastdb.pab', is_binary=True, optional=True)
 #        self.add_composite_file('blastdb.pac', is_binary=True, optional=True)
 # The last 3 lines should be repeated for each WriteDB column, with filename extensions like ('.pba', '.pbb', '.pbc'), ('.pca', '.pcb', '.pcc'), etc.
 
 
 class BlastDomainDb5(_BlastDb):
     """Class for domain BLAST database files."""
-    file_ext = 'blastdbd5'
-    composite_type = 'basic'
+
+    file_ext = "blastdbd5"
+    composite_type = "basic"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('blastdb.phr', is_binary=True)
-        self.add_composite_file('blastdb.pin', is_binary=True)
-        self.add_composite_file('blastdb.psq', is_binary=True)
-        self.add_composite_file('blastdb.freq', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.loo', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psd', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.psi', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.rps', is_binary=True, optional=True)
-        self.add_composite_file('blastdb.aux', is_binary=True, optional=True)
+        self.add_composite_file("blastdb.phr", is_binary=True)
+        self.add_composite_file("blastdb.pin", is_binary=True)
+        self.add_composite_file("blastdb.psq", is_binary=True)
+        self.add_composite_file("blastdb.freq", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.loo", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psd", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.psi", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.rps", is_binary=True, optional=True)
+        self.add_composite_file("blastdb.aux", is_binary=True, optional=True)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/constructive_solid_geometry.py` & `galaxy-data-23.0.1/galaxy/datatypes/constructive_solid_geometry.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,126 +1,143 @@
 # TODO: revisit ignoring type and write some tests for this, the multi-inheritance in this
 # this file is challenging, it should be broken into true mixins.
 """
 Constructive Solid Geometry file formats.
 """
 
 import abc
-from typing import List
+from typing import (
+    List,
+    Optional,
+    Tuple,
+    TYPE_CHECKING,
+)
 
 from galaxy import util
 from galaxy.datatypes import data
 from galaxy.datatypes.binary import Binary
-from galaxy.datatypes.data import get_file_peek
-from galaxy.datatypes.data import nice_size
+from galaxy.datatypes.data import (
+    get_file_peek,
+    nice_size,
+)
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.datatypes.tabular import Tabular
 
+if TYPE_CHECKING:
+    from io import TextIOBase
+
+    from galaxy.model import DatasetInstance
+
 MAX_HEADER_LINES = 500
 MAX_LINE_LEN = 2000
-COLOR_OPTS = ['COLOR_SCALARS', 'red', 'green', 'blue']
+COLOR_OPTS = ["COLOR_SCALARS", "red", "green", "blue"]
 
 
 @build_sniff_from_prefix
 class Ply:
     """
     The PLY format describes an object as a collection of vertices,
     faces and other elements, along with properties such as color and
     normal direction that can be attached to these elements.  A PLY
     file contains the description of exactly one object.
     """
-    subtype = ''
+
+    subtype = ""
     # Add metadata elements.
-    MetadataElement(name="file_format", default=None, desc="File format",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="vertex", default=None, desc="Vertex",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="face", default=None, desc="Face",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="other_elements", default=[], desc="Other elements",
-                    readonly=True, optional=True, visible=True, no_value=[])
+    MetadataElement(name="file_format", default=None, desc="File format", readonly=True, optional=True, visible=True)
+    MetadataElement(name="vertex", default=None, desc="Vertex", readonly=True, optional=True, visible=True)
+    MetadataElement(name="face", default=None, desc="Face", readonly=True, optional=True, visible=True)
+    MetadataElement(
+        name="other_elements",
+        default=[],
+        desc="Other elements",
+        readonly=True,
+        optional=True,
+        visible=True,
+        no_value=[],
+    )
 
     @abc.abstractmethod
     def __init__(self, **kwd):
         raise NotImplementedError
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         The structure of a typical PLY file:
         Header, Vertex List, Face List, (lists of other elements)
         """
-        if not self._is_ply_header(file_prefix.text_io(errors='ignore'), self.subtype):
+        if not self._is_ply_header(file_prefix.text_io(errors="ignore"), self.subtype):
             return False
         return True
 
-    def _is_ply_header(self, fh, subtype):
+    def _is_ply_header(self, fh: "TextIOBase", subtype: str) -> bool:
         """
         The header is a series of carriage-return terminated lines of
         text that describe the remainder of the file.
         """
-        valid_header_items = ['comment', 'obj_info', 'element', 'property']
+        valid_header_items = ["comment", "obj_info", "element", "property"]
         # Line 1: ply
         line = get_next_line(fh)
-        if line != 'ply':
+        if line != "ply":
             return False
         # Line 2: format ascii 1.0
         line = get_next_line(fh)
         if line.find(subtype) < 0:
             return False
         stop_index = 0
         for line in util.iter_start_of_line(fh, MAX_LINE_LEN):
             line = line.strip()
             stop_index += 1
-            if line == 'end_header':
+            if line == "end_header":
                 return True
             items = line.split()
             if items[0] not in valid_header_items:
                 return False
             if stop_index > MAX_HEADER_LINES:
                 # If this is a PLY file, there must be an unusually
                 # large number of comments.
                 break
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if dataset.has_data():
-            with open(dataset.file_name, errors='ignore') as fh:
+            with open(dataset.file_name, errors="ignore") as fh:
                 for line in fh:
                     line = line.strip()
                     if not line:
                         continue
-                    if line.startswith('format'):
+                    if line.startswith("format"):
                         items = line.split()
                         dataset.metadata.file_format = items[1]
-                    elif line == 'end_header':
+                    elif line == "end_header":
                         # Metadata is complete.
                         break
-                    elif line.startswith('element'):
+                    elif line.startswith("element"):
                         items = line.split()
-                        if items[1] == 'face':
+                        if items[1] == "face":
                             dataset.metadata.face = int(items[2])
-                        elif items[1] == 'vertex':
+                        elif items[1] == "vertex":
                             dataset.metadata.vertex = int(items[2])
                         else:
                             element_tuple = (items[1], int(items[2]))
                             dataset.metadata.other_elements.append(element_tuple)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = f"Faces: {str(dataset.metadata.face)}, Vertices: {str(dataset.metadata.vertex)}"
         else:
-            dataset.peek = 'File does not exist'
-            dataset.blurb = 'File purged from disc'
+            dataset.peek = "File does not exist"
+            dataset.blurb = "File purged from disc"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Ply file ({nice_size(dataset.get_size())})"
 
 
 class PlyAscii(Ply, data.Text):  # type: ignore[misc]
@@ -129,24 +146,25 @@
     >>> fname = get_test_fname('test.plyascii')
     >>> PlyAscii().sniff(fname)
     True
     >>> fname = get_test_fname('test.vtkascii')
     >>> PlyAscii().sniff(fname)
     False
     """
+
     file_ext = "plyascii"
-    subtype = 'ascii'
+    subtype = "ascii"
 
     def __init__(self, **kwd):
         data.Text.__init__(self, **kwd)
 
 
 class PlyBinary(Ply, Binary):  # type: ignore[misc]
     file_ext = "plybinary"
-    subtype = 'binary'
+    subtype = "binary"
 
     def __init__(self, **kwd):
         Binary.__init__(self, **kwd)
 
 
 @build_sniff_from_prefix
 class Vtk:
@@ -169,88 +187,88 @@
 
     Binary data must be placed into the file immediately after the newline
     ('\\n') character from the previous ASCII keyword and parameter sequence.
 
     TODO: only legacy formats are currently supported and support for XML formats
     should be added.
     """
-    subtype = ''
+    subtype = ""
     # Add metadata elements.
-    MetadataElement(name="vtk_version", default=None, desc="Vtk version",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="file_format", default=None, desc="File format",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="dataset_type", default=None, desc="Dataset type",
-                    readonly=True, optional=True, visible=True)
+    MetadataElement(name="vtk_version", default=None, desc="Vtk version", readonly=True, optional=True, visible=True)
+    MetadataElement(name="file_format", default=None, desc="File format", readonly=True, optional=True, visible=True)
+    MetadataElement(name="dataset_type", default=None, desc="Dataset type", readonly=True, optional=True, visible=True)
 
     # STRUCTURED_GRID data_type.
-    MetadataElement(name="dimensions", default=[], desc="Dimensions",
-                    readonly=True, optional=True, visible=True, no_value=[])
-    MetadataElement(name="origin", default=[], desc="Origin",
-                    readonly=True, optional=True, visible=True, no_value=[])
-    MetadataElement(name="spacing", default=[], desc="Spacing",
-                    readonly=True, optional=True, visible=True, no_value=[])
+    MetadataElement(
+        name="dimensions", default=[], desc="Dimensions", readonly=True, optional=True, visible=True, no_value=[]
+    )
+    MetadataElement(name="origin", default=[], desc="Origin", readonly=True, optional=True, visible=True, no_value=[])
+    MetadataElement(name="spacing", default=[], desc="Spacing", readonly=True, optional=True, visible=True, no_value=[])
 
     # POLYDATA data_type (Points element is also a component of UNSTRUCTURED_GRID..
-    MetadataElement(name="points", default=None, desc="Points",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="vertices", default=None, desc="Vertices",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="lines", default=None, desc="Lines",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="polygons", default=None, desc="Polygons",
-                    readonly=True, optional=True, visible=True)
-    MetadataElement(name="triangle_strips", default=None, desc="Triangle strips",
-                    readonly=True, optional=True, visible=True)
+    MetadataElement(name="points", default=None, desc="Points", readonly=True, optional=True, visible=True)
+    MetadataElement(name="vertices", default=None, desc="Vertices", readonly=True, optional=True, visible=True)
+    MetadataElement(name="lines", default=None, desc="Lines", readonly=True, optional=True, visible=True)
+    MetadataElement(name="polygons", default=None, desc="Polygons", readonly=True, optional=True, visible=True)
+    MetadataElement(
+        name="triangle_strips", default=None, desc="Triangle strips", readonly=True, optional=True, visible=True
+    )
 
     # UNSTRUCTURED_GRID data_type.
-    MetadataElement(name="cells", default=None, desc="Cells",
-                    readonly=True, optional=True, visible=True)
+    MetadataElement(name="cells", default=None, desc="Cells", readonly=True, optional=True, visible=True)
 
     # Additional elements not categorized by data_type.
-    MetadataElement(name="field_names", default=[], desc="Field names",
-                    readonly=True, optional=True, visible=True, no_value=[])
+    MetadataElement(
+        name="field_names", default=[], desc="Field names", readonly=True, optional=True, visible=True, no_value=[]
+    )
     # The keys in the field_components map to the list of field_names in the above element
     # which ensures order for select list options that are built from it.
-    MetadataElement(name="field_components", default={}, desc="Field names and components",
-                    readonly=True, optional=True, visible=True, no_value={})
+    MetadataElement(
+        name="field_components",
+        default={},
+        desc="Field names and components",
+        readonly=True,
+        optional=True,
+        visible=True,
+        no_value={},
+    )
 
     @abc.abstractmethod
     def __init__(self, **kwd):
         raise NotImplementedError
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         VTK files can be either ASCII or binary, with two different
         styles of file formats: legacy or XML.  We'll assume if the
         file contains a valid VTK header, then it is a valid VTK file.
         """
-        if self._is_vtk_header(file_prefix.text_io(errors='ignore'), self.subtype):
+        if self._is_vtk_header(file_prefix.text_io(errors="ignore"), self.subtype):
             return True
         return False
 
-    def _is_vtk_header(self, fh, subtype):
+    def _is_vtk_header(self, fh: "TextIOBase", subtype: str) -> bool:
         """
         The Header section consists of at least 4, but possibly
         5 lines.  This is tricky because sometimes the 4th line
         is blank (in which case the 5th line consists of the
         data_kind) or the 4th line consists of the data_kind (in
         which case the 5th line is blank).
         """
-        data_kinds = ['STRUCTURED_GRID', 'POLYDATA', 'UNSTRUCTURED_GRID', 'STRUCTURED_POINTS', 'RECTILINEAR_GRID']
+        data_kinds = ["STRUCTURED_GRID", "POLYDATA", "UNSTRUCTURED_GRID", "STRUCTURED_POINTS", "RECTILINEAR_GRID"]
 
         def check_data_kind(line):
             for data_kind in data_kinds:
                 if line.find(data_kind) >= 0:
                     return True
             return False
 
         # Line 1: vtk DataFile Version 3.0
         line = get_next_line(fh)
-        if line.find('vtk') < 0:
+        if line.find("vtk") < 0:
             return False
         # Line 2: can be anything - skip it
         line = get_next_line(fh)
         # Line 3: ASCII or BINARY
         line = get_next_line(fh)
         if line.find(subtype) < 0:
             return False
@@ -260,30 +278,30 @@
             return check_data_kind(line)
         # line 5:
         line = get_next_line(fh)
         if line:
             return check_data_kind(line)
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if dataset.has_data():
             dataset.metadata.field_names = []
             dataset.metadata.field_components = {}
             dataset_type = None
             field_components = {}
             dataset_structure_complete = False
             processing_field_section = False
-            with open(dataset.file_name, errors='ignore') as fh:
+            with open(dataset.file_name, errors="ignore") as fh:
                 for i, line in enumerate(fh):
                     line = line.strip()
                     if not line:
                         continue
                     if i < 3:
                         dataset = self.set_initial_metadata(i, line, dataset)
-                    elif dataset.metadata.file_format == 'ASCII' or not util.is_binary(line):
+                    elif dataset.metadata.file_format == "ASCII" or not util.is_binary(line):
                         if dataset_structure_complete:
                             """
                             The final part of legacy VTK files describes the dataset attributes.
                             This part begins with the keywords POINT_DATA or CELL_DATA, followed
                             by an integer number specifying the number of points or cells,
                             respectively. Other keyword/data combinations then define the actual
                             dataset attribute values (i.e., scalars, vectors, tensors, normals,
@@ -296,15 +314,15 @@
                             data. As a result, more than one attribute data of the same type can be
                             included in a file.  For example, two different scalar fields defined
                             on the dataset points, pressure and temperature, can be contained in
                             the same file.  If the appropriate dataName is not specified in the VTK
                             reader, then the first data of that type is extracted from the file.
                             """
                             items = line.split()
-                            if items[0] == 'SCALARS':
+                            if items[0] == "SCALARS":
                                 # Example: SCALARS surface_field double 3
                                 # Scalar definition includes specification of a lookup table. The
                                 # definition of a lookup table is optional. If not specified, the
                                 # default VTK table will be used, and tableName should be
                                 # "default". Also note that the numComp variable is optional.  By
                                 # default the number of components is equal to one.  The parameter
                                 # numComp must range between (1,4) inclusive; in versions of VTK
@@ -313,15 +331,15 @@
                                 dataset.metadata.field_names.append(field_name)
                                 try:
                                     num_components = int(items[-1])
                                 except Exception:
                                     num_components = 1
                                 field_component_indexes = [str(i) for i in range(num_components)]
                                 field_components[field_name] = field_component_indexes
-                            elif items[0] == 'FIELD':
+                            elif items[0] == "FIELD":
                                 # The dataset consists of CELL_DATA.
                                 # FIELD FieldData 2
                                 processing_field_section = True
                                 num_fields = int(items[-1])
                                 fields_processed: List[str] = []
                             elif processing_field_section:
                                 if len(fields_processed) == num_fields:
@@ -336,122 +354,124 @@
                                         # Example: surface_field1 1 12 double
                                         field_name = items[0]
                                         dataset.metadata.field_names.append(field_name)
                                         num_components = int(items[1])
                                         field_component_indexes = [str(i) for i in range(num_components)]
                                         field_components[field_name] = field_component_indexes
                                         fields_processed.append(field_name)
-                        elif line.startswith('CELL_DATA'):
+                        elif line.startswith("CELL_DATA"):
                             # CELL_DATA 3188
                             dataset_structure_complete = True
                             dataset.metadata.cells = int(line.split()[1])
-                        elif line.startswith('POINT_DATA'):
+                        elif line.startswith("POINT_DATA"):
                             # POINT_DATA 1876
                             dataset_structure_complete = True
                             dataset.metadata.points = int(line.split()[1])
                         else:
                             dataset, dataset_type = self.set_structure_metadata(line, dataset, dataset_type)
             if len(field_components) > 0:
                 dataset.metadata.field_components = field_components
 
-    def set_initial_metadata(self, i, line, dataset):
+    def set_initial_metadata(self, i: int, line: str, dataset: "DatasetInstance") -> "DatasetInstance":
         if i == 0:
             # The first part of legacy VTK files is the file version and
             # identifier. This part contains the single line:
             # # vtk DataFile Version X.Y
-            dataset.metadata.vtk_version = line.lower().split('version')[1]
+            dataset.metadata.vtk_version = line.lower().split("version")[1]
             # The second part of legacy VTK files is the header. The header
             # consists of a character string terminated by end-of-line
             # character \n. The header is 256 characters maximum. The header
             # can be used to describe the data and include any other pertinent
             # information.  We skip the header line...
         elif i == 2:
             # The third part of legacy VTK files is the file format.  The file
             # format describes the type of file, either ASCII or binary. On
             # this line the single word ASCII or BINARY must appear.
             dataset.metadata.file_format = line
         return dataset
 
-    def set_structure_metadata(self, line, dataset, dataset_type):
+    def set_structure_metadata(
+        self, line: str, dataset: "DatasetInstance", dataset_type: Optional[str]
+    ) -> Tuple["DatasetInstance", Optional[str]]:
         """
         The fourth part of legacy VTK files is the dataset structure. The
         geometry part describes the geometry and topology of the dataset.
         This part begins with a line containing the keyword DATASET followed
         by a keyword describing the type of dataset.  Then, depending upon
         the type of dataset, other keyword/ data combinations define the
         actual data.
         """
-        if dataset_type is None and line.startswith('DATASET'):
+        if dataset_type is None and line.startswith("DATASET"):
             dataset_type = line.split()[1]
             dataset.metadata.dataset_type = dataset_type
-        if dataset_type == 'STRUCTURED_GRID':
+        if dataset_type == "STRUCTURED_GRID":
             # The STRUCTURED_GRID format supports 1D, 2D, and 3D structured
             # grid datasets.  The dimensions nx, ny, nz must be greater
             # than or equal to 1.  The point coordinates are defined by the
             # data in the POINTS section. This consists of x-y-z data values
             # for each point.
-            if line.startswith('DIMENSIONS'):
+            if line.startswith("DIMENSIONS"):
                 # DIMENSIONS 10 5 1
                 dataset.metadata.dimensions = [line.split()[1:]]
-            elif line.startswith('ORIGIN'):
+            elif line.startswith("ORIGIN"):
                 # ORIGIN 0 0 0
                 dataset.metadata.origin = [line.split()[1:]]
-            elif line.startswith('SPACING'):
+            elif line.startswith("SPACING"):
                 # SPACING 1 1 1
                 dataset.metadata.spacing = [line.split()[1:]]
-        elif dataset_type == 'POLYDATA':
+        elif dataset_type == "POLYDATA":
             # The polygonal dataset consists of arbitrary combinations
             # of surface graphics primitives vertices, lines, polygons
             # and triangle strips.  Polygonal data is defined by the POINTS,
             # VERTICES, LINES, POLYGONS, or TRIANGLE_STRIPS sections.
-            if line.startswith('POINTS'):
+            if line.startswith("POINTS"):
                 # POINTS 18 float
                 dataset.metadata.points = int(line.split()[1])
-            elif line.startswith('VERTICES'):
+            elif line.startswith("VERTICES"):
                 dataset.metadata.vertices = int(line.split()[1])
-            elif line.startswith('LINES'):
+            elif line.startswith("LINES"):
                 # LINES 5 17
                 dataset.metadata.lines = int(line.split()[1])
-            elif line.startswith('POLYGONS'):
+            elif line.startswith("POLYGONS"):
                 # POLYGONS 6 30
                 dataset.metadata.polygons = int(line.split()[1])
-            elif line.startswith('TRIANGLE_STRIPS'):
+            elif line.startswith("TRIANGLE_STRIPS"):
                 # TRIANGLE_STRIPS 2212 16158
                 dataset.metadata.triangle_strips = int(line.split()[1])
-        elif dataset_type == 'UNSTRUCTURED_GRID':
+        elif dataset_type == "UNSTRUCTURED_GRID":
             # The unstructured grid dataset consists of arbitrary combinations
             # of any possible cell type. Unstructured grids are defined by points,
             # cells, and cell types.
-            if line.startswith('POINTS'):
+            if line.startswith("POINTS"):
                 # POINTS 18 float
                 dataset.metadata.points = int(line.split()[1])
-            if line.startswith('CELLS'):
+            if line.startswith("CELLS"):
                 # CELLS 756 3024
                 dataset.metadata.cells = int(line.split()[1])
         return dataset, dataset_type
 
-    def get_blurb(self, dataset):
+    def get_blurb(self, dataset: "DatasetInstance") -> str:
         blurb = ""
         if dataset.metadata.vtk_version is not None:
-            blurb += f'VTK Version {str(dataset.metadata.vtk_version)}'
+            blurb += f"VTK Version {str(dataset.metadata.vtk_version)}"
         if dataset.metadata.dataset_type is not None:
             if blurb:
-                blurb += ' '
+                blurb += " "
             blurb += str(dataset.metadata.dataset_type)
-        return blurb or 'VTK data'
+        return blurb or "VTK data"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = self.get_blurb(dataset)
         else:
-            dataset.peek = 'File does not exist'
-            dataset.blurb = 'File purged from disc'
+            dataset.peek = "File does not exist"
+            dataset.blurb = "File purged from disc"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Vtk file ({nice_size(dataset.get_size())})"
 
 
 class VtkAscii(Vtk, data.Text):  # type: ignore[misc]
@@ -460,34 +480,35 @@
     >>> fname = get_test_fname('test.vtkascii')
     >>> VtkAscii().sniff(fname)
     True
     >>> fname = get_test_fname('test.vtkbinary')
     >>> VtkAscii().sniff(fname)
     False
     """
+
     file_ext = "vtkascii"
-    subtype = 'ASCII'
+    subtype = "ASCII"
 
     def __init__(self, **kwd):
         data.Text.__init__(self, **kwd)
 
 
-class VtkBinary(Vtk, Binary):   # type: ignore[misc]
+class VtkBinary(Vtk, Binary):  # type: ignore[misc]
     """
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('test.vtkbinary')
     >>> VtkBinary().sniff(fname)
     True
     >>> fname = get_test_fname('test.vtkascii')
     >>> VtkBinary().sniff(fname)
     False
     """
 
     file_ext = "vtkbinary"
-    subtype = 'BINARY'
+    subtype = "BINARY"
 
     def __init__(self, **kwd):
         Binary.__init__(self, **kwd)
 
 
 class STL(data.Data):
     file_ext = "stl"
@@ -504,59 +525,60 @@
         **format
             format
         **general
             dim type
         **cell
             number_of_cells
     """
+
     file_ext = "neper.tess"
     MetadataElement(name="format", default=None, desc="format", readonly=True, visible=True)
     MetadataElement(name="dimension", default=None, desc="dimension", readonly=True, visible=True)
     MetadataElement(name="cells", default=None, desc="cells", readonly=True, visible=True)
 
     def __init__(self, **kwd):
         data.Text.__init__(self, **kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Neper tess format, starts with ``***tess``
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('test.neper.tess')
         >>> NeperTess().sniff(fname)
         True
         >>> fname = get_test_fname('test.neper.tesr')
         >>> NeperTess().sniff(fname)
         False
         """
-        return file_prefix.text_io(errors='ignore').readline(10).startswith('***tess')
+        return file_prefix.text_io(errors="ignore").readline(10).startswith("***tess")
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if dataset.has_data():
-            with open(dataset.file_name, errors='ignore') as fh:
+            with open(dataset.file_name, errors="ignore") as fh:
                 for i, line in enumerate(fh):
                     line = line.strip()
                     if not line or i > 6:
                         break
-                    if i == 0 and not line.startswith('***tess'):
+                    if i == 0 and not line.startswith("***tess"):
                         break
                     if i == 2:
                         dataset.metadata.format = line
                     if i == 4:
                         dataset.metadata.dimension = int(line.split()[0])
                     if i == 6:
                         dataset.metadata.cells = int(line)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = get_file_peek(dataset.file_name, LINE_COUNT=7)
-            dataset.blurb = f'format: {str(dataset.metadata.format)} dim: {str(dataset.metadata.dimension)} cells: {str(dataset.metadata.cells)}'
+            dataset.peek = get_file_peek(dataset.file_name, line_count=7)
+            dataset.blurb = f"format: {str(dataset.metadata.format)} dim: {str(dataset.metadata.dimension)} cells: {str(dataset.metadata.cells)}"
         else:
-            dataset.peek = 'File does not exist'
-            dataset.blurb = 'File purged from disc'
+            dataset.peek = "File does not exist"
+            dataset.blurb = "File purged from disc"
 
 
 @build_sniff_from_prefix
 class NeperTesr(Binary):
     """
     Neper Raster Tessellation File
 
@@ -571,98 +593,100 @@
             voxsize_x voxsize_y [voxsize_z]
         [*origin
             origin_x origin_y [origin_z]]
         [*hasvoid has_void]
         [**cell
             number_of_cells
     """
+
     file_ext = "neper.tesr"
     MetadataElement(name="format", default=None, desc="format", readonly=True, visible=True)
     MetadataElement(name="dimension", default=None, desc="dimension", readonly=True, visible=True)
     MetadataElement(name="size", default=[], desc="size", readonly=True, visible=True)
     MetadataElement(name="voxsize", default=[], desc="voxsize", readonly=True, visible=True)
     MetadataElement(name="origin", default=[], desc="origin", readonly=True, visible=True)
     MetadataElement(name="cells", default=None, desc="cells", readonly=True, visible=True)
 
     def __init__(self, **kwd):
         Binary.__init__(self, **kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Neper tesr format, starts with ``***tesr``
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('test.neper.tesr')
         >>> NeperTesr().sniff(fname)
         True
         >>> fname = get_test_fname('test.neper.tess')
         >>> NeperTesr().sniff(fname)
         False
         """
-        return file_prefix.text_io(errors='ignore').readline(10).startswith('***tesr')
+        return file_prefix.text_io(errors="ignore").readline(10).startswith("***tesr")
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if dataset.has_data():
-            with open(dataset.file_name, errors='ignore') as fh:
-                field = ''
+            with open(dataset.file_name, errors="ignore") as fh:
+                field = ""
                 for i, line in enumerate(fh):
                     line = line.strip()
                     if not line or i > 12:
                         break
-                    if i == 0 and not line.startswith('***tesr'):
+                    if i == 0 and not line.startswith("***tesr"):
                         break
-                    if line.startswith('*'):
+                    if line.startswith("*"):
                         field = line
                         continue
                     if i == 2:
                         dataset.metadata.format = line.split()[0]
                         continue
                     if i == 4:
                         dataset.metadata.dimension = line.split()[0]
                         continue
                     if i == 5:
                         dataset.metadata.size = line.split()
                         continue
                     if i == 6:
                         dataset.metadata.voxsize = line.split()
                         continue
-                    if field.startswith('*origin'):
+                    if field.startswith("*origin"):
                         dataset.metadata.origin = line.split()
                         continue
-                    if field.startswith('**cell'):
+                    if field.startswith("**cell"):
                         dataset.metadata.cells = int(line)
                         break
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = get_file_peek(dataset.file_name, LINE_COUNT=9)
-            dataset.blurb = f'format: {str(dataset.metadata.format)} dim: {str(dataset.metadata.dimension)} cells: {str(dataset.metadata.cells)}'
+            dataset.peek = get_file_peek(dataset.file_name, line_count=9)
+            dataset.blurb = f"format: {str(dataset.metadata.format)} dim: {str(dataset.metadata.dimension)} cells: {str(dataset.metadata.cells)}"
         else:
-            dataset.peek = 'File does not exist'
-            dataset.blurb = 'File purged from disc'
+            dataset.peek = "File does not exist"
+            dataset.blurb = "File purged from disc"
 
 
 class NeperPoints(data.Text):
     """
     Neper Position File
     Neper position format has 1 - 3 floats per line separated by white space.
     """
+
     file_ext = "neper.points"
     MetadataElement(name="dimension", default=None, desc="dimension", readonly=True, visible=True)
 
     def __init__(self, **kwd):
         data.Text.__init__(self, **kwd)
 
-    def set_meta(self, dataset, **kwd):
-        data.Text.set_meta(self, dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        data.Text.set_meta(self, dataset, overwrite=overwrite, **kwd)
         if dataset.has_data():
-            with open(dataset.file_name, errors='ignore') as fh:
+            with open(dataset.file_name, errors="ignore") as fh:
                 dataset.metadata.dimension = self._get_dimension(fh)
 
-    def _get_dimension(self, fh, maxlines=100, sep=None):
+    def _get_dimension(self, fh: "TextIOBase", maxlines: int = 100, sep: Optional[str] = None) -> Optional[float]:
         dim = None
         try:
             for i, line in enumerate(fh):
                 if not line:
                     break
                 pts = len([float(x) for x in line.strip().split(sep=sep)])
                 if dim is not None and pts != dim:
@@ -673,107 +697,113 @@
                     return None
                 if i > maxlines:
                     break
         except Exception:
             return None
         return dim
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         data.Text.set_peek(self, dataset)
         if not dataset.dataset.purged:
-            dataset.blurb += f' dim: {str(dataset.metadata.dimension)}'
+            dataset.blurb += f" dim: {str(dataset.metadata.dimension)}"
 
 
 class NeperPointsTabular(NeperPoints, Tabular):
     """
     Neper Position File
     Neper position format has 1 - 3 floats per line separated by TABs.
     """
+
     file_ext = "neper.points.tsv"
 
     def __init__(self, **kwd):
         Tabular.__init__(self, **kwd)
 
-    def set_meta(self, dataset, **kwd):
-        Tabular.set_meta(self, dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        Tabular.set_meta(self, dataset, overwrite=overwrite, **kwd)
         if dataset.has_data():
-            with open(dataset.file_name, errors='ignore') as fh:
+            with open(dataset.file_name, errors="ignore") as fh:
                 dataset.metadata.dimension = self._get_dimension(fh)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         Tabular.set_peek(self, dataset)
         if not dataset.dataset.purged:
-            dataset.blurb += f' dim: {str(dataset.metadata.dimension)}'
+            dataset.blurb += f" dim: {str(dataset.metadata.dimension)}"
 
 
 class NeperMultiScaleCell(data.Text):
     """
     Neper Multiscale Cell File
     """
+
     file_ext = "neper.mscell"
 
 
 @build_sniff_from_prefix
 class GmshMsh(Binary):
     """Gmsh Mesh File"""
+
     file_ext = "gmsh.msh"
+    is_binary = "maybe"  # type: ignore[assignment]  # https://github.com/python/mypy/issues/8796
     MetadataElement(name="version", default=None, desc="version", readonly=True, visible=True)
     MetadataElement(name="format", default=None, desc="format", readonly=True, visible=True)
 
     def __init__(self, **kwd):
         Binary.__init__(self, **kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Gmsh msh format, starts with ``$MeshFormat``
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('test.gmsh.msh')
         >>> GmshMsh().sniff(fname)
         True
         >>> fname = get_test_fname('test.neper.tesr')
         >>> GmshMsh().sniff(fname)
         False
         """
-        return file_prefix.text_io(errors='ignore').readline().startswith('$MeshFormat')
+        return file_prefix.text_io(errors="ignore").readline().startswith("$MeshFormat")
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if dataset.has_data():
-            with open(dataset.file_name, errors='ignore') as fh:
+            with open(dataset.file_name, errors="ignore") as fh:
                 for i, line in enumerate(fh):
                     line = line.strip()
                     if not line or i > 1:
                         break
-                    if i == 0 and not line.startswith('$MeshFormat'):
+                    if i == 0 and not line.startswith("$MeshFormat"):
                         break
                     if i == 1:
                         fields = line.split()
                         if len(fields) > 0:
                             dataset.metadata.version = fields[0]
                         if len(fields) > 1:
-                            dataset.metadata.format = 'ASCII' if fields[1] == '0' else 'binary'
+                            dataset.metadata.format = "ASCII" if fields[1] == "0" else "binary"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = get_file_peek(dataset.file_name, LINE_COUNT=3)
-            dataset.blurb = f'Gmsh verion: {str(dataset.metadata.version)} {str(dataset.metadata.format)}'
+            dataset.peek = get_file_peek(dataset.file_name, line_count=3)
+            dataset.blurb = f"Gmsh verion: {str(dataset.metadata.version)} {str(dataset.metadata.format)}"
         else:
-            dataset.peek = 'File does not exist'
-            dataset.blurb = 'File purged from disc'
+            dataset.peek = "File does not exist"
+            dataset.blurb = "File purged from disc"
 
 
 class GmshGeo(data.Text):
     """Gmsh geometry File"""
+
     file_ext = "gmsh.geo"
 
 
 class ZsetGeof(data.Text):
     """
     Z-set geof File
     """
+
     file_ext = "zset.geof"
 
 
 # Utility functions
 def get_next_line(fh):
     line = fh.readline(MAX_LINE_LEN)
     if not line.endswith("\n"):
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bam_to_bai.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bam_to_bai.xml`

 * *Files 2% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bam_to_bai.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bam_to_bai.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_Bam_Bai_0" name="Bam to Bai" version="1.0.0" hidden="true" profile="16.04">
+<tool id="CONVERTER_Bam_Bai_0" name="Convert Bam to Bai" version="1.0.0" hidden="true" profile="16.04">
   <requirements>
     <requirement type="package" version="1.10">samtools</requirement>
   </requirements>
   <command>samtools index -@ \${GALAXY_SLOTS:-1} '$input1' '$output1'</command>
   <inputs>
     <param format="bam" name="input1" type="data" label="Choose BAM"/>
   </inputs>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bam_to_bigwig_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bam_to_bigwig_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bcf_bcf_uncompressed_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bcf_bcf_uncompressed_converter.xml`

 * *Files 8% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bcf_bcf_uncompressed_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bcf_bcf_uncompressed_converter.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_bcf_uncompressed_to_bcf" name="Converter for BCF and uncompressed BCF" version="0.0.1" hidden="false" profile="21.09">
+<tool id="CONVERTER_bcf_uncompressed_to_bcf" name="Convert compressed and uncompressed BCF files" version="0.0.1" hidden="false" profile="21.09">
   <!-- <description>__NOT_USED_CURRENTLY_FOR_CONVERTERS__</description> -->
   <requirements>
     <requirement type="package" version="1.12">bcftools</requirement>
   </requirements>
   <command detect_errors="exit_code"><![CDATA[
         bcftools view -o '$output1' 
         #if $__target_datatype__ == 'bcf'
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bed_gff_or_vcf_to_bigwig_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bed_gff_or_vcf_to_bigwig_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_fli_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_fli_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_gff_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_gff_converter.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,69 +8,81 @@
 
 def __main__():
     input_name = sys.argv[1]
     output_name = sys.argv[2]
     skipped_lines = 0
     first_skipped_line = 0
     i = 0
-    with open(input_name) as fh, open(output_name, 'w') as out:
+    with open(input_name) as fh, open(output_name, "w") as out:
         out.write("##gff-version 2\n")
         out.write("##bed_to_gff_converter.py\n\n")
         for i, line in enumerate(fh):
             complete_bed = False
-            line = line.rstrip('\r\n')
-            if line and not line.startswith('#') and not line.startswith('track') and not line.startswith('browser'):
+            line = line.rstrip("\r\n")
+            if line and not line.startswith("#") and not line.startswith("track") and not line.startswith("browser"):
                 try:
-                    elems = line.split('\t')
+                    elems = line.split("\t")
                     if len(elems) == 12:
                         complete_bed = True
                     chrom = elems[0]
                     if complete_bed:
                         feature = "mRNA"
                     else:
                         try:
                             feature = elems[3]
                         except Exception:
-                            feature = 'feature%d' % (i + 1)
+                            feature = "feature%d" % (i + 1)
                     start = int(elems[1]) + 1
                     end = int(elems[2])
                     try:
                         score = elems[4]
                     except Exception:
-                        score = '0'
+                        score = "0"
                     try:
                         strand = elems[5]
                     except Exception:
-                        strand = '+'
+                        strand = "+"
                     try:
                         group = elems[3]
                     except Exception:
-                        group = 'group%d' % (i + 1)
+                        group = "group%d" % (i + 1)
                     if complete_bed:
-                        out.write('%s\tbed2gff\t%s\t%d\t%d\t%s\t%s\t.\t%s %s;\n' % (chrom, feature, start, end, score, strand, feature, group))
+                        out.write(
+                            "%s\tbed2gff\t%s\t%d\t%d\t%s\t%s\t.\t%s %s;\n"
+                            % (chrom, feature, start, end, score, strand, feature, group)
+                        )
                     else:
-                        out.write('%s\tbed2gff\t%s\t%d\t%d\t%s\t%s\t.\t%s;\n' % (chrom, feature, start, end, score, strand, group))
+                        out.write(
+                            "%s\tbed2gff\t%s\t%d\t%d\t%s\t%s\t.\t%s;\n"
+                            % (chrom, feature, start, end, score, strand, group)
+                        )
                     if complete_bed:
                         # We have all the info necessary to annotate exons for genes and mRNAs
                         block_count = int(elems[9])
-                        block_sizes = elems[10].split(',')
-                        block_starts = elems[11].split(',')
+                        block_sizes = elems[10].split(",")
+                        block_starts = elems[11].split(",")
                         for j in range(block_count):
                             exon_start = int(start) + int(block_starts[j])
                             exon_end = exon_start + int(block_sizes[j]) - 1
-                            out.write('%s\tbed2gff\texon\t%d\t%d\t%s\t%s\t.\texon %s;\n' % (chrom, exon_start, exon_end, score, strand, group))
+                            out.write(
+                                "%s\tbed2gff\texon\t%d\t%d\t%s\t%s\t.\texon %s;\n"
+                                % (chrom, exon_start, exon_end, score, strand, group)
+                            )
                 except Exception:
                     skipped_lines += 1
                     if not first_skipped_line:
                         first_skipped_line = i + 1
             else:
                 skipped_lines += 1
                 if not first_skipped_line:
                     first_skipped_line = i + 1
     info_msg = "%i lines converted to GFF version 2.  " % (i + 1 - skipped_lines)
     if skipped_lines > 0:
-        info_msg += "Skipped %d blank/comment/invalid lines starting with line #%d." % (skipped_lines, first_skipped_line)
+        info_msg += "Skipped %d blank/comment/invalid lines starting with line #%d." % (
+            skipped_lines,
+            first_skipped_line,
+        )
     print(info_msg)
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_gff_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_gff_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bed_to_interval_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bed_to_interval_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bedgraph_to_bigwig_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bedgraph_to_bigwig_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/biom.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/biom.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/bz2_to_uncompressed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/bz2_to_uncompressed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/cml_to_smi_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/cml_to_smi_converter.xml`

 * *Files 1% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/cml_to_smi_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/cml_to_smi_converter.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_cml_to_smiles" name="CML to SMILES" version="2.4.1">
+<tool id="CONVERTER_cml_to_smiles" name="Convert CML to SMILES" version="2.4.1">
   <description/>
   <parallelism method="multi" split_inputs="input" split_mode="to_size" split_size="10000" shared_inputs="" merge_outputs="output"/>
   <requirements>
     <requirement type="package" version="2.4.1">openbabel</requirement>
   </requirements>
   <command><![CDATA[
         obabel
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/cram_to_bam_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/to_coordinate_sorted_bam.xml`

 * *Files 18% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/cram_to_bam_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/to_coordinate_sorted_bam.xml`

```diff
@@ -1,26 +1,27 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_cram_to_bam_0" name="Convert CRAM to BAM" version="1.0.0" hidden="true" profile="16.04">
+<tool id="CONVERTER_bam_to_coodinate_sorted_bam" name="Convert BAM to coordinate-sorted BAM" version="1.0.0" hidden="true" profile="18.01">
   <requirements>
-    <requirement type="package" version="0.15.4">pysam</requirement>
+    <requirement type="package" version="1.6">samtools</requirement>
   </requirements>
   <command><![CDATA[
-python '$__tool_directory__/cram_to_bam.py' '$input' '$output'
+         samtools sort
+            -@ \${GALAXY_SLOTS:-1}
+            -o '${output}'
+            -O bam
+            -T dataset
+            '${input}'
     ]]></command>
   <inputs>
-    <param format="cram" name="input" type="data" label="Choose CRAM file"/>
+    <param format="sam,unsorted.bam,qname_sorted.bam" name="input" type="data" label="Choose a BAM native or queryname sortedfile"/>
   </inputs>
   <outputs>
     <data format="bam" name="output"/>
   </outputs>
   <tests>
     <test>
-      <param name="input" ftype="cram" value="2.cram"/>
-      <output name="output" ftype="bam">
-        <assert_contents>
-          <has_size value="57232"/>
-        </assert_contents>
-      </output>
+      <param name="input" ftype="sam" value="bfast_out1.sam"/>
+      <output name="output" ftype="bam" value="bfast_out1.bam"/>
     </test>
   </tests>
   <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/csv_to_tabular.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/csv_to_tabular.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_2bit.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_2bit.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_bowtie_base_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_bowtie_base_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_bowtie_color_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_bowtie_color_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_fai.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_fai.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_len.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_len.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,36 +6,35 @@
 """
 import sys
 
 assert sys.version_info[:2] >= (2, 4)
 
 
 def compute_fasta_length(fasta_file, out_file, keep_first_char, keep_first_word=False):
-
     infile = fasta_file
     keep_first_char = int(keep_first_char)
 
-    fasta_title = ''
+    fasta_title = ""
     seq_len = 0
 
     # number of char to keep in the title
     if keep_first_char == 0:
         keep_first_char = None
     else:
         keep_first_char += 1
 
     first_entry = True
 
-    with open(out_file, 'w') as out:
+    with open(out_file, "w") as out:
         with open(infile) as fh:
             for line in fh:
                 line = line.strip()
-                if not line or line.startswith('#'):
+                if not line or line.startswith("#"):
                     continue
-                if line[0] == '>':
+                if line[0] == ">":
                     if first_entry is False:
                         if keep_first_word:
                             fasta_title = fasta_title.split()[0]
                         out.write("%s\t%d\n" % (fasta_title[1:keep_first_char], seq_len))
                     else:
                         first_entry = False
                     fasta_title = line
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_len.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_len.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_tabular_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_tabular_converter.py`

 * *Files 10% similar despite different names*

```diff
@@ -19,31 +19,31 @@
     outfile = sys.argv[2]
 
     if not os.path.isfile(infile):
         sys.stderr.write(f"Input file {infile!r} not found\n")
         sys.exit(1)
 
     with open(infile) as inp:
-        with open(outfile, 'w') as out:
-            sequence = ''
+        with open(outfile, "w") as out:
+            sequence = ""
             for line in inp:
-                line = line.rstrip('\r\n')
-                if line.startswith('>'):
+                line = line.rstrip("\r\n")
+                if line.startswith(">"):
                     if sequence:
                         # Flush sequence from previous FASTA record,
                         # removing any white space
-                        out.write("".join(sequence.split()) + '\n')
-                        sequence = ''
+                        out.write("".join(sequence.split()) + "\n")
+                        sequence = ""
                     # Strip off the leading '>' and remove any pre-existing
                     # tabs which would trigger extra columns; write with
                     # tab to separate this from the sequence column:
-                    out.write(line[1:].replace('\t', ' ') + '\t')
+                    out.write(line[1:].replace("\t", " ") + "\t")
                 else:
                     # Continuing sequence,
                     sequence += line
             # End of FASTA file, flush last sequence
             if sequence:
-                out.write("".join(sequence.split()) + '\n')
+                out.write("".join(sequence.split()) + "\n")
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fasta_to_tabular_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fasta_to_tabular_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fastq_to_fqtoc.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fastq_to_fqtoc.py`

 * *Files 9% similar despite different names*

```diff
@@ -15,31 +15,33 @@
         ]}
 
     This works only for UNCOMPRESSED fastq files. The Python GzipFile does not provide seekable
     offsets via tell(), so clients just have to split the slow way
     """
     input_fname = sys.argv[1]
     if is_gzip(input_fname):
-        sys.exit('Conversion is only possible for uncompressed files')
+        sys.exit("Conversion is only possible for uncompressed files")
 
     current_line = 0
     sequences = 1000000
     lines_per_chunk = 4 * sequences
     chunk_begin = 0
 
-    with open(input_fname) as in_file, open(sys.argv[2], 'w') as out_file:
+    with open(input_fname) as in_file, open(sys.argv[2], "w") as out_file:
         out_file.write('{"sections" : [')
 
-        for _ in iter(in_file.readline, ''):
+        for _ in iter(in_file.readline, ""):
             current_line += 1
             if 0 == current_line % lines_per_chunk:
                 chunk_end = in_file.tell()
                 out_file.write(f'{{"start":"{chunk_begin}","end":"{chunk_end}","sequences":"{sequences}"}},')
                 chunk_begin = chunk_end
 
         chunk_end = in_file.tell()
-        out_file.write(f'{{"start":"{chunk_begin}","end":"{chunk_end}","sequences":"{current_line % lines_per_chunk / 4}"}}')
-        out_file.write(']}\n')
+        out_file.write(
+            f'{{"start":"{chunk_begin}","end":"{chunk_end}","sequences":"{current_line % lines_per_chunk / 4}"}}'
+        )
+        out_file.write("]}\n")
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fastq_to_fqtoc.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fastq_to_fqtoc.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,32 +21,32 @@
 def stop_err(msg):
     sys.exit(f"{msg}")
 
 
 def __main__():
     infile_name = sys.argv[1]
     fastq_block_lines = 0
-    seq_title_startswith = ''
+    seq_title_startswith = ""
 
-    with open(infile_name) as fh, open(sys.argv[2], 'w') as outfile:
+    with open(infile_name) as fh, open(sys.argv[2], "w") as outfile:
         for i, line in enumerate(fh):
             line = line.rstrip()  # eliminate trailing space and new line characters
-            if not line or line.startswith('#'):
+            if not line or line.startswith("#"):
                 continue
             fastq_block_lines = (fastq_block_lines + 1) % 4
             line_startswith = line[0:1]
             if fastq_block_lines == 1:
                 # line 1 is sequence title
                 if not seq_title_startswith:
                     seq_title_startswith = line_startswith
                 if seq_title_startswith != line_startswith:
-                    stop_err('Invalid fastqsolexa format at line %d: %s.' % (i + 1, line))
-                outfile.write(f'>{line[1:]}\n')
+                    stop_err("Invalid fastqsolexa format at line %d: %s." % (i + 1, line))
+                outfile.write(f">{line[1:]}\n")
             elif fastq_block_lines == 2:
                 # line 2 is nucleotides
-                outfile.write(f'{line}\n')
+                outfile.write(f"{line}\n")
             else:
                 pass
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_fasta_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,52 +21,55 @@
 def stop_err(msg):
     sys.exit(f"{msg}")
 
 
 def __main__():
     infile_name = sys.argv[1]
     # datatype = sys.argv[3]
-    qual_title_startswith = ''
-    seq_title_startswith = ''
+    qual_title_startswith = ""
+    seq_title_startswith = ""
     default_coding_value = 64
     fastq_block_lines = 0
 
-    with open(infile_name) as fh, open(sys.argv[2], 'w') as outfile_score:
+    with open(infile_name) as fh, open(sys.argv[2], "w") as outfile_score:
         for i, line in enumerate(fh):
             line = line.rstrip()
-            if not line or line.startswith('#'):
+            if not line or line.startswith("#"):
                 continue
             fastq_block_lines = (fastq_block_lines + 1) % 4
             line_startswith = line[0:1]
             if fastq_block_lines == 1:
                 # first line is @title_of_seq
                 if not seq_title_startswith:
                     seq_title_startswith = line_startswith
                 if line_startswith != seq_title_startswith:
-                    stop_err('Invalid fastqsolexa format at line %d: %s.' % (i + 1, line))
+                    stop_err("Invalid fastqsolexa format at line %d: %s." % (i + 1, line))
                 read_title = line[1:]
             elif fastq_block_lines == 2:
                 # second line is nucleotides
                 read_length = len(line)
             elif fastq_block_lines == 3:
                 # third line is +title_of_qualityscore (might be skipped)
                 if not qual_title_startswith:
                     qual_title_startswith = line_startswith
                 if line_startswith != qual_title_startswith:
-                    stop_err('Invalid fastqsolexa format at line %d: %s.' % (i + 1, line))
+                    stop_err("Invalid fastqsolexa format at line %d: %s." % (i + 1, line))
                 quality_title = line[1:]
                 if quality_title and read_title != quality_title:
-                    stop_err('Invalid fastqsolexa format at line %d: sequence title "%s" differes from score title "%s".' % (i + 1, read_title, quality_title))
+                    stop_err(
+                        'Invalid fastqsolexa format at line %d: sequence title "%s" differes from score title "%s".'
+                        % (i + 1, read_title, quality_title)
+                    )
                 if not quality_title:
-                    outfile_score.write(f'>{read_title}\n')
+                    outfile_score.write(f">{read_title}\n")
                 else:
-                    outfile_score.write(f'>{line[1:]}\n')
+                    outfile_score.write(f">{line[1:]}\n")
             else:
                 # fourth line is quality scores
-                qual = ''
+                qual = ""
                 fastq_integer = True
                 # peek: ascii or digits?
                 val = line.split()[0]
 
                 fastq_integer = True
                 try:
                     int(val)
@@ -80,16 +83,19 @@
                     quality_score_length = len(line)
                     if quality_score_length == read_length + 1:
                         quality_score_startswith = ord(line[0:1])
                         line = line[1:]
                     elif quality_score_length == read_length:
                         quality_score_startswith = default_coding_value
                     else:
-                        stop_err('Invalid fastqsolexa format at line %d: the number of quality scores ( %d ) is not the same as bases ( %d ).' % (i + 1, quality_score_length, read_length))
+                        stop_err(
+                            "Invalid fastqsolexa format at line %d: the number of quality scores ( %d ) is not the same as bases ( %d )."
+                            % (i + 1, quality_score_length, read_length)
+                        )
                     for char in line:
-                        score = ord(char) - quality_score_startswith    # 64
+                        score = ord(char) - quality_score_startswith  # 64
                         qual = f"{qual}{str(score)} "
-                outfile_score.write(f'{qual}\n')
+                outfile_score.write(f"{qual}\n")
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/fastqsolexa_to_qual_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_bed_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_bed_converter.py`

 * *Files 10% similar despite different names*

```diff
@@ -7,24 +7,24 @@
 
 def __main__():
     input_name = sys.argv[1]
     output_name = sys.argv[2]
     skipped_lines = 0
     first_skipped_line = 0
     i = 0
-    with open(input_name) as fh, open(output_name, 'w') as out:
+    with open(input_name) as fh, open(output_name, "w") as out:
         for i, line in enumerate(fh):
-            line = line.rstrip('\r\n')
-            if line and not line.startswith('#'):
+            line = line.rstrip("\r\n")
+            if line and not line.startswith("#"):
                 try:
-                    elems = line.split('\t')
+                    elems = line.split("\t")
                     start = str(int(elems[3]) - 1)
                     strand = elems[6]
-                    if strand not in ['+', '-']:
-                        strand = '+'
+                    if strand not in ["+", "-"]:
+                        strand = "+"
                     # GFF format: chrom source, name, chromStart, chromEnd, score, strand
                     # Bed format: chrom, chromStart, chromEnd, name, score, strand
                     #
                     # Replace any spaces in the name with underscores so UCSC will not complain
                     name = elems[2].replace(" ", "_")
                     out.write(f"{elems[0]}\t{start}\t{elems[4]}\t{name}\t0\t{strand}\n")
                 except Exception:
@@ -33,13 +33,16 @@
                         first_skipped_line = i + 1
             else:
                 skipped_lines += 1
                 if not first_skipped_line:
                     first_skipped_line = i + 1
     info_msg = "%i lines converted to BED.  " % (i + 1 - skipped_lines)
     if skipped_lines > 0:
-        info_msg += "Skipped %d blank/comment/invalid lines starting with line #%d." % (skipped_lines, first_skipped_line)
+        info_msg += "Skipped %d blank/comment/invalid lines starting with line #%d." % (
+            skipped_lines,
+            first_skipped_line,
+        )
     print(info_msg)
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_bed_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_bed_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_fli_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_fli_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_interval_index_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_interval_index_converter.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,16 +7,20 @@
     python gff_to_interval_index_converter.py [input] [output]
 """
 
 import fileinput
 import sys
 
 from bx.interval_index_file import Indexes
+from bx.intervals.io import GenomicInterval
 
-from galaxy.datatypes.util.gff_util import convert_gff_coords_to_bed, GenomicInterval, GFFReaderWrapper
+from galaxy.datatypes.util.gff_util import (
+    convert_gff_coords_to_bed,
+    GFFReaderWrapper,
+)
 
 
 def main():
     # Arguments
     input_fname, out_fname = sys.argv[1:]
 
     # Do conversion.
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gff_to_interval_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gff_to_interval_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gro_to_pdb.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gro_to_pdb.xml`

 * *Files 2% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gro_to_pdb.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gro_to_pdb.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_Gro_to_Pdb_0" name="GRO to PDB" version="1.0.0" hidden="true" profile="20.09">
+<tool id="CONVERTER_Gro_to_Pdb_0" name="Convert GRO to PDB" version="1.0.0" hidden="true" profile="20.09">
   <requirements>
     <requirement type="package" version="2020.4">gromacs</requirement>
   </requirements>
   <command detect_errors="exit_code"><![CDATA[
         ln -s '$input1' ./input.gro &&
         gmx editconf
             -f ./input.gro
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/gz_to_uncompressed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/gz_to_uncompressed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/inchi_to_mol_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/inchi_to_mol_converter.xml`

 * *Files 10% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/inchi_to_mol_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/inchi_to_mol_converter.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_inchi_to_mol" name="InChI to MOL" version="2.4.1">
+<tool id="CONVERTER_inchi_to_mol" name="Convert InChI to MOL" version="2.4.1">
   <description/>
   <parallelism method="multi" split_inputs="input" split_mode="to_size" split_size="10000" shared_inputs="" merge_outputs="output"/>
   <requirements>
     <requirement type="package" version="2.4.1">openbabel</requirement>
   </requirements>
   <command><![CDATA[
         obabel -iinchi '${input}' -omol -O '${output}' -e 2>&1
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed12_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed12_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed6_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed6_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed_converter.py`

 * *Files 10% similar despite different names*

```diff
@@ -14,45 +14,61 @@
 
 def __main__():
     output_name = sys.argv[1]
     input_name = sys.argv[2]
     try:
         chromCol = int(sys.argv[3]) - 1
     except Exception:
-        stop_err(f"'{str(sys.argv[3])}' is an invalid chrom column, correct the column settings before attempting to convert the data format.")
+        stop_err(
+            f"'{str(sys.argv[3])}' is an invalid chrom column, correct the column settings before attempting to convert the data format."
+        )
     try:
         startCol = int(sys.argv[4]) - 1
     except Exception:
-        stop_err(f"'{str(sys.argv[4])}' is an invalid start column, correct the column settings before attempting to convert the data format.")
+        stop_err(
+            f"'{str(sys.argv[4])}' is an invalid start column, correct the column settings before attempting to convert the data format."
+        )
     try:
         endCol = int(sys.argv[5]) - 1
     except Exception:
-        stop_err(f"'{str(sys.argv[5])}' is an invalid end column, correct the column settings before attempting to convert the data format.")
+        stop_err(
+            f"'{str(sys.argv[5])}' is an invalid end column, correct the column settings before attempting to convert the data format."
+        )
     try:
         strandCol = int(sys.argv[6]) - 1
     except Exception:
         strandCol = -1
     try:
         nameCol = int(sys.argv[7]) - 1
     except Exception:
         nameCol = -1
     skipped_lines = 0
     first_skipped_line = 0
     count = 0
-    with open(input_name) as fh, open(output_name, 'w') as out:
-        for count, region in enumerate(bx.intervals.io.NiceReaderWrapper(fh, chrom_col=chromCol, start_col=startCol, end_col=endCol, strand_col=strandCol, fix_strand=True, return_header=False, return_comments=False)):
+    with open(input_name) as fh, open(output_name, "w") as out:
+        for count, region in enumerate(
+            bx.intervals.io.NiceReaderWrapper(
+                fh,
+                chrom_col=chromCol,
+                start_col=startCol,
+                end_col=endCol,
+                strand_col=strandCol,
+                fix_strand=True,
+                return_header=False,
+                return_comments=False,
+            )
+        ):
             try:
                 if nameCol >= 0:
                     name = region.fields[nameCol]
                 else:
                     raise IndexError
             except Exception:
                 name = "region_%i" % count
             try:
-
                 out.write("%s\t%i\t%i\t%s\t%i\t%s\n" % (region.chrom, region.start, region.end, name, 0, region.strand))
             except Exception:
                 skipped_lines += 1
                 if not first_skipped_line:
                     first_skipped_line = count + 1
     print("%i regions converted to BED." % (count + 1 - skipped_lines))
     if skipped_lines > 0:
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bed_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bed_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,141 +10,182 @@
 
 def stop_err(msg):
     sys.exit(msg)
 
 
 def force_bed_field_count(fields, region_count, force_num_columns):
     if force_num_columns >= 4 and len(fields) < 4:
-        fields.append('region_%i' % (region_count))
+        fields.append("region_%i" % (region_count))
     if force_num_columns >= 5 and len(fields) < 5:
-        fields.append('0')
+        fields.append("0")
     if force_num_columns >= 6 and len(fields) < 6:
-        fields.append('+')
+        fields.append("+")
     if force_num_columns >= 7 and len(fields) < 7:
         fields.append(fields[1])
     if force_num_columns >= 8 and len(fields) < 8:
         fields.append(fields[2])
     if force_num_columns >= 9 and len(fields) < 9:
-        fields.append('0')
+        fields.append("0")
     if force_num_columns >= 10 and len(fields) < 10:
-        fields.append('0')
+        fields.append("0")
     if force_num_columns >= 11 and len(fields) < 11:
-        fields.append(',')
+        fields.append(",")
     if force_num_columns >= 12 and len(fields) < 12:
-        fields.append(',')
+        fields.append(",")
     return fields[:force_num_columns]
 
 
 def __main__():
     output_name = sys.argv[1]
     input_name = sys.argv[2]
     try:
         chromCol = int(sys.argv[3]) - 1
     except Exception:
-        stop_err(f"'{str(sys.argv[3])}' is an invalid chrom column, correct the column settings before attempting to convert the data format.")
+        stop_err(
+            f"'{str(sys.argv[3])}' is an invalid chrom column, correct the column settings before attempting to convert the data format."
+        )
     try:
         startCol = int(sys.argv[4]) - 1
     except Exception:
-        stop_err(f"'{str(sys.argv[4])}' is an invalid start column, correct the column settings before attempting to convert the data format.")
+        stop_err(
+            f"'{str(sys.argv[4])}' is an invalid start column, correct the column settings before attempting to convert the data format."
+        )
     try:
         endCol = int(sys.argv[5]) - 1
     except Exception:
-        stop_err(f"'{str(sys.argv[5])}' is an invalid end column, correct the column settings before attempting to convert the data format.")
+        stop_err(
+            f"'{str(sys.argv[5])}' is an invalid end column, correct the column settings before attempting to convert the data format."
+        )
     try:
         strandCol = int(sys.argv[6]) - 1
     except Exception:
         strandCol = -1
     try:
         nameCol = int(sys.argv[7]) - 1
     except Exception:
         nameCol = -1
     try:
         extension = sys.argv[8]
     except IndexError:
-        extension = 'interval'  # default extension
+        extension = "interval"  # default extension
     try:
         force_num_columns = int(sys.argv[9])
     except Exception:
         force_num_columns = None
 
     skipped_lines = 0
     first_skipped_line = None
     count = 0
     # does file already conform to bed strict?
     # if so, we want to keep extended columns, otherwise we'll create a generic 6 column bed file
     strict_bed = True
-    if extension in ['bed', 'bedstrict', 'bed6', 'bed12'] and (chromCol, startCol, endCol) == (0, 1, 2) and (nameCol < 0 or nameCol == 3) and (strandCol < 0 or strandCol == 5):
-        with open(input_name) as fh, open(output_name, 'w') as out:
+    if (
+        extension in ["bed", "bedstrict", "bed6", "bed12"]
+        and (chromCol, startCol, endCol) == (0, 1, 2)
+        and (nameCol < 0 or nameCol == 3)
+        and (strandCol < 0 or strandCol == 5)
+    ):
+        with open(input_name) as fh, open(output_name, "w") as out:
             for count, line in enumerate(fh):
-                line = line.rstrip('\n\r')
+                line = line.rstrip("\n\r")
                 if line == "" or line.startswith("#"):
                     skipped_lines += 1
                     if first_skipped_line is None:
                         first_skipped_line = count + 1
                     continue
-                fields = line.split('\t')
+                fields = line.split("\t")
                 try:
-                    assert len(fields) >= 3, 'A BED file requires at least 3 columns'  # we can't fix this
+                    assert len(fields) >= 3, "A BED file requires at least 3 columns"  # we can't fix this
                     if len(fields) > 12:
                         strict_bed = False
                         break
                     # name (fields[3]) can be anything, no verification needed
                     if len(fields) > 4:
-                        float(fields[4])  # score - A score between 0 and 1000. If the track line useScore attribute is set to 1 for this annotation data set, the score value will determine the level of gray in which this feature is displayed (higher numbers = darker gray).
+                        float(
+                            fields[4]
+                        )  # score - A score between 0 and 1000. If the track line useScore attribute is set to 1 for this annotation data set, the score value will determine the level of gray in which this feature is displayed (higher numbers = darker gray).
                         if len(fields) > 5:
-                            assert fields[5] in ['+', '-'], 'Invalid strand'  # strand - Defines the strand - either '+' or '-'.
+                            assert fields[5] in [
+                                "+",
+                                "-",
+                            ], "Invalid strand"  # strand - Defines the strand - either '+' or '-'.
                             if len(fields) > 6:
-                                int(fields[6])  # thickStart - The starting position at which the feature is drawn thickly (for example, the start codon in gene displays).
+                                int(
+                                    fields[6]
+                                )  # thickStart - The starting position at which the feature is drawn thickly (for example, the start codon in gene displays).
                                 if len(fields) > 7:
-                                    int(fields[7])  # thickEnd - The ending position at which the feature is drawn thickly (for example, the stop codon in gene displays).
+                                    int(
+                                        fields[7]
+                                    )  # thickEnd - The ending position at which the feature is drawn thickly (for example, the stop codon in gene displays).
                                     if len(fields) > 8:
-                                        if fields[8] != '0':  # itemRgb - An RGB value of the form R,G,B (e.g. 255,0,0). If the track line itemRgb attribute is set to "On", this RBG value will determine the display color of the data contained in this BED line. NOTE: It is recommended that a simple color scheme (eight colors or less) be used with this attribute to avoid overwhelming the color resources of the Genome Browser and your Internet browser.
-                                            fields2 = fields[8].split(',')
-                                            assert len(fields2) == 3, 'RGB value must be 0 or have length of 3'
+                                        if (
+                                            fields[8] != "0"
+                                        ):  # itemRgb - An RGB value of the form R,G,B (e.g. 255,0,0). If the track line itemRgb attribute is set to "On", this RBG value will determine the display color of the data contained in this BED line. NOTE: It is recommended that a simple color scheme (eight colors or less) be used with this attribute to avoid overwhelming the color resources of the Genome Browser and your Internet browser.
+                                            fields2 = fields[8].split(",")
+                                            assert len(fields2) == 3, "RGB value must be 0 or have length of 3"
                                             for field in fields2:
                                                 int(field)  # rgb values are integers
                                         if len(fields) > 9:
                                             int(fields[9])  # blockCount - The number of blocks (exons) in the BED line.
                                             if len(fields) > 10:
-                                                if fields[10] != ',':  # blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount.
-                                                    fields2 = fields[10].rstrip(",").split(",")  # remove trailing comma and split on comma
+                                                if (
+                                                    fields[10] != ","
+                                                ):  # blockSizes - A comma-separated list of the block sizes. The number of items in this list should correspond to blockCount.
+                                                    fields2 = (
+                                                        fields[10].rstrip(",").split(",")
+                                                    )  # remove trailing comma and split on comma
                                                     for field in fields2:
                                                         int(field)
                                                 if len(fields) > 11:
-                                                    if fields[11] != ',':  # blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount.
-                                                        fields2 = fields[11].rstrip(",").split(",")  # remove trailing comma and split on comma
+                                                    if (
+                                                        fields[11] != ","
+                                                    ):  # blockStarts - A comma-separated list of block starts. All of the blockStart positions should be calculated relative to chromStart. The number of items in this list should correspond to blockCount.
+                                                        fields2 = (
+                                                            fields[11].rstrip(",").split(",")
+                                                        )  # remove trailing comma and split on comma
                                                         for field in fields2:
                                                             int(field)
                 except Exception:
                     strict_bed = False
                     break
                 if force_num_columns is not None and len(fields) != force_num_columns:
-                    line = '\t'.join(force_bed_field_count(fields, count, force_num_columns))
+                    line = "\t".join(force_bed_field_count(fields, count, force_num_columns))
                 out.write(f"{line}\n")
     else:
         strict_bed = False
 
     if not strict_bed:
         skipped_lines = 0
         first_skipped_line = None
         count = 0
-        with open(input_name) as fh, open(output_name, 'w') as out:
-            for count, region in enumerate(bx.intervals.io.NiceReaderWrapper(fh, chrom_col=chromCol, start_col=startCol, end_col=endCol, strand_col=strandCol, fix_strand=True, return_header=False, return_comments=False)):
+        with open(input_name) as fh, open(output_name, "w") as out:
+            for count, region in enumerate(
+                bx.intervals.io.NiceReaderWrapper(
+                    fh,
+                    chrom_col=chromCol,
+                    start_col=startCol,
+                    end_col=endCol,
+                    strand_col=strandCol,
+                    fix_strand=True,
+                    return_header=False,
+                    return_comments=False,
+                )
+            ):
                 try:
                     if nameCol >= 0:
                         name = region.fields[nameCol]
                     else:
                         raise IndexError
                 except Exception:
                     name = "region_%i" % count
                 try:
                     fields = [str(item) for item in (region.chrom, region.start, region.end, name, 0, region.strand)]
                     if force_num_columns is not None and len(fields) != force_num_columns:
                         fields = force_bed_field_count(fields, count, force_num_columns)
-                    out.write("%s\n" % '\t'.join(fields))
+                    out.write("%s\n" % "\t".join(fields))
                 except Exception:
                     skipped_lines += 1
                     if first_skipped_line is None:
                         first_skipped_line = count + 1
     print("%i regions converted to BED." % (count + 1 - skipped_lines))
     if skipped_lines > 0:
         print("Skipped %d blank or invalid lines starting with line # %d." % (skipped_lines, first_skipped_line))
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bedstrict_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bgzip_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/uncompressed_to_gz.xml`

 * *Files 18% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bgzip_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/uncompressed_to_gz.xml`

```diff
@@ -1,53 +1,49 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_interval_to_bgzip_0" name="Convert Interval to BGZIP" version="1.0.1" hidden="true" profile="16.04">
-  <!-- <description>__NOT_USED_CURRENTLY_FOR_CONVERTERS__</description> -->
+<tool id="CONVERTER_uncompressed_to_gz" name="Convert uncompressed file to compressed" hidden="true" version="@TOOL_VERSION@+galaxy@VERSION_SUFFIX@" profile="20.01">
+  <macros>
+    <token name="@TOOL_VERSION@">1.16</token>
+    <token name="@VERSION_SUFFIX@">0</token>
+  </macros>
   <requirements>
-    <requirement type="package" version="0.15.4">pysam</requirement>
-    <requirement type="package" version="8.25">coreutils</requirement>
+    <requirement type="package" version="@TOOL_VERSION@">htslib</requirement>
+    <requirement type="package" version="1.0.8">bzip2</requirement>
   </requirements>
   <command><![CDATA[
-        python '$__tool_directory__/bgzip.py'
-        #if $input1.ext in ['bed', 'gff', 'vcf']
-            -P $input1.ext
-        #else
-            -c ${input1.metadata.chromCol}
-            -s ${input1.metadata.startCol}
-            -e ${input1.metadata.endCol}
-        #end if
-        '$input1' '$output1'
+cp '$ext_config' galaxy.json &&
+#if $input1.ext.endswith(".bz2"):
+    bzcat '$input1' | bgzip -@ "\${GALAXY_SLOTS:-1}" -c > '$output1'
+#else:
+    bgzip -@ "\${GALAXY_SLOTS:-1}" -c '$input1' > '$output1'
+#end if
     ]]></command>
+  <configfiles>
+    <configfile name="ext_config">#silent ext = $input1.ext[:-4] if $input1.ext.endswith(&quot;.bz2&quot;) else $input1.ext
+#silent ext = ext + '.gz' if ext != 'vcf' else 'vcf_bgzip'
+{&quot;output1&quot;: {
+  &quot;name&quot;: &quot;${input1.name} compressed&quot;,
+  &quot;ext&quot;: &quot;${ext}&quot;
+}}</configfile>
+  </configfiles>
   <inputs>
-    <!-- gff because gff and vcf are not a subclass of interval -->
-    <param format="gff,vcf,interval" name="input1" type="data" label="Choose Interval file"/>
+    <param name="input1" type="data" format="data" label="Choose uncompressed file"/>
   </inputs>
   <outputs>
-    <data format="bgzip" name="output1"/>
+    <!-- auto doesn't sniff - it defers to galaxy.json in this context. -->
+    <data name="output1" format="auto"/>
   </outputs>
   <tests>
     <test>
-      <param name="input1" ftype="bed" value="droPer1.bed"/>
-      <output name="output1" ftype="bgzip" value="droPer1.bgzip"/>
+      <param name="input1" value="test.vcf" ftype="vcf"/>
+      <output name="output1" file="test.vcf.gz" ftype="vcf_bgzip" decompress="true"/>
     </test>
     <test>
-      <param name="input1" ftype="encodepeak" value="encode.broad.peak"/>
-      <output name="output1" ftype="bgzip">
-        <assert_contents>
-          <has_size value="299" delta="10"/>
-        </assert_contents>
-      </output>
+      <param name="input1" value="1.fasta" ftype="fasta"/>
+      <output name="output1" file="1.fasta.gz" ftype="fasta.gz" decompress="true"/>
     </test>
     <test>
-      <param name="input1" ftype="gff" value="gff_filter_by_feature_count_out2.gff"/>
-      <output name="output1" ftype="bgzip" value="bgzip_filter_by_feature_count_out2.bgzip"/>
-    </test>
-    <test>
-      <param name="input1" ftype="interval" value="2.interval"/>
-      <output name="output1" ftype="bgzip" value="2.bgzip"/>
-    </test>
-    <test>
-      <param name="input1" ftype="vcf" value="vcf_to_maf_in.vcf"/>
-      <output name="output1" ftype="bgzip" value="bgzip_to_maf_in.bgzip"/>
+      <param name="input1" value="1.fastqsanger.bz2" ftype="fastqsanger.bz2"/>
+      <output name="output1" file="1.fastqsanger.gz" ftype="fastqsanger.gz" decompress="true"/>
     </test>
   </tests>
   <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_bigwig_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_bigwig_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_fli.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_fli.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,44 +1,51 @@
-'''
+"""
 Creates a feature location index (FLI) for a given BED/GFF file.
 FLI index has the form::
 
     [line_length]
     <symbol1_in_lowercase><tab><symbol1><tab><location>
     <symbol2_in_lowercase><tab><symbol2><tab><location>
     ...
 
 where location is formatted as:
 
     contig:start-end
 
 and symbols are sorted in lexigraphical order.
-'''
+"""
 import optparse
 
-from bx.tabular.io import Comment, Header
-
-from galaxy.datatypes.util.gff_util import convert_gff_coords_to_bed, GFFReaderWrapper, read_unordered_gtf
+from bx.tabular.io import (
+    Comment,
+    Header,
+)
+
+from galaxy.datatypes.util.gff_util import (
+    convert_gff_coords_to_bed,
+    GFFReaderWrapper,
+    read_unordered_gtf,
+)
 
 
 def main():
     # Process arguments.
     parser = optparse.OptionParser()
-    parser.add_option('-F', '--format', dest="input_format")
+    parser.add_option("-F", "--format", dest="input_format")
     (options, args) = parser.parse_args()
     in_fname, out_fname = args
     input_format = options.input_format.lower()
 
     # Create dict of name-location pairings.
     name_loc_dict = {}
-    if input_format in ['gff', 'gtf']:
+    if input_format in ["gff", "gtf"]:
         # GTF/GFF format
 
         # Create reader.
-        if input_format == 'gff':
+        if input_format == "gff":
             in_reader = GFFReaderWrapper(open(in_fname))
         else:  # input_format == 'gtf'
             in_reader = read_unordered_gtf(open(in_fname))
 
         for feature in in_reader:
             if isinstance(feature, (Header, Comment)):
                 continue
@@ -49,58 +56,50 @@
                     float(val)
                     continue
                 except ValueError:
                     convert_gff_coords_to_bed(feature)
                     # Value is not a number, so it can be indexed.
                     if val not in name_loc_dict:
                         # Value is not in dictionary.
-                        name_loc_dict[val] = {
-                            'contig': feature.chrom,
-                            'start': feature.start,
-                            'end': feature.end
-                        }
+                        name_loc_dict[val] = {"contig": feature.chrom, "start": feature.start, "end": feature.end}
                     else:
                         # Value already in dictionary, so update dictionary.
                         loc = name_loc_dict[val]
-                        if feature.start < loc['start']:
-                            loc['start'] = feature.start
-                        if feature.end > loc['end']:
-                            loc['end'] = feature.end
-    elif input_format == 'bed':
+                        if feature.start < loc["start"]:
+                            loc["start"] = feature.start
+                        if feature.end > loc["end"]:
+                            loc["end"] = feature.end
+    elif input_format == "bed":
         # BED format.
         for line in open(in_fname):
             # Ignore track lines.
             if line.startswith("track"):
                 continue
 
             fields = line.split()
 
             # Ignore lines with no feature name.
             if len(fields) < 4:
                 continue
 
             # Process line
-            name_loc_dict[fields[3]] = {
-                'contig': fields[0],
-                'start': int(fields[1]),
-                'end': int(fields[2])
-            }
+            name_loc_dict[fields[3]] = {"contig": fields[0], "start": int(fields[1]), "end": int(fields[2])}
 
     # Create sorted list of entries.
     max_len = 0
     entries = []
     for name in sorted(name_loc_dict.keys()):
         loc = name_loc_dict[name]
-        entry = '{}\t{}\t{}'.format(name.lower(), name, '%s:%i-%i' % (loc['contig'], loc['start'], loc['end']))
+        entry = "{}\t{}\t{}".format(name.lower(), name, "%s:%i-%i" % (loc["contig"], loc["start"], loc["end"]))
         if len(entry) > max_len:
             max_len = len(entry)
         entries.append(entry)
 
     # Write padded entries.
-    with open(out_fname, 'w') as out:
+    with open(out_fname, "w") as out:
         out.write(f"{str(max_len + 1).ljust(max_len)}\n")
         for entry in entries:
             out.write(f"{entry.ljust(max_len)}\n")
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_interval_index_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_interval_index_converter.py`

 * *Files 22% similar despite different names*

```diff
@@ -11,20 +11,19 @@
 
 import optparse
 
 from bx.interval_index_file import Indexes
 
 
 def main():
-
     # Read options, args.
     parser = optparse.OptionParser()
-    parser.add_option('-c', '--chr-col', type='int', dest='chrom_col', default=1)
-    parser.add_option('-s', '--start-col', type='int', dest='start_col', default=2)
-    parser.add_option('-e', '--end-col', type='int', dest='end_col', default=3)
+    parser.add_option("-c", "--chr-col", type="int", dest="chrom_col", default=1)
+    parser.add_option("-s", "--start-col", type="int", dest="start_col", default=2)
+    parser.add_option("-e", "--end-col", type="int", dest="end_col", default=3)
     (options, args) = parser.parse_args()
     input_fname, output_fname = args
 
     # Make column indices 0-based.
     options.chrom_col -= 1
     options.start_col -= 1
     options.end_col -= 1
@@ -40,13 +39,13 @@
                 continue
             chrom = feature[options.chrom_col]
             chrom_start = int(feature[options.start_col])
             chrom_end = int(feature[options.end_col])
             index.add(chrom, chrom_start, chrom_end, offset)
             offset += len(line)
 
-    with open(output_fname, 'wb') as out:
+    with open(output_fname, "wb") as out:
         index.write(out)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_interval_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/interval_to_interval_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_tabix_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/mdconvert.xml`

 * *Files 20% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/interval_to_tabix_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/mdconvert.xml`

```diff
@@ -1,56 +1,73 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_interval_to_tabix_0" name="Convert Interval to tabix" version="1.0.1" hidden="true" profile="16.04">
-  <!-- <description>__NOT_USED_CURRENTLY_FOR_CONVERTERS__</description> -->
+<tool id="CONVERTER_mdconvert" name="Convert XTC, DCD, and TRR" version="1.0.0" hidden="true" profile="21.09">
   <requirements>
-    <requirement type="package" version="0.15.4">pysam</requirement>
+    <requirement type="package" version="1.9.4">mdtraj</requirement>
   </requirements>
-  <command><![CDATA[
-        python '$__tool_directory__/interval_to_tabix_converter.py'
-        #if $input1.ext in ['bed', 'gff', 'vcf']
-            -P $input1.ext
-        #else
-            -c ${input1.metadata.chromCol}
-            -s ${input1.metadata.startCol}
-            -e ${input1.metadata.endCol}
-        #end if
-        '$input1' '$bgzip' '$output1'
+  <command detect_errors="exit_code"><![CDATA[
+        ln -s '$input1' ./input.${input1.ext} &&
+        mdconvert ./input.${input1.ext} -o ./output.$__target_datatype__ &&
+        mv ./output.$__target_datatype__ '$output1' &&
+        cp '$ext_config' 'galaxy.json'
     ]]></command>
+  <configfiles>
+    <configfile name="ext_config">{&quot;output1&quot;: {
+  &quot;name&quot;: &quot;$input1.name converted to $__target_datatype__&quot;,
+  &quot;ext&quot;: &quot;$__target_datatype__&quot;
+}}</configfile>
+  </configfiles>
   <inputs>
-    <!-- gff because gff and vcf are not a subclass of interval -->
-    <param format="gff,vcf,interval" name="input1" type="data" label="Choose Interval file"/>
-    <!-- the bgzip input is autogenerated by a call to interval_to_bgzip converter
-             (due to `depends_on="bgzip"` in datatypes_conf). note that $input1 is actually
-             ignored in the called python code -->
-    <param format="bgzip" name="bgzip" type="data" label="BGZIP file"/>
+    <param format="xtc,dcd,trr" name="input1" type="data" label="Choose input file"/>
+    <param name="__target_datatype__" type="select" label="Target data type">
+      <option value="dcd">dcd</option>
+      <option value="trr">trr</option>
+      <option value="xtc">xtc</option>
+    </param>
   </inputs>
   <outputs>
-    <data format="tabix" name="output1"/>
+    <data format="auto" name="output1"/>
   </outputs>
   <tests>
+    <!-- to dcd-->
     <test>
-      <param name="input1" ftype="bed" value="droPer1.bed"/>
-      <output name="output1" ftype="tabix" value="droPer1.tabix"/>
+      <param name="input1" ftype="xtc" value="traj.xtc"/>
+      <param name="__target_datatype__" value="dcd"/>
+      <output name="output1" ftype="dcd">
+        <assert_contents>
+          <has_size value="540"/>
+        </assert_contents>
+      </output>
     </test>
     <test>
-      <param name="input1" ftype="encodepeak" value="encode.broad.peak"/>
-      <output name="output1" ftype="tabix">
+      <param name="input1" ftype="trr" value="traj.trr"/>
+      <param name="__target_datatype__" value="dcd"/>
+      <output name="output1" ftype="dcd">
         <assert_contents>
-          <has_size value="110" delta="10"/>
+          <has_size value="540"/>
         </assert_contents>
       </output>
     </test>
+    <!-- to trr-->
+    <test>
+      <param name="input1" ftype="xtc" value="traj.xtc"/>
+      <param name="__target_datatype__" value="trr"/>
+      <output name="output1" ftype="trr" value="traj.trr"/>
+    </test>
     <test>
-      <param name="input1" ftype="gff" value="gff_filter_by_feature_count_out2.gff"/>
-      <output name="output1" ftype="tabix" value="tabix_filter_by_feature_count_out2.tabix"/>
+      <param name="input1" ftype="dcd" value="traj.dcd"/>
+      <param name="__target_datatype__" value="trr"/>
+      <output name="output1" ftype="trr" value="traj.trr"/>
     </test>
+    <!-- to xtc-->
     <test>
-      <param name="input1" ftype="interval" value="2.interval"/>
-      <output name="output1" ftype="tabix" value="2.tabix"/>
+      <param name="input1" ftype="xtc" value="traj.xtc"/>
+      <param name="__target_datatype__" value="xtc"/>
+      <output name="output1" ftype="xtc" value="traj.xtc"/>
     </test>
     <test>
-      <param name="input1" ftype="vcf" value="vcf_to_maf_in.vcf"/>
-      <output name="output1" ftype="tabix" value="tabix_to_maf_in.tabix"/>
+      <param name="input1" ftype="trr" value="traj.trr"/>
+      <param name="__target_datatype__" value="xtc"/>
+      <output name="output1" ftype="xtc" value="traj.xtc"/>
     </test>
   </tests>
   <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/len_to_linecount.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/len_to_linecount.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_fped_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_fped_converter.py`

 * *Files 9% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 # use numeric alleles internally
 
 import os
 import sys
 import time
 
 prog = os.path.split(sys.argv[0])[-1]
-myversion = 'Oct 10 2009'
+myversion = "Oct 10 2009"
 
 galhtmlprefix = """<?xml version="1.0" encoding="utf-8" ?>
 <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
 <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
 <head>
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
 <meta name="generator" content="Galaxy %s tool output - see http://getgalaxy.org" />
@@ -21,56 +21,55 @@
 </head>
 <body>
 <div class="document">
 """
 
 
 def timenow():
-    """return current time as a string
-    """
-    return time.strftime('%d/%m/%Y %H:%M:%S', time.localtime(time.time()))
+    """return current time as a string"""
+    return time.strftime("%d/%m/%Y %H:%M:%S", time.localtime(time.time()))
 
 
 def rgConv(inpedfilepath, outhtmlname, outfilepath):
     """convert linkage ped/map to fbat"""
-    recode = {'A': '1', 'C': '2', 'G': '3', 'T': '4', 'N': '0', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4'}
+    recode = {"A": "1", "C": "2", "G": "3", "T": "4", "N": "0", "0": "0", "1": "1", "2": "2", "3": "3", "4": "4"}
     basename = os.path.split(inpedfilepath)[-1]  # get basename
-    inmap = f'{inpedfilepath}.map'
-    inped = f'{inpedfilepath}.ped'
-    outf = f'{basename}.ped'  # note the fbat exe insists that this is the extension for the ped data
+    inmap = f"{inpedfilepath}.map"
+    inped = f"{inpedfilepath}.ped"
+    outf = f"{basename}.ped"  # note the fbat exe insists that this is the extension for the ped data
     outfpath = os.path.join(outfilepath, outf)  # where to write the fbat format file to
     try:
         mf = open(inmap)
     except Exception:
-        sys.exit(f'{prog} cannot open inmap file {inmap} - do you have permission?\n')
+        sys.exit(f"{prog} cannot open inmap file {inmap} - do you have permission?\n")
     try:
         rsl = [x.split()[1] for x in mf]
     except Exception:
-        sys.exit(f'## cannot parse {inmap}')
+        sys.exit(f"## cannot parse {inmap}")
     try:
         os.makedirs(outfilepath)
     except Exception:
         pass  # already exists
-    head = ' '.join(rsl)  # list of rs numbers
+    head = " ".join(rsl)  # list of rs numbers
     # TODO add anno to rs but fbat will prolly barf?
-    with open(inped) as pedf, open(outfpath, 'w', 2 ** 20) as o:
+    with open(inped) as pedf, open(outfpath, "w", 2**20) as o:
         o.write(head)
-        o.write('\n')
+        o.write("\n")
         for i, row in enumerate(pedf):
             if i == 0:
                 lrow = row.split()
                 try:
                     [int(x) for x in lrow[10:50]]  # look for non numeric codes
                 except Exception:
                     dorecode = 1
             if dorecode:
                 lrow = row.strip().split()
                 p = lrow[:6]
                 g = lrow[6:]
-                gc = [recode.get(z, '0') for z in g]
+                gc = [recode.get(z, "0") for z in g]
                 lrow = p + gc
                 row = f"{' '.join(lrow)}\n"
             o.write(row)
 
 
 def main():
     """call fbater
@@ -78,28 +77,28 @@
     so in and out are html files with data in extrafiles path
     <command>python '$__tool_directory__/rg_convert_lped_fped.py' '$input1/$input1.metadata.base_name'
     '$output1' '$output1.extra_files_path'
     </command>
     """
     nparm = 3
     if len(sys.argv) < nparm:
-        sys.exit('## %s called with %s - needs %d parameters \n' % (prog, sys.argv, nparm))
+        sys.exit("## %s called with %s - needs %d parameters \n" % (prog, sys.argv, nparm))
     inpedfilepath = sys.argv[1]
     outhtmlname = sys.argv[2]
     outfilepath = sys.argv[3]
     try:
         os.makedirs(outfilepath)
     except Exception:
         pass
     rgConv(inpedfilepath, outhtmlname, outfilepath)
     flist = os.listdir(outfilepath)
-    with open(outhtmlname, 'w') as f:
+    with open(outhtmlname, "w") as f:
         f.write(galhtmlprefix % prog)
-        print(f'## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}')  # becomes info
-        f.write(f'<div>## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}\n<ol>')
+        print(f"## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}")  # becomes info
+        f.write(f"<div>## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}\n<ol>")
         for data in flist:
             f.write(f'<li><a href="{os.path.split(data)[-1]}">{os.path.split(data)[-1]}</a></li>\n')
         f.write("</div></body></html>")
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_fped_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_fped_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_pbed_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_pbed_converter.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 import os
 import subprocess
 import sys
 import time
 
 prog = os.path.split(sys.argv[0])[-1]
-myversion = 'Oct 10 2009'
+myversion = "Oct 10 2009"
 
 galhtmlprefix = """<?xml version="1.0" encoding="utf-8" ?>
 <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
 <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
 <head>
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
 <meta name="generator" content="Galaxy %s tool output - see http://getgalaxy.org" />
@@ -24,25 +24,24 @@
 </head>
 <body>
 <div class="document">
 """
 
 
 def timenow():
-    """return current time as a string
-    """
-    return time.strftime('%d/%m/%Y %H:%M:%S', time.localtime(time.time()))
+    """return current time as a string"""
+    return time.strftime("%d/%m/%Y %H:%M:%S", time.localtime(time.time()))
 
 
-def getMissval(inped=''):
+def getMissval(inped=""):
     """
     read some lines...ugly hack - try to guess missing value
     should be N or 0 but might be . or -
     """
-    commonmissvals = {'N': 'N', '0': '0', 'n': 'n', '9': '9', '-': '-', '.': '.'}
+    commonmissvals = {"N": "N", "0": "0", "n": "n", "9": "9", "-": "-", ".": "."}
     try:
         f = open(inped)
     except Exception:
         return None  # signal no in file
     missval = None
     while missval is None:  # doggedly continue until we solve the mystery
         try:
@@ -52,60 +51,60 @@
         ll = line.split()[6:]  # ignore pedigree stuff
         for c in ll:
             if commonmissvals.get(c, None):
                 missval = c
                 f.close()
                 return missval
     if not missval:
-        missval = 'N'  # punt
+        missval = "N"  # punt
     f.close()
     return missval
 
 
 def rgConv(inpedfilepath, outhtmlname, outfilepath, plink):
-    """
-    """
-    pedf = f'{inpedfilepath}.ped'
+    """ """
+    pedf = f"{inpedfilepath}.ped"
     basename = os.path.split(inpedfilepath)[-1]  # get basename
     outroot = os.path.join(outfilepath, basename)
     missval = getMissval(inped=pedf)
     if not missval:
-        print(f'### lped_to_pbed_converter.py cannot identify missing value in {pedf}')
-        missval = '0'
-    subprocess.check_call([plink, '--noweb', '--file', inpedfilepath,
-                           '--make-bed', '--out', outroot,
-                           '--missing-genotype', missval], cwd=outfilepath)
+        print(f"### lped_to_pbed_converter.py cannot identify missing value in {pedf}")
+        missval = "0"
+    subprocess.check_call(
+        [plink, "--noweb", "--file", inpedfilepath, "--make-bed", "--out", outroot, "--missing-genotype", missval],
+        cwd=outfilepath,
+    )
 
 
 def main():
     """
     need to work with rgenetics composite datatypes
     so in and out are html files with data in extrafiles path
     <command>python '$__tool_directory__/lped_to_pbed_converter.py' '$input1/$input1.metadata.base_name'
     '$output1' '$output1.extra_files_path' '${GALAXY_DATA_INDEX_DIR}/rg/bin/plink'
     </command>
     """
     nparm = 4
     if len(sys.argv) < nparm:
-        sys.exit('## %s called with %s - needs %d parameters \n' % (prog, sys.argv, nparm))
+        sys.exit("## %s called with %s - needs %d parameters \n" % (prog, sys.argv, nparm))
     inpedfilepath = sys.argv[1]
     outhtmlname = sys.argv[2]
     outfilepath = sys.argv[3]
     try:
         os.makedirs(outfilepath)
     except Exception:
         pass
     plink = sys.argv[4]
     rgConv(inpedfilepath, outhtmlname, outfilepath, plink)
     flist = os.listdir(outfilepath)
-    with open(outhtmlname, 'w') as f:
+    with open(outhtmlname, "w") as f:
         f.write(galhtmlprefix % prog)
-        s = f'## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}'  # becomes info
+        s = f"## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}"  # becomes info
         print(s)
-        f.write(f'<div>{s}\n<ol>')
+        f.write(f"<div>{s}\n<ol>")
         for data in flist:
             f.write(f'<li><a href="{os.path.split(data)[-1]}">{os.path.split(data)[-1]}</a></li>\n')
         f.write("</ol></div></div></body></html>")
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/lped_to_pbed_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/lped_to_pbed_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_fasta_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_fasta_converter.py`

 * *Files 16% similar despite different names*

```diff
@@ -10,24 +10,31 @@
 assert sys.version_info[:2] >= (2, 6)
 
 
 def __main__():
     output_name = sys.argv.pop(1)
     input_name = sys.argv.pop(1)
     count = 0
-    with open(output_name, 'w') as out, open(input_name) as infile:
+    with open(output_name, "w") as out, open(input_name) as infile:
         for count, block in enumerate(bx.align.maf.Reader(infile)):
             spec_counts = {}
             for c in block.components:
                 spec, chrom = maf_utilities.src_split(c.src)
                 if spec not in spec_counts:
                     spec_counts[spec] = 0
                 else:
                     spec_counts[spec] += 1
-                out.write("%s\n" % maf_utilities.get_fasta_header(c, {'block_index': count, 'species': spec, 'sequence_index': spec_counts[spec]}, suffix="%s_%i_%i" % (spec, count, spec_counts[spec])))
+                out.write(
+                    "%s\n"
+                    % maf_utilities.get_fasta_header(
+                        c,
+                        {"block_index": count, "species": spec, "sequence_index": spec_counts[spec]},
+                        suffix="%s_%i_%i" % (spec, count, spec_counts[spec]),
+                    )
+                )
                 out.write(f"{c.text}\n")
             out.write("\n")
     print("%i MAF blocks converted to FASTA." % (count))
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_fasta_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_fasta_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_interval_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_interval_converter.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,23 +11,31 @@
 
 
 def __main__():
     output_name = sys.argv.pop(1)
     input_name = sys.argv.pop(1)
     species = sys.argv.pop(1)
     count = 0
-    with open(output_name, 'w') as out:
+    with open(output_name, "w") as out:
         # write interval header line
         out.write("#chrom\tstart\tend\tstrand\n")
         try:
             with open(input_name) as fh:
                 for block in bx.align.maf.Reader(fh):
                     for c in maf_utilities.iter_components_by_src_start(block, species):
                         if c is not None:
-                            out.write("%s\t%i\t%i\t%s\n" % (maf_utilities.src_split(c.src)[-1], c.get_forward_strand_start(), c.get_forward_strand_end(), c.strand))
+                            out.write(
+                                "%s\t%i\t%i\t%s\n"
+                                % (
+                                    maf_utilities.src_split(c.src)[-1],
+                                    c.get_forward_strand_start(),
+                                    c.get_forward_strand_end(),
+                                    c.strand,
+                                )
+                            )
                             count += 1
         except Exception as e:
             print(f"There was a problem processing your input: {e}", file=sys.stderr)
     print("%i MAF blocks converted to Genomic Intervals for species %s." % (count, species))
 
 
 if __name__ == "__main__":
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/maf_to_interval_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/maf_to_interval_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/mol2_to_mol_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/mol2_to_mol_converter.xml`

 * *Files 1% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/mol2_to_mol_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/mol2_to_mol_converter.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_mol2_to_mol" name="MOL2 to MOL" version="2.4.1">
+<tool id="CONVERTER_mol2_to_mol" name="Convert MOL2 to MOL" version="2.4.1">
   <description/>
   <parallelism method="multi" split_inputs="input" split_mode="to_size" split_size="10000" shared_inputs="" merge_outputs="output"/>
   <requirements>
     <requirement type="package" version="2.4.1">openbabel</requirement>
   </requirements>
   <command><![CDATA[
         obabel -imol2 '${input}' -omol -O '${output}' -e 2>&1
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/molecules_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/molecules_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/neostorezip_to_neostore_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/neostorezip_to_neostore_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/parquet_to_csv_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/parquet_to_csv_converter.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 #!/usr/bin/env python
 """
 Input: parquet
 Output: csv
 """
 import os
 import sys
+
 try:
     import pyarrow.csv
     import pyarrow.parquet
 except ImportError:
     pyarrow = None
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/parquet_to_csv_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/parquet_to_csv_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_ldreduced_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_ldreduced_converter.py`

 * *Files 25% similar despite different names*

```diff
@@ -18,54 +18,64 @@
 <title></title>
 <link rel="stylesheet" href="/static/style/base.css" type="text/css" />
 </head>
 <body>
 <div class="document">
 """
 
-plinke = 'plink'
+plinke = "plink"
 
 
 def timenow():
-    """return current time as a string
-    """
-    return time.strftime('%d/%m/%Y %H:%M:%S', time.localtime(time.time()))
+    """return current time as a string"""
+    return time.strftime("%d/%m/%Y %H:%M:%S", time.localtime(time.time()))
 
 
-def pruneLD(plinktasks=None, cd='./', vclbase=None):
-    """
-    """
+def pruneLD(plinktasks=None, cd="./", vclbase=None):
+    """ """
     plinktasks = plinktasks or []
     vclbase = vclbase or []
-    alog = ['## Rgenetics: http://rgenetics.org Galaxy Tools rgQC.py Plink pruneLD runner\n']
-    with tempfile.NamedTemporaryFile(mode='r+') as plog:
+    alog = ["## Rgenetics: http://rgenetics.org Galaxy Tools rgQC.py Plink pruneLD runner\n"]
+    with tempfile.NamedTemporaryFile(mode="r+") as plog:
         for task in plinktasks:  # each is a list
             vcl = vclbase + task
             subprocess.check_call(vcl, stdout=plog, stderr=plog, cwd=cd)
             try:
                 plog.seek(0)
-                lplog = [elem for elem in plog.readlines() if elem.find('Pruning SNP') == -1]
+                lplog = [elem for elem in plog.readlines() if elem.find("Pruning SNP") == -1]
                 alog += lplog
-                alog.append('\n')
+                alog.append("\n")
             except Exception:
-                alog.append(f"### {timenow()} Strange - no std out from plink when running command line\n{' '.join(vcl)}\n")
+                alog.append(
+                    f"### {timenow()} Strange - no std out from plink when running command line\n{' '.join(vcl)}\n"
+                )
     return alog
 
 
-def makeLDreduced(basename, infpath=None, outfpath=None, plinke='plink', forcerebuild=False, returnFname=False,
-                  winsize="60", winmove="40", r2thresh="0.1"):
-    """ not there so make and leave in output dir for post job hook to copy back into input extra files path for next time
-    """
+def makeLDreduced(
+    basename,
+    infpath=None,
+    outfpath=None,
+    plinke="plink",
+    forcerebuild=False,
+    returnFname=False,
+    winsize="60",
+    winmove="40",
+    r2thresh="0.1",
+):
+    """not there so make and leave in output dir for post job hook to copy back into input extra files path for next time"""
     outbase = os.path.join(outfpath, basename)
     inbase = os.path.join(infpath)
     plinktasks = []
-    vclbase = [plinke, '--noweb']
-    plinktasks += [['--bfile', inbase, f'--indep-pairwise {winsize} {winmove} {r2thresh}', f'--out {outbase}'],
-                   ['--bfile', inbase, f'--extract {outbase}.prune.in --make-bed --out {outbase}']]
-    vclbase = [plinke, '--noweb']
+    vclbase = [plinke, "--noweb"]
+    plinktasks += [
+        ["--bfile", inbase, f"--indep-pairwise {winsize} {winmove} {r2thresh}", f"--out {outbase}"],
+        ["--bfile", inbase, f"--extract {outbase}.prune.in --make-bed --out {outbase}"],
+    ]
+    vclbase = [plinke, "--noweb"]
     pruneLD(plinktasks=plinktasks, cd=outfpath, vclbase=vclbase)
 
 
 def main():
     """
     need to work with rgenetics composite datatypes
     so in and out are html files with data in extrafiles path
@@ -75,37 +85,46 @@
         <command>
             python '$__tool_directory__/pbed_ldreduced_converter.py' '$input1.extra_files_path/$input1.metadata.base_name' '$winsize' '$winmove' '$r2thresh'
             '$output1' '$output1.files_path' 'plink'
         </command>
     """
     nparm = 7
     if len(sys.argv) < nparm:
-        sys.stderr.write('## %s called with %s - needs %d parameters \n' % (prog, sys.argv, nparm))
+        sys.stderr.write("## %s called with %s - needs %d parameters \n" % (prog, sys.argv, nparm))
         sys.exit(1)
     inpedfilepath = sys.argv[1]
     base_name = os.path.split(inpedfilepath)[-1]
     winsize = sys.argv[2]
     winmove = sys.argv[3]
     r2thresh = sys.argv[4]
     outhtmlname = sys.argv[5]
     outfilepath = sys.argv[6]
     try:
         os.makedirs(outfilepath)
     except Exception:
         pass
     plink = sys.argv[7]
-    makeLDreduced(base_name, infpath=inpedfilepath, outfpath=outfilepath, plinke=plink, forcerebuild=False, returnFname=False,
-                  winsize=winsize, winmove=winmove, r2thresh=r2thresh)
+    makeLDreduced(
+        base_name,
+        infpath=inpedfilepath,
+        outfpath=outfilepath,
+        plinke=plink,
+        forcerebuild=False,
+        returnFname=False,
+        winsize=winsize,
+        winmove=winmove,
+        r2thresh=r2thresh,
+    )
     flist = os.listdir(outfilepath)
-    with open(outhtmlname, 'w') as f:
+    with open(outhtmlname, "w") as f:
         f.write(galhtmlprefix % prog)
-        s1 = f'## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}'  # becomes info
-        s2 = f'Input {base_name}, winsize={winsize}, winmove={winmove}, r2thresh={r2thresh}'
-        print(f'{s1} {s2}')
-        f.write(f'<div>{s1}\n{s2}\n<ol>')
+        s1 = f"## Rgenetics: http://rgenetics.org Galaxy Tools {prog} {timenow()}"  # becomes info
+        s2 = f"Input {base_name}, winsize={winsize}, winmove={winmove}, r2thresh={r2thresh}"
+        print(f"{s1} {s2}")
+        f.write(f"<div>{s1}\n{s2}\n<ol>")
         for data in flist:
             f.write(f'<li><a href="{os.path.split(data)[-1]}">{os.path.split(data)[-1]}</a></li>\n')
         f.write("</div></body></html>")
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_ldreduced_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_ldreduced_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_to_lped_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_to_lped_converter.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,17 +6,16 @@
 # that's a lot of converters
 
 import os
 import subprocess
 import sys
 import time
 
-
 prog = os.path.split(sys.argv[0])[-1]
-myversion = 'Oct 10 2009'
+myversion = "Oct 10 2009"
 
 galhtmlprefix = """<?xml version="1.0" encoding="utf-8" ?>
 <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
 <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
 <head>
 <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
 <meta name="generator" content="Galaxy %s tool output - see http://getgalaxy.org" />
@@ -25,53 +24,51 @@
 </head>
 <body>
 <div class="document">
 """
 
 
 def timenow():
-    """return current time as a string
-    """
-    return time.strftime('%d/%m/%Y %H:%M:%S', time.localtime(time.time()))
+    """return current time as a string"""
+    return time.strftime("%d/%m/%Y %H:%M:%S", time.localtime(time.time()))
 
 
 def rgConv(inpedfilepath, outhtmlname, outfilepath, plink):
-    """
-    """
+    """ """
     basename = os.path.split(inpedfilepath)[-1]  # get basename
     outroot = os.path.join(outfilepath, basename)
-    subprocess.check_call([plink, '--noweb', '--bfile', inpedfilepath, '--recode', '--out', outroot], cwd=outfilepath)
+    subprocess.check_call([plink, "--noweb", "--bfile", inpedfilepath, "--recode", "--out", outroot], cwd=outfilepath)
 
 
 def main():
     """
     need to work with rgenetics composite datatypes
     so in and out are html files with data in extrafiles path
     <command>python '$__tool_directory__/pbed_to_lped_converter.py' '$input1/$input1.metadata.base_name'
     '$output1' '$output1.extra_files_path' '${GALAXY_DATA_INDEX_DIR}/rg/bin/plink'
     </command>
     """
     nparm = 4
     if len(sys.argv) < nparm:
-        sys.exit('PBED to LPED converter called with %s - needs %d parameters \n' % (sys.argv, nparm))
+        sys.exit("PBED to LPED converter called with %s - needs %d parameters \n" % (sys.argv, nparm))
     inpedfilepath = sys.argv[1]
     outhtmlname = sys.argv[2]
     outfilepath = sys.argv[3]
     try:
         os.makedirs(outfilepath)
     except Exception:
         pass
     plink = sys.argv[4]
     rgConv(inpedfilepath, outhtmlname, outfilepath, plink)
     flist = os.listdir(outfilepath)
-    with open(outhtmlname, 'w') as f:
+    with open(outhtmlname, "w") as f:
         f.write(galhtmlprefix % prog)
-        s = f'## Rgenetics: http://bitbucket.org/rgalaxy Galaxy Tools {prog} {timenow()}'  # becomes info
+        s = f"## Rgenetics: http://bitbucket.org/rgalaxy Galaxy Tools {prog} {timenow()}"  # becomes info
         print(s)
-        f.write(f'<div>{s}\n<ol>')
+        f.write(f"<div>{s}\n<ol>")
         for data in flist:
             f.write(f'<li><a href="{os.path.split(data)[-1]}">{os.path.split(data)[-1]}</a></li>\n')
         f.write("</ol></div></div></body></html>")
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pbed_to_lped_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pbed_to_lped_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pdb_to_gro.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pdb_to_gro.xml`

 * *Files 2% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pdb_to_gro.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pdb_to_gro.xml`

```diff
@@ -1,9 +1,9 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_Pdb_to_Gro_0" name="PDB to GRO" version="1.0.0" hidden="true" profile="20.09">
+<tool id="CONVERTER_Pdb_to_Gro_0" name="Convert PDB to GRO" version="1.0.0" hidden="true" profile="20.09">
   <requirements>
     <requirement type="package" version="2020.4">gromacs</requirement>
   </requirements>
   <command detect_errors="exit_code"><![CDATA[
         ln -s '$input1' ./input.pdb &&
         gmx editconf
             -f ./input.pdb
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,43 +1,46 @@
 #!/usr/bin/env python
 # Dan Blankenberg
 
 import sys
 
 assert sys.version_info[:2] >= (2, 6)
-HEADER_STARTS_WITH = ('@')
+HEADER_STARTS_WITH = "@"
 
 
 def __main__():
     input_name = sys.argv[1]
     output_name = sys.argv[2]
     skipped_lines = 0
     first_skipped_line = 0
     header_lines = 0
     i = 0
-    with open(input_name) as fh, open(output_name, 'w') as out:
+    with open(input_name) as fh, open(output_name, "w") as out:
         for i, line in enumerate(fh):
-            line = line.rstrip('\r\n')
+            line = line.rstrip("\r\n")
             if line:
                 if line.startswith(HEADER_STARTS_WITH):
                     header_lines += 1
                 else:
                     try:
-                        elems = line.split('\t')
-                        out.write(f'{elems[0]}\t{int(elems[1]) - 1}\t{elems[2]}\t{elems[4]}\t0\t{elems[3]}\n')
+                        elems = line.split("\t")
+                        out.write(f"{elems[0]}\t{int(elems[1]) - 1}\t{elems[2]}\t{elems[4]}\t0\t{elems[3]}\n")
                     except Exception as e:
                         print(e)
                         skipped_lines += 1
                         if not first_skipped_line:
                             first_skipped_line = i + 1
             else:
                 skipped_lines += 1
                 if not first_skipped_line:
                     first_skipped_line = i + 1
     info_msg = "%i lines converted to BED.  " % (i + 1 - skipped_lines)
     if skipped_lines > 0:
-        info_msg += "Skipped %d blank/comment/invalid lines starting with line #%d." % (skipped_lines, first_skipped_line)
+        info_msg += "Skipped %d blank/comment/invalid lines starting with line #%d." % (
+            skipped_lines,
+            first_skipped_line,
+        )
     print(info_msg)
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/picard_interval_list_to_bed6_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 
 import optparse
 
 from bx.interval_index_file import Indexes
 
 
 def main():
-
     # Read options, args.
     parser = optparse.OptionParser()
     (options, args) = parser.parse_args()
     input_fname, output_fname = args
 
     # Do conversion.
     index = Indexes()
@@ -25,13 +24,13 @@
         for line in in_fh:
             chrom, start = line.split()[0:2]
             # Pileup format is 1-based.
             start = int(start) - 1
             index.add(chrom, start, start + 1, offset)
             offset += len(line)
 
-    with open(output_fname, 'wb') as out:
+    with open(output_fname, "wb") as out:
         index.write(out)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/pileup_to_interval_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,18 +7,18 @@
 import re
 import sys
 
 assert sys.version_info[:2] >= (2, 4)
 
 
 def __main__():
-    with open(sys.argv[1]) as infile, open(sys.argv[2], 'w') as outfile:
+    with open(sys.argv[1]) as infile, open(sys.argv[2], "w") as outfile:
         for line in infile:
             line = line.rstrip()
-            if line and not line.startswith('#'):
-                fields = line.split('\t')
+            if line and not line.startswith("#"):
+                fields = line.split("\t")
                 # make sure the 2nd field (taxonomy) ends with a ;
                 outfile.write(f"{fields[0]}\t{re.sub(';$', '', fields[1])};\n")
 
 
 if __name__ == "__main__":
     __main__()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/ref_to_seq_taxonomy_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/sam_to_bigwig_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/sam_to_bigwig_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/sam_to_unsorted_bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/sam_to_unsorted_bam.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/smi_to_mol_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/wig_to_bigwig_converter.xml`

 * *Files 18% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/smi_to_mol_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/wig_to_bigwig_converter.xml`

```diff
@@ -1,30 +1,24 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_SMILES_to_MOL" name="SMILES to MOL" version="2.4.1">
-  <description/>
-  <parallelism method="multi" split_inputs="input" split_mode="to_size" split_size="10000" shared_inputs="" merge_outputs="output"/>
+<tool id="CONVERTER_wig_to_bigwig" name="Convert Wiggle to BigWig" version="1.0.1" hidden="true">
+  <!-- Used internally to generate track indexes -->
   <requirements>
-    <requirement type="package" version="2.4.1">openbabel</requirement>
+    <requirement type="package" version="357">ucsc-wigtobigwig</requirement>
   </requirements>
   <command><![CDATA[
-        obabel -ismi '${input}' -omol -O '${output}' -e 2>&1
+        grep -v "^track" '$input' | wigToBigWig -clip stdin '$chromInfo' '$output'
+        2>&1 || echo "Error running Wiggle to BigWig converter." >&2
 ]]></command>
   <inputs>
-    <param name="input" type="data" format="smi" label="Molecules in SMILES format"/>
+    <param format="wig" name="input" type="data" label="Choose wiggle"/>
   </inputs>
   <outputs>
-    <data name="output" format="mol"/>
+    <data format="bigwig" name="output"/>
   </outputs>
   <tests>
     <test>
-      <param name="input" ftype="smi" value="drugbank_drugs.smi"/>
-      <output name="output" ftype="mol">
-        <assert_contents>
-          <has_text text="OpenBabel"/>
-          <has_text text="M  END"/>
-        </assert_contents>
-      </output>
+      <param name="input" ftype="wig" value="aggregate_binned_scores_3.wig" dbkey="hg17"/>
+      <output name="output" ftype="bigwig" value="aggregate_binned_scores_3.bigwig"/>
     </test>
   </tests>
-  <help><![CDATA[
-]]></help>
+  <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/smi_to_smi_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/smi_to_smi_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/tabular_csv.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_csv.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,35 +8,35 @@
 import argparse
 import csv
 
 
 def main():
     usage = "Usage: %prog [options]"
     parser = argparse.ArgumentParser(usage=usage)
-    parser.add_argument('-f', '--from-tabular', action='store_true', default=False, dest='fromtabular')
-    parser.add_argument('-i', '--input', type=str, dest='input')
-    parser.add_argument('-o', '--output', type=str, dest='output')
+    parser.add_argument("-f", "--from-tabular", action="store_true", default=False, dest="fromtabular")
+    parser.add_argument("-i", "--input", type=str, dest="input")
+    parser.add_argument("-o", "--output", type=str, dest="output")
     args = parser.parse_args()
     input_fname = args.input
     output_fname = args.output
     if args.fromtabular:
         convert_to_csv(input_fname, output_fname)
     else:
         convert_to_tsv(input_fname, output_fname)
 
 
 def convert_to_tsv(input_fname, output_fname):
-    with open(input_fname, newline="") as csvfile, open(output_fname, 'w') as ofh:
+    with open(input_fname, newline="") as csvfile, open(output_fname, "w") as ofh:
         reader = csv.reader(csvfile)
         for line in reader:
-            ofh.write('\t'.join(line) + '\n')
+            ofh.write("\t".join(line) + "\n")
 
 
 def convert_to_csv(input_fname, output_fname):
-    with open(input_fname) as tabfile, open(output_fname, 'w', newline='') as ofh:
-        writer = csv.writer(ofh, delimiter=',')
+    with open(input_fname) as tabfile, open(output_fname, "w", newline="") as ofh:
+        writer = csv.writer(ofh, delimiter=",")
         for line in tabfile.readlines():
-            writer.writerow(line.strip().split('\t'))
+            writer.writerow(line.strip().split("\t"))
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/tabular_to_csv.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_to_csv.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/tar_to_directory.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/tar_to_directory.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/to_coordinate_sorted_bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/to_qname_sorted_bam.xml`

 * *Files 16% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/to_coordinate_sorted_bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/to_qname_sorted_bam.xml`

```diff
@@ -1,27 +1,28 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_bam_to_coodinate_sorted_bam" name="Convert BAM to coordinate-sorted BAM" version="1.0.0" hidden="true" profile="18.01">
+<tool id="CONVERTER_bam_to_qname_sorted_bam" name="Convert BAM to queryname-sorted BAM" version="1.0.0" hidden="true" profile="18.01">
   <requirements>
     <requirement type="package" version="1.6">samtools</requirement>
   </requirements>
   <command><![CDATA[
          samtools sort
             -@ \${GALAXY_SLOTS:-1}
             -o '${output}'
+            -n
             -O bam
             -T dataset
             '${input}'
     ]]></command>
   <inputs>
-    <param format="sam,unsorted.bam,qname_sorted.bam" name="input" type="data" label="Choose a BAM native or queryname sortedfile"/>
+    <param format="sam,unsorted.bam" name="input" type="data" label="Choose a BAM native or queryname sortedfile"/>
   </inputs>
   <outputs>
-    <data format="bam" name="output"/>
+    <data format="qname_sorted.bam" name="output"/>
   </outputs>
   <tests>
     <test>
       <param name="input" ftype="sam" value="bfast_out1.sam"/>
-      <output name="output" ftype="bam" value="bfast_out1.bam"/>
+      <output name="output" ftype="qname_sorted.bam" value="bfast_out1.qname_sorted.bam"/>
     </test>
   </tests>
   <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_bgzip_to_tabix_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip_converter.xml`

 * *Files 16% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_bgzip_to_tabix_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip_converter.xml`

```diff
@@ -1,21 +1,27 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_vcf_bgzip_to_tabix_0" name="Convert BGZ VCF to tabix" version="1.0.1" hidden="true" profile="16.04">
+<tool id="CONVERTER_vcf_to_vcf_bgzip_0" name="Convert VCF to VCF_BGZIP" version="1.0.3" hidden="true" profile="16.04">
   <!-- <description>__NOT_USED_CURRENTLY_FOR_CONVERTERS__</description> -->
   <requirements>
-    <requirement type="package" version="0.15.4">pysam</requirement>
+    <requirement type="package" version="1.14">htslib</requirement>
   </requirements>
-  <command>python '$__tool_directory__/interval_to_tabix_converter.py' -P 'vcf' '' '$input1' '$output1'</command>
+  <command><![CDATA[
+        bgzip -c '$input1' > '$output1'
+    ]]></command>
   <inputs>
-    <param format="vcf_bgzip" name="input1" type="data" label="Choose BGZIP'd VCF file"/>
+    <param format="vcf" name="input1" type="data" label="Choose Vcf file"/>
   </inputs>
   <outputs>
-    <data format="tabix" name="output1"/>
+    <data format="vcf_bgzip" name="output1"/>
   </outputs>
   <tests>
     <test>
-      <param name="input1" ftype="vcf_bgzip" value="vcf_bgzip_to_maf_in.vcf_bgzip"/>
-      <output name="output1" ftype="tabix" value="tabix_to_maf_in_to_tabix.tabix"/>
+      <param name="input1" ftype="vcf" value="vcf_to_maf_in.vcf"/>
+      <output name="output1" ftype="vcf_bgzip" value="vcf_bgzip_to_maf_in.vcf_bgzip" decompress="true">
+        <assert_contents>
+          <has_size value="717" delta="10"/>
+        </assert_contents>
+      </output>
     </test>
   </tests>
   <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.py`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/vcf_to_interval_index_converter.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/wiggle_to_simple_converter.xml`

 * *Files 21% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/vcf_to_vcf_bgzip_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/wiggle_to_simple_converter.xml`

```diff
@@ -1,22 +1,22 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_vcf_to_vcf_bgzip_0" name="Convert VCF to VCF_BGZIP" version="1.0.2" hidden="true" profile="16.04">
+<tool id="CONVERTER_wiggle_to_interval_0" name="Convert Wiggle to Interval" version="1.0.1" profile="16.04">
   <!-- <description>__NOT_USED_CURRENTLY_FOR_CONVERTERS__</description> -->
+  <!-- Used on the metadata edit page. -->
   <requirements>
-    <requirement type="package" version="0.15.4">pysam</requirement>
-    <requirement type="package" version="8.25">coreutils</requirement>
+    <requirement type="package" version="19.9">galaxy-util</requirement>
+    <requirement type="package" version="0.8.6">bx-python</requirement>
   </requirements>
-  <command>python '$__tool_directory__/vcf_to_vcf_bgzip.py' '$input1' '$output1'</command>
+  <command>python '$__tool_directory__/wiggle_to_simple_converter.py' '$input' '$out_file1'</command>
   <inputs>
-    <param format="vcf" name="input1" type="data" label="Choose Vcf file"/>
+    <param format="wig" name="input" type="data" label="Convert"/>
   </inputs>
   <outputs>
-    <data format="vcf_bgzip" name="output1"/>
+    <data format="interval" name="out_file1"/>
   </outputs>
   <tests>
     <test>
-      <param name="input1" ftype="vcf" value="vcf_to_maf_in.vcf"/>
-      <output name="output1" ftype="vcf_bgzip" value="vcf_bgzip_to_maf_in.vcf_bgzip"/>
+      <param name="input" ftype="wig" value="aggregate_binned_scores_3.wig"/>
+      <output name="out_file1" ftype="interval" value="aggregate_binned_scores_3.interval"/>
     </test>
   </tests>
-  <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/wiggle_to_simple_converter.py` & `galaxy-data-23.0.1/galaxy/datatypes/converters/wiggle_to_simple_converter.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,23 +8,25 @@
 
 import sys
 
 import bx.wiggle
 
 from galaxy.util.ucsc import (
     UCSCLimitException,
-    UCSCOutWrapper
+    UCSCOutWrapper,
 )
 
 
 def main():
     with open(sys.argv[1]) as in_file, open(sys.argv[2], "w") as out_file:
         try:
             for fields in bx.wiggle.IntervalReader(UCSCOutWrapper(in_file)):
                 out_file.write("%s\n" % "\t".join(map(str, fields)))
         except UCSCLimitException:
             # Wiggle data was truncated, at the very least need to warn the user.
-            sys.stderr.write('Encountered message from UCSC: "Reached output limit of 100000 data values", so be aware your data was truncated.')
+            sys.stderr.write(
+                'Encountered message from UCSC: "Reached output limit of 100000 data values", so be aware your data was truncated.'
+            )
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/wiggle_to_simple_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_to_dbnsfp.xml`

 * *Files 26% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/converters/wiggle_to_simple_converter.xml` & `galaxy-data-23.0.1/galaxy/datatypes/converters/tabular_to_dbnsfp.xml`

```diff
@@ -1,22 +1,25 @@
 <?xml version="1.0" encoding="utf-8"?>
-<tool id="CONVERTER_wiggle_to_interval_0" name="Wiggle to Interval" version="1.0.1" profile="16.04">
-  <!-- <description>__NOT_USED_CURRENTLY_FOR_CONVERTERS__</description> -->
-  <!-- Used on the metadata edit page. -->
+<tool id="tabular_to_dbnsfp" name="Convert tabular to dbnsfp" version="1.0.2" profile="16.04">
+  <description/>
   <requirements>
-    <requirement type="package" version="19.9">galaxy-util</requirement>
-    <requirement type="package" version="0.8.6">bx-python</requirement>
+    <requirement type="package" version="1.16">htslib</requirement>
   </requirements>
-  <command>python '$__tool_directory__/wiggle_to_simple_converter.py' '$input' '$out_file1'</command>
+  <command><![CDATA[
+        mkdir -p '$dbnsfp.extra_files_path' &&
+        bgzip -c '$input' > '$dbnsfp.extra_files_path/dbNSFP.gz' &&
+        tabix -s 1 -b 2 -e 2 '$dbnsfp.extra_files_path/dbNSFP.gz'
+    ]]></command>
   <inputs>
-    <param format="wig" name="input" type="data" label="Convert"/>
+    <param format="tabular" name="input" type="data" label="Choose a dbnsfp tabular file"/>
   </inputs>
   <outputs>
-    <data format="interval" name="out_file1"/>
+    <data format="snpsiftdbnsfp" name="dbnsfp"/>
   </outputs>
   <tests>
     <test>
-      <param name="input" ftype="wig" value="aggregate_binned_scores_3.wig"/>
-      <output name="out_file1" ftype="interval" value="aggregate_binned_scores_3.interval"/>
+      <param name="input" ftype="tabular" value="vcf2pgSnp_input.vcf"/>
+      <output name="dbnsfp" ftype="snpsiftdbnsfp" value="vcf2pgSnp_input.vcf.snpsiftdbnsfp"/>
     </test>
   </tests>
+  <help/>
 </tool>
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/data.py` & `galaxy-data-23.0.1/galaxy/datatypes/data.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,33 @@
-import abc
 import logging
 import mimetypes
 import os
 import shutil
 import string
 import tempfile
 from inspect import isclass
 from typing import (
     Any,
+    Callable,
     Dict,
+    Generator,
+    IO,
+    Iterable,
     List,
     Optional,
     Tuple,
     TYPE_CHECKING,
+    Union,
 )
 
 from markupsafe import escape
+from typing_extensions import (
+    Literal,
+    Protocol,
+)
 
 from galaxy import util
 from galaxy.datatypes.metadata import (
     MetadataElement,  # import directly to maintain ease of use in Datatype class definitions
 )
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
@@ -28,102 +36,123 @@
 from galaxy.exceptions import ObjectNotFound
 from galaxy.util import (
     compression_utils,
     file_reader,
     FILENAME_VALID_CHARS,
     inflector,
     iter_start_of_line,
-    smart_str,
     unicodify,
 )
 from galaxy.util.bunch import Bunch
+from galaxy.util.compression_utils import FileObjType
+from galaxy.util.markdown import (
+    indicate_data_truncated,
+    literal_via_fence,
+)
 from galaxy.util.sanitize_html import sanitize_html
 from galaxy.util.zipstream import ZipstreamWrapper
-from . import dataproviders as p_dataproviders
-from . import metadata
+from . import (
+    dataproviders as p_dataproviders,
+    metadata,
+)
 
 if TYPE_CHECKING:
-    from galaxy.model import DatasetInstance
+    from galaxy.datatypes.display_applications.application import DisplayApplication
+    from galaxy.datatypes.registry import Registry
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+        HistoryDatasetCollectionAssociation,
+    )
 
 XSS_VULNERABLE_MIME_TYPES = [
-    'image/svg+xml',  # Unfiltered by Galaxy and may contain JS that would be executed by some browsers.
-    'application/xml',  # Some browsers will evalute SVG embedded JS in such XML documents.
+    "image/svg+xml",  # Unfiltered by Galaxy and may contain JS that would be executed by some browsers.
+    "application/xml",  # Some browsers will evalute SVG embedded JS in such XML documents.
 ]
-DEFAULT_MIME_TYPE = 'text/plain'  # Vulnerable mime types will be replaced with this.
+DEFAULT_MIME_TYPE = "text/plain"  # Vulnerable mime types will be replaced with this.
 
 log = logging.getLogger(__name__)
 
 # Valid first column and strand column values vor bed, other formats
-col1_startswith = ['chr', 'chl', 'groupun', 'reftig_', 'scaffold', 'super_', 'vcho']
-valid_strand = ['+', '-', '.']
+col1_startswith = ["chr", "chl", "groupun", "reftig_", "scaffold", "super_", "vcho"]
+valid_strand = ["+", "-", "."]
 
 DOWNLOAD_FILENAME_PATTERN_DATASET = "Galaxy${hid}-[${name}].${ext}"
 DOWNLOAD_FILENAME_PATTERN_COLLECTION_ELEMENT = "Galaxy${hdca_hid}-[${hdca_name}__${element_identifier}].${ext}"
 DEFAULT_MAX_PEEK_SIZE = 1000000  # 1 MB
 
 Headers = Dict[str, Any]
 
 
+class GeneratePrimaryFileDataset(Protocol):
+    extra_files_path: str
+    metadata: Any
+
+
 class DatatypeConverterNotFoundException(Exception):
     pass
 
 
 class DatatypeValidation:
-
-    def __init__(self, state, message):
+    def __init__(self, state: str, message: str) -> None:
         self.state = state
         self.message = message
 
     @staticmethod
-    def validated():
+    def validated() -> "DatatypeValidation":
         return DatatypeValidation("ok", "Dataset validated by datatype validator.")
 
     @staticmethod
-    def invalid(message):
+    def invalid(message: str) -> "DatatypeValidation":
         return DatatypeValidation("invalid", message)
 
     @staticmethod
-    def unvalidated():
+    def unvalidated() -> "DatatypeValidation":
         return DatatypeValidation("unknown", "Dataset validation unimplemented for this datatype.")
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return f"DatatypeValidation[state={self.state},message={self.message}]"
 
 
-def validate(dataset_instance):
+def validate(dataset_instance: "DatasetInstance") -> DatatypeValidation:
     try:
         datatype_validation = dataset_instance.datatype.validate(dataset_instance)
     except Exception as e:
         datatype_validation = DatatypeValidation.invalid(f"Problem running datatype validation method [{str(e)}]")
     return datatype_validation
 
 
-def get_params_and_input_name(converter, deps, target_context=None):
+def get_params_and_input_name(
+    converter, deps: Optional[Dict], target_context: Optional[Dict] = None
+) -> Tuple[Dict, str]:
     # Generate parameter dictionary
     params = {}
     # determine input parameter name and add to params
-    input_name = 'input1'
+    input_name = "input1"
     for key, value in converter.inputs.items():
         if deps and value.name in deps:
             params[value.name] = deps[value.name]
-        elif value.type == 'data':
+        elif value.type == "data":
             input_name = key
+        elif value.optional:
+            params[value.name] = None
 
     # add potentially required/common internal tool parameters e.g. '__job_resource'
     if target_context:
         for key, value in target_context.items():
-            if key.startswith('__'):
+            if key.startswith("__"):
                 params[key] = value
     return params, input_name
 
 
-class DataMeta(abc.ABCMeta):
+class DataMeta(type):
     """
     Metaclass for Data class.  Sets up metadata spec.
     """
+
     def __init__(cls, name, bases, dict_):
         cls.metadata_spec = metadata.MetadataSpecCollection()
         for base in bases:  # loop through bases (class/types) of cls
             if hasattr(base, "metadata_spec"):  # base of class Data (object) has no metadata
                 cls.metadata_spec.update(base.metadata_spec)  # add contents of metadata spec of base class to cls
         metadata.Statement.process(cls)
 
@@ -140,36 +169,43 @@
     >>> DataTest.metadata_spec.test.name
     'test'
     >>> DataTest.metadata_spec.test.desc
     'test'
     >>> type( DataTest.metadata_spec.test.param )
     <class 'galaxy.model.metadata.MetadataParameter'>
     """
+
     edam_data = "data_0006"
     edam_format = "format_1915"
-    file_ext = 'data'
+    file_ext = "data"
     # Data is not chunkable by default.
     CHUNKABLE = False
 
     #: Dictionary of metadata fields for this datatype
     metadata_spec: metadata.MetadataSpecCollection
 
     # Add metadata elements
-    MetadataElement(name="dbkey", desc="Database/Build", default="?", param=metadata.DBKeyParameter, multiple=False, optional=True, no_value="?")
+    MetadataElement(
+        name="dbkey",
+        desc="Database/Build",
+        default="?",
+        param=metadata.DBKeyParameter,
+        multiple=False,
+        optional=True,
+        no_value="?",
+    )
     # Stores the set of display applications, and viewing methods, supported by this datatype
     supported_display_apps: Dict[str, Any] = {}
-    # If False, the peek is regenerated whenever a dataset of this type is copied
-    copy_safe_peek = True
     # The dataset contains binary data --> do not space_to_tab or convert newlines, etc.
     # Allow binary file uploads of this type when True.
-    is_binary = True
+    is_binary: Union[bool, Literal["maybe"]] = True
     # Composite datatypes
     composite_type: Optional[str] = None
     composite_files: Dict[str, Any] = {}
-    primary_file_name = 'index'
+    primary_file_name = "index"
     # Allow user to change between this datatype and others. If left to None,
     # datatype change is allowed if the datatype is not composite.
     allow_datatype_change: Optional[bool] = None
     # A per datatype setting (inherited): max file size (in bytes) for setting optional metadata
     _max_optional_metadata_filesize = None
 
     # Trackster track type.
@@ -183,53 +219,46 @@
     def __init__(self, **kwd):
         """Initialize the datatype"""
         self.supported_display_apps = self.supported_display_apps.copy()
         self.composite_files = self.composite_files.copy()
         self.display_applications = {}
 
     @classmethod
-    def is_datatype_change_allowed(cls):
+    def is_datatype_change_allowed(cls) -> bool:
         """
         Returns the value of the `allow_datatype_change` class attribute if set
         in a subclass, or True iff the datatype is not composite.
         """
         if cls.allow_datatype_change is not None:
             return cls.allow_datatype_change
         return cls.composite_type is None
 
-    def get_raw_data(self, dataset):
-        """Returns the full data. To stream it open the file_name and read/write as needed"""
-        try:
-            return open(dataset.file_name, 'rb').read(-1)
-        except OSError:
-            log.exception('%s reading a file that does not exist %s', self.__class__.__name__, dataset.file_name)
-            return ''
-
-    def dataset_content_needs_grooming(self, file_name):
+    def dataset_content_needs_grooming(self, file_name: str) -> bool:
         """This function is called on an output dataset file after the content is initially generated."""
         return False
 
-    def groom_dataset_content(self, file_name):
+    def groom_dataset_content(self, file_name: str) -> None:
         """This function is called on an output dataset file if dataset_content_needs_grooming returns True."""
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         # Metadata should be left mostly uninitialized.  Dataset will
         # handle returning default values when metadata is not set.
         # copy_from allows metadata to be passed in that will be
         # copied. (although this seems ambiguous, see
         # Dataset.set_metadata.  It always copies the rhs in order to
         # flag the object as modified for SQLAlchemy.
         if copy_from:
             dataset.metadata = copy_from.metadata
 
-    def set_meta(self, dataset: Any, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", *, overwrite: bool = True, **kwd) -> None:
         """Unimplemented method, allows guessing of metadata from contents of file"""
-        return True
 
-    def missing_meta(self, dataset, check=None, skip=None):
+    def missing_meta(
+        self, dataset: "DatasetInstance", check: Optional[List] = None, skip: Optional[List] = None
+    ) -> bool:
         """
         Checks for empty metadata values.
         Returns False if no non-optional metadata is missing and the missing metadata key otherwise.
         Specifying a list of 'check' values will only check those names provided; when used, optionality is ignored
         Specifying a list of 'skip' items will return True even when a named metadata value is missing; when used, optionality is ignored
         """
         if skip is None:
@@ -239,104 +268,110 @@
         else:
             to_check = dataset.metadata.keys()
         for key in to_check:
             if key in skip:
                 continue
             if not check and len(skip) == 0 and dataset.metadata.spec[key].get("optional"):
                 continue  # we skip check for optional and nonrequested values here
-            if not dataset.metadata.element_is_set(key) and (check or dataset.metadata.spec[key].check_required_metadata):
+            if not dataset.metadata.element_is_set(key) and (
+                check or dataset.metadata.spec[key].check_required_metadata
+            ):
                 # FIXME: Optional metadata isn't always properly annotated,
                 # so skip check if check_required_metadata is false on the datatype that defined the metadata element.
                 # See https://github.com/galaxyproject/tools-iuc/issues/4367
                 return key
         return False
 
-    def set_max_optional_metadata_filesize(self, max_value):
+    def set_max_optional_metadata_filesize(self, max_value: int) -> None:
         try:
             max_value = int(max_value)
         except (TypeError, ValueError):
             return
         self.__class__._max_optional_metadata_filesize = max_value
 
-    def get_max_optional_metadata_filesize(self):
+    def get_max_optional_metadata_filesize(self) -> int:
         rval = self.__class__._max_optional_metadata_filesize
         if rval is None:
             return -1
         return rval
 
     max_optional_metadata_filesize = property(get_max_optional_metadata_filesize, set_max_optional_metadata_filesize)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """
         Set the peek and blurb text
         """
         if not dataset.dataset.purged:
-            dataset.peek = ''
-            dataset.blurb = 'data'
+            dataset.peek = ""
+            dataset.blurb = "data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML table, used for displaying peek"""
         out = ['<table cellspacing="0" cellpadding="3">']
         try:
             if not dataset.peek:
                 dataset.set_peek()
             data = dataset.peek
             lines = data.splitlines()
             for line in lines:
                 line = line.strip()
                 if not line:
                     continue
                 out.append(f"<tr><td>{escape(unicodify(line, 'utf-8'))}</td></tr>")
-            out.append('</table>')
+            out.append("</table>")
             return "".join(out)
         except Exception as exc:
             return f"Can't create peek: {unicodify(exc)}"
 
-    def _archive_main_file(self, archive, display_name, data_filename):
+    def _archive_main_file(
+        self, archive: ZipstreamWrapper, display_name: str, data_filename: str
+    ) -> Tuple[bool, str, str]:
         """Called from _archive_composite_dataset to add central file to archive.
 
         Unless subclassed, this will add the main dataset file (argument data_filename)
         to the archive, as an HTML file with its filename derived from the dataset name
         (argument outfname).
 
         Returns a tuple of boolean, string, string: (error, msg, messagetype)
         """
         error, msg, messagetype = False, "", ""
-        archname = f'{display_name}.html'  # fake the real nature of the html file
+        archname = f"{display_name}.html"  # fake the real nature of the html file
         try:
             archive.write(data_filename, archname)
         except OSError:
             error = True
             log.exception("Unable to add composite parent %s to temporary library download archive", data_filename)
             msg = "Unable to create archive for download, please report this error"
             messagetype = "error"
         return error, msg, messagetype
 
-    def _archive_composite_dataset(self, trans, data, headers: Headers, do_action='zip'):
+    def _archive_composite_dataset(
+        self, trans, data: "DatasetInstance", headers: Headers, do_action: str = "zip"
+    ) -> Tuple[Union[ZipstreamWrapper, str], Headers]:
         # save a composite object into a compressed archive for downloading
         outfname = data.name[0:150]
-        outfname = ''.join(c in FILENAME_VALID_CHARS and c or '_' for c in outfname)
+        outfname = "".join(c in FILENAME_VALID_CHARS and c or "_" for c in outfname)
         archive = ZipstreamWrapper(
             archive_name=outfname,
             upstream_mod_zip=trans.app.config.upstream_mod_zip,
-            upstream_gzip=trans.app.config.upstream_gzip
+            upstream_gzip=trans.app.config.upstream_gzip,
         )
         error = False
-        msg = ''
+        msg = ""
         ext = data.extension
         path = data.file_name
         efp = data.extra_files_path
         # Add any central file to the archive,
 
         display_name = os.path.splitext(outfname)[0]
         if not display_name.endswith(ext):
-            display_name = f'{display_name}_{ext}'
+            display_name = f"{display_name}_{ext}"
 
         error, msg = self._archive_main_file(archive, display_name, path)[:2]
         if not error:
             # Add any child files to the archive,
             for fpath, rpath in self.__archive_extra_files_path(extra_files_path=efp):
                 try:
                     archive.write(fpath, rpath)
@@ -346,146 +381,179 @@
                     msg = "Unable to create archive for download, please report this error"
                     continue
         if not error:
             headers.update(archive.get_headers())
             return archive, headers
         return trans.show_error_message(msg), headers
 
-    def __archive_extra_files_path(self, extra_files_path):
+    def __archive_extra_files_path(self, extra_files_path: str) -> Generator[Tuple[str, str], None, None]:
         """Yield filepaths and relative filepaths for files in extra_files_path"""
         for root, _, files in os.walk(extra_files_path):
             for fname in files:
                 fpath = os.path.join(root, fname)
                 rpath = os.path.relpath(fpath, extra_files_path)
                 yield fpath, rpath
 
-    def _serve_raw(self, dataset, to_ext, headers: Headers, **kwd):
-        headers['Content-Length'] = str(os.stat(dataset.file_name).st_size)
-        headers["content-type"] = "application/octet-stream"  # force octet-stream so Safari doesn't append mime extensions to filename
-        filename = self._download_filename(dataset, to_ext, hdca=kwd.get("hdca"), element_identifier=kwd.get("element_identifier"), filename_pattern=kwd.get("filename_pattern"))
+    def _serve_raw(
+        self, dataset: "HistoryDatasetAssociation", to_ext: Optional[str], headers: Headers, **kwd
+    ) -> Tuple[IO, Headers]:
+        headers["Content-Length"] = str(os.stat(dataset.file_name).st_size)
+        headers[
+            "content-type"
+        ] = "application/octet-stream"  # force octet-stream so Safari doesn't append mime extensions to filename
+        filename = self._download_filename(
+            dataset,
+            to_ext,
+            hdca=kwd.get("hdca"),
+            element_identifier=kwd.get("element_identifier"),
+            filename_pattern=kwd.get("filename_pattern"),
+        )
         headers["Content-Disposition"] = f'attachment; filename="{filename}"'
-        return open(dataset.file_name, mode='rb'), headers
+        return open(dataset.file_name, mode="rb"), headers
 
-    def to_archive(self, dataset, name=""):
+    def to_archive(self, dataset: "DatasetInstance", name: str = "") -> Iterable:
         """
         Collect archive paths and file handles that need to be exported when archiving `dataset`.
 
         :param dataset: HistoryDatasetAssociation
         :param name: archive name, in collection context corresponds to collection name(s) and element_identifier,
                      joined by '/', e.g 'fastq_collection/sample1/forward'
         """
         rel_paths = []
         file_paths = []
-        if dataset.datatype.composite_type or dataset.extension.endswith('html'):
+        if dataset.datatype.composite_type or dataset.extension.endswith("html"):
             main_file = f"{name}.html"
             rel_paths.append(main_file)
             file_paths.append(dataset.file_name)
             for fpath, rpath in self.__archive_extra_files_path(dataset.extra_files_path):
                 rel_paths.append(os.path.join(name, rpath))
                 file_paths.append(fpath)
         else:
             rel_paths.append(f"{name or dataset.file_name}.{dataset.extension}")
             file_paths.append(dataset.file_name)
         return zip(file_paths, rel_paths)
 
-    def display_data(self, trans, data, preview=False, filename=None, to_ext=None, **kwd):
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        **kwd,
+    ):
         """
         Displays data in central pane if preview is `True`, else handles download.
 
         Datatypes should be very careful if overridding this method and this interface
         between datatypes and Galaxy will likely change.
 
         TOOD: Document alternatives to overridding this method (data
         providers?).
         """
         headers = kwd.get("headers", {})
         # Relocate all composite datatype display to a common location.
         composite_extensions = trans.app.datatypes_registry.get_composite_extensions()
-        composite_extensions.append('html')  # for archiving composite datatypes
+        composite_extensions.append("html")  # for archiving composite datatypes
+        composite_extensions.append("data_manager_json")  # for downloading bundles if bundled.
         # Prevent IE8 from sniffing content type since we're explicit about it.  This prevents intentionally text/plain
         # content from being rendered in the browser
-        headers['X-Content-Type-Options'] = 'nosniff'
-        if isinstance(data, str):
-            return smart_str(data), headers
+        headers["X-Content-Type-Options"] = "nosniff"
         if filename and filename != "index":
             # For files in extra_files_path
-            extra_dir = data.dataset.extra_files_path_name
-            file_path = trans.app.object_store.get_filename(data.dataset, extra_dir=extra_dir, alt_name=filename)
+            extra_dir = dataset.dataset.extra_files_path_name
+            file_path = trans.app.object_store.get_filename(dataset.dataset, extra_dir=extra_dir, alt_name=filename)
             if os.path.exists(file_path):
                 if os.path.isdir(file_path):
-                    with tempfile.NamedTemporaryFile(mode='w', delete=False, dir=trans.app.config.new_file_path, prefix='gx_html_autocreate_') as tmp_fh:
+                    with tempfile.NamedTemporaryFile(
+                        mode="w", delete=False, dir=trans.app.config.new_file_path, prefix="gx_html_autocreate_"
+                    ) as tmp_fh:
                         tmp_file_name = tmp_fh.name
                         dir_items = sorted(os.listdir(file_path))
                         base_path, item_name = os.path.split(file_path)
-                        tmp_fh.write('<html><head><h3>Directory %s contents: %d items</h3></head>\n' % (escape(item_name), len(dir_items)))
+                        tmp_fh.write(
+                            "<html><head><h3>Directory %s contents: %d items</h3></head>\n"
+                            % (escape(item_name), len(dir_items))
+                        )
                         tmp_fh.write('<body><p/><table cellpadding="2">\n')
                         for index, fname in enumerate(dir_items):
                             if index % 2 == 0:
-                                bgcolor = '#D8D8D8'
+                                bgcolor = "#D8D8D8"
                             else:
-                                bgcolor = '#FFFFFF'
+                                bgcolor = "#FFFFFF"
                             # Can't have an href link here because there is no route
                             # defined for files contained within multiple subdirectory
                             # levels of the primary dataset.  Something like this is
                             # close, but not quite correct:
                             # href = url_for(controller='dataset', action='display',
-                            # dataset_id=trans.security.encode_id(data.dataset.id),
+                            # dataset_id=trans.security.encode_id(dataset.dataset.id),
                             # preview=preview, filename=fname, to_ext=to_ext)
                             tmp_fh.write(f'<tr bgcolor="{bgcolor}"><td>{escape(fname)}</td></tr>\n')
-                        tmp_fh.write('</table></body></html>\n')
-                    return self._yield_user_file_content(trans, data, tmp_file_name, headers), headers
+                        tmp_fh.write("</table></body></html>\n")
+                    return self._yield_user_file_content(trans, dataset, tmp_file_name, headers), headers
                 mime = mimetypes.guess_type(file_path)[0]
                 if not mime:
                     try:
-                        mime = trans.app.datatypes_registry.get_mimetype_by_extension(".".split(file_path)[-1])
+                        mime = trans.app.datatypes_registry.get_mimetype_by_extension(file_path.split(".")[-1])
                     except Exception:
                         mime = "text/plain"
-                self._clean_and_set_mime_type(trans, mime, headers)
-                return self._yield_user_file_content(trans, data, file_path, headers), headers
+                self._clean_and_set_mime_type(trans, mime, headers)  # type: ignore[arg-type]
+                return self._yield_user_file_content(trans, dataset, file_path, headers), headers
             else:
                 raise ObjectNotFound(f"Could not find '{filename}' on the extra files path {file_path}.")
-        self._clean_and_set_mime_type(trans, data.get_mime(), headers)
+        self._clean_and_set_mime_type(trans, dataset.get_mime(), headers)
 
-        trans.log_event(f"Display dataset id: {str(data.id)}")
+        trans.log_event(f"Display dataset id: {str(dataset.id)}")
         from galaxy.datatypes import (  # DBTODO REMOVE THIS AT REFACTOR
             binary,
             images,
             text,
         )
 
-        if to_ext or isinstance(
-            data.datatype, binary.Binary
-        ):  # Saving the file, or binary file
-            if data.extension in composite_extensions:
-                return self._archive_composite_dataset(trans, data, headers, do_action=kwd.get('do_action', 'zip'))
+        if to_ext or isinstance(dataset.datatype, binary.Binary):  # Saving the file, or binary file
+            if dataset.extension in composite_extensions:
+                return self._archive_composite_dataset(trans, dataset, headers, do_action=kwd.get("do_action", "zip"))
             else:
-                headers['Content-Length'] = str(os.stat(data.file_name).st_size)
-                filename = self._download_filename(data, to_ext, hdca=kwd.get("hdca"), element_identifier=kwd.get("element_identifier"), filename_pattern=kwd.get("filename_pattern"))
-                headers['content-type'] = "application/octet-stream"  # force octet-stream so Safari doesn't append mime extensions to filename
+                headers["Content-Length"] = str(os.stat(dataset.file_name).st_size)
+                filename = self._download_filename(
+                    dataset,
+                    to_ext,
+                    hdca=kwd.get("hdca"),
+                    element_identifier=kwd.get("element_identifier"),
+                    filename_pattern=kwd.get("filename_pattern"),
+                )
+                headers[
+                    "content-type"
+                ] = "application/octet-stream"  # force octet-stream so Safari doesn't append mime extensions to filename
                 headers["Content-Disposition"] = f'attachment; filename="{filename}"'
-                return open(data.file_name, 'rb'), headers
-        if not os.path.exists(data.file_name):
-            raise ObjectNotFound(f"File Not Found ({data.file_name}).")
+                return open(dataset.file_name, "rb"), headers
+        if not os.path.exists(dataset.file_name):
+            raise ObjectNotFound(f"File Not Found ({dataset.file_name}).")
         max_peek_size = DEFAULT_MAX_PEEK_SIZE  # 1 MB
-        if isinstance(data.datatype, text.Html):
+        if isinstance(dataset.datatype, text.Html):
             max_peek_size = 10000000  # 10 MB for html
         preview = util.string_as_bool(preview)
         if (
             not preview
-            or isinstance(data.datatype, images.Image)
-            or os.stat(data.file_name).st_size < max_peek_size
+            or isinstance(dataset.datatype, images.Image)
+            or os.stat(dataset.file_name).st_size < max_peek_size
         ):
-            return self._yield_user_file_content(trans, data, data.file_name, headers), headers
+            return self._yield_user_file_content(trans, dataset, dataset.file_name, headers), headers
         else:
             headers["content-type"] = "text/html"
-            return trans.fill_template_mako("/dataset/large_file.mako",
-                                            truncated_data=open(data.file_name, 'rb').read(max_peek_size),
-                                            data=data), headers
+            return (
+                trans.fill_template_mako(
+                    "/dataset/large_file.mako",
+                    truncated_data=open(dataset.file_name, "rb").read(max_peek_size),
+                    data=dataset,
+                ),
+                headers,
+            )
 
-    def display_as_markdown(self, dataset_instance, markdown_format_helpers):
+    def display_as_markdown(self, dataset_instance: "DatasetInstance") -> str:
         """Prepare for embedding dataset into a basic Markdown document.
 
         This is a somewhat experimental interface and should not be implemented
         on datatypes not tightly tied to a Galaxy version (e.g. datatypes in the
         Tool Shed).
 
         Speaking very losely - the datatype should should load a bounded amount
@@ -493,46 +561,55 @@
         into Markdown. This should be relatively vanilla Markdown - the result of
         this is bleached and it should not contain nested Galaxy Markdown
         directives.
 
         If the data cannot reasonably be displayed, just indicate this and do
         not throw an exception.
         """
-        if self.file_ext in {'png', 'jpg'}:
+        if self.file_ext in {"png", "jpg"}:
             return self.handle_dataset_as_image(dataset_instance)
         if self.is_binary:
             result = "*cannot display binary content*\n"
         else:
             with open(dataset_instance.file_name) as f:
                 contents = f.read(DEFAULT_MAX_PEEK_SIZE)
-            result = markdown_format_helpers.literal_via_fence(contents)
+            result = literal_via_fence(contents)
             if len(contents) == DEFAULT_MAX_PEEK_SIZE:
-                result += markdown_format_helpers.indicate_data_truncated()
+                result += indicate_data_truncated()
         return result
 
-    def _yield_user_file_content(self, trans, from_dataset, filename, headers: Headers):
+    def _yield_user_file_content(self, trans, from_dataset: "DatasetInstance", filename: str, headers: Headers) -> IO:
         """This method is responsible for sanitizing the HTML if needed."""
         if trans.app.config.sanitize_all_html and headers.get("content-type", None) == "text/html":
             # Sanitize anytime we respond with plain text/html content.
             # Check to see if this dataset's parent job is allowlisted
             # We cannot currently trust imported datasets for rendering.
-            if not from_dataset.creating_job.imported and from_dataset.creating_job.tool_id.startswith(tuple(trans.app.config.sanitize_allowlist)):
-                return open(filename, mode='rb')
+            if not from_dataset.creating_job.imported and from_dataset.creating_job.tool_id.startswith(
+                tuple(trans.app.config.sanitize_allowlist)
+            ):
+                return open(filename, mode="rb")
 
             # This is returning to the browser, it needs to be encoded.
             # TODO Ideally this happens a layer higher, but this is a bad
             # issue affecting many tools
             with open(filename) as f:
-                return sanitize_html(f.read()).encode('utf-8')
+                return sanitize_html(f.read()).encode("utf-8")
 
-        return open(filename, mode='rb')
+        return open(filename, mode="rb")
 
-    def _download_filename(self, dataset, to_ext, hdca=None, element_identifier=None, filename_pattern=None):
+    def _download_filename(
+        self,
+        dataset: "HistoryDatasetAssociation",
+        to_ext: Optional[str] = None,
+        hdca: Optional["HistoryDatasetCollectionAssociation"] = None,
+        element_identifier: Optional[str] = None,
+        filename_pattern: Optional[str] = None,
+    ) -> str:
         def escape(raw_identifier):
-            return ''.join(c in FILENAME_VALID_CHARS and c or '_' for c in raw_identifier)[0:150]
+            return "".join(c in FILENAME_VALID_CHARS and c or "_" for c in raw_identifier)[0:150]
 
         if not to_ext or to_ext == "data":
             # If a client requests to_ext with the extension 'data', they are
             # deferring to the server, set it based on datatype.
             to_ext = dataset.extension
 
         template_values = {
@@ -551,142 +628,175 @@
             # Use collection context to build up filename.
             template_values["element_identifier"] = element_identifier
             template_values["hdca_name"] = escape(hdca.name)
             template_values["hdca_hid"] = hdca.hid
 
         return string.Template(filename_pattern).substitute(**template_values)
 
-    def display_name(self, dataset):
+    def display_name(self, dataset: "DatasetInstance") -> str:
         """Returns formatted html of dataset name"""
         try:
-            return escape(unicodify(dataset.name, 'utf-8'))
+            return escape(unicodify(dataset.name, "utf-8"))
         except Exception:
             return "name unavailable"
 
-    def display_info(self, dataset):
+    def display_info(self, dataset: "DatasetInstance") -> str:
         """Returns formatted html of dataset info"""
         try:
             # Change new line chars to html
             info: str = escape(dataset.info)
-            if info.find('\r\n') >= 0:
-                info = info.replace('\r\n', '<br/>')
-            if info.find('\r') >= 0:
-                info = info.replace('\r', '<br/>')
-            if info.find('\n') >= 0:
-                info = info.replace('\n', '<br/>')
+            if info.find("\r\n") >= 0:
+                info = info.replace("\r\n", "<br/>")
+            if info.find("\r") >= 0:
+                info = info.replace("\r", "<br/>")
+            if info.find("\n") >= 0:
+                info = info.replace("\n", "<br/>")
 
-            info = unicodify(info, 'utf-8')
+            info = unicodify(info, "utf-8")
 
             return info
         except Exception:
             return "info unavailable"
 
-    def repair_methods(self, dataset):
-        """Unimplemented method, returns dict with method/option for repairing errors"""
-        return None
-
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/octet-stream'
+        return "application/octet-stream"
 
-    def add_display_app(self, app_id, label, file_function, links_function):
+    def add_display_app(self, app_id: str, label: str, file_function: str, links_function: str) -> None:
         """
         Adds a display app to the datatype.
         app_id is a unique id
         label is the primary display label, e.g., display at 'UCSC'
         file_function is a string containing the name of the function that returns a properly formatted display
         links_function is a string containing the name of the function that returns a list of (link_name,link)
         """
         self.supported_display_apps = self.supported_display_apps.copy()
-        self.supported_display_apps[app_id] = {'label': label, 'file_function': file_function, 'links_function': links_function}
+        self.supported_display_apps[app_id] = {
+            "label": label,
+            "file_function": file_function,
+            "links_function": links_function,
+        }
 
-    def remove_display_app(self, app_id):
+    def remove_display_app(self, app_id: str) -> None:
         """Removes a display app from the datatype"""
         self.supported_display_apps = self.supported_display_apps.copy()
         try:
             del self.supported_display_apps[app_id]
         except Exception:
-            log.exception('Tried to remove display app %s from datatype %s, but this display app is not declared.', type, self.__class__.__name__)
+            log.exception(
+                "Tried to remove display app %s from datatype %s, but this display app is not declared.",
+                type,
+                self.__class__.__name__,
+            )
 
-    def clear_display_apps(self):
+    def clear_display_apps(self) -> None:
         self.supported_display_apps = {}
 
-    def add_display_application(self, display_application):
+    def add_display_application(self, display_application: "DisplayApplication") -> None:
         """New style display applications"""
-        assert display_application.id not in self.display_applications, 'Attempted to add a display application twice'
+        assert display_application.id not in self.display_applications, "Attempted to add a display application twice"
         self.display_applications[display_application.id] = display_application
 
-    def get_display_application(self, key, default=None):
+    def get_display_application(self, key: str, default: Optional["DisplayApplication"] = None) -> "DisplayApplication":
         return self.display_applications.get(key, default)
 
-    def get_display_applications_by_dataset(self, dataset, trans):
+    def get_display_applications_by_dataset(self, dataset: "DatasetInstance", trans) -> Dict[str, "DisplayApplication"]:
         rval = {}
         for key, value in self.display_applications.items():
             value = value.filter_by_dataset(dataset, trans)
             if value.links:
                 rval[key] = value
         return rval
 
-    def get_display_types(self):
+    def get_display_types(self) -> List[str]:
         """Returns display types available"""
         return list(self.supported_display_apps.keys())
 
-    def get_display_label(self, type):
+    def get_display_label(self, type: str) -> str:
         """Returns primary label for display app"""
         try:
-            return self.supported_display_apps[type]['label']
+            return self.supported_display_apps[type]["label"]
         except Exception:
-            return 'unknown'
+            return "unknown"
 
-    def as_display_type(self, dataset, type, **kwd):
-        """Returns modified file contents for a particular display type """
+    def as_display_type(self, dataset: "DatasetInstance", type: str, **kwd) -> Union[FileObjType, str]:
+        """Returns modified file contents for a particular display type"""
         try:
             if type in self.get_display_types():
-                return getattr(self, self.supported_display_apps[type]['file_function'])(dataset, **kwd)
+                return getattr(self, self.supported_display_apps[type]["file_function"])(dataset, **kwd)
         except Exception:
-            log.exception('Function %s is referred to in datatype %s for displaying as type %s, but is not accessible', self.supported_display_apps[type]['file_function'], self.__class__.__name__, type)
+            log.exception(
+                "Function %s is referred to in datatype %s for displaying as type %s, but is not accessible",
+                self.supported_display_apps[type]["file_function"],
+                self.__class__.__name__,
+                type,
+            )
         return f"This display type ({type}) is not implemented for this datatype ({dataset.ext})."
 
-    def get_display_links(self, dataset, type, app, base_url, target_frame='_blank', **kwd):
+    def get_display_links(
+        self, dataset: "DatasetInstance", type: str, app, base_url, request, target_frame: str = "_blank", **kwd
+    ):
         """
         Returns a list of tuples of (name, link) for a particular display type.  No check on
         'access' permissions is done here - if you can view the dataset, you can also save it
         or send it to a destination outside of Galaxy, so Galaxy security restrictions do not
         apply anyway.
         """
         try:
             if app.config.enable_old_display_applications and type in self.get_display_types():
-                return target_frame, getattr(self, self.supported_display_apps[type]['links_function'])(dataset, type, app, base_url, **kwd)
+                return target_frame, getattr(self, self.supported_display_apps[type]["links_function"])(
+                    dataset, type, app, base_url, request, **kwd
+                )
         except Exception:
-            log.exception('Function %s is referred to in datatype %s for generating links for type %s, but is not accessible',
-                          self.supported_display_apps[type]['links_function'], self.__class__.__name__, type)
+            log.exception(
+                "Function %s is referred to in datatype %s for generating links for type %s, but is not accessible",
+                self.supported_display_apps[type]["links_function"],
+                self.__class__.__name__,
+                type,
+            )
         return target_frame, []
 
-    def get_converter_types(self, original_dataset, datatypes_registry):
+    def get_converter_types(
+        self, original_dataset: "DatasetInstance", datatypes_registry: "Registry"
+    ) -> Dict[str, Dict]:
         """Returns available converters by type for this dataset"""
         return datatypes_registry.get_converters_by_datatype(original_dataset.ext)
 
     def find_conversion_destination(
         self, dataset, accepted_formats: List[str], datatypes_registry, **kwd
     ) -> Tuple[bool, Optional[str], Optional["DatasetInstance"]]:
         """Returns ( direct_match, converted_ext, existing converted dataset )"""
-        return datatypes_registry.find_conversion_destination_for_dataset_by_extensions(dataset, accepted_formats, **kwd)
+        return datatypes_registry.find_conversion_destination_for_dataset_by_extensions(
+            dataset, accepted_formats, **kwd
+        )
 
-    def convert_dataset(self, trans, original_dataset, target_type, return_output=False, visible=True, deps=None, target_context=None, history=None):
+    def convert_dataset(
+        self,
+        trans,
+        original_dataset: "HistoryDatasetAssociation",
+        target_type: str,
+        return_output: bool = False,
+        visible: bool = True,
+        deps: Optional[Dict] = None,
+        target_context: Optional[Dict] = None,
+        history=None,
+    ):
         """This function adds a job to the queue to convert a dataset to another type. Returns a message about success/failure."""
         converter = trans.app.datatypes_registry.get_converter_by_target_type(original_dataset.ext, target_type)
 
         if converter is None:
-            raise DatatypeConverterNotFoundException(f"A converter does not exist for {original_dataset.ext} to {target_type}.")
+            raise DatatypeConverterNotFoundException(
+                f"A converter does not exist for {original_dataset.ext} to {target_type}."
+            )
 
         params, input_name = get_params_and_input_name(converter, deps, target_context)
 
         params[input_name] = original_dataset
         # Make the target datatype available to the converter
-        params['__target_datatype__'] = target_type
+        params["__target_datatype__"] = target_type
         # Run converter, job is dispatched through Queue
         job, converted_datasets, *_ = converter.execute(trans, incoming=params, set_output_hid=visible, history=history)
         trans.app.job_manager.enqueue(job, tool=converter)
         if len(params) > 0:
             trans.log_event(f"Converter params: {str(params)}", tool_id=converter.id)
         if not visible:
             for value in converted_datasets.values():
@@ -694,60 +804,60 @@
         if return_output:
             return converted_datasets
         return f"The file conversion of {converter.name} on data {original_dataset.hid} has been added to the Queue."
 
     # We need to clear associated files before we set metadata
     # so that as soon as metadata starts to be set, e.g. implicitly converted datasets are deleted and no longer available 'while' metadata is being set, not just after
     # We'll also clear after setting metadata, for backwards compatibility
-    def after_setting_metadata(self, dataset):
+    def after_setting_metadata(self, dataset: "DatasetInstance") -> None:
         """This function is called on the dataset after metadata is set."""
         dataset.clear_associated_files(metadata_safe=True)
 
-    def before_setting_metadata(self, dataset):
+    def before_setting_metadata(self, dataset: "DatasetInstance") -> None:
         """This function is called on the dataset before metadata is set."""
         dataset.clear_associated_files(metadata_safe=True)
 
     def __new_composite_file(
         self,
-        name,
-        optional=False,
-        mimetype=None,
-        description=None,
-        substitute_name_with_metadata=None,
-        is_binary=False,
-        to_posix_lines=True,
-        space_to_tab=False,
+        name: str,
+        optional: bool = False,
+        mimetype: Optional[str] = None,
+        description: Optional[str] = None,
+        substitute_name_with_metadata: Optional[str] = None,
+        is_binary: bool = False,
+        to_posix_lines: bool = True,
+        space_to_tab: bool = False,
         **kwds,
-    ):
+    ) -> Bunch:
         kwds["name"] = name
         kwds["optional"] = optional
         kwds["mimetype"] = mimetype
         kwds["description"] = description
         kwds["substitute_name_with_metadata"] = substitute_name_with_metadata
         kwds["is_binary"] = is_binary
         kwds["to_posix_lines"] = to_posix_lines
         kwds["space_to_tab"] = space_to_tab
 
         composite_file = Bunch(**kwds)
         return composite_file
 
-    def add_composite_file(self, name, **kwds):
+    def add_composite_file(self, name: str, **kwds) -> None:
         # self.composite_files = self.composite_files.copy()
         self.composite_files[name] = self.__new_composite_file(name, **kwds)
 
     @property
     def writable_files(self):
         files = {}
-        if self.composite_type != 'auto_primary_file':
+        if self.composite_type != "auto_primary_file":
             files[self.primary_file_name] = self.__new_composite_file(self.primary_file_name)
         for key, value in self.get_composite_files().items():
             files[key] = value
         return files
 
-    def get_writable_files_for_dataset(self, dataset):
+    def get_writable_files_for_dataset(self, dataset: "DatasetInstance") -> Dict:
         files = {}
         if self.composite_type != "auto_primary_file":
             files[self.primary_file_name] = self.__new_composite_file(self.primary_file_name)
         for key, value in self.get_composite_files(dataset).items():
             files[key] = value
         return files
 
@@ -756,20 +866,21 @@
             if composite_file.substitute_name_with_metadata:
                 if dataset:
                     meta_value = str(dataset.metadata.get(composite_file.substitute_name_with_metadata))
                 else:
                     meta_value = self.metadata_spec[composite_file.substitute_name_with_metadata].default
                 return key % meta_value
             return key
+
         files = {}
         for key, value in self.composite_files.items():
             files[substitute_composite_key(key, value)] = value
         return files
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         raise Exception("generate_primary_file is not implemented for this datatype.")
 
     @property
     def has_resolution(self):
         return False
 
     def matches_any(self, target_datatypes: List[Any]) -> bool:
@@ -777,148 +888,155 @@
         Check if this datatype is of any of the target_datatypes or is
         a subtype thereof.
         """
         datatype_classes = tuple(datatype if isclass(datatype) else datatype.__class__ for datatype in target_datatypes)
         return isinstance(self, datatype_classes)
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """
-            Merge files with copy.copyfileobj() will not hit the
-            max argument limitation of cat. gz and bz2 files are also working.
+        Merge files with copy.copyfileobj() will not hit the
+        max argument limitation of cat. gz and bz2 files are also working.
         """
         if not split_files:
-            raise ValueError(f'Asked to merge zero files as {output_file}')
+            raise ValueError(f"Asked to merge zero files as {output_file}")
         elif len(split_files) == 1:
-            shutil.copyfileobj(open(split_files[0], 'rb'), open(output_file, 'wb'))
+            shutil.copyfileobj(open(split_files[0], "rb"), open(output_file, "wb"))
         else:
-            with open(output_file, 'wb') as fdst:
+            with open(output_file, "wb") as fdst:
                 for fsrc in split_files:
-                    shutil.copyfileobj(open(fsrc, 'rb'), fdst)
-
-    def get_visualizations(self, dataset):
-        """
-        Returns a list of visualizations for datatype.
-        """
-
-        if self.track_type:
-            return ['trackster', 'circster']
-        return []
+                    shutil.copyfileobj(open(fsrc, "rb"), fdst)
 
     # ------------- Dataproviders
-    def has_dataprovider(self, data_format):
+    def has_dataprovider(self, data_format: str) -> bool:
         """
         Returns True if `data_format` is available in `dataproviders`.
         """
         return data_format in self.dataproviders
 
-    def dataprovider(self, dataset, data_format, **settings):
+    def dataprovider(self, dataset: "DatasetInstance", data_format: str, **settings):
         """
         Base dataprovider factory for all datatypes that returns the proper provider
         for the given `data_format` or raises a `NoProviderAvailable`.
         """
         if self.has_dataprovider(data_format):
             return self.dataproviders[data_format](self, dataset, **settings)
         raise p_dataproviders.exceptions.NoProviderAvailable(self, data_format)
 
-    def validate(self, dataset, **kwd):
+    def validate(self, dataset: "DatasetInstance", **kwd) -> DatatypeValidation:
         return DatatypeValidation.unvalidated()
 
     @p_dataproviders.decorators.dataprovider_factory("base")
-    def base_dataprovider(self, dataset, **settings):
+    def base_dataprovider(self, dataset: "DatasetInstance", **settings) -> p_dataproviders.base.DataProvider:
         dataset_source = p_dataproviders.dataset.DatasetDataProvider(dataset)
         return p_dataproviders.base.DataProvider(dataset_source, **settings)
 
-    @p_dataproviders.decorators.dataprovider_factory(
-        "chunk", p_dataproviders.chunk.ChunkDataProvider.settings
-    )
-    def chunk_dataprovider(self, dataset, **settings):
+    @p_dataproviders.decorators.dataprovider_factory("chunk", p_dataproviders.chunk.ChunkDataProvider.settings)
+    def chunk_dataprovider(self, dataset: "DatasetInstance", **settings) -> p_dataproviders.chunk.ChunkDataProvider:
         dataset_source = p_dataproviders.dataset.DatasetDataProvider(dataset)
         return p_dataproviders.chunk.ChunkDataProvider(dataset_source, **settings)
 
-    @p_dataproviders.decorators.dataprovider_factory(
-        "chunk64", p_dataproviders.chunk.Base64ChunkDataProvider.settings
-    )
-    def chunk64_dataprovider(self, dataset, **settings):
+    @p_dataproviders.decorators.dataprovider_factory("chunk64", p_dataproviders.chunk.Base64ChunkDataProvider.settings)
+    def chunk64_dataprovider(
+        self, dataset: "DatasetInstance", **settings
+    ) -> p_dataproviders.chunk.Base64ChunkDataProvider:
         dataset_source = p_dataproviders.dataset.DatasetDataProvider(dataset)
         return p_dataproviders.chunk.Base64ChunkDataProvider(dataset_source, **settings)
 
-    def _clean_and_set_mime_type(self, trans, mime, headers: Headers):
+    def _clean_and_set_mime_type(self, trans, mime: str, headers: Headers) -> None:
         if mime.lower() in XSS_VULNERABLE_MIME_TYPES:
             if not getattr(trans.app.config, "serve_xss_vulnerable_mimetypes", True):
                 mime = DEFAULT_MIME_TYPE
         headers["content-type"] = mime
 
-    def handle_dataset_as_image(self, hda) -> str:
+    def handle_dataset_as_image(self, hda: "DatasetInstance") -> str:
         raise Exception("Unimplemented Method")
 
+    def __getstate__(self) -> Dict[str, Any]:
+        state = self.__dict__.copy()
+        state.pop("display_applications", None)
+        return state
+
 
 @p_dataproviders.decorators.has_dataproviders
 class Text(Data):
     edam_format = "format_2330"
-    file_ext = 'txt'
-    line_class = 'line'
+    file_ext = "txt"
+    line_class = "line"
 
     is_binary = False
 
     # Add metadata elements
-    MetadataElement(name="data_lines", default=0, desc="Number of data lines", readonly=True, optional=True, visible=False, no_value=0)
+    MetadataElement(
+        name="data_lines",
+        default=0,
+        desc="Number of data lines",
+        readonly=True,
+        optional=True,
+        visible=False,
+        no_value=0,
+    )
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/plain'
+        return "text/plain"
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", *, overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
         dataset.metadata.data_lines = self.count_data_lines(dataset)
 
-    def estimate_file_lines(self, dataset):
+    def estimate_file_lines(self, dataset: "DatasetInstance") -> Optional[int]:
         """
         Perform a rough estimate by extrapolating number of lines from a small read.
         """
         sample_size = 1048576
         try:
             with compression_utils.get_fileobj(dataset.file_name) as dataset_fh:
                 dataset_read = dataset_fh.read(sample_size)
-            sample_lines = dataset_read.count('\n')
+            sample_lines = dataset_read.count("\n")
             return int(sample_lines * (float(dataset.get_size()) / float(sample_size)))
         except UnicodeDecodeError:
-            log.error(f'Unable to estimate lines in file {dataset.file_name}')
+            log.error(f"Unable to estimate lines in file {dataset.file_name}")
             return None
 
-    def count_data_lines(self, dataset):
+    def count_data_lines(self, dataset: "DatasetInstance") -> Optional[int]:
         """
         Count the number of lines of data in dataset,
         skipping all blank lines and comments.
         """
-        CHUNK_SIZE = 2 ** 15  # 32Kb
+        CHUNK_SIZE = 2**15  # 32Kb
         data_lines = 0
         with compression_utils.get_fileobj(dataset.file_name) as in_file:
             # FIXME: Potential encoding issue can prevent the ability to iterate over lines
             # causing set_meta process to fail otherwise OK jobs. A better solution than
             # a silent try/except is desirable.
             try:
                 for line in iter_start_of_line(in_file, CHUNK_SIZE):
                     line = line.strip()
-                    if line and not line.startswith('#'):
+                    if line and not line.startswith("#"):
                         data_lines += 1
             except UnicodeDecodeError:
-                log.error(f'Unable to count lines in file {dataset.file_name}')
+                log.error(f"Unable to count lines in file {dataset.file_name}")
                 return None
         return data_lines
 
-    def set_peek(self, dataset, line_count=None, WIDTH=256, skipchars=None, line_wrap=True, **kwd):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """
         Set the peek.  This method is used by various subclasses of Text.
         """
+        line_count = kwd.get("line_count")
+        width = kwd.get("width", 256)
+        skipchars = kwd.get("skipchars")
+        line_wrap = kwd.get("line_wrap", True)
+
         if not dataset.dataset.purged:
             # The file must exist on disk for the get_file_peek() method
-            dataset.peek = get_file_peek(dataset.file_name, WIDTH=WIDTH, skipchars=skipchars, line_wrap=line_wrap)
+            dataset.peek = get_file_peek(dataset.file_name, width=width, skipchars=skipchars, line_wrap=line_wrap)
             if line_count is None:
                 # See if line_count is stored in the metadata
                 if dataset.metadata.data_lines:
                     dataset.blurb = f"{util.commaify(str(dataset.metadata.data_lines))} {inflector.cond_plural(dataset.metadata.data_lines, self.line_class)}"
                 else:
                     # Number of lines is not known ( this should not happen ), and auto-detect is
                     # needed to set metadata
@@ -936,52 +1054,53 @@
                         if est_lines is not None:
                             dataset.blurb = f"~{util.commaify(util.roundify(str(est_lines)))} {inflector.cond_plural(est_lines, self.line_class)}"
                         else:
                             dataset.blurb = "Error: Cannot estimate lines in dataset"
             else:
                 dataset.blurb = f"{util.commaify(str(line_count))} {inflector.cond_plural(line_count, self.line_class)}"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
         Split the input files by line.
         """
         if split_params is None:
             return
 
         if len(input_datasets) > 1:
             raise Exception("Text file splitting does not support multiple files")
         input_files = [ds.file_name for ds in input_datasets]
 
         lines_per_file = None
         chunk_size = None
-        if split_params['split_mode'] == 'number_of_parts':
+        if split_params["split_mode"] == "number_of_parts":
             lines_per_file = []
 
             # Computing the length is expensive!
             def _file_len(fname):
                 with open(fname) as f:
                     return sum(1 for _ in f)
+
             length = _file_len(input_files[0])
-            parts = int(split_params['split_size'])
+            parts = int(split_params["split_size"])
             if length < parts:
                 parts = length
             len_each, remainder = divmod(length, parts)
             while length > 0:
                 chunk = len_each
                 if remainder > 0:
                     chunk += 1
                 lines_per_file.append(chunk)
                 remainder -= 1
                 length -= chunk
-        elif split_params['split_mode'] == 'to_size':
-            chunk_size = int(split_params['split_size'])
+        elif split_params["split_mode"] == "to_size":
+            chunk_size = int(split_params["split_size"])
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
         f = open(input_files[0])
         try:
             chunk_idx = 0
             file_done = False
@@ -992,167 +1111,158 @@
                 elif chunk_idx < len(lines_per_file):
                     this_chunk_size = lines_per_file[chunk_idx]
                     chunk_idx += 1
                 lines_remaining = this_chunk_size
                 part_file = None
                 while lines_remaining > 0:
                     a_line = f.readline()
-                    if a_line == '':
+                    if a_line == "":
                         file_done = True
                         break
                     if part_file is None:
                         part_dir = subdir_generator_function()
                         part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
-                        part_file = open(part_path, 'w')
+                        part_file = open(part_path, "w")
                     part_file.write(a_line)
                     lines_remaining -= 1
         except Exception as e:
-            log.error('Unable to split files: %s', unicodify(e))
+            log.error("Unable to split files: %s", unicodify(e))
             raise
         finally:
             f.close()
             if part_file:
                 part_file.close()
 
     # ------------- Dataproviders
-    @p_dataproviders.decorators.dataprovider_factory(
-        "line", p_dataproviders.line.FilteredLineDataProvider.settings
-    )
-    def line_dataprovider(self, dataset, **settings):
+    @p_dataproviders.decorators.dataprovider_factory("line", p_dataproviders.line.FilteredLineDataProvider.settings)
+    def line_dataprovider(
+        self, dataset: "DatasetInstance", **settings
+    ) -> p_dataproviders.line.FilteredLineDataProvider:
         """
         Returns an iterator over the dataset's lines (that have been stripped)
         optionally excluding blank lines and lines that start with a comment character.
         """
         dataset_source = p_dataproviders.dataset.DatasetDataProvider(dataset)
         return p_dataproviders.line.FilteredLineDataProvider(dataset_source, **settings)
 
-    @p_dataproviders.decorators.dataprovider_factory(
-        "regex-line", p_dataproviders.line.RegexLineDataProvider.settings
-    )
-    def regex_line_dataprovider(self, dataset, **settings):
+    @p_dataproviders.decorators.dataprovider_factory("regex-line", p_dataproviders.line.RegexLineDataProvider.settings)
+    def regex_line_dataprovider(
+        self, dataset: "DatasetInstance", **settings
+    ) -> p_dataproviders.line.RegexLineDataProvider:
         """
         Returns an iterator over the dataset's lines
         optionally including/excluding lines that match one or more regex filters.
         """
         dataset_source = p_dataproviders.dataset.DatasetDataProvider(dataset)
         return p_dataproviders.line.RegexLineDataProvider(dataset_source, **settings)
 
 
 class Directory(Data):
     """Class representing a directory of files."""
 
 
 class GenericAsn1(Text):
     """Class for generic ASN.1 text format"""
+
     edam_data = "data_0849"
     edam_format = "format_1966"
-    file_ext = 'asn1'
+    file_ext = "asn1"
 
 
 class LineCount(Text):
     """
     Dataset contains a single line with a single integer that denotes the
     line count for a related dataset. Used for custom builds.
     """
 
 
 class Newick(Text):
     """New Hampshire/Newick Format"""
+
     edam_data = "data_0872"
     edam_format = "format_1910"
     file_ext = "newick"
 
-    def sniff(self, filename):
-        """ Returning false as the newick format is too general and cannot be sniffed."""
+    def sniff(self, filename: str) -> bool:
+        """Returning false as the newick format is too general and cannot be sniffed."""
         return False
 
-    def get_visualizations(self, dataset):
-        """
-        Returns a list of visualizations for datatype.
-        """
-        return ['phyloviz']
-
 
 @build_sniff_from_prefix
 class Nexus(Text):
     """Nexus format as used By Paup, Mr Bayes, etc"""
+
     edam_data = "data_0872"
     edam_format = "format_1912"
     file_ext = "nex"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """All Nexus Files Simply puts a '#NEXUS' in its first line"""
         return file_prefix.string_io().read(6).upper() == "#NEXUS"
 
-    def get_visualizations(self, dataset):
-        """
-        Returns a list of visualizations for datatype.
-        """
-        return ['phyloviz']
-
 
 # ------------- Utility methods --------------
 
 # nice_size used to be here, but to resolve cyclical dependencies it's been
 # moved to galaxy.util.  It belongs there anyway since it's used outside
 # datatypes.
 nice_size = util.nice_size
 
 
 def get_test_fname(fname):
     """Returns test data filename"""
     path = os.path.dirname(__file__)
-    full_path = os.path.join(path, 'test', fname)
+    full_path = os.path.join(path, "test", fname)
     return full_path
 
 
-def get_file_peek(file_name, WIDTH=256, LINE_COUNT=5, skipchars=None, line_wrap=True):
+def get_file_peek(file_name, width=256, line_count=5, skipchars=None, line_wrap=True):
     """
-    Returns the first LINE_COUNT lines wrapped to WIDTH.
+    Returns the first line_count lines wrapped to width.
 
     >>> def assert_peek_is(file_name, expected, *args, **kwd):
     ...     path = get_test_fname(file_name)
     ...     peek = get_file_peek(path, *args, **kwd)
     ...     assert peek == expected, "%s != %s" % (peek, expected)
     >>> assert_peek_is('0_nonewline', u'0')
     >>> assert_peek_is('0.txt', u'0\\n')
-    >>> assert_peek_is('4.bed', u'chr22\\t30128507\\t31828507\\tuc003bnx.1_cds_2_0_chr22_29227_f\\t0\\t+\\n', LINE_COUNT=1)
-    >>> assert_peek_is('1.bed', u'chr1\\t147962192\\t147962580\\tCCDS989.1_cds_0_0_chr1_147962193_r\\t0\\t-\\nchr1\\t147984545\\t147984630\\tCCDS990.1_cds_0_0_chr1_147984546_f\\t0\\t+\\n', LINE_COUNT=2)
+    >>> assert_peek_is('4.bed', u'chr22\\t30128507\\t31828507\\tuc003bnx.1_cds_2_0_chr22_29227_f\\t0\\t+\\n', line_count=1)
+    >>> assert_peek_is('1.bed', u'chr1\\t147962192\\t147962580\\tCCDS989.1_cds_0_0_chr1_147962193_r\\t0\\t-\\nchr1\\t147984545\\t147984630\\tCCDS990.1_cds_0_0_chr1_147984546_f\\t0\\t+\\n', line_count=2)
     """
     # Set size for file.readline() to a negative number to force it to
     # read until either a newline or EOF.  Needed for datasets with very
     # long lines.
-    if WIDTH == 'unlimited':
-        WIDTH = -1
+    if width == "unlimited":
+        width = -1
     if skipchars is None:
         skipchars = []
     lines = []
     count = 0
 
     last_line_break = False
     with compression_utils.get_fileobj(file_name) as temp:
-        while count < LINE_COUNT:
+        while count < line_count:
             try:
-                line = temp.readline(WIDTH)
+                line = temp.readline(width)
             except UnicodeDecodeError:
                 return "binary file"
             if line == "":
                 break
             last_line_break = False
-            if line.endswith('\n'):
+            if line.endswith("\n"):
                 line = line[:-1]
                 last_line_break = True
             elif not line_wrap:
                 for i in file_reader(temp, 1):
-                    if i == '\n':
+                    if i == "\n":
                         last_line_break = True
-                    if not i or i == '\n':
+                    if not i or i == "\n":
                         break
             skip_line = False
             for skipchar in skipchars:
                 if line.startswith(skipchar):
                     skip_line = True
                     break
             if not skip_line:
                 lines.append(line)
                 count += 1
-    return '\n'.join(lines) + ('\n' if last_line_break else '')
+    return "\n".join(lines) + ("\n" if last_line_break else "")
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/__init__.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -20,11 +20,11 @@
     chunk,
     column,
     dataset,
     decorators,
     exceptions,
     external,
     hierarchy,
-    line
+    line,
 )
 
-__all__ = ('decorators', 'exceptions', 'base', 'chunk', 'line', 'hierarchy', 'column', 'external', 'dataset')
+__all__ = ("decorators", "exceptions", "base", "chunk", "line", "hierarchy", "column", "external", "dataset")
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/base.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,40 +42,42 @@
     """
     Metaclass for data providers that allows defining and inheriting
     a dictionary named 'settings'.
 
     Useful for allowing class level access to expected variable types
     passed to class `__init__` functions so they can be parsed from a query string.
     """
+
     # yeah - this is all too acrobatic
     def __new__(cls, name, base_classes, attributes):
         settings = {}
         # get settings defined in base classes
         for base_class in base_classes:
-            base_settings = getattr(base_class, 'settings', None)
+            base_settings = getattr(base_class, "settings", None)
             if base_settings:
                 settings.update(base_settings)
         # get settings defined in this class
-        new_settings = attributes.pop('settings', None)
+        new_settings = attributes.pop("settings", None)
         if new_settings:
             settings.update(new_settings)
-        attributes['settings'] = settings
+        attributes["settings"] = settings
         return type.__new__(cls, name, base_classes, attributes)
 
 
 # ----------------------------------------------------------------------------- base classes
 class DataProvider(metaclass=HasSettings):
     """
     Base class for all data providers. Data providers:
 
     - have a source (which must be another file-like object)
     - implement both the iterator and context manager interfaces
     - do not allow write methods (but otherwise implement the other file object interface methods)
 
     """
+
     # a definition of expected types for keyword arguments sent to __init__
     #   useful for controlling how query string dictionaries can be parsed into correct types for __init__
     #   empty in this base class
     settings: Dict[str, str] = {}
 
     def __init__(self, source, **kwargs):
         """Sets up a data provider, validates supplied source.
@@ -91,40 +93,40 @@
         """
         Is this a valid source for this provider?
 
         :raises InvalidDataProviderSource: if the source is considered invalid.
 
         Meant to be overridden in subclasses.
         """
-        if not source or not hasattr(source, '__iter__'):
+        if not source or not hasattr(source, "__iter__"):
             # that's by no means a thorough check
             raise exceptions.InvalidDataProviderSource(source)
         return source
 
     # TODO: (this might cause problems later...)
     # TODO: some providers (such as chunk's seek and read) rely on this... remove
     def __getattr__(self, name):
-        if name == 'source':
+        if name == "source":
             # if we're inside this fn, source hasn't been set - provide some safety just for this attr
             return None
         # otherwise, try to get the attr from the source - allows us to get things like provider.encoding, etc.
         if hasattr(self.source, name):
             return getattr(self.source, name)
         # raise the proper error
         return self.__getattribute__(name)
 
     # write methods should not be allowed
     def truncate(self, size):
-        raise NotImplementedError('Write methods are purposely disabled')
+        raise NotImplementedError("Write methods are purposely disabled")
 
     def write(self, string):
-        raise NotImplementedError('Write methods are purposely disabled')
+        raise NotImplementedError("Write methods are purposely disabled")
 
     def writelines(self, sequence):
-        raise NotImplementedError('Write methods are purposely disabled')
+        raise NotImplementedError("Write methods are purposely disabled")
 
     # TODO: route read methods through next?
     # def readline( self ):
     #    return self.next()
     def readlines(self):
         return [line for line in self]
 
@@ -136,47 +138,48 @@
 
     def __next__(self):
         return next(self.source)
 
     # context manager interface
     def __enter__(self):
         # make the source's context manager interface optional
-        if hasattr(self.source, '__enter__'):
+        if hasattr(self.source, "__enter__"):
             self.source.__enter__()
         return self
 
     def __exit__(self, *args):
         # make the source's context manager interface optional, call on source if there
-        if hasattr(self.source, '__exit__'):
+        if hasattr(self.source, "__exit__"):
             self.source.__exit__(*args)
         # alternately, call close()
-        elif hasattr(self.source, 'close'):
+        elif hasattr(self.source, "close"):
             self.source.close()
 
     def __str__(self):
         """
         String representation for easier debugging.
 
         Will call `__str__` on its source so this will display piped dataproviders.
         """
         # we need to protect against recursion (in __getattr__) if self.source hasn't been set
-        source_str = str(self.source) if hasattr(self, 'source') else ''
-        return f'{self.__class__.__name__}({str(source_str)})'
+        source_str = str(self.source) if hasattr(self, "source") else ""
+        return f"{self.__class__.__name__}({str(source_str)})"
 
 
 class FilteredDataProvider(DataProvider):
     """
     Passes each datum through a filter function and yields it if that function
     returns a non-`None` value.
 
     Also maintains counters:
         - `num_data_read`: how many data have been consumed from the source.
         - `num_valid_data_read`: how many data have been returned from `filter`.
         - `num_data_returned`: how many data has this provider yielded.
     """
+
     # not useful here - we don't want functions over the query string
     # settings.update({ 'filter_fn': 'function' })
 
     def __init__(self, source, filter_fn=None, **kwargs):
         """
         :param filter_fn: a lambda or function that will be passed a datum and
             return either the (optionally modified) datum or None.
@@ -221,19 +224,17 @@
 class LimitedOffsetDataProvider(FilteredDataProvider):
     """
     A provider that uses the counters from FilteredDataProvider to limit the
     number of data and/or skip `offset` number of data before providing.
 
     Useful for grabbing sections from a source (e.g. pagination).
     """
+
     # define the expected types of these __init__ arguments so they can be parsed out from query strings
-    settings = {
-        'limit': 'int',
-        'offset': 'int'
-    }
+    settings = {"limit": "int", "offset": "int"}
 
     # TODO: may want to squash this into DataProvider
     def __init__(self, source, offset=0, limit=None, **kwargs):
         """
         :param offset:  the number of data to skip before providing.
         :param limit:   the final number of data to provide.
         """
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/chunk.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/chunk.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,32 +6,30 @@
 """
 import base64
 import logging
 import os
 
 from . import (
     base,
-    exceptions
+    exceptions,
 )
 
 log = logging.getLogger(__name__)
 
 
 class ChunkDataProvider(base.DataProvider):
     """
     Data provider that yields chunks of data from its file.
 
     Note: this version does not account for lines and works with Binary datatypes.
     """
-    MAX_CHUNK_SIZE = 2 ** 16
+
+    MAX_CHUNK_SIZE = 2**16
     DEFAULT_CHUNK_SIZE = MAX_CHUNK_SIZE
-    settings = {
-        'chunk_index': 'int',
-        'chunk_size': 'int'
-    }
+    settings = {"chunk_index": "int", "chunk_size": "int"}
 
     # TODO: subclass from LimitedOffsetDataProvider?
     # see web/framework/base.iterate_file, util/__init__.file_reader, and datatypes.tabular
     def __init__(self, source, chunk_index=0, chunk_size=DEFAULT_CHUNK_SIZE, **kwargs):
         """
         :param chunk_index: if a source can be divided into N number of
             `chunk_size` sections, this is the index of which section to
@@ -45,15 +43,15 @@
 
     def validate_source(self, source):
         """
         Does the given source have both the methods `seek` and `read`?
         :raises InvalidDataProviderSource: if not.
         """
         source = super().validate_source(source)
-        if((not hasattr(source, 'seek')) or (not hasattr(source, 'read'))):
+        if (not hasattr(source, "seek")) or (not hasattr(source, "read")):
             raise exceptions.InvalidDataProviderSource(source)
         return source
 
     def __iter__(self):
         # not reeeally an iterator per se
         self.__enter__()
         self.source.seek(self.chunk_pos, os.SEEK_SET)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/column.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/column.py`

 * *Files 7% similar despite different names*

```diff
@@ -27,26 +27,36 @@
     re-arrange columns.
 
     If any desired index is outside the actual number of columns
     in the source, this provider will None-pad the output and you are guaranteed
     the same number of columns as the number of indeces asked for (even if they
     are filled with None).
     """
+
     settings = {
-        'indeces': 'list:int',
-        'column_count': 'int',
-        'column_types': 'list:str',
-        'parse_columns': 'bool',
-        'deliminator': 'str',
-        'filters': 'list:str'
+        "indeces": "list:int",
+        "column_count": "int",
+        "column_types": "list:str",
+        "parse_columns": "bool",
+        "deliminator": "str",
+        "filters": "list:str",
     }
 
-    def __init__(self, source, indeces=None,
-                 column_count=None, column_types=None, parsers=None, parse_columns=True,
-                 deliminator='\t', filters=None, **kwargs):
+    def __init__(
+        self,
+        source,
+        indeces=None,
+        column_count=None,
+        column_types=None,
+        parsers=None,
+        parse_columns=True,
+        deliminator="\t",
+        filters=None,
+        **kwargs,
+    ):
         """
 
         :param indeces: a list of indeces of columns to gather from each row
                         Optional: will default to `None`.
                         If `None`, this provider will return all rows (even when a
                         particular row contains more/less than others).
                         If a row/line does not contain an element at a given index, the
@@ -110,28 +120,28 @@
         for filter_ in filters:
             parsed = self.parse_filter(filter_)
             # TODO: might be better to error on bad filter/None here
             if callable(parsed):
                 self.column_filters.append(parsed)
 
     def parse_filter(self, filter_param_str):
-        split = filter_param_str.split('-', 2)
+        split = filter_param_str.split("-", 2)
         if not len(split) >= 3:
             return None
         column, op, val = split
 
         # better checking v. len and indeces
         column = int(column)
         if column > len(self.column_types):
             return None
-        if self.column_types[column] in ('float', 'int'):
+        if self.column_types[column] in ("float", "int"):
             return self.create_numeric_filter(column, op, val)
-        if self.column_types[column] in ('str'):
+        if self.column_types[column] in ("str"):
             return self.create_string_filter(column, op, val)
-        if self.column_types[column] in ('list'):
+        if self.column_types[column] in ("list"):
             return self.create_list_filter(column, op, val)
         return None
 
     def create_numeric_filter(self, column, op, val):
         """
         Return an anonymous filter function that will be passed the array
         of parsed columns. Return None if no filter function can be
@@ -149,25 +159,25 @@
 
         `val` is cast as float here and will return None if there's a parsing error.
         """
         try:
             val = float(val)
         except ValueError:
             return None
-        if 'lt' == op:
+        if "lt" == op:
             return lambda d: d[column] < val
-        elif 'le' == op:
+        elif "le" == op:
             return lambda d: d[column] <= val
-        elif 'eq' == op:
+        elif "eq" == op:
             return lambda d: d[column] == val
-        elif 'ne' == op:
+        elif "ne" == op:
             return lambda d: d[column] != val
-        elif 'ge' == op:
+        elif "ge" == op:
             return lambda d: d[column] >= val
-        elif 'gt' == op:
+        elif "gt" == op:
             return lambda d: d[column] > val
         return None
 
     def create_string_filter(self, column, op, val):
         """
         Return an anonymous filter function that will be passed the array
         of parsed columns. Return None if no filter function can be
@@ -176,19 +186,19 @@
         The function will compare the column at index `column` against `val`
         using the given op where op is one of:
 
         - eq: exactly matches
         - has: the column contains the substring `val`
         - re: the column matches the regular expression in `val`
         """
-        if 'eq' == op:
+        if "eq" == op:
             return lambda d: d[column] == val
-        elif 'has' == op:
+        elif "has" == op:
             return lambda d: val in d[column]
-        elif 're' == op:
+        elif "re" == op:
             val = unquote_plus(val)
             val = re.compile(val)
             return lambda d: val.match(d[column]) is not None
         return None
 
     def create_list_filter(self, column, op, val):
         """
@@ -198,18 +208,18 @@
 
         The function will compare the column at index `column` against `val`
         using the given op where op is one of:
 
         - eq: the list `val` exactly matches the list in the column
         - has: the list in the column contains the sublist `val`
         """
-        if 'eq' == op:
-            val = self.parse_value(val, 'list')
+        if "eq" == op:
+            val = self.parse_value(val, "list")
             return lambda d: d[column] == val
-        elif 'has' == op:
+        elif "has" == op:
             return lambda d: val in d[column]
         return None
 
     def get_default_parsers(self):
         """
         Return parser dictionary keyed for each columnar type
         (as defined in datatypes).
@@ -219,32 +229,28 @@
 
         :returns: a dictionary of the form:
             `{ <parser type name> : <function used to parse type> }`
         """
         # TODO: move to module level (or datatypes, util)
         return {
             # str is default and not needed here
-            'int': int,
-            'float': float,
-            'bool': bool,
-
+            "int": int,
+            "float": float,
+            "bool": bool,
             # unfortunately, 'list' is used in dataset metadata both for
             #   query style maps (9th col gff) AND comma-sep strings.
             #   (disabled for now)
             # 'list'  : lambda v: v.split( ',' ),
             # 'csv'   : lambda v: v.split( ',' ),
             # i don't like how urlparses does sub-lists...
             # 'querystr' : lambda v: dict([ ( p.split( '=', 1 ) if '=' in p else ( p, True ) )
             #                              for p in v.split( ';', 1 ) ])
-
             # 'scifloat': #floating point which may be in scientific notation
-
             # always with the 1 base, biologists?
             # 'int1'  : ( lambda i: int( i ) - 1 ),
-
             # 'gffval': string or '.' for None
             # 'gffint': # int or '.' for None
             # 'gffphase': # 0, 1, 2, or '.' for None
             # 'gffstrand': # -, +, ?, or '.' for None, etc.
         }
 
     def filter(self, line):
@@ -286,15 +292,15 @@
 
         :param val: the column value to parse (often a string)
         :param type: the string type 'name' used to find the appropriate parser
         :returns: the parsed value
             or `value` if no `type` found in `parsers`
             or `None` if there was a parser error (ValueError)
         """
-        if type == 'str' or type is None:
+        if type == "str" or type is None:
             return val
         try:
             return self.parsers[type](val)
         except KeyError:
             # no parser - return as string
             pass
         except ValueError:
@@ -328,16 +334,17 @@
 
     A combination use of both `column_names` and `indeces` allows 'picking'
     key/value pairs from the source.
 
     .. note:: The subclass constructors are passed kwargs - so their
         params (limit, offset, etc.) are also applicable here.
     """
+
     settings = {
-        'column_names': 'list:str',
+        "column_names": "list:str",
     }
 
     def __init__(self, source, column_names=None, **kwargs):
         """
         :param column_names: an ordered list of strings that will be used as the keys
             for each column in the returned dictionaries.
             The number of key, value pairs each returned dictionary has will
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/dataset.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,23 +6,23 @@
         (e.g. parsing genomic regions from their source)
 """
 import logging
 import sys
 
 from bx import (
     seq as bx_seq,
-    wiggle as bx_wig
+    wiggle as bx_wig,
 )
 
 from galaxy.util import sqlite
 from . import (
     base,
     column,
     external,
-    line
+    line,
 )
 
 _TODO = """
 use bx as much as possible
 gff3 hierarchies
 
 change SamtoolsDataProvider to use pysam
@@ -48,43 +48,44 @@
         :param dataset: the Galaxy dataset whose file will be the source
         :type dataset: model.DatasetInstance
         """
         # precondition: dataset is a galaxy.model.DatasetInstance
         self.dataset = dataset
         # this dataset file is obviously the source
         # TODO: this might be a good place to interface with the object_store...
-        mode = 'rb' if dataset.datatype.is_binary else 'r'
+        mode = "rb" if dataset.datatype.is_binary else "r"
         super().__init__(open(dataset.file_name, mode))
 
     # TODO: this is a bit of a mess
     @classmethod
     def get_column_metadata_from_dataset(cls, dataset):
         """
         Convenience class method to get column metadata from a dataset.
 
         :returns: a dictionary of `column_count`, `column_types`, and `column_names`
             if they're available, setting each to `None` if not.
         """
         # re-map keys to fit ColumnarProvider.__init__ kwargs
         params = {}
-        params['column_count'] = dataset.metadata.columns
-        params['column_types'] = dataset.metadata.column_types
-        params['column_names'] = dataset.metadata.column_names or getattr(dataset.datatype, 'column_names', None)
+        params["column_count"] = dataset.metadata.columns
+        params["column_types"] = dataset.metadata.column_types
+        params["column_names"] = dataset.metadata.column_names or getattr(dataset.datatype, "column_names", None)
         return params
 
     def get_metadata_column_types(self, indeces=None):
         """
         Return the list of `column_types` for this dataset or `None` if unavailable.
 
         :param indeces: the indeces for the columns of which to return the types.
             Optional: defaults to None (return all types)
         :type indeces: list of ints
         """
-        metadata_column_types = (self.dataset.metadata.column_types
-                                 or getattr(self.dataset.datatype, 'column_types', None) or None)
+        metadata_column_types = (
+            self.dataset.metadata.column_types or getattr(self.dataset.datatype, "column_types", None) or None
+        )
         if not metadata_column_types:
             return metadata_column_types
         if indeces:
             column_types = []
             for index in indeces:
                 column_type = metadata_column_types[index] if index < len(metadata_column_types) else None
                 column_types.append(column_type)
@@ -95,16 +96,17 @@
         """
         Return the list of `column_names` for this dataset or `None` if unavailable.
 
         :param indeces: the indeces for the columns of which to return the names.
             Optional: defaults to None (return all names)
         :type indeces: list of ints
         """
-        metadata_column_names = (self.dataset.metadata.column_names
-                                 or getattr(self.dataset.datatype, 'column_names', None) or None)
+        metadata_column_names = (
+            self.dataset.metadata.column_names or getattr(self.dataset.datatype, "column_names", None) or None
+        )
         if not metadata_column_names:
             return metadata_column_names
         if indeces:
             column_names = []
             for index in indeces:
                 column_type = metadata_column_names[index] if index < len(metadata_column_names) else None
                 column_names.append(column_type)
@@ -118,19 +120,21 @@
 
         :param list_of_column_names: the names of the columns of which to get indeces.
         :type list_of_column_names: list of strs
 
         :raises KeyError: if column_names are not found
         :raises ValueError: if an entry in list_of_column_names is not in column_names
         """
-        metadata_column_names = (self.dataset.metadata.column_names
-                                 or getattr(self.dataset.datatype, 'column_names', None) or None)
+        metadata_column_names = (
+            self.dataset.metadata.column_names or getattr(self.dataset.datatype, "column_names", None) or None
+        )
         if not metadata_column_names:
-            raise KeyError('No column_names found for '
-                           + f'datatype: {str(self.dataset.datatype)}, dataset: {str(self.dataset)}')
+            raise KeyError(
+                "No column_names found for " + f"datatype: {str(self.dataset.datatype)}, dataset: {str(self.dataset)}"
+            )
         indeces = []  # if indeces and column_names:
         # pull using indeces and re-name with given names - no need to alter (does as super would)
         #    pass
         for column_name in list_of_column_names:
             indeces.append(metadata_column_names.index(column_name))
         return indeces
 
@@ -148,29 +152,29 @@
         a source representing a genomic region.
 
         :param check: if True will raise a ValueError if any were not found.
         :type check: bool
         :raises ValueError: if check is `True` and one or more indeces were not found.
         :returns: list of column indeces for the named columns.
         """
-        region_column_names = ('chromCol', 'startCol', 'endCol')
+        region_column_names = ("chromCol", "startCol", "endCol")
         region_indices = [self.get_metadata_column_index_by_name(name) for name in region_column_names]
         if check and not all(_ is not None for _ in region_indices):
             raise ValueError(f"Could not determine proper column indices for chrom, start, end: {str(region_indices)}")
         return region_indices
 
 
 class ConvertedDatasetDataProvider(DatasetDataProvider):
     """
     Class that uses the file contents of a dataset after conversion to a different
     format.
     """
 
     def __init__(self, dataset, **kwargs):
-        raise NotImplementedError('Abstract class')
+        raise NotImplementedError("Abstract class")
         # self.original_dataset = dataset
         # self.converted_dataset = self.convert_dataset(dataset, **kwargs)
         # super(ConvertedDatasetDataProvider, self).__init__(self.converted_dataset, **kwargs)
         # NOTE: now self.converted_dataset == self.dataset
 
     def convert_dataset(self, dataset, **kwargs):
         """
@@ -194,17 +198,17 @@
 
         If no kwargs are given, this class will attempt to get those kwargs
         from the dataset source's metadata.
         If any kwarg is given, it will override and be used in place of
         any metadata available.
         """
         dataset_source = DatasetDataProvider(dataset)
-        if not kwargs.get('column_types', None):
-            indeces = kwargs.get('indeces', None)
-            kwargs['column_types'] = dataset_source.get_metadata_column_types(indeces=indeces)
+        if not kwargs.get("column_types", None):
+            indeces = kwargs.get("indeces", None)
+            kwargs["column_types"] = dataset_source.get_metadata_column_types(indeces=indeces)
         super().__init__(dataset_source, **kwargs)
 
 
 class DatasetDictDataProvider(column.DictDataProvider):
     """
     Data provider that uses a DatasetDataProvider as its source and the
     dataset's metadata to buuild settings for the DictDataProvider it's
@@ -230,32 +234,32 @@
         | Names NOT given | pull indeces, name w/ meta    | pull all, name w/meta |
         +=================+-------------------------------+-----------------------+
         """
         dataset_source = DatasetDataProvider(dataset)
 
         # TODO: getting too complicated - simplify at some lvl, somehow
         # if no column_types given, get column_types from indeces (or all if indeces == None)
-        indeces = kwargs.get('indeces', None)
-        column_names = kwargs.get('column_names', None)
+        indeces = kwargs.get("indeces", None)
+        column_names = kwargs.get("column_names", None)
 
         if not indeces and column_names:
             # pull columns by name
-            indeces = kwargs['indeces'] = dataset_source.get_indeces_by_column_names(column_names)
+            indeces = kwargs["indeces"] = dataset_source.get_indeces_by_column_names(column_names)
 
         elif indeces and not column_names:
             # pull using indeces, name with meta
-            column_names = kwargs['column_names'] = dataset_source.get_metadata_column_names(indeces=indeces)
+            column_names = kwargs["column_names"] = dataset_source.get_metadata_column_names(indeces=indeces)
 
         elif not indeces and not column_names:
             # pull all indeces and name using metadata
-            column_names = kwargs['column_names'] = dataset_source.get_metadata_column_names(indeces=indeces)
+            column_names = kwargs["column_names"] = dataset_source.get_metadata_column_names(indeces=indeces)
 
         # if no column_types given, use metadata column_types
-        if not kwargs.get('column_types', None):
-            kwargs['column_types'] = dataset_source.get_metadata_column_types(indeces=indeces)
+        if not kwargs.get("column_types", None):
+            kwargs["column_types"] = dataset_source.get_metadata_column_types(indeces=indeces)
 
         super().__init__(dataset_source, **kwargs)
 
 
 # ----------------------------------------------------------------------------- provides a bio-relevant datum
 class GenomicRegionDataProvider(column.ColumnarDataProvider):
     """
@@ -263,21 +267,22 @@
     using the datasets metadata settings.
 
     Is a ColumnarDataProvider that uses a DatasetDataProvider as its source.
 
     If `named_columns` is true, will return dictionaries with the keys
     'chrom', 'start', 'end'.
     """
+
     # dictionary keys when named_columns=True
-    COLUMN_NAMES = ['chrom', 'start', 'end']
+    COLUMN_NAMES = ["chrom", "start", "end"]
     settings = {
-        'chrom_column': 'int',
-        'start_column': 'int',
-        'end_column': 'int',
-        'named_columns': 'bool',
+        "chrom_column": "int",
+        "start_column": "int",
+        "end_column": "int",
+        "named_columns": "bool",
     }
 
     def __init__(self, dataset, chrom_column=None, start_column=None, end_column=None, named_columns=False, **kwargs):
         """
         :param dataset: the Galaxy dataset whose file will be the source
         :type dataset: model.DatasetInstance
 
@@ -293,27 +298,26 @@
             Optional: defaults to False
         :type named_columns: bool
         """
         # TODO: allow passing in a string format e.g. "{chrom}:{start}-{end}"
         dataset_source = DatasetDataProvider(dataset)
 
         if chrom_column is None:
-            chrom_column = dataset_source.get_metadata_column_index_by_name('chromCol')
+            chrom_column = dataset_source.get_metadata_column_index_by_name("chromCol")
         if start_column is None:
-            start_column = dataset_source.get_metadata_column_index_by_name('startCol')
+            start_column = dataset_source.get_metadata_column_index_by_name("startCol")
         if end_column is None:
-            end_column = dataset_source.get_metadata_column_index_by_name('endCol')
+            end_column = dataset_source.get_metadata_column_index_by_name("endCol")
         indeces = [chrom_column, start_column, end_column]
         if not all(_ is not None for _ in indeces):
-            raise ValueError("Could not determine proper column indeces for"
-                             + f" chrom, start, end: {str(indeces)}")
-        kwargs.update({'indeces': indeces})
+            raise ValueError("Could not determine proper column indeces for" + f" chrom, start, end: {str(indeces)}")
+        kwargs.update({"indeces": indeces})
 
-        if not kwargs.get('column_types', None):
-            kwargs.update({'column_types': dataset_source.get_metadata_column_types(indeces=indeces)})
+        if not kwargs.get("column_types", None):
+            kwargs.update({"column_types": dataset_source.get_metadata_column_types(indeces=indeces)})
 
         self.named_columns = named_columns
         if self.named_columns:
             self.column_names = self.COLUMN_NAMES
 
         super().__init__(dataset_source, **kwargs)
 
@@ -332,26 +336,36 @@
     """
     Data provider that parses chromosome, start, and end data (as well as strand
     and name if set in the metadata) using the dataset's metadata settings.
 
     If `named_columns` is true, will return dictionaries with the keys
     'chrom', 'start', 'end' (and 'strand' and 'name' if available).
     """
-    COLUMN_NAMES = ['chrom', 'start', 'end', 'strand', 'name']
+
+    COLUMN_NAMES = ["chrom", "start", "end", "strand", "name"]
     settings = {
-        'chrom_column': 'int',
-        'start_column': 'int',
-        'end_column': 'int',
-        'strand_column': 'int',
-        'name_column': 'int',
-        'named_columns': 'bool',
+        "chrom_column": "int",
+        "start_column": "int",
+        "end_column": "int",
+        "strand_column": "int",
+        "name_column": "int",
+        "named_columns": "bool",
     }
 
-    def __init__(self, dataset, chrom_column=None, start_column=None, end_column=None,
-                 strand_column=None, name_column=None, named_columns=False, **kwargs):
+    def __init__(
+        self,
+        dataset,
+        chrom_column=None,
+        start_column=None,
+        end_column=None,
+        strand_column=None,
+        name_column=None,
+        named_columns=False,
+        **kwargs,
+    ):
         """
         :param dataset: the Galaxy dataset whose file will be the source
         :type dataset: model.DatasetInstance
 
         :param named_columns: optionally return dictionaries keying each column
             with 'chrom', 'start', 'end', 'strand', or 'name'.
             Optional: defaults to False
@@ -361,42 +375,42 @@
         dataset_source = DatasetDataProvider(dataset)
 
         # get genomic indeces and add strand and name
         self.column_names = []
         indeces = []
         # TODO: this is sort of involved and oogly
         if chrom_column is None:
-            chrom_column = dataset_source.get_metadata_column_index_by_name('chromCol')
+            chrom_column = dataset_source.get_metadata_column_index_by_name("chromCol")
             if chrom_column is not None:
-                self.column_names.append('chrom')
+                self.column_names.append("chrom")
                 indeces.append(chrom_column)
         if start_column is None:
-            start_column = dataset_source.get_metadata_column_index_by_name('startCol')
+            start_column = dataset_source.get_metadata_column_index_by_name("startCol")
             if start_column is not None:
-                self.column_names.append('start')
+                self.column_names.append("start")
                 indeces.append(start_column)
         if end_column is None:
-            end_column = dataset_source.get_metadata_column_index_by_name('endCol')
+            end_column = dataset_source.get_metadata_column_index_by_name("endCol")
             if end_column is not None:
-                self.column_names.append('end')
+                self.column_names.append("end")
                 indeces.append(end_column)
         if strand_column is None:
-            strand_column = dataset_source.get_metadata_column_index_by_name('strandCol')
+            strand_column = dataset_source.get_metadata_column_index_by_name("strandCol")
             if strand_column is not None:
-                self.column_names.append('strand')
+                self.column_names.append("strand")
                 indeces.append(strand_column)
         if name_column is None:
-            name_column = dataset_source.get_metadata_column_index_by_name('nameCol')
+            name_column = dataset_source.get_metadata_column_index_by_name("nameCol")
             if name_column is not None:
-                self.column_names.append('name')
+                self.column_names.append("name")
                 indeces.append(name_column)
 
-        kwargs.update({'indeces': indeces})
-        if not kwargs.get('column_types', None):
-            kwargs.update({'column_types': dataset_source.get_metadata_column_types(indeces=indeces)})
+        kwargs.update({"indeces": indeces})
+        if not kwargs.get("column_types", None):
+            kwargs.update({"column_types": dataset_source.get_metadata_column_types(indeces=indeces)})
 
         self.named_columns = named_columns
 
         super().__init__(dataset_source, **kwargs)
 
     def __iter__(self):
         parent_gen = super().__iter__()
@@ -414,16 +428,17 @@
     Class that returns fasta format data in a list of maps of the form::
 
         {
             id: <fasta header id>,
             sequence: <joined lines of nucleotide/amino data>
         }
     """
+
     settings = {
-        'ids': 'list:str',
+        "ids": "list:str",
     }
 
     def __init__(self, source, ids=None, **kwargs):
         """
         :param ids: optionally return only ids (and sequences) that are in this list.
             Optional: defaults to None (provide all ids)
         :type ids: list or None
@@ -433,31 +448,29 @@
         super().__init__(source, **kwargs)
         self.ids = ids
         # how to do ids?
 
     def __iter__(self):
         parent_gen = super().__iter__()
         for fasta_record in parent_gen:
-            yield {
-                'id': fasta_record.name,
-                'seq': fasta_record.text
-            }
+            yield {"id": fasta_record.name, "seq": fasta_record.text}
 
 
 class TwoBitFastaDataProvider(DatasetDataProvider):
     """
     Class that returns fasta format data in a list of maps of the form::
 
         {
             id: <fasta header id>,
             sequence: <joined lines of nucleotide/amino data>
         }
     """
+
     settings = {
-        'ids': 'list:str',
+        "ids": "list:str",
     }
 
     def __init__(self, source, ids=None, **kwargs):
         """
         :param ids: optionally return only ids (and sequences) that are in this list.
             Optional: defaults to None (provide all ids)
         :type ids: list or None
@@ -466,29 +479,27 @@
         # TODO: validate is a 2bit
         super(FastaDataProvider, self).__init__(source, **kwargs)
         # could do in order provided with twobit
         self.ids = ids or self.source.keys()
 
     def __iter__(self):
         for id_ in self.ids:
-            yield {
-                'id': id_,
-                'seq': self.source[id_]
-            }
+            yield {"id": id_, "seq": self.source[id_]}
 
 
 # TODO:
 class WiggleDataProvider(base.LimitedOffsetDataProvider):
     """
     Class that returns chrom, pos, data from a wiggle source.
     """
-    COLUMN_NAMES = ['chrom', 'pos', 'value']
+
+    COLUMN_NAMES = ["chrom", "pos", "value"]
     settings = {
-        'named_columns': 'bool',
-        'column_names': 'list:str',
+        "named_columns": "bool",
+        "column_names": "list:str",
     }
 
     def __init__(self, source, named_columns=False, column_names=None, **kwargs):
         """
         :param named_columns: optionally return dictionaries keying each column
             with 'chrom', 'start', 'end', 'strand', or 'name'.
             Optional: defaults to False
@@ -519,18 +530,19 @@
                 yield list(three_tuple)
 
 
 class BigWigDataProvider(base.LimitedOffsetDataProvider):
     """
     Class that returns chrom, pos, data from a wiggle source.
     """
-    COLUMN_NAMES = ['chrom', 'pos', 'value']
+
+    COLUMN_NAMES = ["chrom", "pos", "value"]
     settings = {
-        'named_columns': 'bool',
-        'column_names': 'list:str',
+        "named_columns": "bool",
+        "column_names": "list:str",
     }
 
     def __init__(self, source, chrom, start, end, named_columns=False, column_names=None, **kwargs):
         """
 
         :param chrom: which chromosome within the bigbed file to extract data for
         :type chrom: str
@@ -547,15 +559,15 @@
         :param column_names: an ordered list of strings that will be used as the keys
                              for each column in the returned dictionaries.
                              The number of key, value pairs each returned dictionary has will
                              be as short as the number of column names provided.
         :type column_names:
 
         """
-        raise NotImplementedError('Work in progress')
+        raise NotImplementedError("Work in progress")
         # TODO: validate is a wig
         # still good to maintain a ref to the raw source bc Reader won't
         # self.raw_source = source
         # self.parser = bx_bbi.bigwig_file.BigWigFile(source)
         # super(BigWigDataProvider, self).__init__(self.parser, **kwargs)
 
         # self.named_columns = named_columns
@@ -575,39 +587,41 @@
 class DatasetSubprocessDataProvider(external.SubprocessDataProvider):
     """
     Create a source from running a subprocess on a dataset's file.
 
     Uses a subprocess as its source and has a dataset (gen. as an input file
     for the process).
     """
+
     # TODO: below should be a subclass of this and not RegexSubprocess
 
     def __init__(self, dataset, *args, **kwargs):
         """
         :param args: the list of strings used to build commands.
         :type args: variadic function args
         """
-        raise NotImplementedError('Abstract class')
+        raise NotImplementedError("Abstract class")
         # super(DatasetSubprocessDataProvider, self).__init__(*args, **kwargs)
         # self.dataset = dataset
 
 
 class SamtoolsDataProvider(line.RegexLineDataProvider):
     """
     Data provider that uses samtools on a Sam or Bam file as its source.
 
     This can be piped through other providers (column, map, genome region, etc.).
 
     .. note:: that only the samtools 'view' command is currently implemented.
     """
-    FLAGS_WO_ARGS = 'bhHSu1xXcB'
-    FLAGS_W_ARGS = 'fFqlrs'
+
+    FLAGS_WO_ARGS = "bhHSu1xXcB"
+    FLAGS_W_ARGS = "fFqlrs"
     VALID_FLAGS = FLAGS_WO_ARGS + FLAGS_W_ARGS
 
-    def __init__(self, dataset, options_string='', options_dict=None, regions=None, **kwargs):
+    def __init__(self, dataset, options_string="", options_dict=None, regions=None, **kwargs):
         """
         :param options_string: samtools options in string form (flags separated
             by spaces)
             Optional: defaults to ''
         :type options_string: str
         :param options_dict: dictionary of samtools options
             Optional: defaults to None
@@ -623,26 +637,26 @@
 
         options_dict = options_dict or {}
         # ensure regions are strings
         regions = [str(r) for r in regions] if regions else []
 
         # TODO: view only for now
         # TODO: not properly using overriding super's validate_opts, command here
-        subcommand = 'view'
+        subcommand = "view"
         # TODO:?? do we need a path to samtools?
         subproc_args = self.build_command_list(subcommand, options_string, options_dict, regions)
-# TODO: the composition/inheritance here doesn't make a lot sense
+        # TODO: the composition/inheritance here doesn't make a lot sense
         subproc_provider = external.SubprocessDataProvider(*subproc_args)
         super().__init__(subproc_provider, **kwargs)
 
     def build_command_list(self, subcommand, options_string, options_dict, regions):
         """
         Convert all init args to list form.
         """
-        command = ['samtools', subcommand]
+        command = ["samtools", subcommand]
         # add options and switches, input file, regions list (if any)
         command.extend(self.to_options_list(options_string, options_dict))
         command.append(self.dataset.file_name)
         command.extend(regions)
         return command
 
     def to_options_list(self, options_string, options_dict):
@@ -650,22 +664,21 @@
         Convert both options_string and options_dict to list form
         while filtering out non-'valid' options.
         """
         opt_list = []
 
         # strip out any user supplied bash switch formating -> string of option chars
         #   then compress to single option string of unique, VALID flags with prefixed bash switch char '-'
-        options_string = options_string.strip('- ')
+        options_string = options_string.strip("- ")
         validated_flag_list = {flag for flag in options_string if flag in self.FLAGS_WO_ARGS}
 
         # if sam add -S
         # TODO: not the best test in the world...
-        if((self.dataset.ext == 'sam')
-                and ('S' not in validated_flag_list)):
-            validated_flag_list.append('S')
+        if (self.dataset.ext == "sam") and ("S" not in validated_flag_list):
+            validated_flag_list.append("S")
 
         if validated_flag_list:
             opt_list.append(f"-{''.join(validated_flag_list)}")
 
         for flag, arg in options_dict.items():
             if flag in self.FLAGS_W_ARGS:
                 opt_list.extend([f"-{flag}", str(arg)])
@@ -692,17 +705,16 @@
 
 class SQliteDataProvider(base.DataProvider):
     """
     Data provider that uses a sqlite database file as its source.
 
     Allows any query to be run and returns the resulting rows as sqlite3 row objects
     """
-    settings = {
-        'query': 'str'
-    }
+
+    settings = {"query": "str"}
 
     def __init__(self, source, query=None, **kwargs):
         self.query = query
         self.connection = sqlite.connect(source.dataset.file_name)
         super().__init__(source, **kwargs)
 
     def __iter__(self):
@@ -713,19 +725,16 @@
 
 
 class SQliteDataTableProvider(base.DataProvider):
     """
     Data provider that uses a sqlite database file as its source.
     Allows any query to be run and returns the resulting rows as arrays of arrays
     """
-    settings = {
-        'query': 'str',
-        'headers': 'bool',
-        'limit': 'int'
-    }
+
+    settings = {"query": "str", "headers": "bool", "limit": "int"}
 
     def __init__(self, source, query=None, headers=False, limit=sys.maxsize, **kwargs):
         self.query = query
         self.headers = headers
         self.limit = limit
         self.connection = sqlite.connect(source.dataset.file_name)
         super().__init__(source, **kwargs)
@@ -745,17 +754,16 @@
 
 
 class SQliteDataDictProvider(base.DataProvider):
     """
     Data provider that uses a sqlite database file as its source.
     Allows any query to be run and returns the resulting rows as arrays of dicts
     """
-    settings = {
-        'query': 'str'
-    }
+
+    settings = {"query": "str"}
 
     def __init__(self, source, query=None, **kwargs):
         self.query = query
         self.connection = sqlite.connect(source.dataset.file_name)
         super().__init__(source, **kwargs)
 
     def __iter__(self):
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/decorators.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/decorators.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,16 +17,16 @@
 import copy
 import logging
 from functools import wraps
 from urllib.parse import unquote
 
 log = logging.getLogger(__name__)
 
-_DATAPROVIDER_CLASS_MAP_KEY = 'dataproviders'
-_DATAPROVIDER_METHOD_NAME_KEY = '_dataprovider_name'
+_DATAPROVIDER_CLASS_MAP_KEY = "dataproviders"
+_DATAPROVIDER_METHOD_NAME_KEY = "_dataprovider_name"
 
 
 def has_dataproviders(cls):
     """
     Wraps a class (generally a Datatype), finds methods within that have been
     decorated with `@dataprovider` and adds them, by their name, to a map
     in the class.
@@ -63,17 +63,19 @@
     dataproviders = getattr(cls, _DATAPROVIDER_CLASS_MAP_KEY)
 
     # scan for methods with dataprovider names and add them to the map
     # note: this has a 'cascading' effect
     #       where it's possible to override a super's provider with a sub's
     for attr_key, attr_value in cls.__dict__.items():
         # can't use isinstance( attr_value, MethodType ) bc of wrapping
-        if((callable(attr_value))
-                and (not attr_key.startswith("__"))
-                and (getattr(attr_value, _DATAPROVIDER_METHOD_NAME_KEY, None))):
+        if (
+            (callable(attr_value))
+            and (not attr_key.startswith("__"))
+            and (getattr(attr_value, _DATAPROVIDER_METHOD_NAME_KEY, None))
+        ):
             name = getattr(attr_value, _DATAPROVIDER_METHOD_NAME_KEY)
             dataproviders[name] = attr_value
     return cls
 
 
 def dataprovider_factory(name, settings=None):
     """
@@ -88,14 +90,15 @@
 
     :param name: what name/key to register the factory under in `cls.dataproviders`
     :type name: any hashable var
     :param settings: dictionary containing key/type pairs for parsing query strings
         to __init__ arguments
     :type settings: dictionary
     """
+
     # TODO:?? use *args for settings allowing mulitple dictionaries
     # make a function available through the name->provider dispatch to parse query strings
     #   callable like:
     # settings_dict = dataproviders[ provider_name ].parse_query_string_settings( query_kwargs )
     # TODO: ugh - overly complicated but the best I could think of
     def parse_query_string_settings(query_kwargs):
         return _parse_query_string_settings(query_kwargs, settings)
@@ -106,47 +109,50 @@
         func.parse_query_string_settings = parse_query_string_settings
         func.settings = settings
         # TODO: I want a way to inherit settings from the previous provider( this_name ) instead of defining over and over
 
         @wraps(func)
         def wrapped_dataprovider_factory(self, *args, **kwargs):
             return func(self, *args, **kwargs)
+
         return wrapped_dataprovider_factory
+
     return named_dataprovider_factory
 
 
 def _parse_query_string_settings(query_kwargs, settings=None):
     """
     Parse the values in `query_kwargs` from strings to the proper types
     listed in the same key in `settings`.
     """
+
     # TODO: this was a relatively late addition: review and re-think
     def list_from_query_string(s):
         # assume csv
-        return s.split(',')
+        return s.split(",")
 
     parsers = {
-        'int': int,
-        'float': float,
-        'bool': bool,
-        'list:str': lambda s: list_from_query_string(s),
-        'list:escaped': lambda s: [unquote(e) for e in list_from_query_string(s)],
-        'list:int': lambda s: [int(i) for i in list_from_query_string(s)],
+        "int": int,
+        "float": float,
+        "bool": bool,
+        "list:str": lambda s: list_from_query_string(s),
+        "list:escaped": lambda s: [unquote(e) for e in list_from_query_string(s)],
+        "list:int": lambda s: [int(i) for i in list_from_query_string(s)],
     }
     settings = settings or {}
     # yay! yet another set of query string parsers! <-- sarcasm
     # work through the keys in settings finding matching keys in query_kwargs
     #   if found in both, get the expected/needed type from settings and store the new parsed value
     #   if we can't parse it (no parser, bad value), delete the key from query_kwargs so the provider will use the defaults
     for key in settings:
         if key in query_kwargs:
             # TODO: this would be the place to sanitize any strings
             query_value = query_kwargs[key]
             needed_type = settings[key]
-            if needed_type != 'str':
+            if needed_type != "str":
                 try:
                     query_kwargs[key] = parsers[needed_type](query_value)
                 except (KeyError, ValueError):
                     del query_kwargs[key]
 
         # TODO:?? do we want to remove query_kwarg entries NOT in settings?
     return query_kwargs
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/exceptions.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/exceptions.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,16 +4,16 @@
 
 
 class InvalidDataProviderSource(TypeError):
     """
     Raised when a unusable source is passed to a provider.
     """
 
-    def __init__(self, source=None, msg=''):
-        msg = msg or f'Invalid source for provider: {source}'
+    def __init__(self, source=None, msg=""):
+        msg = msg or f"Invalid source for provider: {source}"
         super().__init__(msg)
 
 
 class NoProviderAvailable(TypeError):
     """
     Raised when no provider is found for the given `format_requested`.
 
@@ -23,14 +23,14 @@
 
     Both params are attached to this class and accessible to the try-catch
     receiver.
 
     Meant to be used within a class that builds dataproviders (e.g. a Datatype)
     """
 
-    def __init__(self, factory_source, format_requested=None, msg=''):
+    def __init__(self, factory_source, format_requested=None, msg=""):
         self.factory_source = factory_source
         self.format_requested = format_requested
         msg = msg or f'No provider available in factory_source "{str(factory_source)}" for format requested'
         if self.format_requested:
             msg += f': "{self.format_requested}"'
         super().__init__(msg)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/external.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/external.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,15 +11,15 @@
     urlparse,
 )
 from urllib.request import urlopen
 
 from galaxy.util import DEFAULT_SOCKET_TIMEOUT
 from . import (
     base,
-    line
+    line,
 )
 
 _TODO = """
 YAGNI: ftp, image, cryptos, sockets
 job queue
 admin: admin server log rgx/stats, ps aux
 """
@@ -29,14 +29,15 @@
 
 # ----------------------------------------------------------------------------- server subprocess / external prog
 class SubprocessDataProvider(base.DataProvider):
     """
     Data provider that uses the output from an intermediate program and
     subprocess as its data source.
     """
+
     # TODO: need better ways of checking returncode, stderr for errors and raising
 
     def __init__(self, *args, **kwargs):
         """
         :param args: the list of strings used to build commands.
         :type args: variadic function args
         """
@@ -53,40 +54,41 @@
         """
         :param args: the list of strings used as commands.
         :type args: variadic function args
         """
         try:
             # how expensive is this?
             popen = subprocess.Popen(command_list, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
-            log.info(f'opened subrocess ({str(command_list)}), PID: {str(popen.pid)}')
+            log.info(f"opened subrocess ({str(command_list)}), PID: {str(popen.pid)}")
 
         except OSError as os_err:
-            command_str = ' '.join(self.command)
-            raise OSError(' '.join((str(os_err), ':', command_str)))
+            command_str = " ".join(self.command)
+            raise OSError(" ".join((str(os_err), ":", command_str)))
 
         return popen
 
     def __exit__(self, *args):
         # poll the subrocess for an exit code
         self.exit_code = self.popen.poll()
-        log.info(f'{str(self)}.__exit__, exit_code: {str(self.exit_code)}')
+        log.info(f"{str(self)}.__exit__, exit_code: {str(self.exit_code)}")
         return super().__exit__(*args)
 
     def __str__(self):
         # provide the pid and current return code
-        source_str = ''
-        if hasattr(self, 'popen'):
-            source_str = f'{str(self.popen.pid)}:{str(self.popen.poll())}'
-        return f'{self.__class__.__name__}({str(source_str)})'
+        source_str = ""
+        if hasattr(self, "popen"):
+            source_str = f"{str(self.popen.pid)}:{str(self.popen.poll())}"
+        return f"{self.__class__.__name__}({str(source_str)})"
 
 
 class RegexSubprocessDataProvider(line.RegexLineDataProvider):
     """
     RegexLineDataProvider that uses a SubprocessDataProvider as its data source.
     """
+
     # this is a conv. class and not really all that necc...
 
     def __init__(self, *args, **kwargs):
         # using subprocess as proxy data source in filtered line prov.
         subproc_provider = SubprocessDataProvider(*args)
         super().__init__(subproc_provider, **kwargs)
 
@@ -94,17 +96,18 @@
 # ----------------------------------------------------------------------------- other apis
 class URLDataProvider(base.DataProvider):
     """
     Data provider that uses the contents of a URL for its data source.
 
     This can be piped through other providers (column, map, genome region, etc.).
     """
-    VALID_METHODS = ('GET', 'POST')
 
-    def __init__(self, url, method='GET', data=None, **kwargs):
+    VALID_METHODS = ("GET", "POST")
+
+    def __init__(self, url, method="GET", data=None, **kwargs):
         """
         :param url: the base URL to open.
         :param method: the HTTP method to use.
             Optional: defaults to 'GET'
         :param data: any data to pass (either in query for 'GET'
             or as post data with 'POST')
         :type data: dict
@@ -112,23 +115,23 @@
         self.url = url
         self.method = method
 
         self.data = data or {}
         encoded_data = urlencode(self.data)
 
         scheme = urlparse(url).scheme
-        assert scheme in ('http', 'https', 'ftp'), f'Invalid URL scheme: {scheme}'
+        assert scheme in ("http", "https", "ftp"), f"Invalid URL scheme: {scheme}"
 
-        if method == 'GET':
-            self.url += f'?{encoded_data}'
+        if method == "GET":
+            self.url += f"?{encoded_data}"
             opened = urlopen(url, timeout=DEFAULT_SOCKET_TIMEOUT)
-        elif method == 'POST':
+        elif method == "POST":
             opened = urlopen(url, encoded_data, timeout=DEFAULT_SOCKET_TIMEOUT)
         else:
-            raise ValueError(f'Not a valid method: {method}')
+            raise ValueError(f"Not a valid method: {method}")
 
         super().__init__(opened, **kwargs)
         # NOTE: the request object is now accessible as self.source
 
     def __enter__(self):
         pass
 
@@ -141,15 +144,15 @@
     """
     Data provider that uses g(un)zip on a file as its source.
 
     This can be piped through other providers (column, map, genome region, etc.).
     """
 
     def __init__(self, source, **kwargs):
-        unzipped = gzip.GzipFile(source, 'rb')
+        unzipped = gzip.GzipFile(source, "rb")
         super().__init__(unzipped, **kwargs)
         # NOTE: the GzipFile is now accessible in self.source
 
 
 # ----------------------------------------------------------------------------- intermediate tempfile
 class TempfileDataProvider(base.DataProvider):
     """
@@ -167,10 +170,10 @@
 
     def create_file(self):
         self.tmp_file = tempfile.NamedTemporaryFile()
         return self.tmp_file
 
     def write_to_file(self):
         parent_gen = super().__iter__()
-        with open(self.tmp_file, 'w') as open_file:
+        with open(self.tmp_file, "w") as open_file:
             for datum in parent_gen:
                 open_file.write(f"{datum}\n")
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/hierarchy.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/hierarchy.py`

 * *Files 6% similar despite different names*

```diff
@@ -27,21 +27,22 @@
 
 
 # ----------------------------------------------------------------------------- xml
 class XMLDataProvider(HierarchalDataProvider):
     """
     Data provider that converts selected XML elements to dictionaries.
     """
+
     # using lxml.etree's iterparse method to keep mem down
     # TODO:   this, however (AFAIK), prevents the use of xpath
     settings = {
-        'selector': 'str',  # urlencoded
-        'max_depth': 'int',
+        "selector": "str",  # urlencoded
+        "max_depth": "int",
     }
-    ITERPARSE_ALL_EVENTS = ('start', 'end', 'start-ns', 'end-ns')
+    ITERPARSE_ALL_EVENTS = ("start", "end", "start-ns", "end-ns")
     # TODO: move appropo into super
 
     def __init__(self, source, selector=None, max_depth=None, **kwargs):
         """
         :param  selector:   some partial string in the desired tags to return
         :param  max_depth:  the number of generations of descendents to return
         """
@@ -60,30 +61,29 @@
 
         Change point for more sophisticated selectors.
         """
         # search for partial match of selector to the element tag
         # TODO: add more flexibility here w/o re-implementing xpath
         # TODO: fails with '#' - browser thinks it's an anchor - use urlencode
         # TODO: need removal/replacement of etree namespacing here - then move to string match
-        Element = getattr(etree, '_Element', etree.Element)
-        return bool((selector is None)
-                    or (isinstance(element, Element) and selector in element.tag))
+        Element = getattr(etree, "_Element", etree.Element)
+        return bool((selector is None) or (isinstance(element, Element) and selector in element.tag))
 
     def element_as_dict(self, element):
         """
         Converts an XML element (its text, tag, and attributes) to dictionary form.
 
         :param  element:    an XML ``Element``
         """
         # TODO: Key collision is unlikely here, but still should be better handled
         return {
-            'tag': element.tag,
-            'text': element.text.strip() if element.text else None,
+            "tag": element.tag,
+            "text": element.text.strip() if element.text else None,
             # needs shallow copy to protect v. element.clear()
-            'attrib': dict(element.attrib)
+            "attrib": dict(element.attrib),
         }
 
     def get_children(self, element, max_depth=None):
         """
         Yield all children of element (and their children - recursively)
         in dictionary form.
         :param  element:    an XML ``Element``
@@ -92,45 +92,44 @@
         if not isinstance(max_depth, int) or max_depth >= 1:
             for child in element:
                 child_data = self.element_as_dict(child)
 
                 next_depth = max_depth - 1 if isinstance(max_depth, int) else None
                 grand_children = list(self.get_children(child, next_depth))
                 if grand_children:
-                    child_data['children'] = grand_children
+                    child_data["children"] = grand_children
 
                 yield child_data
 
     def __iter__(self):
         context = etree.iterparse(self.source, events=self.ITERPARSE_ALL_EVENTS)
         context = iter(context)
 
         selected_element = None
         for event, element in context:
-            if event == 'start-ns':
+            if event == "start-ns":
                 ns, uri = element
                 self.namespaces[ns] = uri
 
-            elif event == 'start':
-                if((selected_element is None)
-                        and (self.matches_selector(element, self.selector))):
+            elif event == "start":
+                if (selected_element is None) and (self.matches_selector(element, self.selector)):
                     # start tag of selected element - wait for 'end' to emit/yield
                     selected_element = element
 
-            elif event == 'end':
-                if((selected_element is not None) and (element == selected_element)):
+            elif event == "end":
+                if (selected_element is not None) and (element == selected_element):
                     self.num_valid_data_read += 1
 
                     # offset
                     if self.num_valid_data_read > self.offset:
                         # convert to dict and yield
                         selected_element_dict = self.element_as_dict(selected_element)
                         children = list(self.get_children(selected_element, self.max_depth))
                         if children:
-                            selected_element_dict['children'] = children
+                            selected_element_dict["children"] = children
                         yield selected_element_dict
 
                         # limit
                         self.num_data_returned += 1
                         if self.limit is not None and self.num_data_returned >= self.limit:
                             break
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/dataproviders/line.py` & `galaxy-data-23.0.1/galaxy/datatypes/dataproviders/line.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,24 +19,32 @@
 
 class FilteredLineDataProvider(base.LimitedOffsetDataProvider):
     """
     Data provider that yields lines of data from its source allowing
     optional control over which line to start on and how many lines
     to return.
     """
-    DEFAULT_COMMENT_CHAR = '#'
+
+    DEFAULT_COMMENT_CHAR = "#"
     settings = {
-        'strip_lines': 'bool',
-        'strip_newlines': 'bool',
-        'provide_blank': 'bool',
-        'comment_char': 'str',
+        "strip_lines": "bool",
+        "strip_newlines": "bool",
+        "provide_blank": "bool",
+        "comment_char": "str",
     }
 
-    def __init__(self, source, strip_lines=True, strip_newlines=False, provide_blank=False,
-                 comment_char=DEFAULT_COMMENT_CHAR, **kwargs):
+    def __init__(
+        self,
+        source,
+        strip_lines=True,
+        strip_newlines=False,
+        provide_blank=False,
+        comment_char=DEFAULT_COMMENT_CHAR,
+        **kwargs,
+    ):
         """
         :param strip_lines: remove whitespace from the beginning an ending
             of each line (or not).
             Optional: defaults to True
         :type strip_lines: bool
 
         :param strip_newlines: remove newlines only
@@ -68,16 +76,16 @@
         :returns: a line or `None`
         """
         if line is not None:
             # ??: shouldn't it strip newlines regardless, if not why not use on of the base.dprovs
             if self.strip_lines:
                 line = line.strip()
             elif self.strip_newlines:
-                line = line.strip('\n')
-            if not self.provide_blank and line == '':
+                line = line.strip("\n")
+            if not self.provide_blank and line == "":
                 return None
             elif self.comment_char and line.startswith(self.comment_char):
                 return None
 
         return super().filter(line)
 
 
@@ -86,17 +94,18 @@
     Data provider that yields only those lines of data from its source
     that do (or do not when `invert` is True) match one or more of the given list
     of regexs.
 
     .. note:: the regex matches are effectively OR'd (if **any** regex matches
         the line it is considered valid and will be provided).
     """
+
     settings = {
-        'regex_list': 'list:escaped',
-        'invert': 'bool',
+        "regex_list": "list:escaped",
+        "invert": "bool",
     }
 
     def __init__(self, source, regex_list=None, invert=False, **kwargs):
         """
         :param regex_list: list of strings or regular expression strings that will
             be `match`ed to each line
             Optional: defaults to `None` (no matching)
@@ -149,16 +158,15 @@
         :param block_filter_fn: function that determines if a block is valid and
             will be provided.
             Optional: defaults to `None` (no filtering)
         :type block_filter_fn: function
         """
         # composition - not inheritance
         # TODO: not a fan of this:
-        (filter_fn, limit, offset) = (kwargs.pop('filter_fn', None),
-                                      kwargs.pop('limit', None), kwargs.pop('offset', 0))
+        (filter_fn, limit, offset) = (kwargs.pop("filter_fn", None), kwargs.pop("limit", None), kwargs.pop("offset", 0))
         line_provider = FilteredLineDataProvider(source, **kwargs)
         super().__init__(line_provider, filter_fn=filter_fn, limit=limit, offset=offset)
 
         self.new_block_delim_fn = new_block_delim_fn
         self.block_filter_fn = block_filter_fn
         self.init_new_block()
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/application.py` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/application.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 # Contains objects for using external display applications
 import logging
 from copy import deepcopy
 from urllib.parse import quote_plus
 
 from galaxy.util import (
     parse_xml,
-    string_as_bool
+    string_as_bool,
 )
 from galaxy.util.template import fill_template
 from .parameters import (
     DEFAULT_DATASET_NAME,
     DisplayApplicationDataParameter,
-    DisplayApplicationParameter
+    DisplayApplicationParameter,
 )
 from .util import encode_dataset_user
 
 log = logging.getLogger(__name__)
 
 
 def quote_plus_string(value, **kwds):
@@ -25,24 +25,24 @@
     return quote_plus(str(value), **kwds)
 
 
 class DisplayApplicationLink:
     @classmethod
     def from_elem(cls, elem, display_application, other_values=None):
         rval = DisplayApplicationLink(display_application)
-        rval.id = elem.get('id', None)
-        assert rval.id, 'Link elements require a id.'
-        rval.name = elem.get('name', rval.id)
-        rval.url = elem.find('url')
-        assert rval.url is not None, 'A url element must be provided for link elements.'
+        rval.id = elem.get("id", None)
+        assert rval.id, "Link elements require a id."
+        rval.name = elem.get("name", rval.id)
+        rval.url = elem.find("url")
+        assert rval.url is not None, "A url element must be provided for link elements."
         rval.other_values = other_values
-        rval.filters = elem.findall('filter')
-        for param_elem in elem.findall('param'):
+        rval.filters = elem.findall("filter")
+        for param_elem in elem.findall("param"):
             param = DisplayApplicationParameter.from_elem(param_elem, rval)
-            assert param, f'Unable to load parameter from element: {param_elem}'
+            assert param, f"Unable to load parameter from element: {param_elem}"
             rval.parameters[param.name] = param
             rval.url_param_name_map[param.url] = param.name
         return rval
 
     def __init__(self, display_application):
         self.display_application = display_application
         self.parameters = {}
@@ -58,144 +58,152 @@
             controller="dataset",
             action="display_application",
             dataset_id=dataset_hash,
             user_id=user_hash,
             app_name=quote_plus(self.display_application.id),
             link_name=quote_plus(self.id),
             app_action=None,
+            environ=trans.request.environ,
         )
 
     def get_inital_values(self, data, trans):
         if self.other_values:
             rval = dict(self.other_values)
         else:
             rval = {}
-        rval.update({'BASE_URL': trans.request.base, 'APP': trans.app})  # trans automatically appears as a response, need to add properties of trans that we want here
-        BASE_PARAMS = {'qp': quote_plus_string, 'url_for': trans.app.url_for}
+        rval.update(
+            {"BASE_URL": trans.request.base, "APP": trans.app}
+        )  # trans automatically appears as a response, need to add properties of trans that we want here
+        BASE_PARAMS = {"qp": quote_plus_string, "url_for": trans.app.url_for}
         for key, value in BASE_PARAMS.items():  # add helper functions/variables
             rval[key] = value
         rval[DEFAULT_DATASET_NAME] = data  # always have the display dataset name available
         return rval
 
     def build_parameter_dict(self, data, dataset_hash, user_hash, trans, app_kwds):
         other_values = self.get_inital_values(data, trans)
-        other_values['DATASET_HASH'] = dataset_hash
-        other_values['USER_HASH'] = user_hash
+        other_values["DATASET_HASH"] = dataset_hash
+        other_values["USER_HASH"] = user_hash
         ready = True
         for name, param in self.parameters.items():
             assert name not in other_values, f"The display parameter '{name}' has been defined more than once."
             if param.ready(other_values):
                 if name in app_kwds and param.allow_override:
                     other_values[name] = app_kwds[name]
                 else:
-                    other_values[name] = param.get_value(other_values, dataset_hash, user_hash, trans)  # subsequent params can rely on this value
+                    other_values[name] = param.get_value(
+                        other_values, dataset_hash, user_hash, trans
+                    )  # subsequent params can rely on this value
             else:
                 ready = False
-                other_values[name] = param.get_value(other_values, dataset_hash, user_hash, trans)  # subsequent params can rely on this value
+                other_values[name] = param.get_value(
+                    other_values, dataset_hash, user_hash, trans
+                )  # subsequent params can rely on this value
                 if other_values[name] is None:
                     # Need to stop here, next params may need this value to determine its own value
                     return False, other_values
         return ready, other_values
 
     def filter_by_dataset(self, data, trans):
         context = self.get_inital_values(data, trans)
         for filter_elem in self.filters:
-            if fill_template(filter_elem.text, context=context) != filter_elem.get('value', 'True'):
+            if fill_template(filter_elem.text, context=context) != filter_elem.get("value", "True"):
                 return False
         return True
 
 
 class DynamicDisplayApplicationBuilder:
-
     def __init__(self, elem, display_application, build_sites):
         filename = None
         data_table = None
-        if elem.get('site_type', None) is not None:
-            filename = build_sites.get(elem.get('site_type'))
+        if elem.get("site_type", None) is not None:
+            filename = build_sites.get(elem.get("site_type"))
         else:
-            filename = elem.get('from_file', None)
+            filename = elem.get("from_file", None)
         if filename is None:
-            data_table_name = elem.get('from_data_table', None)
+            data_table_name = elem.get("from_data_table", None)
             if data_table_name:
                 data_table = display_application.app.tool_data_tables.get(data_table_name, None)
                 assert data_table is not None, f'Unable to find data table named "{data_table_name}".'
 
-        assert filename is not None or data_table is not None, 'Filename or data Table is required for dynamic_links.'
-        skip_startswith = elem.get('skip_startswith', None)
-        separator = elem.get('separator', '\t')
-        id_col = elem.get('id', None)
+        assert filename is not None or data_table is not None, "Filename or data Table is required for dynamic_links."
+        skip_startswith = elem.get("skip_startswith", None)
+        separator = elem.get("separator", "\t")
+        id_col = elem.get("id", None)
         try:
             id_col = int(id_col)
         except (TypeError, ValueError):
             if data_table:
                 if id_col is None:
-                    id_col = data_table.columns.get('id', None)
+                    id_col = data_table.columns.get("id", None)
                 if id_col is None:
-                    id_col = data_table.columns.get('value', None)
+                    id_col = data_table.columns.get("value", None)
                 try:
                     id_col = int(id_col)
                 except (TypeError, ValueError):
                     # id is set to a string or None, use column by that name if available
                     id_col = data_table.columns.get(id_col, None)
                     id_col = int(id_col)
-        name_col = elem.get('name', None)
+        name_col = elem.get("name", None)
         try:
             name_col = int(name_col)
         except (TypeError, ValueError):
             if data_table:
                 if name_col is None:
-                    name_col = data_table.columns.get('name', None)
+                    name_col = data_table.columns.get("name", None)
                 else:
                     name_col = data_table.columns.get(name_col, None)
             else:
                 name_col = None
         if name_col is None:
             name_col = id_col
         max_col = max(id_col, name_col)
         dynamic_params = {}
         if data_table is not None:
             max_col = max([max_col] + list(data_table.columns.values()))
             for key, value in data_table.columns.items():
-                dynamic_params[key] = {'column': value, 'split': False, 'separator': ','}
-        for dynamic_param in elem.findall('dynamic_param'):
-            name = dynamic_param.get('name')
-            value = int(dynamic_param.get('value'))
-            split = string_as_bool(dynamic_param.get('split', False))
-            param_separator = dynamic_param.get('separator', ',')
+                dynamic_params[key] = {"column": value, "split": False, "separator": ","}
+        for dynamic_param in elem.findall("dynamic_param"):
+            name = dynamic_param.get("name")
+            value = int(dynamic_param.get("value"))
+            split = string_as_bool(dynamic_param.get("split", False))
+            param_separator = dynamic_param.get("separator", ",")
             max_col = max(max_col, value)
-            dynamic_params[name] = {'column': value, 'split': split, 'separator': param_separator}
+            dynamic_params[name] = {"column": value, "split": split, "separator": param_separator}
         if filename:
             data_iter = open(filename)
         elif data_table:
             version, data_iter = data_table.get_version_fields()
             display_application.add_data_table_watch(data_table.name, version)
         links = []
         for line in data_iter:
             if isinstance(line, str):
                 if not skip_startswith or not line.startswith(skip_startswith):
-                    line = line.rstrip('\n\r')
+                    line = line.rstrip("\n\r")
                     if not line:
                         continue
                     fields = line.split(separator)
                 else:
                     continue
             else:
                 fields = line
             if len(fields) > max_col:
                 new_elem = deepcopy(elem)
-                new_elem.set('id', fields[id_col])
-                new_elem.set('name', fields[name_col])
+                new_elem.set("id", fields[id_col])
+                new_elem.set("name", fields[name_col])
                 dynamic_values = {}
                 for key, attributes in dynamic_params.items():
-                    value = fields[attributes['column']]
-                    if attributes['split']:
-                        value = value.split(attributes['separator'])
+                    value = fields[attributes["column"]]
+                    if attributes["split"]:
+                        value = value.split(attributes["separator"])
                     dynamic_values[key] = value
                 # now populate
-                links.append(DisplayApplicationLink.from_elem(new_elem, display_application, other_values=dynamic_values))
+                links.append(
+                    DisplayApplicationLink.from_elem(new_elem, display_application, other_values=dynamic_values)
+                )
             else:
                 log.warning(f'Invalid dynamic display application link specified in {filename}: "{line}"')
         self.links = links
 
     def __iter__(self):
         return iter(self.links)
 
@@ -203,24 +211,26 @@
 class PopulatedDisplayApplicationLink:
     def __init__(self, display_application_link, data, dataset_hash, user_hash, trans, app_kwds):
         self.link = display_application_link
         self.data = data
         self.dataset_hash = dataset_hash
         self.user_hash = user_hash
         self.trans = trans
-        self.ready, self.parameters = self.link.build_parameter_dict(self.data, self.dataset_hash, self.user_hash, trans, app_kwds)
+        self.ready, self.parameters = self.link.build_parameter_dict(
+            self.data, self.dataset_hash, self.user_hash, trans, app_kwds
+        )
 
     def display_ready(self):
         return self.ready
 
     def get_param_value(self, name):
         value = None
         if self.ready:
             value = self.parameters.get(name, None)
-            assert value, 'Unknown parameter requested'
+            assert value, "Unknown parameter requested"
         return value
 
     def preparing_display(self):
         if not self.ready:
             return self.link.parameters[list(self.parameters.keys())[-1]].is_preparing(self.parameters)
         return False
 
@@ -229,32 +239,32 @@
         found_last = False
         if not self.ready and not self.preparing_display():
             other_values = self.parameters
             for name, param in self.link.parameters.items():
                 if found_last or list(other_values.keys())[-1] == name:  # found last parameter to be populated
                     found_last = True
                     value = param.prepare(other_values, self.dataset_hash, self.user_hash, self.trans)
-                    rval.append({'name': name, 'value': value, 'param': param})
+                    rval.append({"name": name, "value": value, "param": param})
                     other_values[name] = value
                     if value is None:
                         # We can go no further until we have a value for this parameter
                         return rval
         return rval
 
     def get_prepare_steps(self, datasets_only=True):
         rval = []
         for name, param in self.link.parameters.items():
             if datasets_only and not isinstance(param, DisplayApplicationDataParameter):
                 continue
             value = self.parameters.get(name, None)
-            rval.append({'name': name, 'value': value, 'param': param, 'ready': param.ready(self.parameters)})
+            rval.append({"name": name, "value": value, "param": param, "ready": param.ready(self.parameters)})
         return rval
 
     def display_url(self):
-        assert self.display_ready(), 'Display is not yet ready, cannot generate display link'
+        assert self.display_ready(), "Display is not yet ready, cannot generate display link"
         return fill_template(self.link.url.text, context=self.parameters)
 
     def get_param_name_by_url(self, url):
         for name, parameter in self.link.parameters.items():
             if parameter.build_url(self.parameters) == url:
                 return name
         raise ValueError(f"Unknown URL parameter name provided: {url}")
@@ -268,24 +278,26 @@
     @classmethod
     def from_file(cls, filename, app):
         return cls.from_elem(parse_xml(filename).getroot(), app, filename=filename)
 
     @classmethod
     def from_elem(cls, elem, app, filename=None):
         att_dict = cls._get_attributes_from_elem(elem)
-        rval = DisplayApplication(att_dict['id'], att_dict['name'], app, att_dict['version'], filename=filename, elem=elem)
+        rval = DisplayApplication(
+            att_dict["id"], att_dict["name"], app, att_dict["version"], filename=filename, elem=elem
+        )
         rval._load_links_from_elem(elem)
         return rval
 
     @classmethod
     def _get_attributes_from_elem(cls, elem):
-        display_id = elem.get('id', None)
+        display_id = elem.get("id", None)
         assert display_id, "ID tag is required for a Display Application"
-        name = elem.get('name', display_id)
-        version = elem.get('version', None)
+        name = elem.get("name", display_id)
+        version = elem.get("version", None)
         return dict(id=display_id, name=name, version=version)
 
     def __init__(self, display_id, name, app, version=None, filename=None, elem=None):
         self.id = display_id
         self.name = name
         self.app = app
         if version is None:
@@ -293,21 +305,23 @@
         self.version = version
         self.links = {}
         self._filename = filename
         self._elem = elem
         self._data_table_versions = {}
 
     def _load_links_from_elem(self, elem):
-        for link_elem in elem.findall('link'):
+        for link_elem in elem.findall("link"):
             link = DisplayApplicationLink.from_elem(link_elem, self)
             if link:
                 self.links[link.id] = link
         try:
-            for dynamic_links in elem.findall('dynamic_links'):
-                for link in DynamicDisplayApplicationBuilder(dynamic_links, self, self.app.datatypes_registry.build_sites):
+            for dynamic_links in elem.findall("dynamic_links"):
+                for link in DynamicDisplayApplicationBuilder(
+                    dynamic_links, self, self.app.datatypes_registry.build_sites
+                ):
                     self.links[link.id] = link
         except Exception as e:
             log.error("Error loading a set of Dynamic Display Application links: %s", e)
 
     def get_link(self, link_name, data, dataset_hash, user_hash, trans, app_kwds):
         # returns a link object with data knowledge to generate links
         self._check_and_reload()
@@ -327,15 +341,17 @@
         elif self._elem:
             elem = self._elem
         else:
             raise Exception(f"Unable to reload DisplayApplication {self.name}.")
         # All toolshed-specific attributes added by e.g the registry will remain
         attr_dict = self._get_attributes_from_elem(elem)
         # We will not allow changing the id at this time (we'll need to fix several mappings upstream to handle this case)
-        assert attr_dict.get('id') == self.id, ValueError("You cannot reload a Display application where the ID has changed. You will need to restart the server instead.")
+        assert attr_dict.get("id") == self.id, ValueError(
+            "You cannot reload a Display application where the ID has changed. You will need to restart the server instead."
+        )
         # clear old links
         self.links = {}
         # clear data table versions:
         self._data_table_versions = {}
         # Set new attributes
         for key, value in attr_dict.items():
             setattr(self, key, value)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_bam.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_gff.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_gff.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_interval_as_bed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ensembl/ensembl_interval_as_bed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_gff.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_gff.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_interval_as_bed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_interval_as_bed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_wig.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/gbrowse/gbrowse_wig.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bam.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bb.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bb.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bedgraph.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bedgraph.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/bigwig.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/bigwig.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/gtf.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/gtf.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igb/wig.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igb/wig.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/bam.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/bigwig.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/bigwig.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/genbank.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/genbank.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/genome_fasta.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/genome_fasta.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/gff.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/gff.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/interval_as_bed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/interval_as_bed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/vcf.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/vcf.xml`

 * *Files 5% similar despite different names*

#### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/igv/vcf.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/igv/vcf.xml`

```diff
@@ -84,11 +84,11 @@
   </dynamic_links>
   <dynamic_links from_data_table="igv_broad_genomes" skip_startswith="#" id="value" name="name">
     <!-- Our input data table is one line per dbkey -->
     <filter>${ $dataset.dbkey == $value }</filter>
     <!-- We define url and params as normal, but values defined in dynamic_param are available by specified name -->
     <url>http://www.broadinstitute.org/igv/projects/current/igv.php?sessionURL=${bgzip_file.qp}&amp;genome=${qp($bgzip_file.dbkey)}&amp;merge=true&amp;name=${qp( ( $bgzip_file.name or $DATASET_HASH ).replace( ',', ';' ) )}</url>
     <param type="data" name="bgzip_file" url="galaxy_${DATASET_HASH}.vcf.gz" format="vcf_bgzip" mimetype="application/octet-stream"/>
-    <param type="data" name="tabix_file" metadata="tabix_index" url="galaxy_${DATASET_HASH}.vcf.gz.tbi" format="vcf_bgzip" mimetype="application/octet-stream"/>
+    <param type="data" name="tabix_file" metadata="tabix_index" url="galaxy_${DATASET_HASH}.vcf.gz.tbi" mimetype="application/octet-stream"/>
   </dynamic_links>
 </display>
 <!-- Dan Blankenberg -->
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/rviewer/bed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/rviewer/bed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/rviewer/vcf.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/rviewer/vcf.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/bam.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/bam.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/bigbed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/bigbed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/bigwig.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/bigwig.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/interval_as_bed.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/interval_as_bed.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/maf_customtrack.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/maf_customtrack.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/configs/ucsc/vcf.xml` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/configs/ucsc/vcf.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/parameters.py` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/parameters.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,43 +3,49 @@
 from typing import Optional
 from urllib.parse import quote_plus
 
 from galaxy.util import string_as_bool
 from galaxy.util.bunch import Bunch
 from galaxy.util.template import fill_template
 
-DEFAULT_DATASET_NAME = 'dataset'
+DEFAULT_DATASET_NAME = "dataset"
 
 
 class DisplayApplicationParameter:
-    """ Abstract Class for Display Application Parameters """
+    """Abstract Class for Display Application Parameters"""
 
     type: Optional[str] = None
 
     @classmethod
     def from_elem(cls, elem, link):
-        param_type = elem.get('type', None)
-        assert param_type, 'DisplayApplicationParameter requires a type'
+        param_type = elem.get("type", None)
+        assert param_type, "DisplayApplicationParameter requires a type"
         return parameter_type_to_class[param_type](elem, link)
 
     def __init__(self, elem, link):
-        self.name = elem.get('name', None)
-        assert self.name, 'DisplayApplicationParameter requires a name'
+        self.name = elem.get("name", None)
+        assert self.name, "DisplayApplicationParameter requires a name"
         self.link = link
-        self.url = elem.get('url', self.name)  # name used in url for display purposes defaults to name; e.g. want the form of file.ext, where a '.' is not allowed as python variable name/keyword
-        self.mime_type = elem.get('mimetype', None)
-        self.guess_mime_type = string_as_bool(elem.get('guess_mimetype', 'False'))
-        self.viewable = string_as_bool(elem.get('viewable', 'False'))  # only allow these to be viewed via direct url when explicitly set to viewable
-        self.strip = string_as_bool(elem.get('strip', 'False'))
-        self.strip_https = string_as_bool(elem.get('strip_https', 'False'))
-        self.allow_override = string_as_bool(elem.get('allow_override', 'False'))  # Passing query param app_<name>=<value> to dataset controller allows override if this is true.
-        self.allow_cors = string_as_bool(elem.get('allow_cors', 'False'))
+        self.url = elem.get(
+            "url", self.name
+        )  # name used in url for display purposes defaults to name; e.g. want the form of file.ext, where a '.' is not allowed as python variable name/keyword
+        self.mime_type = elem.get("mimetype", None)
+        self.guess_mime_type = string_as_bool(elem.get("guess_mimetype", "False"))
+        self.viewable = string_as_bool(
+            elem.get("viewable", "False")
+        )  # only allow these to be viewed via direct url when explicitly set to viewable
+        self.strip = string_as_bool(elem.get("strip", "False"))
+        self.strip_https = string_as_bool(elem.get("strip_https", "False"))
+        self.allow_override = string_as_bool(
+            elem.get("allow_override", "False")
+        )  # Passing query param app_<name>=<value> to dataset controller allows override if this is true.
+        self.allow_cors = string_as_bool(elem.get("allow_cors", "False"))
 
     def get_value(self, other_values, dataset_hash, user_hash, trans):
-        raise Exception('get_value() is unimplemented for DisplayApplicationDataParameter')
+        raise Exception("get_value() is unimplemented for DisplayApplicationDataParameter")
 
     def prepare(self, other_values, dataset_hash, user_hash, trans):
         return self.get_value(other_values, dataset_hash, user_hash, trans)
 
     def ready(self, other_values):
         return True
 
@@ -47,55 +53,69 @@
         return False
 
     def build_url(self, other_values):
         return fill_template(self.url, context=other_values)
 
 
 class DisplayApplicationDataParameter(DisplayApplicationParameter):
-    """ Parameter that returns a file_name containing the requested content """
+    """Parameter that returns a file_name containing the requested content"""
 
-    type = 'data'
+    type = "data"
 
     def __init__(self, elem, link):
         DisplayApplicationParameter.__init__(self, elem, link)
-        self.extensions = elem.get('format', None)
+        self.extensions = elem.get("format", None)
         if self.extensions:
             self.extensions = self.extensions.split(",")
-        self.metadata = elem.get('metadata', None)
-        self.allow_extra_files_access = string_as_bool(elem.get('allow_extra_files_access', 'False'))
-        self.dataset = elem.get('dataset', DEFAULT_DATASET_NAME)  # 'dataset' is default name assigned to dataset to be displayed
-        assert not (self.extensions and self.metadata), 'A format or a metadata can be defined for a DisplayApplicationParameter, but not both.'
-        assert not (self.allow_extra_files_access and self.metadata), 'allow_extra_files_access or metadata can be defined for a DisplayApplicationParameter, but not both.'
-        self.viewable = string_as_bool(elem.get('viewable', 'True'))  # data params should be viewable
-        self.force_url_param = string_as_bool(elem.get('force_url_param', 'False'))
-        self.force_conversion = string_as_bool(elem.get('force_conversion', 'False'))
+        self.metadata = elem.get("metadata", None)
+        self.allow_extra_files_access = string_as_bool(elem.get("allow_extra_files_access", "False"))
+        self.dataset = elem.get(
+            "dataset", DEFAULT_DATASET_NAME
+        )  # 'dataset' is default name assigned to dataset to be displayed
+        assert not (
+            self.extensions and self.metadata
+        ), "A format or a metadata can be defined for a DisplayApplicationParameter, but not both."
+        assert not (
+            self.allow_extra_files_access and self.metadata
+        ), "allow_extra_files_access or metadata can be defined for a DisplayApplicationParameter, but not both."
+        self.viewable = string_as_bool(elem.get("viewable", "True"))  # data params should be viewable
+        self.force_url_param = string_as_bool(elem.get("force_url_param", "False"))
+        self.force_conversion = string_as_bool(elem.get("force_conversion", "False"))
 
     @property
     def formats(self):
         if self.extensions:
-            return tuple(map(type, map(self.link.display_application.app.datatypes_registry.get_datatype_by_extension, self.extensions)))
+            return tuple(
+                map(
+                    type,
+                    map(
+                        self.link.display_application.app.datatypes_registry.get_datatype_by_extension, self.extensions
+                    ),
+                )
+            )
         return None
 
     def _get_dataset_like_object(self, other_values):
         # this returned object has file_name, state, and states attributes equivalent to a DatasetAssociation
         data = other_values.get(self.dataset, None)
-        assert data, 'Base dataset could not be found in values provided to DisplayApplicationDataParameter'
+        assert data, "Base dataset could not be found in values provided to DisplayApplicationDataParameter"
         if isinstance(data, DisplayDataValueWrapper):
             data = data.value
+        if data.state != data.states.OK:
+            return None
         if self.metadata:
             rval = getattr(data.metadata, self.metadata, None)
             assert rval, f'Unknown metadata name "{self.metadata}" provided for dataset type "{data.ext}".'
-            return Bunch(file_name=rval.file_name, state=data.state, states=data.states, extension='data')
+            return Bunch(file_name=rval.file_name, state=data.state, states=data.states, extension="data")
         elif self.extensions and (self.force_conversion or not isinstance(data.datatype, self.formats)):
             for ext in self.extensions:
                 rval = data.get_converted_files_by_type(ext)
                 if rval:
                     return rval
-
-            direct_match, target_ext, converted_dataset = data.find_conversion_destination(self.formats)
+            direct_match, target_ext, _ = data.find_conversion_destination(self.formats)
             assert direct_match or target_ext is not None, f"No conversion path found for data param: {self.name}"
             return None
         return data
 
     def get_value(self, other_values, dataset_hash, user_hash, trans):
         data = self._get_dataset_like_object(other_values)
         if data:
@@ -106,24 +126,34 @@
         data = self._get_dataset_like_object(other_values)
         if not data and self.formats:
             data = other_values.get(self.dataset, None)
             trans.sa_session.refresh(data)
             # start conversion
             # FIXME: Much of this is copied (more than once...); should be some abstract method elsewhere called from here
             # find target ext
-            direct_match, target_ext, converted_dataset = data.find_conversion_destination(self.formats, converter_safe=True)
+            direct_match, target_ext, converted_dataset = data.find_conversion_destination(
+                self.formats, converter_safe=True
+            )
             if not direct_match:
                 if target_ext and not converted_dataset:
                     if isinstance(data, DisplayDataValueWrapper):
                         data = data.value
-                    new_data = next(iter(data.datatype.convert_dataset(trans, data, target_ext, return_output=True, visible=False).values()))
+                    new_data = next(
+                        iter(
+                            data.datatype.convert_dataset(
+                                trans, data, target_ext, return_output=True, visible=False
+                            ).values()
+                        )
+                    )
                     new_data.hid = data.hid
                     new_data.name = data.name
                     trans.sa_session.add(new_data)
-                    assoc = trans.app.model.ImplicitlyConvertedDatasetAssociation(parent=data, file_type=target_ext, dataset=new_data, metadata_safe=False)
+                    assoc = trans.app.model.ImplicitlyConvertedDatasetAssociation(
+                        parent=data, file_type=target_ext, dataset=new_data, metadata_safe=False
+                    )
                     trans.sa_session.add(assoc)
                     trans.sa_session.flush()
                 elif converted_dataset and converted_dataset.state == converted_dataset.states.ERROR:
                     raise Exception(f"Dataset conversion failed for data parameter: {self.name}")
         return self.get_value(other_values, dataset_hash, user_hash, trans)
 
     def is_preparing(self, other_values):
@@ -134,40 +164,42 @@
 
     def ready(self, other_values):
         value = self._get_dataset_like_object(other_values)
         if value:
             if value.state == value.states.OK:
                 return True
             elif value.state == value.states.ERROR:
-                raise Exception(f'A data display parameter is in the error state: {self.name}')
+                raise Exception(f"A data display parameter is in the error state: {self.name}")
         return False
 
 
 class DisplayApplicationTemplateParameter(DisplayApplicationParameter):
-    """ Parameter that returns a string containing the requested content """
+    """Parameter that returns a string containing the requested content"""
 
-    type = 'template'
+    type = "template"
 
     def __init__(self, elem, link):
         DisplayApplicationParameter.__init__(self, elem, link)
-        self.text = elem.text or ''
+        self.text = elem.text or ""
 
     def get_value(self, other_values, dataset_hash, user_hash, trans):
         value = fill_template(self.text, context=other_values)
         if self.strip:
             value = value.strip()
         return DisplayParameterValueWrapper(value, self, other_values, dataset_hash, user_hash, trans)
 
 
-parameter_type_to_class = {DisplayApplicationDataParameter.type: DisplayApplicationDataParameter,
-                           DisplayApplicationTemplateParameter.type: DisplayApplicationTemplateParameter}
+parameter_type_to_class = {
+    DisplayApplicationDataParameter.type: DisplayApplicationDataParameter,
+    DisplayApplicationTemplateParameter.type: DisplayApplicationTemplateParameter,
+}
 
 
 class DisplayParameterValueWrapper:
-    ACTION_NAME = 'param'
+    ACTION_NAME = "param"
 
     def __init__(self, value, parameter, other_values, dataset_hash, user_hash, trans):
         self.value = value
         self.parameter = parameter
         self.other_values = other_values
         self.trans = trans
         self._dataset_hash = dataset_hash
@@ -179,49 +211,55 @@
 
     def mime_type(self, action_param_extra=None):
         if self.parameter.mime_type is not None:
             return self.parameter.mime_type
         if self.parameter.guess_mime_type:
             mime, encoding = mimetypes.guess_type(self._url)
             if not mime:
-                mime = self.trans.app.datatypes_registry.get_mimetype_by_extension(".".split(self._url)[-1], None)
+                mime = self.trans.app.datatypes_registry.get_mimetype_by_extension(self._url.split(".")[-1], None)
             if mime:
                 return mime
-        return 'text/plain'
+        return "text/plain"
 
     @property
     def url(self):
         base_url = self.trans.request.base
-        if self.parameter.strip_https and base_url[: 5].lower() == 'https':
+        if self.parameter.strip_https and base_url[:5].lower() == "https":
             base_url = f"http{base_url[5:]}"
-        return "{}{}".format(base_url,
-                             self.trans.app.url_for(controller='dataset',
-                                                    action="display_application",
-                                                    dataset_id=self._dataset_hash,
-                                                    user_id=self._user_hash,
-                                                    app_name=quote_plus(self.parameter.link.display_application.id),
-                                                    link_name=quote_plus(self.parameter.link.id),
-                                                    app_action=self.action_name,
-                                                    action_param=self._url))
+        return "{}{}".format(
+            base_url,
+            self.trans.app.legacy_url_for(
+                mapper=self.trans.app.legacy_mapper,
+                controller="dataset",
+                action="display_application",
+                dataset_id=self._dataset_hash,
+                user_id=self._user_hash,
+                app_name=quote_plus(self.parameter.link.display_application.id),
+                link_name=quote_plus(self.parameter.link.id),
+                app_action=self.action_name,
+                action_param=self._url,
+                environ=self.trans.request.environ,
+            ),
+        )
 
     @property
     def action_name(self):
         return self.ACTION_NAME
 
     @property
     def qp(self):
         # returns quoted str contents
-        return self.other_values['qp'](str(self))
+        return self.other_values["qp"](str(self))
 
     def __getattr__(self, key):
         return getattr(self.value, key)
 
 
 class DisplayDataValueWrapper(DisplayParameterValueWrapper):
-    ACTION_NAME = 'data'
+    ACTION_NAME = "data"
 
     def __str__(self):
         # string of data param is filename
         return str(self.value.file_name)
 
     def mime_type(self, action_param_extra=None):
         if self.parameter.mime_type is not None:
@@ -229,26 +267,28 @@
         if self.parameter.guess_mime_type:
             if action_param_extra:
                 mime, encoding = mimetypes.guess_type(action_param_extra)
             else:
                 mime, encoding = mimetypes.guess_type(self._url)
             if not mime:
                 if action_param_extra:
-                    mime = self.trans.app.datatypes_registry.get_mimetype_by_extension(".".split(action_param_extra)[-1], None)
+                    mime = self.trans.app.datatypes_registry.get_mimetype_by_extension(
+                        action_param_extra.split(".")[-1], None
+                    )
                 if not mime:
-                    mime = self.trans.app.datatypes_registry.get_mimetype_by_extension(".".split(self._url)[-1], None)
+                    mime = self.trans.app.datatypes_registry.get_mimetype_by_extension(self._url.split(".")[-1], None)
             if mime:
                 return mime
-        if hasattr(self.value, 'get_mime'):
+        if hasattr(self.value, "get_mime"):
             return self.value.get_mime()
         return self.other_values[DEFAULT_DATASET_NAME].get_mime()
 
     @property
     def action_name(self):
         if self.parameter.force_url_param:
             return super(DisplayParameterValueWrapper, self).action_name
         return self.ACTION_NAME
 
     @property
     def qp(self):
         # returns quoted url contents
-        return self.other_values['qp'](self.url)
+        return self.other_values["qp"](self.url)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/util.py` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/util.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,28 +2,28 @@
 
 
 def encode_dataset_user(trans, dataset, user):
     # encode dataset id as usual
     # encode user id using the dataset create time as the key
     dataset_hash = trans.security.encode_id(dataset.id)
     if user is None:
-        user_hash = 'None'
+        user_hash = "None"
     else:
         security = IdEncodingHelper(id_secret=dataset.create_time)
         user_hash = security.encode_id(user.id)
     return dataset_hash, user_hash
 
 
 def decode_dataset_user(trans, dataset_hash, user_hash):
     # decode dataset id as usual
     # decode user id using the dataset create time as the key
     dataset_id = trans.security.decode_id(dataset_hash)
     dataset = trans.sa_session.query(trans.app.model.HistoryDatasetAssociation).get(dataset_id)
     assert dataset, "Bad Dataset id provided to decode_dataset_user"
-    if user_hash in [None, 'None']:
+    if user_hash in [None, "None"]:
         user = None
     else:
         security = IdEncodingHelper(id_secret=dataset.create_time)
         user_id = security.decode_id(user_hash)
         user = trans.sa_session.query(trans.app.model.User).get(user_id)
         assert user, "A Bad user id was passed to decode_dataset_user"
     return dataset, user
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/display_applications/xsd/geda.xsd` & `galaxy-data-23.0.1/galaxy/datatypes/display_applications/xsd/geda.xsd`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/flow.py` & `galaxy-data-23.0.1/galaxy/datatypes/flow.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,52 +1,57 @@
 """
 Flow analysis datatypes.
 """
 
 import logging
+from typing import TYPE_CHECKING
 
 from galaxy.datatypes.binary import Binary
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from . import data
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class FCS(Binary):
     """Class describing an FCS binary file"""
+
     file_ext = "fcs"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary FCS file"
             dataset.blurb = data.nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Binary FCS file"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checking if the file is in FCS format. Should read FCS2.0, FCS3.0
         and FCS3.1
 
         Based on flowcore:
         https://github.com/RGLab/flowCore/blob/27141b792ad65ae8bd0aeeef26e757c39cdaefe7/R/IO.R#L667
         """
         content = file_prefix.contents_header_bytes[:42].decode()
         version = content[:6]
         if version not in ["FCS2.0", "FCS3.0", "FCS3.1"]:
             return False
-        if content[6:10] != '    ':
+        if content[6:10] != "    ":
             return False
         # we only need to check ioffs 2 to 5
-        int(content[10:42].replace(' ', ''))
+        int(content[10:42].replace(" ", ""))
         return True
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/genetics.py` & `galaxy-data-23.0.1/galaxy/datatypes/genetics.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,78 +11,90 @@
 ross lazarus for rgenetics
 august 20 2007
 """
 import logging
 import os
 import re
 import sys
+from typing import (
+    Dict,
+    IO,
+    List,
+    Optional,
+    TYPE_CHECKING,
+    Union,
+)
 from urllib.parse import quote_plus
 
 from markupsafe import escape
 
 from galaxy.datatypes import metadata
 from galaxy.datatypes.data import (
     DatatypeValidation,
+    GeneratePrimaryFileDataset,
     Text,
 )
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.datatypes.tabular import Tabular
 from galaxy.datatypes.text import Html
 from galaxy.util import (
     nice_size,
     unicodify,
 )
+from galaxy.util.compression_utils import FileObjType
+
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
 
 gal_Log = logging.getLogger(__name__)
 verbose = False
 
 # https://genome.ucsc.edu/goldenpath/help/hgGenomeHelp.html
-VALID_GENOME_GRAPH_MARKERS = re.compile(r'^(chr.*|RH.*|rs.*|SNP_.*|CN.*|A_.*)')
-VALID_GENOTYPES_LINE = re.compile(r'^([a-zA-Z0-9]+)(\s([0-9]{2}|[A-Z]{2}|NC|\?\?))+\s*$')
+VALID_GENOME_GRAPH_MARKERS = re.compile(r"^(chr.*|RH.*|rs.*|SNP_.*|CN.*|A_.*)")
+VALID_GENOTYPES_LINE = re.compile(r"^([a-zA-Z0-9]+)(\s([0-9]{2}|[A-Z]{2}|NC|\?\?))+\s*$")
 
 
 @build_sniff_from_prefix
 class GenomeGraphs(Tabular):
     """
     Tab delimited data containing a marker id and any number of numeric values
     """
 
     MetadataElement(name="markerCol", default=1, desc="Marker ID column", param=metadata.ColumnParameter)
     MetadataElement(name="columns", default=3, desc="Number of columns", readonly=True)
     MetadataElement(name="column_types", default=[], desc="Column types", readonly=True, visible=False)
-    file_ext = 'gg'
+    file_ext = "gg"
 
     def __init__(self, **kwd):
         """
         Initialize gg datatype, by adding UCSC display apps
         """
         super().__init__(**kwd)
-        self.add_display_app('ucsc', 'Genome Graph', 'as_ucsc_display_file', 'ucsc_links')
+        self.add_display_app("ucsc", "Genome Graph", "as_ucsc_display_file", "ucsc_links")
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         dataset.metadata.markerCol = 1
-        header = open(dataset.file_name).readlines()[0].strip().split('\t')
+        header = open(dataset.file_name).readlines()[0].strip().split("\t")
         dataset.metadata.columns = len(header)
-        t = ['numeric' for x in header]
-        t[0] = 'string'
+        t = ["numeric" for x in header]
+        t[0] = "string"
         dataset.metadata.column_types = t
-        return True
 
-    def as_ucsc_display_file(self, dataset, **kwd):
+    def as_ucsc_display_file(self, dataset: "DatasetInstance", **kwd) -> Union[FileObjType, str]:
         """
         Returns file
         """
-        return open(dataset.file_name, 'rb')
+        return open(dataset.file_name, "rb")
 
-    def ucsc_links(self, dataset, type, app, base_url):
+    def ucsc_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         """
         from the ever-helpful angie hinrichs angie@soe.ucsc.edu
         a genome graphs call looks like this
 
         http://genome.ucsc.edu/cgi-bin/hgGenome?clade=mammal&org=Human&db=hg18&hgGenome_dataSetName=dname
         &hgGenome_dataSetDescription=test&hgGenome_formatType=best%20guess&hgGenome_markerType=best%20guess
         &hgGenome_columnLabels=best%20guess&hgGenome_maxVal=&hgGenome_labelVals=
@@ -93,87 +105,96 @@
 
         http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg18&position=chr1:1-1000&hgt.customText=
         http%3A%2F%2Fgalaxy.esphealth.org%2Fdisplay_as%3Fid%3D339%26display_app%3Ducsc
 
         """
         ret_val = []
         if not dataset.dbkey:
-            dataset.dbkey = 'hg18'  # punt!
+            dataset.dbkey = "hg18"  # punt!
         if dataset.has_data():
-            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build('ucsc', dataset.dbkey):
-                if site_name in app.datatypes_registry.get_display_sites('ucsc'):
-                    site_url = site_url.replace('/hgTracks?', '/hgGenome?')  # for genome graphs
-                    internal_url = "%s" % app.url_for(controller='dataset',
-                                                      dataset_id=dataset.id,
-                                                      action='display_at',
-                                                      filename=f"ucsc_{site_name}")
-                    display_url = "%s%s/display_as?id=%i&display_app=%s&authz_method=display_at" % (base_url, app.url_for(controller='root'), dataset.id, type)
+            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build("ucsc", dataset.dbkey):
+                if site_name in app.datatypes_registry.get_display_sites("ucsc"):
+                    site_url = site_url.replace("/hgTracks?", "/hgGenome?")  # for genome graphs
+                    internal_url = "%s" % app.legacy_url_for(
+                        mapper=app.legacy_mapper,
+                        environ=request.environ,
+                        controller="dataset",
+                        dataset_id=dataset.id,
+                        action="display_at",
+                        filename=f"ucsc_{site_name}",
+                    )
+                    display_url = "%s%s/display_as?id=%i&display_app=%s&authz_method=display_at" % (
+                        base_url,
+                        app.legacy_url_for(mapper=app.legacy_mapper, environ=request.environ, controller="root"),
+                        dataset.id,
+                        type,
+                    )
                     display_url = quote_plus(display_url)
                     # was display_url = quote_plus( "%s/display_as?id=%i&display_app=%s" % (base_url, dataset.id, type) )
                     # redirect_url = quote_plus( "%sdb=%s&position=%s:%s-%s&hgt.customText=%%s" % (site_url, dataset.dbkey, chrom, start, stop) )
-                    sl = [f"{site_url}db={dataset.dbkey}", ]
+                    sl = [
+                        f"{site_url}db={dataset.dbkey}",
+                    ]
                     # sl.append("&hgt.customText=%s")
                     sl.append(f"&hgGenome_dataSetName={dataset.name}&hgGenome_dataSetDescription=GalaxyGG_data")
                     sl.append("&hgGenome_formatType=best guess&hgGenome_markerType=best guess")
                     sl.append("&hgGenome_columnLabels=first row&hgGenome_maxVal=&hgGenome_labelVals=")
                     sl.append("&hgGenome_doSubmitUpload=submit")
                     sl.append(f"&hgGenome_maxGapToFill=25000000&hgGenome_uploadFile={display_url}")
-                    s = ''.join(sl)
+                    s = "".join(sl)
                     s = quote_plus(s)
                     redirect_url = s
-                    link = f'{internal_url}?redirect_url={redirect_url}&display_url={display_url}'
+                    link = f"{internal_url}?redirect_url={redirect_url}&display_url={display_url}"
                     ret_val.append((site_name, link))
         return ret_val
 
-    def make_html_table(self, dataset):
+    def make_html_table(self, dataset: "DatasetInstance", **kwargs) -> str:
         """
         Create HTML table, used for displaying peek
         """
-        out = ['<table cellspacing="0" cellpadding="3">']
         try:
+            out = ['<table cellspacing="0" cellpadding="3">']
             with open(dataset.file_name) as f:
                 d = f.readlines()[:5]
             if len(d) == 0:
-                out = f"Cannot find anything to parse in {dataset.name}"
-                return out
+                return f"Cannot find anything to parse in {dataset.name}"
             hasheader = 0
             try:
-                [f'{x:f}' for x in d[0][1:]]  # first is name - see if starts all numerics
+                [f"{x:f}" for x in d[0][1:]]  # first is name - see if starts all numerics
             except Exception:
                 hasheader = 1
             # Generate column header
-            out.append('<tr>')
+            out.append("<tr>")
             if hasheader:
                 for i, name in enumerate(d[0].split()):
-                    out.append(f'<th>{i + 1}.{name}</th>')
+                    out.append(f"<th>{i + 1}.{name}</th>")
                 d.pop(0)
-                out.append('</tr>')
+                out.append("</tr>")
             for row in d:
-                out.append('<tr>')
-                out.append(''.join(f'<td>{x}</td>' for x in row.split()))
-                out.append('</tr>')
-            out.append('</table>')
-            out = "".join(out)
+                out.append("<tr>")
+                out.append("".join(f"<td>{x}</td>" for x in row.split()))
+                out.append("</tr>")
+            out.append("</table>")
+            return "".join(out)
         except Exception as exc:
-            out = f"Can't create peek {exc}"
-        return out
+            return f"Can't create peek {exc}"
 
-    def validate(self, dataset, **kwd):
+    def validate(self, dataset: "DatasetInstance", **kwd) -> DatatypeValidation:
         """
         Validate a gg file - all numeric after header row
         """
         with open(dataset.file_name) as infile:
             next(infile)  # header
             for row in infile:
-                ll = row.strip().split('\t')[1:]  # first is alpha feature identifier
+                ll = row.strip().split("\t")[1:]  # first is alpha feature identifier
                 for x in ll:
-                    x = float(x)
+                    float(x)
         return DatatypeValidation.validated()
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in gg format
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'test_space.txt' )
         >>> GenomeGraphs().sniff( fname )
         False
@@ -197,320 +218,336 @@
             rest_row = row[1:]
             try:
                 [float(x) for x in rest_row]  # first col has been removed
             except ValueError:
                 return False
         return True
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/vnd.ms-excel'
+        return "application/vnd.ms-excel"
 
 
 class rgTabList(Tabular):
     """
     for sampleid and for featureid lists of exclusions or inclusions in the clean tool
     featureid subsets on statistical criteria -> specialized display such as gg
     """
+
     file_ext = "rgTList"
 
     def __init__(self, **kwd):
         """
         Initialize featurelistt datatype
         """
         super().__init__(**kwd)
         self.column_names = []
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/html'
+        return "text/html"
 
 
 class rgSampleList(rgTabList):
     """
     for sampleid exclusions or inclusions in the clean tool
     output from QC eg excess het, gender error, ibd pair member,eigen outlier,excess mendel errors,...
     since they can be uploaded, should be flexible
     but they are persistent at least
     same infrastructure for expression?
     """
+
     file_ext = "rgSList"
 
     def __init__(self, **kwd):
         """
         Initialize samplelist datatype
         """
         super().__init__(**kwd)
-        self.column_names[0] = 'FID'
-        self.column_names[1] = 'IID'
+        self.column_names[0] = "FID"
+        self.column_names[1] = "IID"
         # this is what Plink wants as at 2009
 
 
 class rgFeatureList(rgTabList):
     """
     for featureid lists of exclusions or inclusions in the clean tool
     output from QC eg low maf, high missingness, bad hwe in controls, excess mendel errors,...
     featureid subsets on statistical criteria -> specialized display such as gg
     same infrastructure for expression?
     """
+
     file_ext = "rgFList"
 
     def __init__(self, **kwd):
         """Initialize featurelist datatype"""
         super().__init__(**kwd)
-        for i, s in enumerate(['#FeatureId', 'Chr', 'Genpos', 'Mappos']):
+        for i, s in enumerate(["#FeatureId", "Chr", "Genpos", "Mappos"]):
             self.column_names[i] = s
 
 
 class Rgenetics(Html):
     """
     base class to use for rgenetics datatypes
     derived from html - composite datatype elements
     stored in extra files path
     """
 
-    MetadataElement(name="base_name", desc="base name for all transformed versions of this genetic dataset", default='RgeneticsData',
-                    readonly=True, set_in_upload=True)
-
-    composite_type = 'auto_primary_file'
-    file_ext = 'rgenetics'
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>Rgenetics Galaxy Composite Dataset </title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+    MetadataElement(
+        name="base_name",
+        desc="base name for all transformed versions of this genetic dataset",
+        default="RgeneticsData",
+        readonly=True,
+        set_in_upload=True,
+    )
+
+    composite_type = "auto_primary_file"
+    file_ext = "rgenetics"
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Rgenetics Galaxy Composite Dataset </title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"application/binary\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"application/binary\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="application/binary">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         """
         cannot do this until we are setting metadata
         """
         efp = dataset.extra_files_path
         flist = os.listdir(efp)
-        rval = [f'<html><head><title>Files for Composite Dataset {dataset.name}</title></head><body><p/>Composite {dataset.name} contains:<p/><ul>']
+        rval = [
+            f"<html><head><title>Files for Composite Dataset {dataset.name}</title></head><body><p/>Composite {dataset.name} contains:<p/><ul>"
+        ]
         for fname in flist:
             sfname = os.path.split(fname)[-1]
             f, e = os.path.splitext(fname)
             rval.append(f'<li><a href="{sfname}">{sfname}</a></li>')
-        rval.append('</ul></body></html>')
-        with open(dataset.file_name, 'w') as f:
+        rval.append("</ul></body></html>")
+        with open(dataset.file_name, "w") as f:
             f.write("\n".join(rval))
-            f.write('\n')
+            f.write("\n")
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/html'
+        return "text/html"
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         for lped/pbed eg
 
         """
-        super().set_meta(dataset, **kwd)
-        if not kwd.get('overwrite'):
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
+        if not overwrite:
             if verbose:
-                gal_Log.debug('@@@ rgenetics set_meta called with overwrite = False')
-            return True
+                gal_Log.debug("@@@ rgenetics set_meta called with overwrite = False")
+            return
         try:
             efp = dataset.extra_files_path
         except Exception:
             if verbose:
-                gal_Log.debug(f'@@@rgenetics set_meta failed {sys.exc_info()[0]} - dataset {dataset.name} has no efp ?')
-            return False
+                gal_Log.debug(f"@@@rgenetics set_meta failed {sys.exc_info()[0]} - dataset {dataset.name} has no efp ?")
+            return
         try:
             flist = os.listdir(efp)
         except Exception:
             if verbose:
-                gal_Log.debug(f'@@@rgenetics set_meta failed {sys.exc_info()[0]} - dataset {dataset.name} has no efp ?')
-            return False
+                gal_Log.debug(f"@@@rgenetics set_meta failed {sys.exc_info()[0]} - dataset {dataset.name} has no efp ?")
+            return
         if len(flist) == 0:
             if verbose:
-                gal_Log.debug(f'@@@rgenetics set_meta failed - {dataset.name} efp {efp} is empty?')
-            return False
+                gal_Log.debug(f"@@@rgenetics set_meta failed - {dataset.name} efp {efp} is empty?")
+            return
         self.regenerate_primary_file(dataset)
         if not dataset.info:
-            dataset.info = 'Galaxy genotype datatype object'
+            dataset.info = "Galaxy genotype datatype object"
         if not dataset.blurb:
-            dataset.blurb = 'Composite file - Rgenetics Galaxy toolkit'
-        return True
+            dataset.blurb = "Composite file - Rgenetics Galaxy toolkit"
 
 
 class SNPMatrix(Rgenetics):
     """
     BioC SNPMatrix Rgenetics data collections
     """
+
     file_ext = "snpmatrix"
 
-    def set_peek(self, dataset, **kwd):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Binary RGenetics file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
-        """ need to check the file header hex code
-        """
+    def sniff(self, filename: str) -> bool:
+        """need to check the file header hex code"""
         with open(filename, "b") as infile:
             head = infile.read(16)
         head = [hex(x) for x in head]
-        if head != '':
+        if head != "":
             return False
         else:
             return True
 
 
 class Lped(Rgenetics):
     """
     linkage pedigree (ped,map) Rgenetics data collections
     """
+
     file_ext = "lped"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.ped',
-                                description='Pedigree File',
-                                substitute_name_with_metadata='base_name',
-                                is_binary=False)
-        self.add_composite_file('%s.map',
-                                description='Map File',
-                                substitute_name_with_metadata='base_name',
-                                is_binary=False)
+        self.add_composite_file(
+            "%s.ped", description="Pedigree File", substitute_name_with_metadata="base_name", is_binary=False
+        )
+        self.add_composite_file(
+            "%s.map", description="Map File", substitute_name_with_metadata="base_name", is_binary=False
+        )
 
 
 class Pphe(Rgenetics):
     """
     Plink phenotype file - header must have FID\tIID... Rgenetics data collections
     """
+
     file_ext = "pphe"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.pphe',
-                                description='Plink Phenotype File',
-                                substitute_name_with_metadata='base_name',
-                                is_binary=False)
+        self.add_composite_file(
+            "%s.pphe", description="Plink Phenotype File", substitute_name_with_metadata="base_name", is_binary=False
+        )
 
 
 class Fphe(Rgenetics):
     """
     fbat pedigree file - mad format with ! as first char on header row
     Rgenetics data collections
     """
+
     file_ext = "fphe"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.fphe',
-                                description='FBAT Phenotype File',
-                                substitute_name_with_metadata='base_name')
+        self.add_composite_file("%s.fphe", description="FBAT Phenotype File", substitute_name_with_metadata="base_name")
 
 
 class Phe(Rgenetics):
     """
     Phenotype file
     """
+
     file_ext = "phe"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.phe',
-                                description='Phenotype File',
-                                substitute_name_with_metadata='base_name',
-                                is_binary=False)
+        self.add_composite_file(
+            "%s.phe", description="Phenotype File", substitute_name_with_metadata="base_name", is_binary=False
+        )
 
 
 class Fped(Rgenetics):
     """
     FBAT pedigree format - single file, map is header row of rs numbers. Strange.
     Rgenetics data collections
     """
+
     file_ext = "fped"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.fped', description='FBAT format pedfile',
-                                substitute_name_with_metadata='base_name',
-                                is_binary=False)
+        self.add_composite_file(
+            "%s.fped", description="FBAT format pedfile", substitute_name_with_metadata="base_name", is_binary=False
+        )
 
 
 class Pbed(Rgenetics):
     """
     Plink Binary compressed 2bit/geno Rgenetics data collections
     """
+
     file_ext = "pbed"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.bim', substitute_name_with_metadata='base_name', is_binary=False)
-        self.add_composite_file('%s.bed', substitute_name_with_metadata='base_name', is_binary=True)
-        self.add_composite_file('%s.fam', substitute_name_with_metadata='base_name', is_binary=False)
+        self.add_composite_file("%s.bim", substitute_name_with_metadata="base_name", is_binary=False)
+        self.add_composite_file("%s.bed", substitute_name_with_metadata="base_name", is_binary=True)
+        self.add_composite_file("%s.fam", substitute_name_with_metadata="base_name", is_binary=False)
 
 
 class ldIndep(Rgenetics):
     """
     LD (a good measure of redundancy of information) depleted Plink Binary compressed 2bit/geno
     This is really a plink binary, but some tools work better with less redundancy so are constrained to
     these files
     """
+
     file_ext = "ldreduced"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.bim', substitute_name_with_metadata='base_name', is_binary=False)
-        self.add_composite_file('%s.bed', substitute_name_with_metadata='base_name', is_binary=True)
-        self.add_composite_file('%s.fam', substitute_name_with_metadata='base_name', is_binary=False)
+        self.add_composite_file("%s.bim", substitute_name_with_metadata="base_name", is_binary=False)
+        self.add_composite_file("%s.bed", substitute_name_with_metadata="base_name", is_binary=True)
+        self.add_composite_file("%s.fam", substitute_name_with_metadata="base_name", is_binary=False)
 
 
 class Eigenstratgeno(Rgenetics):
     """
     Eigenstrat format - may be able to get rid of this
     if we move to shellfish
     Rgenetics data collections
     """
+
     file_ext = "eigenstratgeno"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.eigenstratgeno', substitute_name_with_metadata='base_name', is_binary=False)
-        self.add_composite_file('%s.ind', substitute_name_with_metadata='base_name', is_binary=False)
-        self.add_composite_file('%s.map', substitute_name_with_metadata='base_name', is_binary=False)
+        self.add_composite_file("%s.eigenstratgeno", substitute_name_with_metadata="base_name", is_binary=False)
+        self.add_composite_file("%s.ind", substitute_name_with_metadata="base_name", is_binary=False)
+        self.add_composite_file("%s.map", substitute_name_with_metadata="base_name", is_binary=False)
 
 
 class Eigenstratpca(Rgenetics):
     """
     Eigenstrat PCA file for case control adjustment
     Rgenetics data collections
     """
+
     file_ext = "eigenstratpca"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.eigenstratpca',
-                                description='Eigenstrat PCA file', substitute_name_with_metadata='base_name')
+        self.add_composite_file(
+            "%s.eigenstratpca", description="Eigenstrat PCA file", substitute_name_with_metadata="base_name"
+        )
 
 
 class Snptest(Rgenetics):
     """
     BioC snptest Rgenetics data collections
     """
+
     file_ext = "snptest"
 
 
 class IdeasPre(Html):
     """
     This datatype defines the input format required by IDEAS:
     https://academic.oup.com/nar/article/44/14/6721/2468150
@@ -519,107 +556,125 @@
     contains the following files and directories.
     - chromosome_windows.txt (optional)
     - chromosomes.bed (optional)
     - IDEAS_input_config.txt
     - compressed archived tmp directory containing a number of compressed bed files.
     """
 
-    MetadataElement(name="base_name", desc="Base name for this dataset", default='IDEASData', readonly=True, set_in_upload=True)
+    MetadataElement(
+        name="base_name", desc="Base name for this dataset", default="IDEASData", readonly=True, set_in_upload=True
+    )
     MetadataElement(name="chrom_bed", desc="Bed file specifying window positions", default=None, readonly=True)
     MetadataElement(name="chrom_windows", desc="Chromosome window positions", default=None, readonly=True)
     MetadataElement(name="input_config", desc="IDEAS input config", default=None, readonly=True)
     MetadataElement(name="tmp_archive", desc="Compressed archive of compressed bed files", default=None, readonly=True)
 
-    composite_type = 'auto_primary_file'
-    file_ext = 'ideaspre'
+    composite_type = "auto_primary_file"
+    file_ext = "ideaspre"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('chromosome_windows.txt', description='Chromosome window positions', is_binary=False, optional=True)
-        self.add_composite_file('chromosomes.bed', description='Bed file specifying window positions', is_binary=False, optional=True)
-        self.add_composite_file('IDEAS_input_config.txt', description='IDEAS input config', is_binary=False)
-        self.add_composite_file('tmp.tar.gz', description='Compressed archive of compressed bed files', is_binary=True)
+        self.add_composite_file(
+            "chromosome_windows.txt", description="Chromosome window positions", is_binary=False, optional=True
+        )
+        self.add_composite_file(
+            "chromosomes.bed", description="Bed file specifying window positions", is_binary=False, optional=True
+        )
+        self.add_composite_file("IDEAS_input_config.txt", description="IDEAS input config", is_binary=False)
+        self.add_composite_file("tmp.tar.gz", description="Compressed archive of compressed bed files", is_binary=True)
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         for fname in os.listdir(dataset.extra_files_path):
             if fname.startswith("chromosomes"):
                 dataset.metadata.chrom_bed = os.path.join(dataset.extra_files_path, fname)
             elif fname.startswith("chromosome_windows"):
                 dataset.metadata.chrom_windows = os.path.join(dataset.extra_files_path, fname)
             elif fname.startswith("IDEAS_input_config"):
                 dataset.metadata.input_config = os.path.join(dataset.extra_files_path, fname)
             elif fname.startswith("tmp"):
                 dataset.metadata.tmp_archive = os.path.join(dataset.extra_files_path, fname)
         self.regenerate_primary_file(dataset)
 
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head></head><body>']
-        rval.append('<h3>Files prepared for IDEAS</h3>')
-        rval.append('<ul>')
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head></head><body>"]
+        rval.append("<h3>Files prepared for IDEAS</h3>")
+        rval.append("<ul>")
         for composite_name in self.get_composite_files(dataset=dataset).keys():
             fn = composite_name
             rval.append(f'<li><a href="{fn}>{fn}</a></li>')
-        rval.append('</ul></body></html>\n')
+        rval.append("</ul></body></html>\n")
         return "\n".join(rval)
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         # Cannot do this until we are setting metadata.
-        rval = ['<html><head></head><body>']
-        rval.append('<h3>Files prepared for IDEAS</h3>')
-        rval.append('<ul>')
+        rval = ["<html><head></head><body>"]
+        rval.append("<h3>Files prepared for IDEAS</h3>")
+        rval.append("<ul>")
         for fname in os.listdir(dataset.extra_files_path):
             fn = os.path.split(fname)[-1]
             rval.append(f'<li><a href="{fn}">{fn}</a></li>')
-        rval.append('</ul></body></html>')
-        with open(dataset.file_name, 'w') as f:
+        rval.append("</ul></body></html>")
+        with open(dataset.file_name, "w") as f:
             f.write("\n".join(rval))
-            f.write('\n')
+            f.write("\n")
 
 
 class Pheno(Tabular):
     """
     base class for pheno files
     """
-    file_ext = 'pheno'
+
+    file_ext = "pheno"
 
 
 class RexpBase(Html):
     """
     base class for BioC data structures in Galaxy
     must be constructed with the pheno data in place since that
     goes into the metadata for each instance
     """
+
     MetadataElement(name="columns", default=0, desc="Number of columns", visible=True)
     MetadataElement(name="column_names", default=[], desc="Column names", visible=True)
     MetadataElement(name="pheCols", default=[], desc="Select list for potentially interesting variables", visible=True)
-    MetadataElement(name="base_name",
-                    desc="base name for all transformed versions of this expression dataset", default='rexpression', set_in_upload=True)
-    MetadataElement(name="pheno_path", desc="Path to phenotype data for this experiment", default="rexpression.pheno", visible=True)
-    file_ext = 'rexpbase'
+    MetadataElement(
+        name="base_name",
+        desc="base name for all transformed versions of this expression dataset",
+        default="rexpression",
+        set_in_upload=True,
+    )
+    MetadataElement(
+        name="pheno_path", desc="Path to phenotype data for this experiment", default="rexpression.pheno", visible=True
+    )
+    file_ext = "rexpbase"
     html_table = None
-    composite_type = 'auto_primary_file'
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.pheno', description='Phenodata tab text file',
-                                substitute_name_with_metadata='base_name', is_binary=False)
+        self.add_composite_file(
+            "%s.pheno",
+            description="Phenodata tab text file",
+            substitute_name_with_metadata="base_name",
+            is_binary=False,
+        )
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """
         This is called only at upload to write the html file
         cannot rename the datasets here - they come with the default unfortunately
         """
-        return '<html><head></head><body>AutoGenerated Primary File for Composite Dataset</body></html>'
+        return "<html><head></head><body>AutoGenerated Primary File for Composite Dataset</body></html>"
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/html'
+        return "text/html"
 
-    def get_phecols(self, phenolist, maxConc=20):
+    def get_phecols(self, phenolist: List, maxConc: int = 20) -> List:
         """
         sept 2009: cannot use whitespace to split - make a more complex structure here
         and adjust the methods that rely on this structure
         return interesting phenotype column names for an rexpression eset or affybatch
         to use in array subsetting and so on. Returns a data structure for a
         dynamic Galaxy select parameter.
         A column with only 1 value doesn't change, so is not interesting for
@@ -628,23 +683,26 @@
         are removed after the concordance (count of unique terms) is constructed for each
         column. Then a complication - each remaining pair of columns is tested for
         redundancy - if two columns are always paired, then only one is needed :)
         """
         for nrows, row in enumerate(phenolist):  # construct concordance
             if len(row.strip()) == 0:
                 break
-            row = row.strip().split('\t')
+            row = row.strip().split("\t")
             if nrows == 0:  # set up from header
                 head = row
                 totcols = len(row)
-                concordance = [{} for x in head]  # list of dicts
+                concordance: List[Dict] = [{} for x in head]
             else:
                 for col, code in enumerate(row):  # keep column order correct
                     if col >= totcols:
-                        gal_Log.warning('### get_phecols error in pheno file - row %d col %d (%s) longer than header %s' % (nrows, col, row, head))
+                        gal_Log.warning(
+                            "### get_phecols error in pheno file - row %d col %d (%s) longer than header %s"
+                            % (nrows, col, row, head)
+                        )
                     else:
                         concordance[col].setdefault(code, 0)  # first one is zero
                         concordance[col][code] += 1
         useCols = []
         useConc = []  # columns of interest to keep
         nrows = len(phenolist)
         nrows -= 1  # drop head from count
@@ -652,23 +710,23 @@
             if (len(conc) > 1) and (len(conc) < min(nrows, maxConc)):  # not all same and not all different!!
                 useConc.append(conc)  # keep concordance
                 useCols.append(c)  # keep column
         nuse = len(useCols)
         # now to check for pairs of concordant columns - drop one of these.
         delme = []
         p = phenolist[1:]  # drop header
-        plist = [x.strip().split('\t') for x in p]  # list of lists
+        plist = [x.strip().split("\t") for x in p]  # list of lists
         phe = [[x[i] for i in useCols] for x in plist if len(x) >= totcols]  # strip unused data
         for i in range(0, (nuse - 1)):  # for each interesting column
             for j in range(i + 1, nuse):
                 kdict = {}
                 for row in phe:  # row is a list of lists
-                    k = f'{row[i]}{row[j]}'  # composite key
+                    k = f"{row[i]}{row[j]}"  # composite key
                     kdict[k] = k
-                if (len(kdict.keys()) == len(concordance[useCols[j]])):  # i and j are always matched
+                if len(kdict.keys()) == len(concordance[useCols[j]]):  # i and j are always matched
                     delme.append(j)
         delme = list(set(delme))  # remove dupes
         listCol = []
         delme.sort()
         delme.reverse()  # must delete from far end!
         for i in delme:
             del useConc[i]  # get rid of concordance
@@ -678,174 +736,190 @@
             cc = [(x[1], x[0]) for x in ccounts]  # list of code count tuples
             codeDetails = (head[useCols[i]], cc)  # ('foo',[('a',3),('b',11),..])
             listCol.append(codeDetails)
         if len(listCol) > 0:
             res = listCol
             # metadata.pheCols becomes [('bar;22,zot;113','foo'), ...]
         else:
-            res = [('no usable phenotype columns found', [('?', 0), ]), ]
+            res = [
+                (
+                    "no usable phenotype columns found",
+                    [
+                        ("?", 0),
+                    ],
+                ),
+            ]
         return res
 
     def get_pheno(self, dataset):
         """
         expects a .pheno file in the extra_files_dir - ugh
         note that R is wierd and adds the row.name in
         the header so the columns are all wrong - unless you tell it not to.
         A file can be written as
         write.table(file='foo.pheno',pData(foo),sep='\t',quote=F,row.names=F)
         """
         p = open(dataset.metadata.pheno_path).readlines()
         if len(p) > 0:  # should only need to fix an R pheno file once
-            head = p[0].strip().split('\t')
-            line1 = p[1].strip().split('\t')
+            head = p[0].strip().split("\t")
+            line1 = p[1].strip().split("\t")
             if len(head) < len(line1):
-                head.insert(0, 'ChipFileName')  # fix R write.table b0rken-ness
-                p[0] = '\t'.join(head)
+                head.insert(0, "ChipFileName")  # fix R write.table b0rken-ness
+                p[0] = "\t".join(head)
         else:
             p = []
-        return '\n'.join(p)
+        return "\n".join(p)
 
-    def set_peek(self, dataset, **kwd):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """
         expects a .pheno file in the extra_files_dir - ugh
         note that R is weird and does not include the row.name in
         the header. why?"""
         if not dataset.dataset.purged:
-            pp = os.path.join(dataset.extra_files_path, f'{dataset.metadata.base_name}.pheno')
+            pp = os.path.join(dataset.extra_files_path, f"{dataset.metadata.base_name}.pheno")
             try:
                 with open(pp) as f:
                     p = f.readlines()
             except Exception:
-                p = [f'##failed to find {pp}', ]
-            dataset.peek = ''.join(p[:5])
-            dataset.blurb = 'Galaxy Rexpression composite file'
+                p = [
+                    f"##failed to find {pp}",
+                ]
+            dataset.peek = "".join(p[:5])
+            dataset.blurb = "Galaxy Rexpression composite file"
         else:
-            dataset.peek = 'file does not exist\n'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist\n"
+            dataset.blurb = "file purged from disk"
 
     def get_peek(self, dataset):
         """
         expects a .pheno file in the extra_files_dir - ugh
         """
-        pp = os.path.join(dataset.extra_files_path, f'{dataset.metadata.base_name}.pheno')
+        pp = os.path.join(dataset.extra_files_path, f"{dataset.metadata.base_name}.pheno")
         try:
             with open(pp) as f:
                 p = f.readlines()
         except Exception:
-            p = [f'##failed to find {pp}']
-        return ''.join(p[:5])
+            p = [f"##failed to find {pp}"]
+        return "".join(p[:5])
 
-    def get_file_peek(self, filename):
+    def get_file_peek(self, filename: str) -> str:
         """
-        can't really peek at a filename - need the extra_files_path and such?
+        Read and return the first `max_lines`.
+        (Can't really peek at a filename - need the extra_files_path and such?)
         """
-        h = '## rexpression get_file_peek: no file found'
+        max_lines = 5
         try:
             with open(filename) as f:
-                h = f.readlines()
+                lines = []
+                for line in f:
+                    lines.append(line)
+                    if len(lines) == max_lines:
+                        break
+                return "".join(lines)
         except Exception:
-            pass
-        return ''.join(h[:5])
+            return "## rexpression get_file_peek: no file found"
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         """
         cannot do this until we are setting metadata
         """
         bn = dataset.metadata.base_name
         flist = os.listdir(dataset.extra_files_path)
-        rval = [f'<html><head><title>Files for Composite Dataset {bn}</title></head><p/>Comprises the following files:<p/><ul>']
+        rval = [
+            f"<html><head><title>Files for Composite Dataset {bn}</title></head><p/>Comprises the following files:<p/><ul>"
+        ]
         for fname in flist:
             sfname = os.path.split(fname)[-1]
             rval.append(f'<li><a href="{sfname}">{sfname}</a>')
-        rval.append('</ul></html>')
-        with open(dataset.file_name, 'w') as f:
+        rval.append("</ul></html>")
+        with open(dataset.file_name, "w") as f:
             f.write("\n".join(rval))
-            f.write('\n')
+            f.write("\n")
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         if copy_from:
             dataset.metadata = copy_from.metadata
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         NOTE we apply the tabular machinary to the phenodata extracted
         from a BioC eSet or affybatch.
 
         """
-        super().set_meta(dataset, **kwd)
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         try:
             flist = os.listdir(dataset.extra_files_path)
         except Exception:
             if verbose:
-                gal_Log.debug('@@@rexpression set_meta failed - no dataset?')
-            return False
+                gal_Log.debug("@@@rexpression set_meta failed - no dataset?")
+            return
         bn = dataset.metadata.base_name
         if not bn:
             for f in flist:
                 n = os.path.splitext(f)[0]
                 bn = n
                 dataset.metadata.base_name = bn
         if not bn:
-            bn = '?'
+            bn = "?"
             dataset.metadata.base_name = bn
-        pn = f'{bn}.pheno'
+        pn = f"{bn}.pheno"
         pp = os.path.join(dataset.extra_files_path, pn)
         dataset.metadata.pheno_path = pp
         try:
             with open(pp) as f:
                 pf = f.readlines()  # read the basename.phenodata in the extra_files_path
         except Exception:
             pf = None
         if pf:
             h = pf[0].strip()
-            h = h.split('\t')  # hope is header
+            h = h.split("\t")  # hope is header
             h = [escape(x) for x in h]
             dataset.metadata.column_names = h
             dataset.metadata.columns = len(h)
-            dataset.peek = ''.join(pf[:5])
+            dataset.peek = "".join(pf[:5])
         else:
             dataset.metadata.column_names = []
             dataset.metadata.columns = 0
-            dataset.peek = 'No pheno file found'
+            dataset.peek = "No pheno file found"
         if pf and len(pf) > 1:
             dataset.metadata.pheCols = self.get_phecols(phenolist=pf)
         else:
-            dataset.metadata.pheCols = [('', 'No useable phenotypes found', False), ]
+            dataset.metadata.pheCols = [
+                ("", "No useable phenotypes found", False),
+            ]
         if not dataset.info:
-            dataset.info = 'Galaxy Expression datatype object'
+            dataset.info = "Galaxy Expression datatype object"
         if not dataset.blurb:
-            dataset.blurb = 'R loadable BioC expression object for the Rexpression Galaxy toolkit'
-        return True
+            dataset.blurb = "R loadable BioC expression object for the Rexpression Galaxy toolkit"
 
-    def make_html_table(self, pp='nothing supplied from peek\n'):
+    def make_html_table(self, pp: str = "nothing supplied from peek\n") -> str:
         """
         Create HTML table, used for displaying peek
         """
-        out = ['<table cellspacing="0" cellpadding="3">', ]
         try:
+            out = ['<table cellspacing="0" cellpadding="3">']
             # Generate column header
-            p = pp.split('\n')
+            p = pp.split("\n")
             for i, row in enumerate(p):
-                lrow = row.strip().split('\t')
+                lrow = row.strip().split("\t")
                 if i == 0:
-                    orow = [f'<th>{escape(x)}</th>' for x in lrow]
-                    orow.insert(0, '<tr>')
-                    orow.append('</tr>')
+                    orow = [f"<th>{escape(x)}</th>" for x in lrow]
+                    orow.insert(0, "<tr>")
+                    orow.append("</tr>")
                 else:
-                    orow = [f'<td>{escape(x)}</td>' for x in lrow]
-                    orow.insert(0, '<tr>')
-                    orow.append('</tr>')
-                out.append(''.join(orow))
-            out.append('</table>')
-            out = "\n".join(out)
+                    orow = [f"<td>{escape(x)}</td>" for x in lrow]
+                    orow.insert(0, "<tr>")
+                    orow.append("</tr>")
+                out.append("".join(orow))
+            out.append("</table>")
+            return "\n".join(out)
         except Exception as exc:
-            out = f"Can't create html table {unicodify(exc)}"
-        return out
+            return f"Can't create html table {unicodify(exc)}"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """
         Returns formatted html of peek
         """
         out = self.make_html_table(dataset.peek)
         return out
 
 
@@ -854,84 +928,99 @@
     derived class for BioC data structures in Galaxy
     """
 
     file_ext = "affybatch"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.affybatch',
-                                description='AffyBatch R object saved to file',
-                                substitute_name_with_metadata='base_name', is_binary=True)
+        self.add_composite_file(
+            "%s.affybatch",
+            description="AffyBatch R object saved to file",
+            substitute_name_with_metadata="base_name",
+            is_binary=True,
+        )
 
 
 class Eset(RexpBase):
     """
     derived class for BioC data structures in Galaxy
     """
+
     file_ext = "eset"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.eset',
-                                description='ESet R object saved to file',
-                                substitute_name_with_metadata='base_name', is_binary=True)
+        self.add_composite_file(
+            "%s.eset",
+            description="ESet R object saved to file",
+            substitute_name_with_metadata="base_name",
+            is_binary=True,
+        )
 
 
 class MAlist(RexpBase):
     """
     derived class for BioC data structures in Galaxy
     """
+
     file_ext = "malist"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.malist',
-                                description='MAlist R object saved to file',
-                                substitute_name_with_metadata='base_name', is_binary=True)
+        self.add_composite_file(
+            "%s.malist",
+            description="MAlist R object saved to file",
+            substitute_name_with_metadata="base_name",
+            is_binary=True,
+        )
 
 
 class LinkageStudies(Text):
     """
     superclass for classical linkage analysis suites
     """
+
     test_files = [
-        'linkstudies.allegro_fparam', 'linkstudies.alohomora_gts',
-        'linkstudies.linkage_datain', 'linkstudies.linkage_map'
+        "linkstudies.allegro_fparam",
+        "linkstudies.alohomora_gts",
+        "linkstudies.linkage_datain",
+        "linkstudies.linkage_map",
     ]
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self.max_lines = 10
 
 
 @build_sniff_from_prefix
 class GenotypeMatrix(LinkageStudies):
     """
     Sample matrix of genotypes
     - GTs as columns
     """
+
     file_ext = "alohomora_gts"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def header_check(self, fio):
-        header_elems = fio.readline().split('\t')
+    def header_check(self, fio: IO) -> bool:
+        header_elems = fio.readline().split("\t")
 
         if header_elems[0] != "Name":
             return False
 
         try:
             return all(int(sid) > 0 for sid in header_elems[1:])
         except ValueError:
             return False
 
         return True
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> classname = GenotypeMatrix
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> extn_true = classname().file_ext
         >>> file_true = get_test_fname("linkstudies." + extn_true)
         >>> classname().sniff(file_true)
         True
@@ -953,15 +1042,15 @@
         if not self.header_check(fio):
             return False
 
         for lcount, line in enumerate(fio):
             if lcount > self.max_lines:
                 return True
 
-            tokens = line.split('\t')
+            tokens = line.split("\t")
 
             if num_cols == -1:
                 num_cols = len(tokens)
             elif num_cols != len(tokens):
                 return False
             if not VALID_GENOTYPES_LINE.match(line):
                 return False
@@ -973,25 +1062,26 @@
 class MarkerMap(LinkageStudies):
     """
     Map of genetic markers including physical and genetic distance
     Common input format for linkage programs
 
     chrom, genetic pos, markername, physical pos, Nr
     """
+
     file_ext = "linkage_map"
 
-    def header_check(self, fio):
+    def header_check(self, fio: IO) -> bool:
         headers = fio.readline().split()
 
         if len(headers) == 5 and headers[0] == "#Chr":
             return True
 
         return False
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> classname = MarkerMap
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> extn_true = classname().file_ext
         >>> file_true = get_test_fname("linkstudies." + extn_true)
         >>> classname().sniff(file_true)
         True
@@ -1019,35 +1109,36 @@
                 chrm, gpos, nam, bpos, row = line.split()
                 float(gpos)
                 int(bpos)
 
                 try:
                     int(chrm)
                 except ValueError:
-                    if not chrm.lower()[0] in ('x', 'y', 'm'):
+                    if chrm.lower()[0] not in ("x", "y", "m"):
                         return False
 
             except ValueError:
                 return False
 
         return True
 
 
 @build_sniff_from_prefix
 class DataIn(LinkageStudies):
     """
     Common linkage input file for intermarker distances
     and recombination rates
     """
+
     file_ext = "linkage_datain"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> classname = DataIn
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> extn_true = classname().file_ext
         >>> file_true = get_test_fname("linkstudies." + extn_true)
         >>> classname().sniff(file_true)
         True
@@ -1104,26 +1195,25 @@
 
 
 @build_sniff_from_prefix
 class AllegroLOD(LinkageStudies):
     """
     Allegro output format for LOD scores
     """
+
     file_ext = "allegro_fparam"
 
-    def header_check(self, fio):
+    def header_check(self, fio: IO) -> bool:
         header = fio.readline().splitlines()[0].split()
-        if len(header) == 4 and header == [
-                "family", "location", "LOD", "marker"
-        ]:
+        if len(header) == 4 and header == ["family", "location", "LOD", "marker"]:
             return True
 
         return False
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> classname = AllegroLOD
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> extn_true = classname().file_ext
         >>> file_true = get_test_fname("linkstudies." + extn_true)
         >>> classname().sniff(file_true)
         True
@@ -1157,12 +1247,7 @@
                 if tokens[2] != "-inf":
                     float(tokens[2])
 
             except (ValueError, IndexError):
                 return False
 
         return True
-
-
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/gis.py` & `galaxy-data-23.0.1/galaxy/datatypes/gis.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,63 +1,100 @@
 """
 GIS classes
 """
+from typing import TYPE_CHECKING
 
 from galaxy.datatypes.binary import Binary
+from galaxy.datatypes.data import GeneratePrimaryFileDataset
+
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
 
 
 class Shapefile(Binary):
-    """ The Shapefile data format:
-            For more information please see http://en.wikipedia.org/wiki/Shapefile
+    """The Shapefile data format:
+    For more information please see http://en.wikipedia.org/wiki/Shapefile
     """
 
-    composite_type = 'auto_primary_file'
+    composite_type = "auto_primary_file"
     file_ext = "shp"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('shapefile.shp', description='Geometry File (shp)', is_binary=True, optional=False)
-        self.add_composite_file('shapefile.shx', description='Geometry index File (shx)', is_binary=True, optional=False)
-        self.add_composite_file('shapefile.dbf', description='Columnar attributes for each shape (dbf)', is_binary=True, optional=False)
+        self.add_composite_file("shapefile.shp", description="Geometry File (shp)", is_binary=True, optional=False)
+        self.add_composite_file(
+            "shapefile.shx", description="Geometry index File (shx)", is_binary=True, optional=False
+        )
+        self.add_composite_file(
+            "shapefile.dbf", description="Columnar attributes for each shape (dbf)", is_binary=True, optional=False
+        )
         # optional
-        self.add_composite_file('shapefile.prj', description='Projection description (prj)', is_binary=False, optional=True)
-        self.add_composite_file('shapefile.sbn', description='Spatial index of the features (sbn)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.sbx', description='Spatial index of the features (sbx)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.fbn', description='Read only spatial index of the features (fbn)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.fbx', description='Read only spatial index of the features (fbx)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.ain', description='Attribute index of the active fields in a table (ain)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.aih', description='Attribute index of the active fields in a table (aih)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.atx', description='Attribute index for the dbf file (atx)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.ixs', description='Geocoding index (ixs)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.mxs', description='Geocoding index in ODB format (mxs)', is_binary=True, optional=True)
-        self.add_composite_file('shapefile.shp.xml', description='Geospatial metadata in XML format (xml)', is_binary=False, optional=True)
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>Shapefile Galaxy Composite Dataset</title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+        self.add_composite_file(
+            "shapefile.prj", description="Projection description (prj)", is_binary=False, optional=True
+        )
+        self.add_composite_file(
+            "shapefile.sbn", description="Spatial index of the features (sbn)", is_binary=True, optional=True
+        )
+        self.add_composite_file(
+            "shapefile.sbx", description="Spatial index of the features (sbx)", is_binary=True, optional=True
+        )
+        self.add_composite_file(
+            "shapefile.fbn", description="Read only spatial index of the features (fbn)", is_binary=True, optional=True
+        )
+        self.add_composite_file(
+            "shapefile.fbx", description="Read only spatial index of the features (fbx)", is_binary=True, optional=True
+        )
+        self.add_composite_file(
+            "shapefile.ain",
+            description="Attribute index of the active fields in a table (ain)",
+            is_binary=True,
+            optional=True,
+        )
+        self.add_composite_file(
+            "shapefile.aih",
+            description="Attribute index of the active fields in a table (aih)",
+            is_binary=True,
+            optional=True,
+        )
+        self.add_composite_file(
+            "shapefile.atx", description="Attribute index for the dbf file (atx)", is_binary=True, optional=True
+        )
+        self.add_composite_file("shapefile.ixs", description="Geocoding index (ixs)", is_binary=True, optional=True)
+        self.add_composite_file(
+            "shapefile.mxs", description="Geocoding index in ODB format (mxs)", is_binary=True, optional=True
+        )
+        self.add_composite_file(
+            "shapefile.shp.xml", description="Geospatial metadata in XML format (xml)", is_binary=False, optional=True
+        )
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Shapefile Galaxy Composite Dataset</title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"application/binary\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"application/binary\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="application/binary">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>\n')
+        rval.append("</ul></div></html>\n")
         return "\n".join(rval)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text."""
         if not dataset.dataset.purged:
             dataset.peek = "Shapefile data"
             dataset.blurb = "Shapefile data"
         else:
             dataset.peek = "file does not exist"
             dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "Shapefile data"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/goldenpath.py` & `galaxy-data-23.0.1/galaxy/datatypes/goldenpath.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,34 +1,42 @@
 import abc
+import logging
 import os
 from typing import (
     Set,
+    TYPE_CHECKING,
     Union,
 )
 
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     iter_headers,
 )
 from .tabular import Tabular
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
+log = logging.getLogger(__name__)
+
 
 @build_sniff_from_prefix
 class GoldenPath(Tabular):
     """Class describing NCBI's Golden Path assembly format"""
-    edam_format = 'format_3693'
-    file_ext = 'agp'
 
-    def set_meta(self, dataset, **kwd):
+    edam_format = "format_3693"
+    file_ext = "agp"
+
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         # AGPFile reads and validates entire file.
         AGPFile(dataset.file_name)
-        super().set_meta(dataset, **kwd)
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checks for and does cursory validation on data that looks like AGP
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('eg1.agp')
         >>> GoldenPath().sniff(fname)
         True
@@ -40,29 +48,48 @@
         False
         >>> fname = get_test_fname('2.tabular')
         >>> GoldenPath().sniff(fname)
         False
         """
         found_non_comment_lines = False
         try:
-            for line in iter_headers(file_prefix, '\t', comment_designator='#'):
+            for line in iter_headers(file_prefix, "\t", comment_designator="#"):
                 if line:
                     if len(line) != 9:
                         return False
-                    assert line[4] in ['A', 'D', 'F', 'G', 'O', 'P', 'W', 'N', 'U']
+                    assert line[4] in ["A", "D", "F", "G", "O", "P", "W", "N", "U"]
                     ostensible_numbers = line[1:3]
-                    if line[4] in ['U', 'N']:
+                    if line[4] in ["U", "N"]:
                         ostensible_numbers.append(line[5])
-                        assert line[6] in ['scaffold', 'contig', 'centromere', 'short_arm', 'heterochromatin', 'telomere', 'repeat']
-                        assert line[7] in ['yes', 'no']
-                        assert line[8] in ['na', 'paired-ends', 'align_genus', 'align_xgenus', 'align_trnscript', 'within_clone', 'clone_contig', 'map', 'strobe', 'unspecified']
+                        assert line[6] in [
+                            "scaffold",
+                            "contig",
+                            "centromere",
+                            "short_arm",
+                            "heterochromatin",
+                            "telomere",
+                            "repeat",
+                        ]
+                        assert line[7] in ["yes", "no"]
+                        assert line[8] in [
+                            "na",
+                            "paired-ends",
+                            "align_genus",
+                            "align_xgenus",
+                            "align_trnscript",
+                            "within_clone",
+                            "clone_contig",
+                            "map",
+                            "strobe",
+                            "unspecified",
+                        ]
                     else:
                         ostensible_numbers.extend([line[6], line[7]])
-                        assert line[8] in ['+', '-', '?', '0', 'na']
-                    if line[4] == 'U':
+                        assert line[8] in ["+", "-", "?", "0", "na"]
+                    if line[4] == "U":
                         assert int(line[5]) == 100
                     assert all(map(lambda x: str(x).isnumeric() and int(x) > 0, ostensible_numbers))
                     found_non_comment_lines = True
         except Exception:
             return False
         return found_non_comment_lines
 
@@ -94,15 +121,14 @@
     Common abbreviations:
       "comp": AGP component
       "obj":  AGP object
       "pid":  AGP part number
     """
 
     def __init__(self, in_file):
-
         self._agp_version = "2.1"
         self._fname = os.path.abspath(in_file)
 
         # Store comment and AGP lines separately.
         self._comment_lines = []
         self._objects = []
 
@@ -127,15 +153,15 @@
         self._comment_lines = []
         self._objects = []
         self._current_obj = None
         self._seen_objs = set()
 
         line_number = 0
         in_body = False
-        with open(self.fname, "r") as f:
+        with open(self.fname) as f:
             for line in f:
                 line_number += 1
                 line = line.rstrip("\n")
                 if line.startswith("#"):
                     if not in_body:
                         self._comment_lines.append(line)
                     else:
@@ -162,15 +188,14 @@
                     agp_line = AGPSeqLine(self.fname, line_number, *fields)
 
                 self._add_line(agp_line)
 
     def _add_line(self, agp_line):
         # Perform validity checks if this is a new object
         if agp_line.obj != self._current_obj:
-
             # Check if we have already seen this object before
             if agp_line.obj in self._seen_objs:
                 raise AGPError(self.fname, agp_line.line_number, "object identifier out of order")
 
             # Add the new object to our master list
             agp_obj = AGPObject(self.fname, agp_line)
             self._objects.append(agp_obj)
@@ -188,27 +213,25 @@
 
     @property
     def fname(self):
         return self._fname
 
     @property
     def num_lines(self):
-        """ Calculate the number of lines in the current state of the AGP file. """
+        """Calculate the number of lines in the current state of the AGP file."""
         return len(self._comment_lines) + sum(obj.num_lines for obj in self._objects)
 
     def iterate_objs(self):
-        """ Iterate over the objects of the AGP file. """
-        for obj in self._objects:
-            yield obj
+        """Iterate over the objects of the AGP file."""
+        yield from self._objects
 
     def iterate_lines(self):
-        """ Iterate over the non-comment lines of AGP file. """
+        """Iterate over the non-comment lines of AGP file."""
         for obj in self.iterate_objs():
-            for j in obj.iterate_lines():
-                yield j
+            yield from obj.iterate_lines()
 
 
 class AGPObject:
     """
     Represents an AGP object. Objects will consist of AGP lines, and have to adhere to
     certain rules. By organizing AGP lines into the objects that they comprise, we can easily calculate stats
     about the assembly (the collection of objects).
@@ -267,26 +290,29 @@
         # Check that our PID is sequential
         if agp_line.pid - self.previous_pid != 1:
             raise AGPError(self.fname, agp_line.line_number, "non-sequential part_numbers")
 
         # Check that the object intervals are sequential
         if self.obj_intervals:
             if self.obj_intervals[-1][1] != agp_line.obj_beg - 1:
-                raise AGPError(self.fname, agp_line.line_number, f"some positions in {agp_line.obj} are not accounted for or overlapping")
+                raise AGPError(
+                    self.fname,
+                    agp_line.line_number,
+                    f"some positions in {agp_line.obj} are not accounted for or overlapping",
+                )
 
         self.previous_pid = agp_line.pid
         self.obj_intervals.append((agp_line.obj_beg - 1, agp_line.obj_end))
         self._agp_lines.append(agp_line)
 
     def iterate_lines(self):
-        for i in self._agp_lines:
-            yield i
+        yield from self._agp_lines
 
 
-class AGPLine(object, metaclass=abc.ABCMeta):
+class AGPLine(metaclass=abc.ABCMeta):
     """
     An abstract base class representing a single AGP file line. Inheriting subclasses should
     override or implement new methods to check the validity of a single AFP line. Validity
     checks that involve multiple lines should not be considered.
     """
 
     allowed_comp_types: Set[str] = set()
@@ -308,87 +334,90 @@
         self._validate_strings()
         self._validate_obj_coords()
         self._validate_component_type()
         self._validate_line()
 
     @abc.abstractmethod
     def __str__(self):
-        """ Return the tab delimited AGP line"""
-        pass
+        """Return the tab delimited AGP line"""
 
     @abc.abstractmethod
     def __iter__(self):
-        """ Return the AGP line's iterator"""
-        pass
+        """Return the AGP line's iterator"""
 
     @abc.abstractmethod
     def _validate_numerics(self):
-        """ Ensure all numeric fields and positive integers. """
-        pass
+        """Ensure all numeric fields and positive integers."""
 
     @abc.abstractmethod
     def _validate_strings(self):
-        """ Ensure all text fields are strings. """
-        pass
+        """Ensure all text fields are strings."""
 
     def _validate_obj_coords(self):
         if self.obj_beg > self.obj_end:
-            raise AGPError(self.fname, self.line_number, f"object_beg ({self.obj_beg}) must be <= object_end ({self.obj_end})")
+            raise AGPError(
+                self.fname, self.line_number, f"object_beg ({self.obj_beg}) must be <= object_end ({self.obj_end})"
+            )
 
     def _validate_component_type(self):
         if self.comp_type not in self.allowed_comp_types:
             raise AGPError(self.fname, self.line_number, f"invalid component type: {self.comp_type}")
 
     @abc.abstractmethod
     def _validate_line(self):
-        """ Final remaining validations specific to the gap or sequence AGP lines. """
-        pass
+        """Final remaining validations specific to the gap or sequence AGP lines."""
 
 
 class AGPSeqLine(AGPLine):
 
     """
     A subclass of AGPLine specifically for AGP lines that represent sequences.
     """
 
     allowed_comp_types = {"A", "D", "F", "G", "O", "P", "W"}
     allowed_orientations = {"+", "-", "?", "0", "na"}
 
-    def __init__(self, fname, line_number, obj, obj_beg, obj_end, pid, comp_type, comp, comp_beg, comp_end, orientation):
+    def __init__(
+        self, fname, line_number, obj, obj_beg, obj_end, pid, comp_type, comp, comp_beg, comp_end, orientation
+    ):
         self.comp = comp
         self.comp_beg = comp_beg
         self.comp_end = comp_end
         self.orientation = orientation
 
         # Set the object attributes and perform superclass-defined validations
-        super(AGPSeqLine, self).__init__(fname, line_number, obj, obj_beg, obj_end, pid, comp_type)
+        super().__init__(fname, line_number, obj, obj_beg, obj_end, pid, comp_type)
 
         self.is_gap = False
-        self.seqdict = dict(obj=str(self.obj),
-                            obj_beg=int(self.obj_beg),
-                            obj_end=int(self.obj_end),
-                            pid=int(self.pid),
-                            comp_type=str(self.comp_type),
-                            comp=str(self.comp),
-                            comp_beg=int(self.comp_beg),
-                            comp_end=int(self.comp_end),
-                            orientation=str(self.orientation))
+        self.seqdict = dict(
+            obj=str(self.obj),
+            obj_beg=int(self.obj_beg),
+            obj_end=int(self.obj_end),
+            pid=int(self.pid),
+            comp_type=str(self.comp_type),
+            comp=str(self.comp),
+            comp_beg=int(self.comp_beg),
+            comp_end=int(self.comp_end),
+            orientation=str(self.orientation),
+        )
 
     def __str__(self):
-        return "\t".join([
-            self.obj,
-            str(self.obj_beg),
-            str(self.obj_end),
-            str(self.pid),
-            self.comp_type,
-            self.comp,
-            str(self.comp_beg),
-            str(self.comp_end),
-            self.orientation
-        ])
+        return "\t".join(
+            [
+                self.obj,
+                str(self.obj_beg),
+                str(self.obj_end),
+                str(self.pid),
+                self.comp_type,
+                self.comp,
+                str(self.comp_beg),
+                str(self.comp_end),
+                self.orientation,
+            ]
+        )
 
     def __iter__(self):
         for key in self.seqdict:
             yield str(key), self.seqdict[key]
 
     def _validate_numerics(self):
         # Convert all numeric types to integers
@@ -399,29 +428,31 @@
             self.pid = int(self.pid)
             self.comp_beg = int(self.comp_beg)
             self.comp_end = int(self.comp_end)
         except TypeError:
             raise AGPError(self.fname, self.line_number, "encountered an invalid non-integer numeric AGP field")
 
         # Ensure that all numeric values are positive
-        if not all([
-            self.obj_beg > 0,
-            self.obj_end > 0,
-            self.pid > 0,
-            self.comp_beg > 0,
-            self.comp_end > 0
-        ]):
+        if not all([self.obj_beg > 0, self.obj_end > 0, self.pid > 0, self.comp_beg > 0, self.comp_end > 0]):
             raise AGPError(self.fname, self.line_number, "encountered an invalid zero or negative numeric AGP field.")
 
         # Check the coordinates
         if self.comp_beg > self.comp_end:
-            raise AGPError(self.fname, self.line_number, f"component_beg ({self.comp_beg}) must be <= component_end ({self.comp_end})")
+            raise AGPError(
+                self.fname,
+                self.line_number,
+                f"component_beg ({self.comp_beg}) must be <= component_end ({self.comp_end})",
+            )
 
         if self.obj_end - (self.obj_beg - 1) != self.comp_end - (self.comp_beg - 1):
-            raise AGPError(self.fname, self.line_number, f"object coordinates ({self.obj_beg}, {self.obj_end}) and component coordinates ({self.comp_beg}, {self.comp_end}) do not have the same length")
+            raise AGPError(
+                self.fname,
+                self.line_number,
+                f"object coordinates ({self.obj_beg}, {self.obj_end}) and component coordinates ({self.comp_beg}, {self.comp_end}) do not have the same length",
+            )
 
     def _validate_strings(self):
         try:
             self.obj = str(self.obj)
             self.comp_type = str(self.comp_type)
             self.comp = str(self.comp)
             self.orientation = str(self.orientation)
@@ -438,54 +469,76 @@
     """
     A subclass of AGPLine specifically for AGP lines that represent sequence gaps.
     """
 
     allowed_comp_types = {"N", "U"}
     allowed_linkage_types = {"yes", "no"}
     allowed_gap_types = {
-        "scaffold", "contig", "centromere", "short_arm", "heterochromatin", "telomere", "repeat", "contamination"
+        "scaffold",
+        "contig",
+        "centromere",
+        "short_arm",
+        "heterochromatin",
+        "telomere",
+        "repeat",
+        "contamination",
     }
     allowed_evidence_types = {
-        "na", "paired-ends", "align_genus", "align_xgenus",
-        "align_trnscpt", "within_clone", "clone_contig", "map",
-        "pcr", "proximity_ligation", "strobe", "unspecified"
+        "na",
+        "paired-ends",
+        "align_genus",
+        "align_xgenus",
+        "align_trnscpt",
+        "within_clone",
+        "clone_contig",
+        "map",
+        "pcr",
+        "proximity_ligation",
+        "strobe",
+        "unspecified",
     }
 
-    def __init__(self, fname, line_number, obj, obj_beg, obj_end, pid, comp_type, gap_len, gap_type, linkage, linkage_evidence):
+    def __init__(
+        self, fname, line_number, obj, obj_beg, obj_end, pid, comp_type, gap_len, gap_type, linkage, linkage_evidence
+    ):
         self.gap_len = gap_len
         self.gap_type = gap_type
         self.linkage = linkage
         self.linkage_evidence = linkage_evidence
 
         # Set the object attributes and perform superclass-defined validations
-        super(AGPGapLine, self).__init__(fname, line_number, obj, obj_beg, obj_end, pid, comp_type)
+        super().__init__(fname, line_number, obj, obj_beg, obj_end, pid, comp_type)
 
         self.is_gap = True
-        self.gapdict = dict(obj=str(self.obj),
-                            obj_beg=int(self.obj_beg),
-                            obj_end=int(self.obj_end),
-                            pid=int(self.pid),
-                            comp_type=str(self.comp_type),
-                            gap_len=int(self.gap_len),
-                            gap_type=str(self.gap_type),
-                            linkage=str(self.linkage),
-                            linkage_evidence=str(self.linkage_evidence))
+        self.gapdict = dict(
+            obj=str(self.obj),
+            obj_beg=int(self.obj_beg),
+            obj_end=int(self.obj_end),
+            pid=int(self.pid),
+            comp_type=str(self.comp_type),
+            gap_len=int(self.gap_len),
+            gap_type=str(self.gap_type),
+            linkage=str(self.linkage),
+            linkage_evidence=str(self.linkage_evidence),
+        )
 
     def __str__(self):
-        return "\t".join([
-            self.obj,
-            str(self.obj_beg),
-            str(self.obj_end),
-            str(self.pid),
-            self.comp_type,
-            str(self.gap_len),
-            self.gap_type,
-            self.linkage,
-            self.linkage_evidence
-        ])
+        return "\t".join(
+            [
+                self.obj,
+                str(self.obj_beg),
+                str(self.obj_end),
+                str(self.pid),
+                self.comp_type,
+                str(self.gap_len),
+                self.gap_type,
+                self.linkage,
+                self.linkage_evidence,
+            ]
+        )
 
     def __iter__(self):
         for key in self.gapdict:
             yield str(key), self.gapdict[key]
 
     def _validate_numerics(self):
         # Convert all numeric types to integers
@@ -495,40 +548,43 @@
             self.obj_end = int(self.obj_end)
             self.pid = int(self.pid)
             self.gap_len = int(self.gap_len)
         except TypeError:
             raise AGPError(self.fname, self.line_number, "encountered an invalid non-integer numeric AGP field")
 
         # Ensure that all numeric values are positive
-        if not all([
-            self.obj_beg > 0,
-            self.obj_end > 0,
-            self.pid > 0,
-            self.gap_len > 0
-        ]):
+        if not all([self.obj_beg > 0, self.obj_end > 0, self.pid > 0, self.gap_len > 0]):
             raise AGPError(self.fname, self.line_number, "encountered an invalid negative numeric AGP field")
 
         # Make sure the coordinates match
         if self.obj_end - (self.obj_beg - 1) != self.gap_len:
-            raise AGPError(self.fname, self.line_number, f"object coordinates ({self.obj_beg}, {self.obj_end}) and gap length ({self.gap_len}) are not the same length")
+            raise AGPError(
+                self.fname,
+                self.line_number,
+                f"object coordinates ({self.obj_beg}, {self.obj_end}) and gap length ({self.gap_len}) are not the same length",
+            )
 
     def _validate_strings(self):
         try:
             self.obj = str(self.obj)
             self.comp_type = str(self.comp_type)
             self.gap_type = str(self.gap_type)
             self.linkage = str(self.linkage)
             self.linkage_evidence = str(self.linkage_evidence)
         except TypeError:
             raise AGPError(self.fname, self.line_number, "encountered an invalid type for an AGP text field")
 
     def _validate_line(self):
-        """ Validation specific to AGP gap lines. """
+        """Validation specific to AGP gap lines."""
         if self.comp_type == "U" and self.gap_len != 100:
-            raise AGPError(self.fname, self.line_number, f"invalid gap length for component type 'U': {self.gap_len} (should be 100)")
+            raise AGPError(
+                self.fname,
+                self.line_number,
+                f"invalid gap length for component type 'U': {self.gap_len} (should be 100)",
+            )
 
         if self.gap_type not in AGPGapLine.allowed_gap_types:
             raise AGPError(self.fname, self.line_number, f"invalid gap type: {self.gap_type}")
 
         if self.linkage not in AGPGapLine.allowed_linkage_types:
             raise AGPError(self.fname, self.line_number, f"invalid linkage field: {self.linkage}")
 
@@ -538,11 +594,17 @@
                 raise AGPError(self.fname, self.line_number, f"invalid linkage evidence: {e}")
 
         if self.linkage == "no":
             if self.gap_type == "scaffold":
                 raise AGPError(self.fname, self.line_number, "invalid 'scaffold' gap without linkage evidence")
 
             if self.linkage_evidence != "na":
-                raise AGPError(self.fname, self.line_number, f"linkage evidence must be 'na' when not asserting linkage. Got {self.linkage_evidence}")
+                raise AGPError(
+                    self.fname,
+                    self.line_number,
+                    f"linkage evidence must be 'na' when not asserting linkage. Got {self.linkage_evidence}",
+                )
         else:
             if "na" in all_evidence:
-                raise AGPError(self.fname, self.line_number, "'na' is invalid linkage evidence when asserting linkage")
+                log.warning(
+                    AGPError(self.fname, self.line_number, "'na' is invalid linkage evidence when asserting linkage")
+                )
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/graph.py` & `galaxy-data-23.0.1/galaxy/datatypes/graph.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,106 +1,119 @@
 """
 Graph content classes.
 """
 import logging
+from typing import (
+    List,
+    TYPE_CHECKING,
+)
 
+from galaxy.datatypes.dataproviders.column import ColumnarDataProvider
+from galaxy.datatypes.dataproviders.dataset import DatasetDataProvider
+from galaxy.datatypes.dataproviders.hierarchy import XMLDataProvider
 from galaxy.util import simplegraph
 from . import (
     data,
     dataproviders,
     tabular,
-    xml
+    xml,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 @dataproviders.decorators.has_dataproviders
 class Xgmml(xml.GenericXml):
     """
     XGMML graph format
     (http://wiki.cytoscape.org/Cytoscape_User_Manual/Network_Formats).
     """
+
     file_ext = "xgmml"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """
         Set the peek and blurb text
         """
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'XGMML data'
+            dataset.blurb = "XGMML data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Returns false and the user must manually set.
         """
         return False
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """
         Merging multiple XML files is non-trivial and must be done in subclasses.
         """
         if len(split_files) > 1:
-            raise NotImplementedError("Merging multiple XML files is non-trivial "
-                                      + "and must be implemented for each XML type")
+            raise NotImplementedError(
+                "Merging multiple XML files is non-trivial " + "and must be implemented for each XML type"
+            )
         # For one file only, use base class method (move/copy)
         data.Text.merge(split_files, output_file)
 
-    @dataproviders.decorators.dataprovider_factory('node-edge', dataproviders.hierarchy.XMLDataProvider.settings)
-    def node_edge_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
+    @dataproviders.decorators.dataprovider_factory("node-edge", XMLDataProvider.settings)
+    def node_edge_dataprovider(self, dataset: "DatasetInstance", **settings) -> "XGMMLGraphDataProvider":
+        dataset_source = DatasetDataProvider(dataset)
         return XGMMLGraphDataProvider(dataset_source, **settings)
 
 
 @dataproviders.decorators.has_dataproviders
 class Sif(tabular.Tabular):
     """
     SIF graph format
     (http://wiki.cytoscape.org/Cytoscape_User_Manual/Network_Formats).
 
     First column: node id
     Second column: relationship type
     Third to Nth column: target ids for link
     """
+
     file_ext = "sif"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """
         Set the peek and blurb text
         """
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'SIF data'
+            dataset.blurb = "SIF data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Returns false and the user must manually set.
         """
         return False
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         data.Text.merge(split_files, output_file)
 
-    @dataproviders.decorators.dataprovider_factory('node-edge', dataproviders.column.ColumnarDataProvider.settings)
-    def node_edge_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
+    @dataproviders.decorators.dataprovider_factory("node-edge", ColumnarDataProvider.settings)
+    def node_edge_dataprovider(self, dataset: "DatasetInstance", **settings) -> "SIFGraphDataProvider":
+        dataset_source = DatasetDataProvider(dataset)
         return SIFGraphDataProvider(dataset_source, **settings)
 
 
 # ----------------------------------------------------------------------------- graph specific data providers
-class XGMMLGraphDataProvider(dataproviders.hierarchy.XMLDataProvider):
+class XGMMLGraphDataProvider(XMLDataProvider):
     """
     Provide two lists: nodes, edges::
 
         'nodes': contains objects of the form:
             { 'id' : <some string id>, 'data': <any extra data> }
         'edges': contains objects of the form:
             { 'source' : <an index into nodes>, 'target': <an index into nodes>, 'data': <any extra data> }
@@ -109,32 +122,32 @@
     def __iter__(self):
         # use simple graph to store nodes and links, later providing them as a dict
         #   essentially this is a form of aggregation
         graph = simplegraph.SimpleGraph()
 
         parent_gen = super().__iter__()
         for graph_elem in parent_gen:
-            if 'children' not in graph_elem:
+            if "children" not in graph_elem:
                 continue
-            for elem in graph_elem['children']:
+            for elem in graph_elem["children"]:
                 # use endswith to work around Elementtree namespaces
-                if elem['tag'].endswith('node'):
-                    node_id = elem['attrib']['id']
+                if elem["tag"].endswith("node"):
+                    node_id = elem["attrib"]["id"]
                     # pass the entire, parsed xml element as the data
                     graph.add_node(node_id, **elem)
 
-                elif elem['tag'].endswith('edge'):
-                    source_id = elem['attrib']['source']
-                    target_id = elem['attrib']['target']
+                elif elem["tag"].endswith("edge"):
+                    source_id = elem["attrib"]["source"]
+                    target_id = elem["attrib"]["target"]
                     graph.add_edge(source_id, target_id, **elem)
 
         yield graph.as_dict()
 
 
-class SIFGraphDataProvider(dataproviders.column.ColumnarDataProvider):
+class SIFGraphDataProvider(ColumnarDataProvider):
     """
     Provide two lists: nodes, edges::
 
         'nodes': contains objects of the form:
             { 'id' : <some string id>, 'data': <any extra data> }
         'edges': contains objects of the form:
             { 'source' : <an index into nodes>, 'target': <an index into nodes>, 'data': <any extra data> }
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/hdf5.py` & `galaxy-data-23.0.1/galaxy/datatypes/hdf5.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,15 +1,25 @@
 """Composite datatype for the HDF5SummarizedExperiment R data object.
 
 This datatype was created for use with the iSEE interactive tool.
 """
-
-from galaxy.datatypes.data import Data
+from typing import (
+    Optional,
+    TYPE_CHECKING,
+)
+
+from galaxy.datatypes.data import (
+    Data,
+    GeneratePrimaryFileDataset,
+)
 from galaxy.datatypes.metadata import MetadataElement
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 
 class HDF5SummarizedExperiment(Data):
     """Composite datatype to represent HDF5SummarizedExperiment objects.
 
     A lightweight shell file `se.rds` is read into memory by R, and provides an
     interface to the much larger `assays.h5` files which contains the
     experiment data.
@@ -24,55 +34,55 @@
         name="base_name",
         desc="SummarisedExperiment object name",
         default="HDF5 SE object",
         readonly=True,
         set_in_upload=True,
     )
 
-    file_ext = 'rdata.se'
-    composite_type = 'auto_primary_file'
+    file_ext = "rdata.se"
+    composite_type = "auto_primary_file"
     allow_datatype_change = False
 
     def __init__(self, **kwd):
         """Construct object from input files."""
         Data.__init__(self, **kwd)
         self.add_composite_file(
-            'se.rds',
+            "se.rds",
             is_binary=True,
             description="Summarized experiment RDS object",
         )
         self.add_composite_file(
-            'assays.h5',
+            "assays.h5",
             is_binary=True,
             description="Summarized experiment data array",
         )
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         """Override parent init metadata."""
         Data.init_meta(self, dataset, copy_from=copy_from)
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """Generate primary file to represent dataset."""
-        return (
-            f'''
+        return f"""
               <html>
                 <head>
                   <title> Files for Composite Dataset ({self.file_ext})</title>
                 </head>
                 <p/>
                   This composite dataset is composed of the following files:
                 </p>
                 <ul>
                   <li><a href="se.rds">se.rds</a>
                   <li><a href="array.h5">array.h5</a>
                 </ul>
               </html>
-              '''
-        )
+              """
 
-    def sniff(self, filename):
-        """Not sure whether this is necessary (or possible) with binaries."""
-        pass
+    def sniff(self, filename: str) -> bool:
+        """
+        Returns false and the user must manually set.
+        """
+        return False
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Return the mime type of the datatype."""
-        return 'text/html'
+        return "text/html"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/images.py` & `galaxy-data-23.0.1/galaxy/datatypes/images.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,36 +1,41 @@
 """
 Image classes
 """
 import base64
 import json
 import logging
-import zipfile
-from io import StringIO
-from urllib.parse import quote_plus
+from typing import (
+    Optional,
+    TYPE_CHECKING,
+)
 
 import mrcfile
 import numpy as np
 import tifffile
 
 from galaxy.datatypes.binary import Binary
+from galaxy.datatypes.data import GeneratePrimaryFileDataset
 from galaxy.datatypes.metadata import (
     FileParameter,
     MetadataElement,
 )
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.datatypes.text import Html as HtmlFromText
 from galaxy.util import nice_size
 from galaxy.util.image_util import check_image_type
 from . import data
 from .xml import GenericXml
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 # TODO: Uploading image files of various types is supported in Galaxy, but on
 # the main public instance, the display_in_upload is not set for these data
 # types in datatypes_conf.xml because we do not allow image files to be uploaded
 # there.  There is currently no API feature that allows uploading files outside
 # of a data library ( where it requires either the upload_paths or upload_directory
@@ -38,77 +43,90 @@
 # of this, we're currently safe, but when the api is enhanced to allow other uploads,
 # we need to ensure that the implementation is such that image files cannot be uploaded
 # to our main public instance.
 
 
 class Image(data.Data):
     """Class describing an image"""
-    edam_data = 'data_2968'
+
+    edam_data = "data_2968"
     edam_format = "format_3547"
-    file_ext = ''
+    file_ext = ""
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self.image_formats = [self.file_ext.upper()]
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = f'Image in {dataset.extension} format'
+            dataset.peek = f"Image in {dataset.extension} format"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """Determine if the file is in this format"""
         return check_image_type(filename, self.image_formats)
 
-    def handle_dataset_as_image(self, hda):
+    def handle_dataset_as_image(self, hda: "DatasetInstance") -> str:
         dataset = hda.dataset
-        name = hda.name or ''
+        name = hda.name or ""
         with open(dataset.file_name, "rb") as f:
             base64_image_data = base64.b64encode(f.read()).decode("utf-8")
         return f"![{name}](data:image/{self.file_ext};base64,{base64_image_data})"
 
 
 class Jpg(Image):
     edam_format = "format_3579"
     file_ext = "jpg"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.image_formats = ['JPEG']
+        self.image_formats = ["JPEG"]
 
 
 class Png(Image):
     edam_format = "format_3603"
     file_ext = "png"
 
 
 class Tiff(Image):
     edam_format = "format_3591"
     file_ext = "tiff"
 
 
 class OMETiff(Tiff):
     file_ext = "ome.tiff"
-    MetadataElement(name="offsets", desc="Offsets File", param=FileParameter, file_ext="json", readonly=True, visible=False, optional=True)
-
-    def set_meta(self, dataset, overwrite=True, **kwd):
-        spec_key = 'offsets'
+    MetadataElement(
+        name="offsets",
+        desc="Offsets File",
+        param=FileParameter,
+        file_ext="json",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
+        spec_key = "offsets"
         offsets_file = dataset.metadata.offsets
         if not offsets_file:
-            offsets_file = dataset.metadata.spec[spec_key].param.new_file(dataset=dataset)
+            offsets_file = dataset.metadata.spec[spec_key].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
         with tifffile.TiffFile(dataset.file_name) as tif:
             offsets = [page.offset for page in tif.pages]
-        with open(offsets_file.file_name, 'w') as f:
+        with open(offsets_file.file_name, "w") as f:
             json.dump(offsets, f)
         dataset.metadata.offsets = offsets_file
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         with tifffile.TiffFile(filename) as tif:
             if tif.is_ome:
                 return True
         return False
 
 
 class Hamamatsu(Image):
@@ -197,60 +215,45 @@
     file_ext = "rast"
 
 
 class Pdf(Image):
     edam_format = "format_3508"
     file_ext = "pdf"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """Determine if the file is in pdf format."""
-        with open(filename, 'rb') as fh:
+        with open(filename, "rb") as fh:
             return fh.read(4) == b"%PDF"
 
 
-def create_applet_tag_peek(class_name, archive, params):
-    text = f"""
-<object classid="java:{class_name}"
-      type="application/x-java-applet"
-      height="30" width="200" align="center" >
-      <param name="archive" value="{archive}"/>"""
-    for name, value in params.items():
-        text += f"""<param name="{name}" value="{value}"/>"""
-    text += f"""
-<object classid="clsid:8AD9C840-044E-11D1-B3E9-00805F499D93"
-        height="30" width="200" >
-        <param name="code" value="{class_name}" />
-        <param name="archive" value="{archive}"/>"""
-    for name, value in params.items():
-        text += f"""<param name="{name}" value="{value}"/>"""
-    text += """<div class="errormessage">You must install and enable Java in your browser in order to access this applet.<div></object>
-</object>
-"""
-    return f"""<div><p align="center">{text}</p></div>"""
-
-
 @build_sniff_from_prefix
 class Tck(Binary):
     """
     Tracks file format (.tck) format
     https://mrtrix.readthedocs.io/en/latest/getting_started/image_data.html#tracks-file-format-tck
 
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('fibers_sparse_top_6_lines.tck')
     >>> Tck().sniff( fname )
     True
     >>> fname = get_test_fname('2.txt')
     >>> Tck().sniff( fname )
     False
     """
-    file_ext = 'tck'
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        format_def = [[b'mrtrix tracks'], [b'datatype: Float32LE', b'datatype: Float32BE', b'datatype: Float64BE', b'datatype: Float64LE'],
-                      [b'count: '], [b'file: .'], [b'END']]
+    file_ext = "tck"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        format_def = [
+            [b"mrtrix tracks"],
+            [b"datatype: Float32LE", b"datatype: Float32BE", b"datatype: Float64BE", b"datatype: Float64LE"],
+            [b"count: "],
+            [b"file: ."],
+            [b"END"],
+        ]
         matches = 0
 
         for elem in format_def:
             for identifier in elem:
                 if identifier in file_prefix.contents_header_bytes:
                     matches += 1
         if matches == 5:
@@ -268,48 +271,49 @@
     >>> fname = get_test_fname('IIT2mean_top_2000bytes.trk')
     >>> Trk().sniff( fname )
     True
     >>> fname = get_test_fname('2.txt')
     >>> Trk().sniff( fname )
     False
     """
-    file_ext = 'trk'
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    file_ext = "trk"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # quick check
         header_raw = None
         header_raw = file_prefix.contents_header_bytes[:1000]
 
-        if header_raw[:5] != b'TRACK':
+        if header_raw[:5] != b"TRACK":
             return False
         # detailed check
         header_def = [
-            ('magic', 'S6'),
-            ('dim', 'h', 3),
-            ('voxel_size', 'f4', 3),
-            ('origin', 'f4', 3),
-            ('n_scalars', 'h'),
-            ('scalar_name', 'S20', 10),
-            ('n_properties', 'h'),
-            ('property_name', 'S20', 10),
-            ('vox_to_ras', 'f4', (4, 4)),
-            ('reserved', 'S444'),
-            ('voxel_order', 'S4'),
-            ('pad2', 'S4'),
-            ('image_orientation_patient', 'f4', 6),
-            ('pad1', 'S2'),
-            ('invert_x', 'S1'),
-            ('invert_y', 'S1'),
-            ('invert_z', 'S1'),
-            ('swap_xy', 'S1'),
-            ('swap_yz', 'S1'),
-            ('swap_zx', 'S1'),
-            ('n_count', 'i4'),
-            ('version', 'i4'),
-            ('header_size', 'i4'),
+            ("magic", "S6"),
+            ("dim", "h", 3),
+            ("voxel_size", "f4", 3),
+            ("origin", "f4", 3),
+            ("n_scalars", "h"),
+            ("scalar_name", "S20", 10),
+            ("n_properties", "h"),
+            ("property_name", "S20", 10),
+            ("vox_to_ras", "f4", (4, 4)),
+            ("reserved", "S444"),
+            ("voxel_order", "S4"),
+            ("pad2", "S4"),
+            ("image_orientation_patient", "f4", 6),
+            ("pad1", "S2"),
+            ("invert_x", "S1"),
+            ("invert_y", "S1"),
+            ("invert_z", "S1"),
+            ("swap_xy", "S1"),
+            ("swap_yz", "S1"),
+            ("swap_zx", "S1"),
+            ("n_count", "i4"),
+            ("version", "i4"),
+            ("header_size", "i4"),
         ]
         np_dtype = np.dtype(header_def)
         header: np.ndarray = np.ndarray(shape=(), dtype=np_dtype, buffer=header_raw)
         if (
             header["header_size"] == 1000
             and b"TRACK" in header["magic"]
             and header["version"] == 2
@@ -328,131 +332,76 @@
     >>> fname = get_test_fname('1.mrc')
     >>> Mrc2014().sniff(fname)
     True
     >>> fname = get_test_fname('2.txt')
     >>> Mrc2014().sniff(fname)
     False
     """
-    file_ext = 'mrc'
 
-    def sniff(self, filename):
-        # Handle the wierdness of mrcfile:
-        # https://github.com/ccpem/mrcfile/blob/master/mrcfile/validator.py#L88
+    file_ext = "mrc"
+
+    def sniff(self, filename: str) -> bool:
         try:
             # An exception is thrown
             # if the file is not an
             # mrc2014 file.
-            if mrcfile.validate(filename, print_file=StringIO()):
-                return True
+            mrcfile.load_functions.open(filename, header_only=True)
+            return True
         except Exception:
             return False
-        return False
 
 
 class Gmaj(data.Data):
-    """Class describing a GMAJ Applet"""
+    """Deprecated class. Exists for limited backwards compatibility."""
+
     edam_format = "format_3547"
     file_ext = "gmaj.zip"
-    copy_safe_peek = False
-
-    def set_peek(self, dataset):
-        if not dataset.dataset.purged:
-            if hasattr(dataset, 'history_id'):
-                params = {
-                    "bundle": f"display?id={dataset.id}&tofile=yes&toext=.zip",
-                    "buttonlabel": "Launch GMAJ",
-                    "nobutton": "false",
-                    "urlpause": "100",
-                    "debug": "false",
-                    "posturl": "history_add_to?%s" % "&".join(f"{x[0]}={quote_plus(str(x[1]))}" for x in [('copy_access_from', dataset.id), ('history_id', dataset.history_id), ('ext', 'maf'), ('name', f'GMAJ Output on data {dataset.hid}'), ('info', 'Added by GMAJ'), ('dbkey', dataset.dbkey)])
-                }
-                class_name = "edu.psu.bx.gmaj.MajApplet.class"
-                archive = "/static/gmaj/gmaj.jar"
-                dataset.peek = create_applet_tag_peek(class_name, archive, params)
-                dataset.blurb = 'GMAJ Multiple Alignment Viewer'
-            else:
-                dataset.peek = "After you add this item to your history, you will be able to launch the GMAJ applet."
-                dataset.blurb = 'GMAJ Multiple Alignment Viewer'
-        else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
 
-    def display_peek(self, dataset):
-        try:
-            return dataset.peek
-        except Exception:
-            return "peek unavailable"
-
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/zip'
-
-    def sniff(self, filename):
-        """
-        NOTE: the sniff.convert_newlines() call in the upload utility will keep Gmaj data types from being
-        correctly sniffed, but the files can be uploaded (they'll be sniffed as 'txt').  This sniff function
-        is here to provide an example of a sniffer for a zip file.
-        """
-        if not zipfile.is_zipfile(filename):
-            return False
-        contains_gmaj_file = False
-        with zipfile.ZipFile(filename, "r") as zip_file:
-            for name in zip_file.namelist():
-                if name.split(".")[1].strip().lower() == 'gmaj':
-                    contains_gmaj_file = True
-                    break
-        if not contains_gmaj_file:
-            return False
-        return True
+        return "application/zip"
 
 
 class Analyze75(Binary):
     """
-        Mayo Analyze 7.5 files
-        http://www.imzml.org
+    Mayo Analyze 7.5 files
+    http://www.imzml.org
     """
-    file_ext = 'analyze75'
-    composite_type = 'auto_primary_file'
+
+    file_ext = "analyze75"
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
         # The header file provides information about dimensions, identification,
         # and processing history.
-        self.add_composite_file(
-            'hdr',
-            description='The Analyze75 header file.',
-            is_binary=True)
+        self.add_composite_file("hdr", description="The Analyze75 header file.", is_binary=True)
 
         # The image file contains the actual data, whose data type and ordering
         # are described by the header file.
-        self.add_composite_file(
-            'img',
-            description='The Analyze75 image file.',
-            is_binary=True)
-
-        self.add_composite_file(
-            't2m',
-            description='The Analyze75 t2m file.',
-            optional=True,
-            is_binary=True)
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>Analyze75 Composite Dataset.</title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+        self.add_composite_file("img", description="The Analyze75 image file.", is_binary=True)
+
+        self.add_composite_file("t2m", description="The Analyze75 t2m file.", optional=True, is_binary=True)
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Analyze75 Composite Dataset.</title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
 
 
 @build_sniff_from_prefix
 class Nifti1(Binary):
     """
     Nifti1 format
@@ -462,19 +411,20 @@
     >>> fname = get_test_fname('T1_top_350bytes.nii1')
     >>> Nifti1().sniff( fname )
     True
     >>> fname = get_test_fname('2.txt')
     >>> Nifti1().sniff( fname )
     False
     """
-    file_ext = 'nii1'
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    file_ext = "nii1"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         magic = file_prefix.contents_header_bytes[344:348]
-        if magic == b'n+1\0':
+        if magic == b"n+1\0":
             return True
         return False
 
 
 @build_sniff_from_prefix
 class Nifti2(Binary):
     """
@@ -485,29 +435,31 @@
     >>> fname = get_test_fname('avg152T1_LR_nifti2_top_100bytes.nii2')
     >>> Nifti2().sniff( fname )
     True
     >>> fname = get_test_fname('T1_top_350bytes.nii1')
     >>> Nifti2().sniff( fname )
     False
     """
-    file_ext = 'nii2'
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    file_ext = "nii2"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         magic = file_prefix.contents_header_bytes[4:8]
-        if magic in [b'n+2\0', b'ni2\0']:
+        if magic in [b"n+2\0", b"ni2\0"]:
             return True
         return False
 
 
 @build_sniff_from_prefix
 class Gifti(GenericXml):
     """Class describing a Gifti format"""
+
     file_ext = "gii"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """Determines whether the file is a Gifti file
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('Human.colin.R.activations.label.gii')
         >>> Gifti().sniff(fname)
         True
         >>> fname = get_test_fname('interval.interval')
@@ -524,36 +476,37 @@
         line = handle.readline()
         if not line.strip().startswith('<?xml version="1.0"'):
             return False
         line = handle.readline()
         if line.strip() == '<!DOCTYPE GIFTI SYSTEM "http://www.nitrc.org/frs/download.php/1594/gifti.dtd">':
             return True
         line = handle.readline()
-        if line.strip().startswith('<GIFTI'):
+        if line.strip().startswith("<GIFTI"):
             return True
         return False
 
 
 @build_sniff_from_prefix
 class Star(data.Text):
     """Base format class for Relion STAR (Self-defining
     Text Archiving and Retrieval) image files.
     https://relion.readthedocs.io/en/latest/Reference/Conventions.html"""
+
     file_ext = "star"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'Relion STAR data'
+            dataset.blurb = "Relion STAR data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """Each file must have one or more data blocks.
         The start of a data block is defined by the keyword
         ``data_`` followed by an optional string for
         identification (e.g., ``data_images``).  All text
         before the first ``data_`` keyword are comments
 
         >>> from galaxy.datatypes.sniff import get_test_fname
@@ -588,36 +541,8 @@
 
 class Html(HtmlFromText):
     """Deprecated class. This class should not be used anymore, but the galaxy.datatypes.text:Html one.
     This is for backwards compatibilities only."""
 
 
 class Laj(data.Text):
-    """Class describing a LAJ Applet"""
-    file_ext = "laj"
-    copy_safe_peek = False
-
-    def set_peek(self, dataset):
-        if not dataset.dataset.purged:
-            if hasattr(dataset, 'history_id'):
-                params = {
-                    "alignfile1": f"display?id={dataset.id}",
-                    "buttonlabel": "Launch LAJ",
-                    "title": "LAJ in Galaxy",
-                    "posturl": quote_plus(f"history_add_to?{'&'.join(f'{key}={value}' for key, value in {'history_id': dataset.history_id, 'ext': 'lav', 'name': 'LAJ Output', 'info': 'Added by LAJ', 'dbkey': dataset.dbkey, 'copy_access_from': dataset.id}.items())}"),
-                    "noseq": "true"
-                }
-                class_name = "edu.psu.cse.bio.laj.LajApplet.class"
-                archive = "/static/laj/laj.jar"
-                dataset.peek = create_applet_tag_peek(class_name, archive, params)
-            else:
-                dataset.peek = "After you add this item to your history, you will be able to launch the LAJ applet."
-                dataset.blurb = 'LAJ Multiple Alignment Viewer'
-        else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
-
-    def display_peek(self, dataset):
-        try:
-            return dataset.peek
-        except Exception:
-            return "peek unavailable"
+    """Deprecated class. Exists for limited backwards compatibility."""
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/interval.py` & `galaxy-data-23.0.1/galaxy/datatypes/interval.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,46 +1,80 @@
 """
 Interval datatypes
 """
 import logging
-import math
 import sys
 import tempfile
+from typing import (
+    cast,
+    List,
+    Optional,
+    Tuple,
+    TYPE_CHECKING,
+    Union,
+)
 from urllib.parse import quote_plus
 
-from bx.intervals.io import GenomicIntervalReader, ParseError
+import pysam
+from bx.intervals.io import (
+    GenomicIntervalReader,
+    ParseError,
+)
 
 from galaxy import util
 from galaxy.datatypes import metadata
 from galaxy.datatypes.data import DatatypeValidation
+from galaxy.datatypes.dataproviders.dataset import (
+    DatasetDataProvider,
+    GenomicRegionDataProvider,
+    IntervalDataProvider,
+    WiggleDataProvider,
+)
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     get_headers,
-    iter_headers
+    iter_headers,
 )
 from galaxy.datatypes.tabular import Tabular
-from galaxy.datatypes.util.gff_util import parse_gff3_attributes, parse_gff_attributes
+from galaxy.datatypes.util.gff_util import (
+    parse_gff3_attributes,
+    parse_gff_attributes,
+)
 from galaxy.util import compression_utils
+from galaxy.util.compression_utils import FileObjType
 from . import (
     data,
-    dataproviders
+    dataproviders,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 # Contains the meta columns and the words that map to it; list aliases on the
 # right side of the : in decreasing order of priority
 alias_spec = {
-    'chromCol': ['chrom', 'CHROMOSOME', 'CHROM', 'Chromosome Name'],
-    'startCol': ['start', 'START', 'chromStart', 'txStart', 'Start Position (bp)'],
-    'endCol': ['end', 'END', 'STOP', 'chromEnd', 'txEnd', 'End Position (bp)'],
-    'strandCol': ['strand', 'STRAND', 'Strand'],
-    'nameCol': ['name', 'NAME', 'Name', 'name2', 'NAME2', 'Name2', 'Ensembl Gene ID', 'Ensembl Transcript ID', 'Ensembl Peptide ID']
+    "chromCol": ["chrom", "CHROMOSOME", "CHROM", "Chromosome Name"],
+    "startCol": ["start", "START", "chromStart", "txStart", "Start Position (bp)"],
+    "endCol": ["end", "END", "STOP", "chromEnd", "txEnd", "End Position (bp)"],
+    "strandCol": ["strand", "STRAND", "Strand"],
+    "nameCol": [
+        "name",
+        "NAME",
+        "Name",
+        "name2",
+        "NAME2",
+        "Name2",
+        "Ensembl Gene ID",
+        "Ensembl Transcript ID",
+        "Ensembl Peptide ID",
+    ],
 }
 
 # a little faster lookup
 alias_helper = {}
 for key, value in alias_spec.items():
     for elem in value:
         alias_helper[elem] = key
@@ -52,109 +86,134 @@
 VIEWPORT_MAX_READS_PER_LINE = 10
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class Interval(Tabular):
     """Tab delimited data containing interval information"""
+
     edam_data = "data_3002"
     edam_format = "format_3475"
     file_ext = "interval"
     line_class = "region"
     track_type = "FeatureTrack"
     data_sources = {"data": "tabix", "index": "bigwig"}
 
     MetadataElement(name="chromCol", default=1, desc="Chrom column", param=metadata.ColumnParameter)
     MetadataElement(name="startCol", default=2, desc="Start column", param=metadata.ColumnParameter)
     MetadataElement(name="endCol", default=3, desc="End column", param=metadata.ColumnParameter)
-    MetadataElement(name="strandCol", desc="Strand column (click box & select)", param=metadata.ColumnParameter, optional=True, no_value=0)
-    MetadataElement(name="nameCol", desc="Name/Identifier column (click box & select)", param=metadata.ColumnParameter, optional=True, no_value=0)
+    MetadataElement(
+        name="strandCol",
+        default=0,
+        desc="Strand column (click box & select)",
+        param=metadata.ColumnParameter,
+        optional=True,
+        no_value=0,
+    )
+    MetadataElement(
+        name="nameCol",
+        desc="Name/Identifier column (click box & select)",
+        param=metadata.ColumnParameter,
+        optional=True,
+        no_value=0,
+    )
     MetadataElement(name="columns", default=3, desc="Number of columns", readonly=True, visible=False)
 
     def __init__(self, **kwd):
         """Initialize interval datatype, by adding UCSC display apps"""
         Tabular.__init__(self, **kwd)
-        self.add_display_app('ucsc', 'display at UCSC', 'as_ucsc_display_file', 'ucsc_links')
+        self.add_display_app("ucsc", "display at UCSC", "as_ucsc_display_file", "ucsc_links")
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         Tabular.init_meta(self, dataset, copy_from=copy_from)
 
-    def set_meta(self, dataset, overwrite=True, first_line_is_header=False, **kwd):
+    def set_meta(
+        self, dataset: "DatasetInstance", *, overwrite: bool = True, first_line_is_header: bool = False, **kwd
+    ) -> None:
         """Tries to guess from the line the location number of the column for the chromosome, region start-end and strand"""
         Tabular.set_meta(self, dataset, overwrite=overwrite, skip=0)
         if dataset.has_data():
             empty_line_count = 0
             num_check_lines = 100  # only check up to this many non empty lines
             with compression_utils.get_fileobj(dataset.file_name) as in_fh:
                 for i, line in enumerate(in_fh):
-                    line = line.rstrip('\r\n')
+                    line = line.rstrip("\r\n")
                     if line:
-                        if (first_line_is_header or line[0] == '#'):
+                        if first_line_is_header or line[0] == "#":
                             self.init_meta(dataset)
-                            line = line.strip('#')
-                            elems = line.split('\t')
+                            line = line.strip("#")
+                            elems = line.split("\t")
                             for meta_name, header_list in alias_spec.items():
                                 for header_val in header_list:
                                     if header_val in elems:
                                         # found highest priority header to meta_name
                                         setattr(dataset.metadata, meta_name, elems.index(header_val) + 1)
                                         break  # next meta_name
                             break  # Our metadata is set, so break out of the outer loop
                         else:
                             # Header lines in Interval files are optional. For example, BED is Interval but has no header.
                             # We'll make a best guess at the location of the metadata columns.
-                            elems = line.split('\t')
+                            elems = line.split("\t")
                             if len(elems) > 2:
-                                if overwrite or not dataset.metadata.element_is_set('chromCol'):
+                                if overwrite or not dataset.metadata.element_is_set("chromCol"):
                                     dataset.metadata.chromCol = 1
                                 try:
                                     int(elems[1])
-                                    if overwrite or not dataset.metadata.element_is_set('startCol'):
+                                    if overwrite or not dataset.metadata.element_is_set("startCol"):
                                         dataset.metadata.startCol = 2
                                 except Exception:
                                     pass  # Metadata default will be used
                                 try:
                                     int(elems[2])
-                                    if overwrite or not dataset.metadata.element_is_set('endCol'):
+                                    if overwrite or not dataset.metadata.element_is_set("endCol"):
                                         dataset.metadata.endCol = 3
                                 except Exception:
                                     pass  # Metadata default will be used
                                 # we no longer want to guess that this column is the 'name', name must now be set manually for interval files
                                 # we will still guess at the strand, as we can make a more educated guess
                                 # if len( elems ) > 3:
                                 #    try:
                                 #        int( elems[3] )
                                 #    except Exception:
                                 #        if overwrite or not dataset.metadata.element_is_set( 'nameCol' ):
                                 #            dataset.metadata.nameCol = 4
                                 if len(elems) < 6 or elems[5] not in data.valid_strand:
-                                    if overwrite or not dataset.metadata.element_is_set('strandCol'):
+                                    if overwrite or not dataset.metadata.element_is_set("strandCol"):
                                         dataset.metadata.strandCol = 0
                                 else:
-                                    if overwrite or not dataset.metadata.element_is_set('strandCol'):
+                                    if overwrite or not dataset.metadata.element_is_set("strandCol"):
                                         dataset.metadata.strandCol = 6
                                 break
                             if (i - empty_line_count) > num_check_lines:
                                 break  # Our metadata is set or we examined 100 non-empty lines, so break out of the outer loop
                     else:
                         empty_line_count += 1
 
-    def displayable(self, dataset):
+    def displayable(self, dataset: "DatasetInstance"):
         try:
-            return dataset.has_data() \
-                and dataset.state == dataset.states.OK \
-                and dataset.metadata.columns > 0 \
-                and dataset.metadata.data_lines != 0 \
-                and dataset.metadata.chromCol \
-                and dataset.metadata.startCol \
+            return (
+                not dataset.dataset.purged
+                and dataset.has_data()
+                and dataset.state == dataset.states.OK
+                and dataset.metadata.columns > 0
+                and dataset.metadata.data_lines != 0
+                and dataset.metadata.chromCol
+                and dataset.metadata.startCol
                 and dataset.metadata.endCol
+            )
         except Exception:
             return False
 
-    def get_estimated_display_viewport(self, dataset, chrom_col=None, start_col=None, end_col=None):
+    def get_estimated_display_viewport(
+        self,
+        dataset: "DatasetInstance",
+        chrom_col: Optional[int] = None,
+        start_col: Optional[int] = None,
+        end_col: Optional[int] = None,
+    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
         """Return a chrom, start, stop tuple for viewing a file."""
         viewport_feature_count = 100  # viewport should check at least 100 features; excludes comment lines
         max_line_count = max(viewport_feature_count, 500)  # maximum number of lines to check; includes comment lines
         if not self.displayable(dataset):
             return (None, None, None)
         try:
             # If column indexes were not passwed, determine from metadata
@@ -168,33 +227,35 @@
             chrom = None
             start = sys.maxsize
             end = 0
             max_col = max(chrom_col, start_col, end_col)
             with compression_utils.get_fileobj(dataset.file_name) as fh:
                 for line in util.iter_start_of_line(fh, VIEWPORT_READLINE_BUFFER_SIZE):
                     # Skip comment lines
-                    if not line.startswith('#'):
+                    if not line.startswith("#"):
                         try:
-                            fields = line.rstrip().split('\t')
+                            fields = line.rstrip().split("\t")
                             if len(fields) > max_col:
                                 if chrom is None or chrom == fields[chrom_col]:
                                     start = min(start, int(fields[start_col]))
                                     end = max(end, int(fields[end_col]))
                                     # Set chrom last, in case start and end are not integers
                                     chrom = fields[chrom_col]
                                 viewport_feature_count -= 1
                         except Exception:
                             # Most likely a non-integer field has been encountered
                             # for start / stop. Just ignore and make sure we finish
                             # reading the line and decrementing the counters.
                             pass
                     # Make sure we are at the next new line
                     readline_count = VIEWPORT_MAX_READS_PER_LINE
-                    while line.rstrip('\n\r') == line:
-                        assert readline_count > 0, Exception(f'Viewport readline count exceeded for dataset {dataset.id}.')
+                    while line.rstrip("\n\r") == line:
+                        assert readline_count > 0, Exception(
+                            f"Viewport readline count exceeded for dataset {dataset.id}."
+                        )
                         line = fh.readline(VIEWPORT_READLINE_BUFFER_SIZE)
                         if not line:
                             break  # EOF
                         readline_count -= 1
                     max_line_count -= 1
                     if not viewport_feature_count or not max_line_count:
                         # exceeded viewport or total line count to check
@@ -202,101 +263,126 @@
             if chrom is not None:
                 return (chrom, str(start), str(end))  # Necessary to return strings?
         except Exception:
             # Unexpected error, possibly missing metadata
             log.exception("Exception caught attempting to generate viewport for dataset '%d'", dataset.id)
         return (None, None, None)
 
-    def as_ucsc_display_file(self, dataset, **kwd):
+    def as_ucsc_display_file(self, dataset: "DatasetInstance", **kwd) -> Union[FileObjType, str]:
         """Returns file contents with only the bed data"""
-        with tempfile.NamedTemporaryFile(delete=False, mode='w') as fh:
-            c, s, e, t, n = dataset.metadata.chromCol, dataset.metadata.startCol, dataset.metadata.endCol, dataset.metadata.strandCol or 0, dataset.metadata.nameCol or 0
+        with tempfile.NamedTemporaryFile(delete=False, mode="w") as fh:
+            c, s, e, t, n = (
+                dataset.metadata.chromCol,
+                dataset.metadata.startCol,
+                dataset.metadata.endCol,
+                dataset.metadata.strandCol or 0,
+                dataset.metadata.nameCol or 0,
+            )
             c, s, e, t, n = int(c) - 1, int(s) - 1, int(e) - 1, int(t) - 1, int(n) - 1
             if t >= 0:  # strand column (should) exists
                 for i, elems in enumerate(compression_utils.file_iter(dataset.file_name)):
                     strand = "+"
                     name = "region_%i" % i
                     if n >= 0 and n < len(elems):
-                        name = elems[n]
+                        name = cast(str, elems[n])
                     if t < len(elems):
-                        strand = elems[t]
-                    tmp = [elems[c], elems[s], elems[e], name, '0', strand]
-                    fh.write('%s\n' % '\t'.join(tmp))
+                        strand = cast(str, elems[t])
+                    tmp = [elems[c], elems[s], elems[e], name, "0", strand]
+                    fh.write("%s\n" % "\t".join(tmp))
             elif n >= 0:  # name column (should) exists
                 for i, elems in enumerate(compression_utils.file_iter(dataset.file_name)):
                     name = "region_%i" % i
                     if n >= 0 and n < len(elems):
-                        name = elems[n]
+                        name = cast(str, elems[n])
                     tmp = [elems[c], elems[s], elems[e], name]
-                    fh.write('%s\n' % '\t'.join(tmp))
+                    fh.write("%s\n" % "\t".join(tmp))
             else:
                 for elems in compression_utils.file_iter(dataset.file_name):
                     tmp = [elems[c], elems[s], elems[e]]
-                    fh.write('%s\n' % '\t'.join(tmp))
-            return compression_utils.get_fileobj(fh.name, mode='rb')
+                    fh.write("%s\n" % "\t".join(tmp))
+            return compression_utils.get_fileobj(fh.name, mode="rb")
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
-        return self.make_html_table(dataset, column_parameter_alias={'chromCol': 'Chrom', 'startCol': 'Start', 'endCol': 'End', 'strandCol': 'Strand', 'nameCol': 'Name'})
+        return self.make_html_table(
+            dataset,
+            column_parameter_alias={
+                "chromCol": "Chrom",
+                "startCol": "Start",
+                "endCol": "End",
+                "strandCol": "Strand",
+                "nameCol": "Name",
+            },
+        )
 
-    def ucsc_links(self, dataset, type, app, base_url):
+    def ucsc_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         """
         Generate links to UCSC genome browser sites based on the dbkey
         and content of dataset.
         """
         # Filter UCSC sites to only those that are supported by this build and
         # enabled.
-        valid_sites = [(name, url)
-                       for name, url in app.datatypes_registry.get_legacy_sites_by_build('ucsc', dataset.dbkey)
-                       if name in app.datatypes_registry.get_display_sites('ucsc')]
+        valid_sites = [
+            (name, url)
+            for name, url in app.datatypes_registry.get_legacy_sites_by_build("ucsc", dataset.dbkey)
+            if name in app.datatypes_registry.get_display_sites("ucsc")
+        ]
         if not valid_sites:
             return []
         # If there are any valid sites, we need to generate the estimated
         # viewport
         chrom, start, stop = self.get_estimated_display_viewport(dataset)
         if chrom is None:
             return []
         # Accumulate links for valid sites
         ret_val = []
         for site_name, site_url in valid_sites:
-            internal_url = app.url_for(controller='dataset', dataset_id=dataset.id,
-                                       action='display_at', filename='ucsc_' + site_name)
-            display_url = quote_plus("%s%s/display_as?id=%i&display_app=%s&authz_method=display_at" %
-                                     (base_url, app.url_for(controller='root'), dataset.id, type))
-            redirect_url = quote_plus("%sdb=%s&position=%s:%s-%s&hgt.customText=%%s" %
-                                      (site_url, dataset.dbkey, chrom, start, stop))
-            link = f'{internal_url}?redirect_url={redirect_url}&display_url={display_url}'
+            internal_url = app.legacy_url_for(
+                mapper=app.legacy_mapper,
+                environ=request.environ,
+                controller="dataset",
+                dataset_id=dataset.id,
+                action="display_at",
+                filename="ucsc_" + site_name,
+            )
+            display_url = quote_plus(
+                "%s%s/display_as?id=%i&display_app=%s&authz_method=display_at"
+                % (
+                    base_url,
+                    app.legacy_url_for(mapper=app.legacy_mapper, environ=request.environ, controller="root"),
+                    dataset.id,
+                    type,
+                )
+            )
+            redirect_url = quote_plus(f"{site_url}db={dataset.dbkey}&position={chrom}:{start}-{stop}&hgt.customText=%s")
+            link = f"{internal_url}?redirect_url={redirect_url}&display_url={display_url}"
             ret_val.append((site_name, link))
         return ret_val
 
-    def validate(self, dataset, **kwd):
+    def validate(self, dataset: "DatasetInstance", **kwd) -> DatatypeValidation:
         """Validate an interval file using the bx GenomicIntervalReader"""
-        c, s, e, t = dataset.metadata.chromCol, dataset.metadata.startCol, dataset.metadata.endCol, dataset.metadata.strandCol
+        c, s, e, t = (
+            dataset.metadata.chromCol,
+            dataset.metadata.startCol,
+            dataset.metadata.endCol,
+            dataset.metadata.strandCol,
+        )
         c, s, e, t = int(c) - 1, int(s) - 1, int(e) - 1, int(t) - 1
         with compression_utils.get_fileobj(dataset.file_name, "r") as infile:
-            reader = GenomicIntervalReader(
-                infile,
-                chrom_col=c,
-                start_col=s,
-                end_col=e,
-                strand_col=t)
+            reader = GenomicIntervalReader(infile, chrom_col=c, start_col=s, end_col=e, strand_col=t)
 
             while True:
                 try:
                     next(reader)
                 except ParseError as e:
                     return DatatypeValidation.invalid(util.unicodify(e))
                 except StopIteration:
-                    return DatatypeValidation.valid()
-
-    def repair_methods(self, dataset):
-        """Return options for removing errors along with a description"""
-        return [("lines", "Remove erroneous lines")]
+                    return DatatypeValidation.validated()
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checks for 'intervalness'
 
         This format is mostly used by galaxy itself.  Valid interval files should include
         a valid header comment, but this seems to be loosely regulated.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
@@ -305,15 +391,15 @@
         False
         >>> fname = get_test_fname( 'interval.interval' )
         >>> Interval().sniff( fname )
         True
         """
         found_valid_lines = False
         try:
-            headers = iter_headers(file_prefix, '\t', comment_designator='#')
+            headers = iter_headers(file_prefix, "\t", comment_designator="#")
             # If we got here, we already know the file is_column_based and is not bed,
             # so we'll just look for some valid data.
             for hdr in headers:
                 if hdr:
                     if len(hdr) < 3:
                         return False
                     # Assume chrom start and end are in column positions 1 and 2
@@ -321,140 +407,174 @@
                     int(hdr[1])
                     int(hdr[2])
                     found_valid_lines = True
         except Exception:
             return False
         return found_valid_lines
 
-    def get_track_resolution(self, dataset, start, end):
-        return None
-
     # ------------- Dataproviders
-    @dataproviders.decorators.dataprovider_factory('genomic-region',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dataprovider(self, dataset, **settings):
-        return dataproviders.dataset.GenomicRegionDataProvider(dataset, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('genomic-region-dict',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dict_dataprovider(self, dataset, **settings):
-        settings['named_columns'] = True
+    @dataproviders.decorators.dataprovider_factory("genomic-region", GenomicRegionDataProvider.settings)
+    def genomic_region_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        return GenomicRegionDataProvider(dataset, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("genomic-region-dict", GenomicRegionDataProvider.settings)
+    def genomic_region_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        settings["named_columns"] = True
         return self.genomic_region_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('interval',
-                                                   dataproviders.dataset.IntervalDataProvider.settings)
-    def interval_dataprovider(self, dataset, **settings):
-        return dataproviders.dataset.IntervalDataProvider(dataset, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('interval-dict',
-                                                   dataproviders.dataset.IntervalDataProvider.settings)
-    def interval_dict_dataprovider(self, dataset, **settings):
-        settings['named_columns'] = True
+    @dataproviders.decorators.dataprovider_factory("interval", IntervalDataProvider.settings)
+    def interval_dataprovider(self, dataset: "DatasetInstance", **settings) -> IntervalDataProvider:
+        return IntervalDataProvider(dataset, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("interval-dict", IntervalDataProvider.settings)
+    def interval_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> IntervalDataProvider:
+        settings["named_columns"] = True
         return self.interval_dataprovider(dataset, **settings)
 
 
 class BedGraph(Interval):
     """Tab delimited chrom/start/end/datavalue dataset"""
+
     edam_format = "format_3583"
     file_ext = "bedgraph"
     track_type = "LineTrack"
     data_sources = {"data": "bigwig", "index": "bigwig"}
 
-    def as_ucsc_display_file(self, dataset, **kwd):
+    def as_ucsc_display_file(self, dataset: "DatasetInstance", **kwd) -> Union[FileObjType, str]:
         """
-            Returns file contents as is with no modifications.
-            TODO: this is a functional stub and will need to be enhanced moving forward to provide additional support for bedgraph.
+        Returns file contents as is with no modifications.
+        TODO: this is a functional stub and will need to be enhanced moving forward to provide additional support for bedgraph.
         """
-        return open(dataset.file_name, 'rb')
+        return open(dataset.file_name, "rb")
 
-    def get_estimated_display_viewport(self, dataset, chrom_col=0, start_col=1, end_col=2):
-        """
-            Set viewport based on dataset's first 100 lines.
-        """
-        return Interval.get_estimated_display_viewport(self, dataset, chrom_col=chrom_col, start_col=start_col, end_col=end_col)
+    def get_estimated_display_viewport(
+        self,
+        dataset: "DatasetInstance",
+        chrom_col: Optional[int] = 0,
+        start_col: Optional[int] = 1,
+        end_col: Optional[int] = 2,
+    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
+        """
+        Set viewport based on dataset's first 100 lines.
+        """
+        return Interval.get_estimated_display_viewport(
+            self, dataset, chrom_col=chrom_col, start_col=start_col, end_col=end_col
+        )
 
 
 class Bed(Interval):
     """Tab delimited data in BED format"""
+
     edam_format = "format_3003"
     file_ext = "bed"
     data_sources = {"data": "tabix", "index": "bigwig", "feature_search": "fli"}
     track_type = Interval.track_type
     check_required_metadata = True
 
-    column_names = ['Chrom', 'Start', 'End', 'Name', 'Score', 'Strand', 'ThickStart', 'ThickEnd', 'ItemRGB', 'BlockCount', 'BlockSizes', 'BlockStarts']
+    column_names = [
+        "Chrom",
+        "Start",
+        "End",
+        "Name",
+        "Score",
+        "Strand",
+        "ThickStart",
+        "ThickEnd",
+        "ItemRGB",
+        "BlockCount",
+        "BlockSizes",
+        "BlockStarts",
+    ]
 
     MetadataElement(name="chromCol", default=1, desc="Chrom column", param=metadata.ColumnParameter)
     MetadataElement(name="startCol", default=2, desc="Start column", param=metadata.ColumnParameter)
     MetadataElement(name="endCol", default=3, desc="End column", param=metadata.ColumnParameter)
-    MetadataElement(name="strandCol", desc="Strand column (click box & select)", param=metadata.ColumnParameter, optional=True, no_value=0)
+    MetadataElement(
+        name="strandCol",
+        default=0,
+        desc="Strand column (click box & select)",
+        param=metadata.ColumnParameter,
+        optional=True,
+        no_value=0,
+    )
     MetadataElement(name="columns", default=3, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="viz_filter_cols", desc="Score column for visualization", default=[4], param=metadata.ColumnParameter, optional=True, multiple=True)
+    MetadataElement(
+        name="viz_filter_cols",
+        desc="Score column for visualization",
+        default=[4],
+        param=metadata.ColumnParameter,
+        optional=True,
+        multiple=True,
+    )
     # do we need to repeat these? they are the same as should be inherited from interval type
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """Sets the metadata information for datasets previously determined to be in bed format."""
         if dataset.has_data():
             i = 0
             for i, line in enumerate(open(dataset.file_name)):  # noqa: B007
-                line = line.rstrip('\r\n')
-                if line and not line.startswith('#'):
-                    elems = line.split('\t')
+                line = line.rstrip("\r\n")
+                if line and not line.startswith("#"):
+                    elems = line.split("\t")
                     if len(elems) > 2:
                         if len(elems) > 3:
-                            if overwrite or not dataset.metadata.element_is_set('nameCol'):
+                            if overwrite or not dataset.metadata.element_is_set("nameCol"):
                                 dataset.metadata.nameCol = 4
                         if len(elems) < 6:
-                            if overwrite or not dataset.metadata.element_is_set('strandCol'):
+                            if overwrite or not dataset.metadata.element_is_set("strandCol"):
                                 dataset.metadata.strandCol = 0
                         else:
-                            if overwrite or not dataset.metadata.element_is_set('strandCol'):
+                            if overwrite or not dataset.metadata.element_is_set("strandCol"):
                                 dataset.metadata.strandCol = 6
                         break
             Tabular.set_meta(self, dataset, overwrite=overwrite, skip=i)
 
-    def as_ucsc_display_file(self, dataset, **kwd):
+    def as_ucsc_display_file(self, dataset: "DatasetInstance", **kwd) -> Union[FileObjType, str]:
         """Returns file contents with only the bed data. If bed 6+, treat as interval."""
         for line in open(dataset.file_name):
             line = line.strip()
             if line == "" or line.startswith("#"):
                 continue
-            fields = line.split('\t')
+            fields = line.split("\t")
             """check to see if this file doesn't conform to strict genome browser accepted bed"""
             try:
                 if len(fields) > 12:
                     return Interval.as_ucsc_display_file(self, dataset)  # too many fields
                 if len(fields) > 6:
                     int(fields[6])
                     if len(fields) > 7:
                         int(fields[7])
                         if len(fields) > 8:
                             if int(fields[8]) != 0:
                                 return Interval.as_ucsc_display_file(self, dataset)
                             if len(fields) > 9:
                                 int(fields[9])
                                 if len(fields) > 10:
-                                    fields2 = fields[10].rstrip(",").split(",")  # remove trailing comma and split on comma
+                                    fields2 = (
+                                        fields[10].rstrip(",").split(",")
+                                    )  # remove trailing comma and split on comma
                                     for field in fields2:
                                         int(field)
                                     if len(fields) > 11:
-                                        fields2 = fields[11].rstrip(",").split(",")  # remove trailing comma and split on comma
+                                        fields2 = (
+                                            fields[11].rstrip(",").split(",")
+                                        )  # remove trailing comma and split on comma
                                         for field in fields2:
                                             int(field)
             except Exception:
                 return Interval.as_ucsc_display_file(self, dataset)
             # only check first line for proper form
             break
 
         try:
-            return open(dataset.file_name, 'rb')
+            return open(dataset.file_name, "rb")
         except Exception:
             return "This item contains no content"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checks for 'bedness'
 
         BED lines have three required fields and nine additional optional fields.
         The number of fields per line must be consistent throughout any single set of data in
         an annotation track.  The order of the optional fields is binding: lower-numbered
         fields must always be populated if higher-numbered fields are used.  The data type of
@@ -470,20 +590,20 @@
         >>> fname = get_test_fname( 'interv1.bed' )
         >>> Bed().sniff( fname )
         True
         >>> fname = get_test_fname( 'complete.bed' )
         >>> Bed().sniff( fname )
         True
         """
-        if not get_headers(file_prefix, '\t', comment_designator='#', count=1):
+        if not get_headers(file_prefix, "\t", comment_designator="#", count=1):
             return False
         try:
             found_valid_lines = False
-            for hdr in iter_headers(file_prefix, '\t', comment_designator='#'):
-                if not hdr or hdr == ['']:
+            for hdr in iter_headers(file_prefix, "\t", comment_designator="#"):
+                if not hdr or hdr == [""]:
                     continue
                 if len(hdr) < 3 or len(hdr) > 12:
                     return False
                 try:
                     int(hdr[1])
                     int(hdr[2])
                 except Exception:
@@ -514,147 +634,218 @@
                         return False
                 if len(hdr) > 8:
                     # hdr[8] is itemRgb, an RGB value of the form R,G,B (e.g. 255,0,0).  However, this could also be an int (e.g., 0)
                     try:
                         int(hdr[8])
                     except Exception:
                         try:
-                            hdr[8].split(',')
+                            hdr[8].split(",")
                         except Exception:
                             return False
                 if len(hdr) > 9:
                     # hdr[9] is blockCount, the number of blocks (exons) in the BED line.
                     try:
                         block_count = int(hdr[9])
                     except Exception:
                         return False
                 if len(hdr) > 10:
                     # hdr[10] is blockSizes - A comma-separated list of the block sizes.
                     # Sometimes the blosck_sizes and block_starts lists end in extra commas
                     try:
-                        block_sizes = hdr[10].rstrip(',').split(',')
+                        block_sizes = hdr[10].rstrip(",").split(",")
                     except Exception:
                         return False
                 if len(hdr) > 11:
                     # hdr[11] is blockStarts - A comma-separated list of block starts.
                     try:
-                        block_starts = hdr[11].rstrip(',').split(',')
+                        block_starts = hdr[11].rstrip(",").split(",")
                     except Exception:
                         return False
                     if len(block_sizes) != block_count or len(block_starts) != block_count:
                         return False
                 found_valid_lines = True
             return found_valid_lines
         except Exception:
             return False
 
 
 class ProBed(Bed):
     """Tab delimited data in proBED format - adaptation of BED for proteomics data."""
+
     edam_format = "format_3827"
     file_ext = "probed"
-    column_names = ['Chrom', 'Start', 'End', 'Name', 'Score', 'Strand', 'ThickStart', 'ThickEnd', 'ItemRGB', 'BlockCount', 'BlockSizes', 'BlockStarts', 'ProteinAccession', 'PeptideSequence', 'Uniqueness', 'GenomeReferenceVersion', 'PsmScore', 'Fdr', 'Modifications', 'Charge', 'ExpMassToCharge', 'CalcMassToCharge', 'PsmRank', 'DatasetID', 'Uri']
+    column_names = [
+        "Chrom",
+        "Start",
+        "End",
+        "Name",
+        "Score",
+        "Strand",
+        "ThickStart",
+        "ThickEnd",
+        "ItemRGB",
+        "BlockCount",
+        "BlockSizes",
+        "BlockStarts",
+        "ProteinAccession",
+        "PeptideSequence",
+        "Uniqueness",
+        "GenomeReferenceVersion",
+        "PsmScore",
+        "Fdr",
+        "Modifications",
+        "Charge",
+        "ExpMassToCharge",
+        "CalcMassToCharge",
+        "PsmRank",
+        "DatasetID",
+        "Uri",
+    ]
 
 
 class BedStrict(Bed):
     """Tab delimited data in strict BED format - no non-standard columns allowed"""
+
     edam_format = "format_3584"
     file_ext = "bedstrict"
 
     # no user change of datatype allowed
     allow_datatype_change = False
 
     # Read only metadata elements
     MetadataElement(name="chromCol", default=1, desc="Chrom column", readonly=True, param=metadata.MetadataParameter)
-    MetadataElement(name="startCol", default=2, desc="Start column", readonly=True, param=metadata.MetadataParameter)  # TODO: start and end should be able to be set to these or the proper thick[start/end]?
+    MetadataElement(
+        name="startCol", default=2, desc="Start column", readonly=True, param=metadata.MetadataParameter
+    )  # TODO: start and end should be able to be set to these or the proper thick[start/end]?
     MetadataElement(name="endCol", default=3, desc="End column", readonly=True, param=metadata.MetadataParameter)
-    MetadataElement(name="strandCol", desc="Strand column (click box & select)", readonly=True, param=metadata.MetadataParameter, no_value=0, optional=True)
-    MetadataElement(name="nameCol", desc="Name/Identifier column (click box & select)", readonly=True, param=metadata.MetadataParameter, no_value=0, optional=True)
+    MetadataElement(
+        name="strandCol",
+        default=0,
+        desc="Strand column (click box & select)",
+        readonly=True,
+        param=metadata.MetadataParameter,
+        no_value=0,
+        optional=True,
+    )
+    MetadataElement(
+        name="nameCol",
+        desc="Name/Identifier column (click box & select)",
+        readonly=True,
+        param=metadata.MetadataParameter,
+        no_value=0,
+        optional=True,
+    )
     MetadataElement(name="columns", default=3, desc="Number of columns", readonly=True, visible=False)
 
     def __init__(self, **kwd):
         Tabular.__init__(self, **kwd)
         self.clear_display_apps()  # only new style display applications for this datatype
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         Tabular.set_meta(self, dataset, overwrite=overwrite, **kwd)  # need column count first
         if dataset.metadata.columns >= 4:
             dataset.metadata.nameCol = 4
             if dataset.metadata.columns >= 6:
                 dataset.metadata.strandCol = 6
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         return False  # NOTE: This would require aggressively validating the entire file
 
 
 class Bed6(BedStrict):
     """Tab delimited data in strict BED format - no non-standard columns allowed; column count forced to 6"""
+
     edam_format = "format_3585"
     file_ext = "bed6"
 
 
 class Bed12(BedStrict):
     """Tab delimited data in strict BED format - no non-standard columns allowed; column count forced to 12"""
+
     edam_format = "format_3586"
     file_ext = "bed12"
 
 
 class _RemoteCallMixin:
-    def _get_remote_call_url(self, redirect_url, site_name, dataset, type, app, base_url):
+    def _get_remote_call_url(
+        self, redirect_url: str, site_name: str, dataset: "DatasetInstance", type: str, app, base_url: str, request
+    ) -> str:
         """Retrieve the URL to call out to an external site and retrieve data.
         This routes our external URL through a local galaxy instance which makes
         the data available, followed by redirecting to the remote site with a
         link back to the available information.
         """
-        internal_url = f"{app.url_for(controller='dataset', dataset_id=dataset.id, action='display_at', filename=f'{type}_{site_name}')}"
+        internal_url = f"{app.legacy_url_for(mapper=app.legacy_mapper, environ=request.environ, controller='dataset', dataset_id=dataset.id, action='display_at', filename=f'{type}_{site_name}')}"
         base_url = app.config.get("display_at_callback", base_url)
-        display_url = quote_plus("%s%s/display_as?id=%i&display_app=%s&authz_method=display_at" %
-                                 (base_url, app.url_for(controller='root'), dataset.id, type))
-        link = f'{internal_url}?redirect_url={redirect_url}&display_url={display_url}'
+        display_url = quote_plus(
+            "%s%s/display_as?id=%i&display_app=%s&authz_method=display_at"
+            % (
+                base_url,
+                app.legacy_url_for(mapper=app.legacy_mapper, environ=request.environ, controller="root"),
+                dataset.id,
+                type,
+            )
+        )
+        link = f"{internal_url}?redirect_url={redirect_url}&display_url={display_url}"
         return link
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class Gff(Tabular, _RemoteCallMixin):
     """Tab delimited data in Gff format"""
+
     edam_data = "data_1255"
     edam_format = "format_2305"
     file_ext = "gff"
-    valid_gff_frame = ['.', '0', '1', '2']
-    column_names = ['Seqname', 'Source', 'Feature', 'Start', 'End', 'Score', 'Strand', 'Frame', 'Group']
+    valid_gff_frame = [".", "0", "1", "2"]
+    column_names = ["Seqname", "Source", "Feature", "Start", "End", "Score", "Strand", "Frame", "Group"]
     data_sources = {"data": "interval_index", "index": "bigwig", "feature_search": "fli"}
     track_type = Interval.track_type
 
     MetadataElement(name="columns", default=9, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=['str', 'str', 'str', 'int', 'int', 'int', 'str', 'str', 'str'],
-                    param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False)
+    MetadataElement(
+        name="column_types",
+        default=["str", "str", "str", "int", "int", "int", "str", "str", "str"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+    )
 
     MetadataElement(name="attributes", default=0, desc="Number of attributes", readonly=True, visible=False, no_value=0)
-    MetadataElement(name="attribute_types", default={}, desc="Attribute types", param=metadata.DictParameter, readonly=True, visible=False, no_value=[])
+    MetadataElement(
+        name="attribute_types",
+        default={},
+        desc="Attribute types",
+        param=metadata.DictParameter,
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
 
     def __init__(self, **kwd):
         """Initialize datatype, by adding GBrowse display app"""
         Tabular.__init__(self, **kwd)
-        self.add_display_app('ucsc', 'display at UCSC', 'as_ucsc_display_file', 'ucsc_links')
-        self.add_display_app('gbrowse', 'display in Gbrowse', 'as_gbrowse_display_file', 'gbrowse_links')
+        self.add_display_app("ucsc", "display at UCSC", "as_ucsc_display_file", "ucsc_links")
+        self.add_display_app("gbrowse", "display in Gbrowse", "as_gbrowse_display_file", "gbrowse_links")
 
-    def set_attribute_metadata(self, dataset):
+    def set_attribute_metadata(self, dataset: "DatasetInstance") -> None:
         """
         Sets metadata elements for dataset's attributes.
         """
 
         # Use first N lines to set metadata for dataset attributes. Attributes
         # not found in the first N lines will not have metadata.
         num_lines = 200
         attribute_types = {}
         with compression_utils.get_fileobj(dataset.file_name) as in_fh:
             for i, line in enumerate(in_fh):
-                if line and not line.startswith('#'):
-                    elems = line.split('\t')
+                if line and not line.startswith("#"):
+                    elems = line.split("\t")
                     if len(elems) == 9:
                         try:
                             # Loop through attributes to set types.
                             for name, value in parse_gff_attributes(elems[8]).items():
                                 # Default type is string.
                                 value_type = "str"
                                 try:
@@ -674,136 +865,140 @@
                     if i + 1 == num_lines:
                         break
 
         # Set attribute metadata and then set additional metadata.
         dataset.metadata.attribute_types = attribute_types
         dataset.metadata.attributes = len(attribute_types)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         self.set_attribute_metadata(dataset)
 
         i = 0
         with compression_utils.get_fileobj(dataset.file_name) as in_fh:
             for i, line in enumerate(in_fh):  # noqa: B007
-                line = line.rstrip('\r\n')
-                if line and not line.startswith('#'):
-                    elems = line.split('\t')
+                line = line.rstrip("\r\n")
+                if line and not line.startswith("#"):
+                    elems = line.split("\t")
                     if len(elems) == 9:
                         try:
                             int(elems[3])
                             int(elems[4])
                             break
                         except Exception:
                             pass
         Tabular.set_meta(self, dataset, overwrite=overwrite, skip=i)
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def get_estimated_display_viewport(self, dataset):
+    def get_estimated_display_viewport(
+        self, dataset: "DatasetInstance"
+    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
         """
         Return a chrom, start, stop tuple for viewing a file.  There are slight differences between gff 2 and gff 3
         formats.  This function should correctly handle both...
         """
         viewport_feature_count = 100  # viewport should check at least 100 features; excludes comment lines
         max_line_count = max(viewport_feature_count, 500)  # maximum number of lines to check; includes comment lines
         if self.displayable(dataset):
             try:
                 seqid = None
                 start = sys.maxsize
                 stop = 0
                 with compression_utils.get_fileobj(dataset.file_name) as fh:
                     for line in util.iter_start_of_line(fh, VIEWPORT_READLINE_BUFFER_SIZE):
                         try:
-                            if line.startswith('##sequence-region'):  # ##sequence-region IV 6000000 6030000
-                                elems = line.rstrip('\n\r').split()
+                            if line.startswith("##sequence-region"):  # ##sequence-region IV 6000000 6030000
+                                elems = line.rstrip("\n\r").split()
                                 if len(elems) > 3:
                                     # line looks like:
                                     # sequence-region   ctg123 1 1497228
                                     seqid = elems[1]  # IV
                                     start = int(elems[2])  # 6000000
                                     stop = int(elems[3])  # 6030000
                                     break  # use location declared in file
-                                elif len(elems) == 2 and elems[1].find('..') > 0:
+                                elif len(elems) == 2 and elems[1].find("..") > 0:
                                     # line looks like this:
                                     # sequence-region X:120000..140000
-                                    elems = elems[1].split(':')
+                                    elems = elems[1].split(":")
                                     seqid = elems[0]
-                                    start = int(elems[1].split('..')[0])
-                                    stop = int(elems[1].split('..')[1])
+                                    start = int(elems[1].split("..")[0])
+                                    stop = int(elems[1].split("..")[1])
                                     break  # use location declared in file
                                 else:
-                                    log.debug(f"line ({str(line)}) uses an unsupported ##sequence-region definition.")
+                                    log.debug(f"line ({line}) uses an unsupported ##sequence-region definition.")
                                     # break #no break, if bad definition, we try another line
                             elif line.startswith("browser position"):
                                 # Allow UCSC style browser and track info in the GFF file
                                 pos_info = line.split()[-1]
                                 seqid, startend = pos_info.split(":")
                                 start, stop = map(int, startend.split("-"))
                                 break  # use location declared in file
-                            elif not line.startswith(('#', 'track', 'browser')):
+                            elif not line.startswith(("#", "track", "browser")):
                                 viewport_feature_count -= 1
-                                elems = line.rstrip('\n\r').split('\t')
+                                elems = line.rstrip("\n\r").split("\t")
                                 if len(elems) > 3:
                                     if not seqid:
                                         # We can only set the viewport for a single chromosome
                                         seqid = elems[0]
                                     if seqid == elems[0]:
                                         # Make sure we have not spanned chromosomes
                                         start = min(start, int(elems[3]))
                                         stop = max(stop, int(elems[4]))
                         except Exception:
                             # most likely start/stop is not an int or not enough fields
                             pass
                         # make sure we are at the next new line
                         readline_count = VIEWPORT_MAX_READS_PER_LINE
-                        while line.rstrip('\n\r') == line:
-                            assert readline_count > 0, Exception(f'Viewport readline count exceeded for dataset {dataset.id}.')
+                        while line.rstrip("\n\r") == line:
+                            assert readline_count > 0, Exception(
+                                f"Viewport readline count exceeded for dataset {dataset.id}."
+                            )
                             line = fh.readline(VIEWPORT_READLINE_BUFFER_SIZE)
                             if not line:
                                 break  # EOF
                             readline_count -= 1
                         max_line_count -= 1
                         if not viewport_feature_count or not max_line_count:
                             # exceeded viewport or total line count to check
                             break
                     if seqid is not None:
                         return (seqid, str(start), str(stop))  # Necessary to return strings?
             except Exception:
-                log.exception('Unexpected error')
+                log.exception("Unexpected error")
         return (None, None, None)  # could not determine viewport
 
-    def ucsc_links(self, dataset, type, app, base_url):
+    def ucsc_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         ret_val = []
         seqid, start, stop = self.get_estimated_display_viewport(dataset)
         if seqid is not None:
-            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build('ucsc', dataset.dbkey):
-                if site_name in app.datatypes_registry.get_display_sites('ucsc'):
+            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build("ucsc", dataset.dbkey):
+                if site_name in app.datatypes_registry.get_display_sites("ucsc"):
                     redirect_url = quote_plus(
-                        "%sdb=%s&position=%s:%s-%s&hgt.customText=%%s" %
-                        (site_url, dataset.dbkey, seqid, start, stop))
-                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url)
+                        f"{site_url}db={dataset.dbkey}&position={seqid}:{start}-{stop}&hgt.customText=%s"
+                    )
+                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url, request)
                     ret_val.append((site_name, link))
         return ret_val
 
-    def gbrowse_links(self, dataset, type, app, base_url):
+    def gbrowse_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         ret_val = []
         seqid, start, stop = self.get_estimated_display_viewport(dataset)
         if seqid is not None:
-            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build('gbrowse', dataset.dbkey):
-                if site_name in app.datatypes_registry.get_display_sites('gbrowse'):
-                    if seqid.startswith('chr') and len(seqid) > 3:
+            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build("gbrowse", dataset.dbkey):
+                if site_name in app.datatypes_registry.get_display_sites("gbrowse"):
+                    if seqid.startswith("chr") and len(seqid) > 3:
                         seqid = seqid[3:]
                     redirect_url = quote_plus(f"{site_url}/?q={seqid}:{start}..{stop}&eurl=%s")
-                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url)
+                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url, request)
                     ret_val.append((site_name, link))
         return ret_val
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in gff format
 
         GFF lines have nine required fields that must be tab-separated.
 
         For complete details see http://genome.ucsc.edu/FAQ/FAQformat#format3
 
@@ -811,35 +1006,35 @@
         >>> fname = get_test_fname('gff.gff3')
         >>> Gff().sniff( fname )
         False
         >>> fname = get_test_fname('test.gff')
         >>> Gff().sniff( fname )
         True
         """
-        if len(get_headers(file_prefix, '\t', count=2)) < 2:
+        if len(get_headers(file_prefix, "\t", count=2)) < 2:
             return False
         try:
             found_valid_lines = False
-            for hdr in iter_headers(file_prefix, '\t'):
-                if not hdr or hdr == ['']:
+            for hdr in iter_headers(file_prefix, "\t"):
+                if not hdr or hdr == [""]:
                     continue
                 hdr0_parts = hdr[0].split()
-                if hdr0_parts[0] == '##gff-version':
-                    return hdr0_parts[1].startswith('2')
+                if hdr0_parts[0] == "##gff-version":
+                    return hdr0_parts[1].startswith("2")
                 # The gff-version header comment may have been stripped, so inspect the data
-                if hdr[0].startswith('#'):
+                if hdr[0].startswith("#"):
                     continue
                 if len(hdr) != 9:
                     return False
                 try:
                     int(hdr[3])
                     int(hdr[4])
                 except Exception:
                     return False
-                if hdr[5] != '.':
+                if hdr[5] != ".":
                     try:
                         float(hdr[5])
                     except Exception:
                         return False
                 if hdr[6] not in data.valid_strand:
                     return False
                 if hdr[7] not in self.valid_gff_frame:
@@ -847,83 +1042,92 @@
                 found_valid_lines = True
             return found_valid_lines
         except Exception:
             return False
 
     # ------------- Dataproviders
     # redefine bc super is Tabular
-    @dataproviders.decorators.dataprovider_factory('genomic-region',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dataprovider(self, dataset, **settings):
-        return dataproviders.dataset.GenomicRegionDataProvider(dataset, 0, 3, 4, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('genomic-region-dict',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dict_dataprovider(self, dataset, **settings):
-        settings['named_columns'] = True
+    @dataproviders.decorators.dataprovider_factory("genomic-region", GenomicRegionDataProvider.settings)
+    def genomic_region_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        return GenomicRegionDataProvider(dataset, 0, 3, 4, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("genomic-region-dict", GenomicRegionDataProvider.settings)
+    def genomic_region_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        settings["named_columns"] = True
         return self.genomic_region_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('interval',
-                                                   dataproviders.dataset.IntervalDataProvider.settings)
-    def interval_dataprovider(self, dataset, **settings):
-        return dataproviders.dataset.IntervalDataProvider(dataset, 0, 3, 4, 6, 2, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('interval-dict',
-                                                   dataproviders.dataset.IntervalDataProvider.settings)
-    def interval_dict_dataprovider(self, dataset, **settings):
-        settings['named_columns'] = True
+    @dataproviders.decorators.dataprovider_factory("interval", IntervalDataProvider.settings)
+    def interval_dataprovider(self, dataset: "DatasetInstance", **settings):
+        return IntervalDataProvider(dataset, 0, 3, 4, 6, 2, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("interval-dict", IntervalDataProvider.settings)
+    def interval_dict_dataprovider(self, dataset: "DatasetInstance", **settings):
+        settings["named_columns"] = True
         return self.interval_dataprovider(dataset, **settings)
 
 
 class Gff3(Gff):
     """Tab delimited data in Gff3 format"""
+
     edam_format = "format_1975"
     file_ext = "gff3"
-    valid_gff3_strand = ['+', '-', '.', '?']
+    valid_gff3_strand = ["+", "-", ".", "?"]
     valid_gff3_phase = Gff.valid_gff_frame
-    column_names = ['Seqid', 'Source', 'Type', 'Start', 'End', 'Score', 'Strand', 'Phase', 'Attributes']
+    column_names = ["Seqid", "Source", "Type", "Start", "End", "Score", "Strand", "Phase", "Attributes"]
     track_type = Interval.track_type
 
-    MetadataElement(name="column_types", default=['str', 'str', 'str', 'int', 'int', 'float', 'str', 'int', 'list'],
-                    param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False)
+    MetadataElement(
+        name="column_types",
+        default=["str", "str", "str", "int", "int", "float", "str", "int", "list"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+    )
 
     def __init__(self, **kwd):
         """Initialize datatype, by adding GBrowse display app"""
         Gff.__init__(self, **kwd)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         self.set_attribute_metadata(dataset)
         i = 0
         with compression_utils.get_fileobj(dataset.file_name) as in_fh:
             for i, line in enumerate(in_fh):  # noqa: B007
-                line = line.rstrip('\r\n')
-                if line and not line.startswith('#'):
-                    elems = line.split('\t')
+                line = line.rstrip("\r\n")
+                if line and not line.startswith("#"):
+                    elems = line.split("\t")
                     valid_start = False
                     valid_end = False
                     if len(elems) == 9:
                         try:
                             start = int(elems[3])
                             valid_start = True
                         except Exception:
-                            if elems[3] == '.':
+                            if elems[3] == ".":
                                 valid_start = True
                         try:
                             end = int(elems[4])
                             valid_end = True
                         except Exception:
-                            if elems[4] == '.':
+                            if elems[4] == ".":
                                 valid_end = True
                         strand = elems[6]
                         phase = elems[7]
-                        if valid_start and valid_end and start < end and strand in self.valid_gff3_strand and phase in self.valid_gff3_phase:
+                        if (
+                            valid_start
+                            and valid_end
+                            and start < end
+                            and strand in self.valid_gff3_strand
+                            and phase in self.valid_gff3_phase
+                        ):
                             break
         Tabular.set_meta(self, dataset, overwrite=overwrite, skip=i)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in GFF version 3 format
 
         GFF 3 format:
 
         1) adds a mechanism for representing more than one level
            of hierarchical grouping of features and subfeatures.
@@ -951,40 +1155,40 @@
         >>> fname = get_test_fname('gff.gff3')
         >>> Gff3().sniff( fname )
         True
         >>> fname = get_test_fname( 'grch37.75.gtf' )
         >>> Gff3().sniff( fname )
         False
         """
-        if len(get_headers(file_prefix, '\t', count=2)) < 2:
+        if len(get_headers(file_prefix, "\t", count=2)) < 2:
             return False
         try:
             found_valid_lines = False
-            for hdr in iter_headers(file_prefix, '\t'):
-                if not hdr or hdr == ['']:
+            for hdr in iter_headers(file_prefix, "\t"):
+                if not hdr or hdr == [""]:
                     continue
                 hdr0_parts = hdr[0].split()
-                if hdr0_parts[0] == '##gff-version':
-                    return hdr0_parts[1].startswith('3')
+                if hdr0_parts[0] == "##gff-version":
+                    return hdr0_parts[1].startswith("3")
                 # The gff-version header comment may have been stripped, so inspect the data
-                if hdr[0].startswith('#'):
+                if hdr[0].startswith("#"):
                     continue
                 if len(hdr) != 9:
                     return False
                 try:
                     int(hdr[3])
                 except Exception:
-                    if hdr[3] != '.':
+                    if hdr[3] != ".":
                         return False
                 try:
                     int(hdr[4])
                 except Exception:
-                    if hdr[4] != '.':
+                    if hdr[4] != ".":
                         return False
-                if hdr[5] != '.':
+                if hdr[5] != ".":
                     try:
                         float(hdr[5])
                     except Exception:
                         return False
                 if hdr[6] not in self.valid_gff3_strand:
                     return False
                 if hdr[7] not in self.valid_gff3_phase:
@@ -994,24 +1198,31 @@
             return found_valid_lines
         except Exception:
             return False
 
 
 class Gtf(Gff):
     """Tab delimited data in Gtf format"""
+
     edam_format = "format_2306"
     file_ext = "gtf"
-    column_names = ['Seqname', 'Source', 'Feature', 'Start', 'End', 'Score', 'Strand', 'Frame', 'Attributes']
+    column_names = ["Seqname", "Source", "Feature", "Start", "End", "Score", "Strand", "Frame", "Attributes"]
     track_type = Interval.track_type
 
     MetadataElement(name="columns", default=9, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=['str', 'str', 'str', 'int', 'int', 'float', 'str', 'int', 'list'],
-                    param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False)
+    MetadataElement(
+        name="column_types",
+        default=["str", "str", "str", "int", "int", "float", "str", "int", "list"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+    )
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in gtf format
 
         GTF lines have nine required fields that must be tab-separated. The first eight GTF fields are the same as GFF.
         The group field has been expanded into a list of attributes. Each attribute consists of a type/value pair.
         Attributes must end in a semi-colon, and be separated from any following attribute by exactly one space.
         The attribute list must begin with the two mandatory attributes:
@@ -1031,166 +1242,175 @@
         >>> fname = get_test_fname( 'test.gtf' )
         >>> Gtf().sniff( fname )
         True
         >>> fname = get_test_fname( 'grch37.75.gtf' )
         >>> Gtf().sniff( fname )
         True
         """
-        if len(get_headers(file_prefix, '\t', count=2)) < 2:
+        if len(get_headers(file_prefix, "\t", count=2)) < 2:
             return False
         try:
             found_valid_lines = False
-            for hdr in iter_headers(file_prefix, '\t'):
-                if not hdr or hdr == ['']:
+            for hdr in iter_headers(file_prefix, "\t"):
+                if not hdr or hdr == [""]:
                     continue
                 hdr0_parts = hdr[0].split()
-                if hdr0_parts[0] == '##gff-version' and not hdr0_parts[1].startswith('2'):
+                if hdr0_parts[0] == "##gff-version" and not hdr0_parts[1].startswith("2"):
                     return False
                 # The gff-version header comment may have been stripped, so inspect the data
-                if hdr[0].startswith('#'):
+                if hdr[0].startswith("#"):
                     continue
                 if len(hdr) != 9:
                     return False
                 try:
                     int(hdr[3])
                     int(hdr[4])
                 except Exception:
                     return False
-                if hdr[5] != '.':
+                if hdr[5] != ".":
                     try:
                         float(hdr[5])
                     except Exception:
                         return False
                 if hdr[6] not in data.valid_strand:
                     return False
                 if hdr[7] not in self.valid_gff_frame:
                     return False
                 # Check attributes for gene_id (transcript_id is also mandatory
                 # but not for genes)
                 attributes = parse_gff_attributes(hdr[8])
-                if 'gene_id' not in attributes:
+                if "gene_id" not in attributes:
                     return False
                 found_valid_lines = True
             return found_valid_lines
         except Exception:
             return False
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class Wiggle(Tabular, _RemoteCallMixin):
     """Tab delimited data in wiggle format"""
+
     edam_format = "format_3005"
     file_ext = "wig"
     track_type = "LineTrack"
     data_sources = {"data": "bigwig", "index": "bigwig"}
 
     MetadataElement(name="columns", default=3, desc="Number of columns", readonly=True, visible=False)
 
     def __init__(self, **kwd):
         Tabular.__init__(self, **kwd)
-        self.add_display_app('ucsc', 'display at UCSC', 'as_ucsc_display_file', 'ucsc_links')
-        self.add_display_app('gbrowse', 'display in Gbrowse', 'as_gbrowse_display_file', 'gbrowse_links')
+        self.add_display_app("ucsc", "display at UCSC", "as_ucsc_display_file", "ucsc_links")
+        self.add_display_app("gbrowse", "display in Gbrowse", "as_gbrowse_display_file", "gbrowse_links")
 
-    def get_estimated_display_viewport(self, dataset):
+    def get_estimated_display_viewport(
+        self, dataset: "DatasetInstance"
+    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
         """Return a chrom, start, stop tuple for viewing a file."""
         viewport_feature_count = 100  # viewport should check at least 100 features; excludes comment lines
         max_line_count = max(viewport_feature_count, 500)  # maximum number of lines to check; includes comment lines
         if self.displayable(dataset):
             try:
                 chrom = None
                 start = sys.maxsize
                 end = 0
                 span = 1
                 step = None
                 with open(dataset.file_name) as fh:
                     for line in util.iter_start_of_line(fh, VIEWPORT_READLINE_BUFFER_SIZE):
                         try:
                             if line.startswith("browser"):
-                                chr_info = line.rstrip('\n\r').split()[-1]
+                                chr_info = line.rstrip("\n\r").split()[-1]
                                 chrom, coords = chr_info.split(":")
                                 start, end = map(int, coords.split("-"))
                                 break  # use the browser line
                             # variableStep chrom=chr20
-                            if line and (line.lower().startswith("variablestep") or line.lower().startswith("fixedstep")):
+                            if line and (
+                                line.lower().startswith("variablestep") or line.lower().startswith("fixedstep")
+                            ):
                                 if chrom is not None:
                                     break  # different chrom or different section of the chrom
-                                chrom = line.rstrip('\n\r').split("chrom=")[1].split()[0]
-                                if 'span=' in line:
-                                    span = int(line.rstrip('\n\r').split("span=")[1].split()[0])
-                                if 'step=' in line:
-                                    step = int(line.rstrip('\n\r').split("step=")[1].split()[0])
-                                    start = int(line.rstrip('\n\r').split("start=")[1].split()[0])
+                                chrom = line.rstrip("\n\r").split("chrom=")[1].split()[0]
+                                if "span=" in line:
+                                    span = int(line.rstrip("\n\r").split("span=")[1].split()[0])
+                                if "step=" in line:
+                                    step = int(line.rstrip("\n\r").split("step=")[1].split()[0])
+                                    start = int(line.rstrip("\n\r").split("start=")[1].split()[0])
                             else:
-                                fields = line.rstrip('\n\r').split()
+                                fields = line.rstrip("\n\r").split()
                                 if fields:
                                     if step is not None:
                                         if not end:
                                             end = start + span
                                         else:
                                             end += step
                                     else:
                                         start = min(int(fields[0]), start)
                                         end = max(end, int(fields[0]) + span)
                                     viewport_feature_count -= 1
                         except Exception:
                             pass
                         # make sure we are at the next new line
                         readline_count = VIEWPORT_MAX_READS_PER_LINE
-                        while line.rstrip('\n\r') == line:
-                            assert readline_count > 0, Exception(f'Viewport readline count exceeded for dataset {dataset.id}.')
+                        while line.rstrip("\n\r") == line:
+                            assert readline_count > 0, Exception(
+                                f"Viewport readline count exceeded for dataset {dataset.id}."
+                            )
                             line = fh.readline(VIEWPORT_READLINE_BUFFER_SIZE)
                             if not line:
                                 break  # EOF
                             readline_count -= 1
                         max_line_count -= 1
                         if not viewport_feature_count or not max_line_count:
                             # exceeded viewport or total line count to check
                             break
                 if chrom is not None:
                     return (chrom, str(start), str(end))  # Necessary to return strings?
             except Exception:
-                log.exception('Unexpected error')
+                log.exception("Unexpected error")
         return (None, None, None)  # could not determine viewport
 
-    def gbrowse_links(self, dataset, type, app, base_url):
+    def gbrowse_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         ret_val = []
         chrom, start, stop = self.get_estimated_display_viewport(dataset)
         if chrom is not None:
-            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build('gbrowse', dataset.dbkey):
-                if site_name in app.datatypes_registry.get_display_sites('gbrowse'):
-                    if chrom.startswith('chr') and len(chrom) > 3:
+            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build("gbrowse", dataset.dbkey):
+                if site_name in app.datatypes_registry.get_display_sites("gbrowse"):
+                    if chrom.startswith("chr") and len(chrom) > 3:
                         chrom = chrom[3:]
                     redirect_url = quote_plus(f"{site_url}/?q={chrom}:{start}..{stop}&eurl=%s")
-                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url)
+                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url, request)
                     ret_val.append((site_name, link))
         return ret_val
 
-    def ucsc_links(self, dataset, type, app, base_url):
+    def ucsc_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         ret_val = []
         chrom, start, stop = self.get_estimated_display_viewport(dataset)
         if chrom is not None:
-            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build('ucsc', dataset.dbkey):
-                if site_name in app.datatypes_registry.get_display_sites('ucsc'):
-                    redirect_url = quote_plus(f"{site_url}db={dataset.dbkey}&position={chrom}:{start}-{stop}&hgt.customText=%s")
-                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url)
+            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build("ucsc", dataset.dbkey):
+                if site_name in app.datatypes_registry.get_display_sites("ucsc"):
+                    redirect_url = quote_plus(
+                        f"{site_url}db={dataset.dbkey}&position={chrom}:{start}-{stop}&hgt.customText=%s"
+                    )
+                    link = self._get_remote_call_url(redirect_url, site_name, dataset, type, app, base_url, request)
                     ret_val.append((site_name, link))
         return ret_val
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
-        return self.make_html_table(dataset, skipchars=['track', '#'])
+        return self.make_html_table(dataset, skipchars=["track", "#"])
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         max_data_lines = None
         i = 0
         for i, line in enumerate(open(dataset.file_name)):  # noqa: B007
-            line = line.rstrip('\r\n')
-            if line and not line.startswith('#'):
-                elems = line.split('\t')
+            line = line.rstrip("\r\n")
+            if line and not line.startswith("#"):
+                elems = line.split("\t")
                 try:
                     # variableStep format is nucleotide position\tvalue\n,
                     # fixedStep is value\n
                     # "Wiggle track data values can be integer or real, positive or negative values"
                     float(elems[0])
                     break
                 except Exception:
@@ -1200,15 +1420,15 @@
             # we'll arbitrarily only use the first 100 data lines in this wig file to calculate tabular attributes (column types)
             # this should be sufficient, except when we have mixed wig track types (bed, variable, fixed),
             #    but those cases are not a single table that would have consistant column definitions
             # optional metadata values set in Tabular class will be 'None'
             max_data_lines = 100
         Tabular.set_meta(self, dataset, overwrite=overwrite, skip=i, max_data_lines=max_data_lines)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines wether the file is in wiggle format
 
         The .wig format is line-oriented. Wiggle data is preceeded by a track definition line,
         which adds a number of options for controlling the default display of this track.
         Following the track definition line is the track data, which can be entered in several
         different formats.
@@ -1226,128 +1446,140 @@
         >>> fname = get_test_fname( 'wiggle.wig' )
         >>> Wiggle().sniff( fname )
         True
         """
         try:
             headers = iter_headers(file_prefix, None)
             for hdr in headers:
-                if len(hdr) > 1 and hdr[0] == 'track' and hdr[1].startswith('type=wiggle'):
+                if len(hdr) > 1 and hdr[0] == "track" and hdr[1].startswith("type=wiggle"):
                     return True
             return False
         except Exception:
             return False
 
-    def get_track_resolution(self, dataset, start, end):
-        range = end - start
-        # Determine appropriate resolution to plot ~1000 points
-        resolution = math.ceil(10 ** math.ceil(math.log10(range / 1000)))
-        # Restrict to valid range
-        resolution = min(resolution, 100000)
-        resolution = max(resolution, 1)
-        return resolution
-
     # ------------- Dataproviders
-    @dataproviders.decorators.dataprovider_factory('wiggle', dataproviders.dataset.WiggleDataProvider.settings)
-    def wiggle_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        return dataproviders.dataset.WiggleDataProvider(dataset_source, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('wiggle-dict', dataproviders.dataset.WiggleDataProvider.settings)
-    def wiggle_dict_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        settings['named_columns'] = True
-        return dataproviders.dataset.WiggleDataProvider(dataset_source, **settings)
+    @dataproviders.decorators.dataprovider_factory("wiggle", WiggleDataProvider.settings)
+    def wiggle_dataprovider(self, dataset: "DatasetInstance", **settings) -> WiggleDataProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        return WiggleDataProvider(dataset_source, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("wiggle-dict", WiggleDataProvider.settings)
+    def wiggle_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> WiggleDataProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        settings["named_columns"] = True
+        return WiggleDataProvider(dataset_source, **settings)
 
 
 @build_sniff_from_prefix
 class CustomTrack(Tabular):
     """UCSC CustomTrack"""
+
     edam_format = "format_3588"
     file_ext = "customtrack"
 
     def __init__(self, **kwd):
         """Initialize interval datatype, by adding UCSC display app"""
         Tabular.__init__(self, **kwd)
-        self.add_display_app('ucsc', 'display at UCSC', 'as_ucsc_display_file', 'ucsc_links')
+        self.add_display_app("ucsc", "display at UCSC", "as_ucsc_display_file", "ucsc_links")
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         Tabular.set_meta(self, dataset, overwrite=overwrite, skip=1)
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
-        return self.make_html_table(dataset, skipchars=['track', '#'])
+        return self.make_html_table(dataset, skipchars=["track", "#"])
 
-    def get_estimated_display_viewport(self, dataset, chrom_col=None, start_col=None, end_col=None):
+    def get_estimated_display_viewport(
+        self,
+        dataset: "DatasetInstance",
+        chrom_col: Optional[int] = None,
+        start_col: Optional[int] = None,
+        end_col: Optional[int] = None,
+    ) -> Tuple[Optional[str], Optional[str], Optional[str]]:
         """Return a chrom, start, stop tuple for viewing a file."""
         # FIXME: only BED and WIG custom tracks are currently supported
         # As per previously existing behavior, viewport will only be over the first intervals
         max_line_count = 100  # maximum number of lines to check; includes comment lines
         variable_step_wig = False
         chrom = None
         span = 1
         if self.displayable(dataset):
             try:
                 with open(dataset.file_name) as fh:
                     for line in util.iter_start_of_line(fh, VIEWPORT_READLINE_BUFFER_SIZE):
-                        if not line.startswith('#'):
+                        if not line.startswith("#"):
                             try:
                                 if variable_step_wig:
                                     fields = line.rstrip().split()
                                     if len(fields) == 2:
                                         start = int(fields[0])
                                         return (chrom, str(start), str(start + span))
-                                elif line and (line.lower().startswith("variablestep") or line.lower().startswith("fixedstep")):
-                                    chrom = line.rstrip('\n\r').split("chrom=")[1].split()[0]
-                                    if 'span=' in line:
-                                        span = int(line.rstrip('\n\r').split("span=")[1].split()[0])
-                                    if 'start=' in line:
-                                        start = int(line.rstrip('\n\r').split("start=")[1].split()[0])
+                                elif line and (
+                                    line.lower().startswith("variablestep") or line.lower().startswith("fixedstep")
+                                ):
+                                    chrom = line.rstrip("\n\r").split("chrom=")[1].split()[0]
+                                    if "span=" in line:
+                                        span = int(line.rstrip("\n\r").split("span=")[1].split()[0])
+                                    if "start=" in line:
+                                        start = int(line.rstrip("\n\r").split("start=")[1].split()[0])
                                         return (chrom, str(start), str(start + span))
                                     else:
                                         variable_step_wig = True
                                 else:
-                                    fields = line.rstrip().split('\t')
+                                    fields = line.rstrip().split("\t")
                                     if len(fields) >= 3:
                                         chrom = fields[0]
                                         start = int(fields[1])
                                         end = int(fields[2])
                                         return (chrom, str(start), str(end))
                             except Exception:
                                 # most likely a non-integer field has been encountered for start / stop
                                 continue
                         # make sure we are at the next new line
                         readline_count = VIEWPORT_MAX_READS_PER_LINE
-                        while line.rstrip('\n\r') == line:
-                            assert readline_count > 0, Exception(f'Viewport readline count exceeded for dataset {dataset.id}.')
+                        while line.rstrip("\n\r") == line:
+                            assert readline_count > 0, Exception(
+                                f"Viewport readline count exceeded for dataset {dataset.id}."
+                            )
                             line = fh.readline(VIEWPORT_READLINE_BUFFER_SIZE)
                             if not line:
                                 break  # EOF
                             readline_count -= 1
                         max_line_count -= 1
                         if not max_line_count:
                             # exceeded viewport or total line count to check
                             break
             except Exception:
-                log.exception('Unexpected error')
+                log.exception("Unexpected error")
         return (None, None, None)  # could not determine viewport
 
-    def ucsc_links(self, dataset, type, app, base_url):
+    def ucsc_links(self, dataset: "DatasetInstance", type: str, app, base_url: str, request) -> List:
         ret_val = []
         chrom, start, stop = self.get_estimated_display_viewport(dataset)
         if chrom is not None:
-            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build('ucsc', dataset.dbkey):
-                if site_name in app.datatypes_registry.get_display_sites('ucsc'):
-                    internal_url = f"{app.url_for(controller='dataset', dataset_id=dataset.id, action='display_at', filename='ucsc_' + site_name)}"
-                    display_url = quote_plus("%s%s/display_as?id=%i&display_app=%s&authz_method=display_at" % (base_url, app.url_for(controller='root'), dataset.id, type))
-                    redirect_url = quote_plus(f"{site_url}db={dataset.dbkey}&position={chrom}:{start}-{stop}&hgt.customText=%s")
-                    link = f'{internal_url}?redirect_url={redirect_url}&display_url={display_url}'
+            for site_name, site_url in app.datatypes_registry.get_legacy_sites_by_build("ucsc", dataset.dbkey):
+                if site_name in app.datatypes_registry.get_display_sites("ucsc"):
+                    internal_url = f"{app.legacy_url_for(mapper=app.legacy_mapper, environ=request.environ, controller='dataset', dataset_id=dataset.id, action='display_at', filename='ucsc_' + site_name)}"
+                    display_url = quote_plus(
+                        "%s%s/display_as?id=%i&display_app=%s&authz_method=display_at"
+                        % (
+                            base_url,
+                            app.legacy_url_for(mapper=app.legacy_mapper, environ=request.environ, controller="root"),
+                            dataset.id,
+                            type,
+                        )
+                    )
+                    redirect_url = quote_plus(
+                        f"{site_url}db={dataset.dbkey}&position={chrom}:{start}-{stop}&hgt.customText=%s"
+                    )
+                    link = f"{internal_url}?redirect_url={redirect_url}&display_url={display_url}"
                     ret_val.append((site_name, link))
         return ret_val
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in customtrack format.
 
         CustomTrack files are built within Galaxy and are basically bed or interval files with the first line looking
         something like this.
 
         track name="User Track" description="User Supplied Track (from Galaxy)" color=0,0,0 visibility=1
@@ -1363,94 +1595,103 @@
         headers = iter_headers(file_prefix, None)
         found_at_least_one_track = False
         first_line = True
         for hdr in headers:
             if first_line:
                 first_line = False
                 try:
-                    if hdr[0].startswith('track'):
+                    if hdr[0].startswith("track"):
                         color_found = False
                         visibility_found = False
                         for elem in hdr[1:]:
-                            if elem.startswith('color'):
+                            if elem.startswith("color"):
                                 color_found = True
-                            if elem.startswith('visibility'):
+                            if elem.startswith("visibility"):
                                 visibility_found = True
                             if color_found and visibility_found:
                                 break
                         if not color_found or not visibility_found:
                             return False
                     else:
                         return False
                 except Exception:
                     return False
             else:
                 try:
-                    if hdr[0] and not hdr[0].startswith('#'):
+                    if hdr[0] and not hdr[0].startswith("#"):
                         if len(hdr) < 3:
                             return False
                         try:
                             int(hdr[1])
                             int(hdr[2])
                         except Exception:
                             return False
                         found_at_least_one_track = True
                 except Exception:
                     return False
         return found_at_least_one_track
 
 
 class ENCODEPeak(Interval):
-    '''
+    """
     Human ENCODE peak format. There are both broad and narrow peak formats.
     Formats are very similar; narrow peak has an additional column, though.
 
     Broad peak ( http://genome.ucsc.edu/FAQ/FAQformat#format13 ):
     This format is used to provide called regions of signal enrichment based
     on pooled, normalized (interpreted) data. It is a BED 6+3 format.
 
     Narrow peak http://genome.ucsc.edu/FAQ/FAQformat#format12 and :
     This format is used to provide called peaks of signal enrichment based on
     pooled, normalized (interpreted) data. It is a BED6+4 format.
-    '''
+    """
+
     edam_format = "format_3612"
     file_ext = "encodepeak"
-    column_names = ['Chrom', 'Start', 'End', 'Name', 'Score', 'Strand', 'SignalValue', 'pValue', 'qValue', 'Peak']
+    column_names = ["Chrom", "Start", "End", "Name", "Score", "Strand", "SignalValue", "pValue", "qValue", "Peak"]
     data_sources = {"data": "tabix", "index": "bigwig"}
 
     MetadataElement(name="chromCol", default=1, desc="Chrom column", param=metadata.ColumnParameter)
     MetadataElement(name="startCol", default=2, desc="Start column", param=metadata.ColumnParameter)
     MetadataElement(name="endCol", default=3, desc="End column", param=metadata.ColumnParameter)
-    MetadataElement(name="strandCol", desc="Strand column (click box & select)", param=metadata.ColumnParameter, optional=True, no_value=0)
+    MetadataElement(
+        name="strandCol",
+        default=0,
+        desc="Strand column (click box & select)",
+        param=metadata.ColumnParameter,
+        optional=True,
+        no_value=0,
+    )
     MetadataElement(name="columns", default=3, desc="Number of columns", readonly=True, visible=False)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         return False
 
 
 class ChromatinInteractions(Interval):
-    '''
+    """
     Chromatin interactions obtained from 3C/5C/Hi-C experiments.
-    '''
+    """
+
     file_ext = "chrint"
     track_type = "DiagonalHeatmapTrack"
     data_sources = {"data": "tabix", "index": "bigwig"}
-    column_names = ['Chrom1', 'Start1', 'End1', 'Chrom2', 'Start2', 'End2', 'Value']
+    column_names = ["Chrom1", "Start1", "End1", "Chrom2", "Start2", "End2", "Value"]
 
     MetadataElement(name="chrom1Col", default=1, desc="Chrom1 column", param=metadata.ColumnParameter)
     MetadataElement(name="start1Col", default=2, desc="Start1 column", param=metadata.ColumnParameter)
     MetadataElement(name="end1Col", default=3, desc="End1 column", param=metadata.ColumnParameter)
     MetadataElement(name="chrom2Col", default=4, desc="Chrom2 column", param=metadata.ColumnParameter)
     MetadataElement(name="start2Col", default=5, desc="Start2 column", param=metadata.ColumnParameter)
     MetadataElement(name="end2Col", default=6, desc="End2 column", param=metadata.ColumnParameter)
     MetadataElement(name="valueCol", default=7, desc="Value column", param=metadata.ColumnParameter)
 
     MetadataElement(name="columns", default=7, desc="Number of columns", readonly=True, visible=False)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         return False
 
 
 @build_sniff_from_prefix
 class ScIdx(Tabular):
     """
     ScIdx files are 1-based and consist of strand-specific coordinate counts.
@@ -1468,38 +1709,47 @@
     True
     >>> Bed().sniff(fname)
     False
     >>> fname = get_test_fname('empty.txt')
     >>> ScIdx().sniff(fname)
     False
     """
+
     file_ext = "scidx"
 
     MetadataElement(name="columns", default=0, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=[], param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False, no_value=[])
+    MetadataElement(
+        name="column_types",
+        default=[],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
 
     def __init__(self, **kwd):
         """
         Initialize scidx datatype.
         """
         Tabular.__init__(self, **kwd)
         # Don't set column names since the first
         # line of the dataset displays them.
-        self.column_names = ['chrom', 'index', 'forward', 'reverse', 'value']
+        self.column_names = ["chrom", "index", "forward", "reverse", "value"]
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checks for 'scidx-ness.'
         """
         count = 0
-        for count, line in enumerate(iter_headers(file_prefix, '\t')):
+        for count, line in enumerate(iter_headers(file_prefix, "\t")):
             # The first line is always a comment like this:
             # 2015-11-23 20:18:56.51;input.bam;READ1
             if count == 0:
-                if not line[0].startswith('#'):
+                if not line[0].startswith("#"):
                     return False
             # The 2nd line is always a specific header
             elif count == 1:
                 if line != ["chrom", "index", "forward", "reverse", "value"]:
                     return False
             # data line columns 2:5 need to be integers and
             # the fwd and rev column need to sum to value
@@ -1514,10 +1764,113 @@
                 break
         # at least the comment and header are required
         if count >= 1:
             return True
         return False
 
 
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
+class IntervalTabix(Interval):
+    """
+    Class describing the bgzip format (http://samtools.github.io/hts-specs/SAMv1.pdf)
+    As tabix is just a bgzip sorted for chr,start,end with an index
+    """
+
+    file_ext = "interval_tabix.gz"
+    edam_format = "format_3616"
+    compressed = True
+    compressed_format = "gzip"
+
+    # The MetadataElements are readonly so the user cannot change them (as the index is generated only once)
+    MetadataElement(name="chromCol", default=1, desc="Chrom column", param=metadata.ColumnParameter, readonly=True)
+    MetadataElement(name="startCol", default=2, desc="Start column", param=metadata.ColumnParameter, readonly=True)
+    MetadataElement(name="endCol", default=3, desc="End column", param=metadata.ColumnParameter, readonly=True)
+
+    # Add metadata elements
+    MetadataElement(
+        name="tabix_index",
+        desc="Tabix Index File",
+        param=metadata.FileParameter,
+        file_ext="tbi",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+
+    # We don't want to have a good sniff as the index would be created before the metadata on columns is set.
+    def sniff_prefix(self, file_prefix: FilePrefix):
+        return False
+
+    # Ideally the tabix_index would be regenerated when the metadataElements are updated
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        first_line_is_header: bool = False,
+        metadata_tmp_files_dir: Optional[str] = None,
+        **kwd,
+    ) -> None:
+        # We don't use the method Interval.set_meta as we don't want to guess the columns for chr start end
+        Tabular.set_meta(self, dataset, overwrite=overwrite, skip=0)
+        # Try to create the index for the Tabix file.
+        # These metadata values are not accessible by users, always overwrite
+        index_file = dataset.metadata.tabix_index
+        if not index_file:
+            index_file = dataset.metadata.spec["tabix_index"].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
+
+        try:
+            # tabix_index columns are 0-based while in the command line it is 1-based
+            pysam.tabix_index(
+                dataset.file_name,
+                index=index_file.file_name,
+                seq_col=dataset.metadata.chromCol - 1,
+                start_col=dataset.metadata.startCol - 1,
+                end_col=dataset.metadata.endCol - 1,
+                keep_original=True,
+                force=True,
+            )
+        except Exception as e:
+            raise Exception(f"Error setting tabix metadata: {util.unicodify(e)}")
+        else:
+            dataset.metadata.tabix_index = index_file
+
+
+class JuicerMediumTabix(IntervalTabix):
+    """
+    Class describing a tabix file built from a juicer medium format:
+    https://github.com/aidenlab/juicer/wiki/Pre#medium-format
+    <readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2>
+
+    str = strand (0 for forward, anything else for reverse)
+    chr = chromosome (must be a chromosome in the genome)
+    pos = position
+    frag = restriction site fragment
+    mapq = mapping quality score
+    """
+
+    file_ext = "juicer_medium_tabix.gz"
+
+    # The MetadataElements are readonly so the user cannot change them (as the index is generated only once)
+    MetadataElement(name="chromCol", default=3, desc="Chrom column", param=metadata.ColumnParameter, readonly=True)
+    MetadataElement(name="startCol", default=4, desc="Start column", param=metadata.ColumnParameter, readonly=True)
+    MetadataElement(name="endCol", default=4, desc="End column", param=metadata.ColumnParameter, readonly=True)
+
+
+class BedTabix(IntervalTabix):
+    """
+    Class describing a tabix file built from a bed file
+    """
+
+    file_ext = "bed_tabix.gz"
+
+
+class GffTabix(IntervalTabix):
+    """
+    Class describing a tabix file built from a bed file
+    """
+
+    file_ext = "gff_tabix.gz"
+
+    # The MetadataElements are readonly so the user cannot change them (as the index is generated only once)
+    MetadataElement(name="startCol", default=4, desc="Start column", param=metadata.ColumnParameter, readonly=True)
+    MetadataElement(name="endCol", default=5, desc="End column", param=metadata.ColumnParameter, readonly=True)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/isa.py` & `galaxy-data-23.0.1/galaxy/datatypes/isa.py`

 * *Files 18% similar despite different names*

```diff
@@ -7,28 +7,44 @@
 import json
 import logging
 import os
 import os.path
 import re
 import shutil
 import tempfile
+from typing import (
+    List,
+    Optional,
+    TYPE_CHECKING,
+)
 
 # Imports isatab after turning off warnings inside logger settings to avoid pandas warning making uploads fail.
 logging.getLogger("isatools.isatab").setLevel(logging.ERROR)
 from isatools import (
     isajson,
-    isatab_meta
+    isatab_meta,
 )
 from markupsafe import escape
 
 from galaxy import util
-from galaxy.datatypes import data
+from galaxy.datatypes.data import (
+    Data,
+    GeneratePrimaryFileDataset,
+)
 from galaxy.util.compression_utils import CompressedFile
 from galaxy.util.sanitize_html import sanitize_html
 
+if TYPE_CHECKING:
+    from isatools.model import Investigation
+
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+
 # CONSTANTS {{{1
 ################################################################
 
 # Main files regex
 JSON_FILE_REGEX = re.compile(r"^.*\.json$", flags=re.IGNORECASE)
 INVESTIGATION_FILE_REGEX = re.compile(r"^i_\w+\.txt$", flags=re.IGNORECASE)
 
@@ -46,180 +62,185 @@
 # Function for opening correctly a CSV file for csv.reader() for both Python 2 and 3 {{{1
 ################################################################
 
 
 # ISA class {{{1
 ################################################################
 
-class _Isa(data.Data):
-    """ Base class for implementing ISA datatypes """
-    composite_type = 'auto_primary_file'
+
+class _Isa(Data):
+    """Base class for implementing ISA datatypes"""
+
+    composite_type = "auto_primary_file"
     is_binary = True
-    _main_file_regex = None
 
     # Make investigation instance {{{2
     ################################################################
 
-    def _make_investigation_instance(self, filename):
+    def _make_investigation_instance(self, filename: str) -> "Investigation":
         raise NotImplementedError()
 
     # Constructor {{{2
     ################################################################
 
-    def __init__(self, main_file_regex, **kwd):
+    def __init__(self, main_file_regex: re.Pattern, **kwd) -> None:
         super().__init__(**kwd)
         self._main_file_regex = main_file_regex
 
         # Add the archive file as the only composite file
         self.add_composite_file(ISA_ARCHIVE_NAME, is_binary=True, optional=True)
 
     # Get ISA folder path {{{2
     ################################################################
 
-    def _get_isa_folder_path(self, dataset):
+    def _get_isa_folder_path(self, dataset: "DatasetInstance") -> str:
         isa_folder = dataset.extra_files_path
         if not isa_folder:
-            raise Exception('Unvalid dataset object, or no extra files path found for this dataset.')
+            raise Exception("Unvalid dataset object, or no extra files path found for this dataset.")
         return isa_folder
 
     # Get main file {{{2
     ################################################################
 
-    def _get_main_file(self, dataset):
+    def _get_main_file(self, dataset: "DatasetInstance") -> Optional[str]:
         """Get the main file of the ISA archive. Either the investigation file i_*.txt for ISA-Tab, or the JSON file for ISA-JSON."""
 
         main_file = None
         isa_folder = self._get_isa_folder_path(dataset)
 
         if os.path.exists(isa_folder):
-
             # Get ISA archive older
             isa_files = os.listdir(isa_folder)
 
             # Try to find main file
             main_file = self._find_main_file_in_archive(isa_files)
 
             if main_file is None:
-                raise Exception('Invalid ISA archive. No main file found.')
+                raise Exception("Invalid ISA archive. No main file found.")
 
             # Make full path
+            assert main_file
             main_file = os.path.join(isa_folder, main_file)
 
         return main_file
 
     # Get investigation {{{2
     ################################################################
 
-    def _get_investigation(self, dataset):
+    def _get_investigation(self, dataset: "DatasetInstance") -> Optional["Investigation"]:
         """Create a contained instance specific to the exact ISA type (Tab or Json).
-           We will use it to parse and access information from the archive."""
+        We will use it to parse and access information from the archive."""
 
         investigation = None
         main_file = self._get_main_file(dataset)
         if main_file is not None:
             investigation = self._make_investigation_instance(main_file)
 
         return investigation
 
     # Find main file in archive {{{2
     ################################################################
 
-    def _find_main_file_in_archive(self, files_list):
+    def _find_main_file_in_archive(self, files_list: List) -> Optional[str]:
         """Find the main file inside the ISA archive."""
 
         found_file = None
 
         for f in files_list:
             match = self._main_file_regex.match(f)
             if match:
                 if found_file is None:
-                    found_file = match.group()
+                    matched = match.group()  # can be string or tuple
+                    found_file = matched if isinstance(matched, str) else matched[0]
                 else:
-                    raise Exception('More than one file match the pattern "', str(self._main_file_regex), '" to identify the investigation file')
+                    raise Exception(
+                        'More than one file match the pattern "',
+                        str(self._main_file_regex),
+                        '" to identify the investigation file',
+                    )
 
         return found_file
 
     # Set peek {{{2
     ################################################################
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text. Get first lines of the main file and set it as the peek."""
 
         main_file = self._get_main_file(dataset)
 
         if main_file is None:
             raise RuntimeError("Unable to find the main file within the 'files_path' folder")
 
         # Read first lines of main file
-        with open(main_file, encoding='utf-8') as f:
-            data = []
+        with open(main_file, encoding="utf-8") as f:
+            data: List = []
             for line in f:
                 if len(data) < _MAX_LINES_HISTORY_PEEK:
                     data.append(line)
                 else:
                     break
             if not dataset.dataset.purged and data:
                 dataset.peek = json.dumps({"data": data})
-                dataset.blurb = 'data'
+                dataset.blurb = "data"
             else:
-                dataset.peek = 'file does not exist'
-                dataset.blurb = 'file purged from disk'
+                dataset.peek = "file does not exist"
+                dataset.blurb = "file purged from disk"
 
     # Display peek {{{2
     ################################################################
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create the HTML table used for displaying peek, from the peek text found by set_peek() method."""
 
         out = ['<table cellspacing="0" cellpadding="3">']
         try:
             if not dataset.peek:
                 dataset.set_peek()
             json_data = json.loads(dataset.peek)
             for line in json_data["data"]:
                 line = line.strip()
                 if not line:
                     continue
                 out.append(f"<tr><td>{escape(util.unicodify(line, 'utf-8'))}</td></tr>")
-            out.append('</table>')
-            out = "".join(out)
+            out.append("</table>")
+            return "".join(out)
         except Exception as exc:
-            out = f"Can't create peek: {util.unicodify(exc)}"
-        return out
+            return f"Can't create peek: {util.unicodify(exc)}"
 
     # Generate primary file {{{2
     ################################################################
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """Generate the primary file. It is an HTML file containing description of the composite dataset
-           as well as a list of the composite files that it contains."""
+        as well as a list of the composite files that it contains."""
 
         if dataset:
-            rval = ['<html><head><title>ISA Dataset </title></head><p/>']
+            rval = ["<html><head><title>ISA Dataset </title></head><p/>"]
             if hasattr(dataset, "extra_files_path"):
-                rval.append('<div>ISA Dataset composed of the following files:<p/><ul>')
+                rval.append("<div>ISA Dataset composed of the following files:<p/><ul>")
                 for cmp_file in os.listdir(dataset.extra_files_path):
                     rval.append(f'<li><a href="{cmp_file}" type="text/plain">{escape(cmp_file)}</a></li>')
-                rval.append('</ul></div></html>')
+                rval.append("</ul></div></html>")
             else:
-                rval.append('<div>ISA Dataset is empty!<p/><ul>')
+                rval.append("<div>ISA Dataset is empty!<p/><ul>")
             return "\n".join(rval)
         return "<div>No dataset available</div>"
 
     # Dataset content needs grooming {{{2
     ################################################################
 
-    def dataset_content_needs_grooming(self, file_name):
+    def dataset_content_needs_grooming(self, file_name: str) -> bool:
         """This function is called on an output dataset file after the content is initially generated."""
         return os.path.basename(file_name) == ISA_ARCHIVE_NAME
 
     # Groom dataset content {{{2
     ################################################################
 
-    def groom_dataset_content(self, file_name):
+    def groom_dataset_content(self, file_name: str) -> None:
         """This method is called by Galaxy to extract files contained in a composite data type."""
         # XXX Is the right place to extract files? Should this step not be a cleaning step instead?
         # Could extracting be done earlier and composite files declared as files contained inside the archive
         # instead of the archive itself?
 
         # extract basename and folder of the current file whose content has to be groomed
         basename = os.path.basename(file_name)
@@ -228,31 +249,41 @@
         if basename == ISA_ARCHIVE_NAME:
             # perform extraction
             # For some ZIP files CompressedFile::extract() extract the file inside <output_folder>/<file_name> instead of outputing it inside <output_folder>. So we first create a temporary folder, extract inside it, and move content to final destination.
             temp_folder = tempfile.mkdtemp()
             CompressedFile(file_name).extract(temp_folder)
             shutil.rmtree(output_path)
             extracted_files = os.listdir(temp_folder)
-            logger.debug(' '.join(extracted_files))
+            logger.debug(" ".join(extracted_files))
             if len(extracted_files) == 0:
                 os.makedirs(output_path)
                 shutil.rmtree(temp_folder)
             elif len(extracted_files) == 1 and os.path.isdir(os.path.join(temp_folder, extracted_files[0])):
                 shutil.move(os.path.join(temp_folder, extracted_files[0]), output_path)
                 shutil.rmtree(temp_folder)
             else:
                 shutil.move(temp_folder, output_path)
 
     # Display data {{{2
     ################################################################
 
-    def display_data(self, trans, dataset, preview=False, filename=None, to_ext=None, offset=None, ck_size=None, **kwd):
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        offset: Optional[int] = None,
+        ck_size: Optional[int] = None,
+        **kwd,
+    ):
         """Downloads the ISA dataset if `preview` is `False`;
-           if `preview` is `True`, it returns a preview of the ISA dataset as a HTML page.
-           The preview is triggered when user clicks on the eye icon of the composite dataset."""
+        if `preview` is `True`, it returns a preview of the ISA dataset as a HTML page.
+        The preview is triggered when user clicks on the eye icon of the composite dataset."""
 
         headers = kwd.get("headers", {})
         # if it is not required a preview use the default behaviour of `display_data`
         if not preview:
             return super().display_data(trans, dataset, preview, filename, to_ext, **kwd)
 
         # prepare the preview of the ISA dataset
@@ -261,71 +292,71 @@
             html = """<html><header><title>Error while reading ISA archive.</title></header>
                    <body>
                         <h1>An error occurred while reading content of ISA archive.</h1>
                         <p>If you have tried to load your archive with the uploader by selecting isa-tab as composite data type, then try to load it again with isa-json instead. Conversely, if you have tried to load your archive with the uploader by selecting isa-json as composite data type, then try isa-tab instead.</p>
                         <p>You may also try to look into your zip file in order to find out if this is a proper ISA archive. If you see a file i_Investigation.txt inside, then it is an ISA-Tab archive. If you see a file with extension .json inside, then it is an ISA-JSON archive. If you see nothing like that, then either your ISA archive is corrupted, or it is not an ISA archive.</p>
                    </body></html>"""
         else:
-            html = '<html><body>'
-            html += f'<h1>{investigation.title} {investigation.identifier}</h1>'
+            html = "<html><body>"
+            html += f"<h1>{investigation.title} {investigation.identifier}</h1>"
 
             # Loop on all studies
             for study in investigation.studies:
-                html += f'<h2>Study {study.identifier}</h2>'
-                html += f'<h3>{study.title}</h3>'
-                html += f'<p>{study.description}</p>'
-                html += f'<p>Submitted the {study.submission_date}</p>'
-                html += f'<p>Released on {study.public_release_date}</p>'
+                html += f"<h2>Study {study.identifier}</h2>"
+                html += f"<h3>{study.title}</h3>"
+                html += f"<p>{study.description}</p>"
+                html += f"<p>Submitted the {study.submission_date}</p>"
+                html += f"<p>Released on {study.public_release_date}</p>"
 
                 html += f"<p>Experimental factors used: {', '.join(x.name for x in study.factors)}</p>"
 
                 # Loop on all assays of this study
                 for assay in study.assays:
-                    html += f'<h3>Assay {assay.filename}</h3>'
-                    html += f'<p>Measurement type: {assay.measurement_type.term}</p>'  # OntologyAnnotation
-                    html += f'<p>Technology type: {assay.technology_type.term}</p>'    # OntologyAnnotation
-                    html += f'<p>Technology platform: {assay.technology_platform}</p>'
+                    html += f"<h3>Assay {assay.filename}</h3>"
+                    html += f"<p>Measurement type: {assay.measurement_type.term}</p>"  # OntologyAnnotation
+                    html += f"<p>Technology type: {assay.technology_type.term}</p>"  # OntologyAnnotation
+                    html += f"<p>Technology platform: {assay.technology_platform}</p>"
                     if assay.data_files is not None:
-                        html += '<p>Data files:</p>'
-                        html += '<ul>'
+                        html += "<p>Data files:</p>"
+                        html += "<ul>"
                         for data_file in assay.data_files:
-                            if data_file.filename != '':
+                            if data_file.filename != "":
                                 html += f"<li>{escape(util.unicodify(str(data_file.filename), 'utf-8'))} - {escape(util.unicodify(str(data_file.label), 'utf-8'))}</li>"
-                        html += '</ul>'
+                        html += "</ul>"
 
-            html += '</body></html>'
+            html += "</body></html>"
 
         # Set mime type
-        mime = 'text/html'
+        mime = "text/html"
         self._clean_and_set_mime_type(trans, mime, headers)
 
-        return sanitize_html(html).encode('utf-8'), headers
+        return sanitize_html(html).encode("utf-8"), headers
 
 
 # ISA-Tab class {{{1
 ################################################################
 
+
 class IsaTab(_Isa):
     file_ext = "isa-tab"
 
     # Constructor {{{2
     ################################################################
 
     def __init__(self, **kwd):
         super().__init__(main_file_regex=INVESTIGATION_FILE_REGEX, **kwd)
 
     # Make investigation instance {{{2
     ################################################################
 
-    def _make_investigation_instance(self, filename):
-
+    def _make_investigation_instance(self, filename: str) -> "Investigation":
         # Parse ISA-Tab investigation file
         parser = isatab_meta.InvestigationParser()
         isa_dir = os.path.dirname(filename)
-        with open(filename, newline='', encoding='utf8') as fp:
+        with open(filename, newline="", encoding="utf8") as fp:
             parser.parse(fp)
         for study in parser.isa.studies:
             s_parser = isatab_meta.LazyStudySampleTableParser(parser.isa)
             s_parser.parse(os.path.join(isa_dir, study.filename))
             for assay in study.assays:
                 a_parser = isatab_meta.LazyAssayTableParser(parser.isa)
                 a_parser.parse(os.path.join(isa_dir, assay.filename))
@@ -333,26 +364,26 @@
 
         return isa
 
 
 # ISA-JSON class {{{1
 ################################################################
 
+
 class IsaJson(_Isa):
     file_ext = "isa-json"
 
     # Constructor {{{2
     ################################################################
 
     def __init__(self, **kwd):
         super().__init__(main_file_regex=JSON_FILE_REGEX, **kwd)
 
     # Make investigation instance {{{2
     ################################################################
 
-    def _make_investigation_instance(self, filename):
-
+    def _make_investigation_instance(self, filename: str) -> "Investigation":
         # Parse JSON file
-        with open(filename, newline='', encoding='utf8') as fp:
+        with open(filename, newline="", encoding="utf8") as fp:
             isa = isajson.load(fp)
 
         return isa
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/media.py` & `galaxy-data-23.0.1/galaxy/datatypes/media.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,167 +1,293 @@
 """Video classes"""
 import json
 import subprocess
 import wave
+from functools import lru_cache
+from typing import (
+    List,
+    Tuple,
+    TYPE_CHECKING,
+)
 
 from galaxy.datatypes.binary import Binary
-from galaxy.datatypes.metadata import ListParameter, MetadataElement
+from galaxy.datatypes.metadata import (
+    ListParameter,
+    MetadataElement,
+)
 from galaxy.util import which
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
+
+@lru_cache(maxsize=128)
+def _ffprobe(path):
+    return subprocess.run(
+        ["ffprobe", "-loglevel", "quiet", "-show_format", "-show_streams", "-of", "json", path], capture_output=True
+    )
+
 
 def ffprobe(path):
-    data = json.loads(subprocess.check_output(['ffprobe', '-loglevel', 'quiet', '-show_format', '-show_streams', '-of', 'json', path]).decode("utf-8"))
-    return data['format'], data['streams']
+    completed_process = _ffprobe(path)
+    completed_process.check_returncode()
+    data = json.loads(completed_process.stdout.decode("utf-8"))
+    return data["format"], data["streams"]
 
 
 class Audio(Binary):
+    MetadataElement(
+        name="duration",
+        default=0,
+        desc="Length of audio sample",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
+    MetadataElement(
+        name="audio_codecs",
+        default=[],
+        desc="Audio codec(s)",
+        param=ListParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="sample_rates",
+        default=[],
+        desc="Sampling Rate(s)",
+        param=ListParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="audio_streams",
+        default=0,
+        desc="Number of audio streams",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    MetadataElement(name="duration", default=0, desc="Length of audio sample", readonly=True, visible=True, optional=True, no_value=0)
-    MetadataElement(name="audio_codecs", default=[], desc="Audio codec(s)", param=ListParameter, readonly=True, visible=True, optional=True, no_value=[])
-    MetadataElement(name="sample_rates", default=[], desc="Sampling Rate(s)", param=ListParameter, readonly=True, visible=True, optional=True, no_value=[])
-    MetadataElement(name="audio_streams", default=0, desc="Number of audio streams", readonly=True, visible=True, optional=True, no_value=0)
-
-    def set_meta(self, dataset, **kwd):
-        if which('ffprobe'):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        if which("ffprobe"):
             metadata, streams = ffprobe(dataset.file_name)
 
-            dataset.metadata.duration = metadata['duration']
-            dataset.metadata.audio_codecs = [stream['codec_name'] for stream in streams if stream['codec_type'] == 'audio']
-            dataset.metadata.sample_rates = [stream['sample_rate'] for stream in streams if stream['codec_type'] == 'audio']
-            dataset.metadata.audio_streams = len([stream for stream in streams if stream['codec_type'] == 'audio'])
+            dataset.metadata.duration = metadata["duration"]
+            dataset.metadata.audio_codecs = [
+                stream["codec_name"] for stream in streams if stream["codec_type"] == "audio"
+            ]
+            dataset.metadata.sample_rates = [
+                stream["sample_rate"] for stream in streams if stream["codec_type"] == "audio"
+            ]
+            dataset.metadata.audio_streams = len([stream for stream in streams if stream["codec_type"] == "audio"])
 
 
 class Video(Binary):
+    MetadataElement(
+        name="resolution_w",
+        default=0,
+        desc="Width of video stream",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
+    MetadataElement(
+        name="resolution_h",
+        default=0,
+        desc="Height of video stream",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
+    MetadataElement(
+        name="fps", default=0, desc="FPS of video stream", readonly=True, visible=True, optional=True, no_value=0
+    )
+    MetadataElement(
+        name="video_codecs",
+        default=[],
+        desc="Video codec(s)",
+        param=ListParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="audio_codecs",
+        default=[],
+        desc="Audio codec(s)",
+        param=ListParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="video_streams",
+        default=0,
+        desc="Number of video streams",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
+    MetadataElement(
+        name="audio_streams",
+        default=0,
+        desc="Number of audio streams",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    MetadataElement(name="resolution_w", default=0, desc="Width of video stream", readonly=True, visible=True, optional=True, no_value=0)
-    MetadataElement(name="resolution_h", default=0, desc="Height of video stream", readonly=True, visible=True, optional=True, no_value=0)
-    MetadataElement(name="fps", default=0, desc="FPS of video stream", readonly=True, visible=True, optional=True, no_value=0)
-    MetadataElement(name="video_codecs", default=[], desc="Video codec(s)", param=ListParameter, readonly=True, visible=True, optional=True, no_value=[])
-    MetadataElement(name="audio_codecs", default=[], desc="Audio codec(s)", param=ListParameter, readonly=True, visible=True, optional=True, no_value=[])
-    MetadataElement(name="video_streams", default=0, desc="Number of video streams", readonly=True, visible=True, optional=True, no_value=0)
-    MetadataElement(name="audio_streams", default=0, desc="Number of audio streams", readonly=True, visible=True, optional=True, no_value=0)
-
-    def _get_resolution(self, streams):
+    def _get_resolution(self, streams: List) -> Tuple[int, int, float]:
         for stream in streams:
-            if stream['codec_type'] == 'video':
-                w = stream['width']
-                h = stream['height']
-                dividend, divisor = stream['avg_frame_rate'].split('/')
+            if stream["codec_type"] == "video":
+                w = stream["width"]
+                h = stream["height"]
+                dividend, divisor = stream["avg_frame_rate"].split("/")
                 fps = float(dividend) / float(divisor)
         else:
             w = h = fps = 0
         return w, h, fps
 
-    def set_meta(self, dataset, **kwd):
-        if which('ffprobe'):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        if which("ffprobe"):
             metadata, streams = ffprobe(dataset.file_name)
             (w, h, fps) = self._get_resolution(streams)
             dataset.metadata.resolution_w = w
             dataset.metadata.resolution_h = h
             dataset.metadata.fps = fps
 
-            dataset.metadata.audio_codecs = [stream['codec_name'] for stream in streams if stream['codec_type'] == 'audio']
-            dataset.metadata.video_codecs = [stream['codec_name'] for stream in streams if stream['codec_type'] == 'video']
+            dataset.metadata.audio_codecs = [
+                stream["codec_name"] for stream in streams if stream["codec_type"] == "audio"
+            ]
+            dataset.metadata.video_codecs = [
+                stream["codec_name"] for stream in streams if stream["codec_type"] == "video"
+            ]
 
-            dataset.metadata.audio_streams = len([stream for stream in streams if stream['codec_type'] == 'audio'])
-            dataset.metadata.video_streams = len([stream for stream in streams if stream['codec_type'] == 'video'])
+            dataset.metadata.audio_streams = len([stream for stream in streams if stream["codec_type"] == "audio"])
+            dataset.metadata.video_streams = len([stream for stream in streams if stream["codec_type"] == "video"])
 
 
 class Mkv(Video):
     file_ext = "mkv"
 
-    def sniff(self, filename):
-        if which('ffprobe'):
+    def sniff(self, filename: str) -> bool:
+        if which("ffprobe"):
             metadata, streams = ffprobe(filename)
-            return 'matroska' in metadata['format_name'].split(',')
+            return "matroska" in metadata["format_name"].split(",")
+        return False
 
 
 class Mp4(Video):
     """
     Class that reads MP4 video file.
     >>> from galaxy.datatypes.sniff import sniff_with_cls
     >>> sniff_with_cls(Mp4, 'video_1.mp4')
     True
     >>> sniff_with_cls(Mp4, 'audio_1.mp4')
     False
     """
 
     file_ext = "mp4"
 
-    def sniff(self, filename):
-        if which('ffprobe'):
+    def sniff(self, filename: str) -> bool:
+        if which("ffprobe"):
             metadata, streams = ffprobe(filename)
-            return 'mp4' in metadata['format_name'].split(',')
+            return "mp4" in metadata["format_name"].split(",")
+        return False
 
 
 class Flv(Video):
     file_ext = "flv"
 
-    def sniff(self, filename):
-        if which('ffprobe'):
+    def sniff(self, filename: str) -> bool:
+        if which("ffprobe"):
             metadata, streams = ffprobe(filename)
-            return 'flv' in metadata['format_name'].split(',')
+            return "flv" in metadata["format_name"].split(",")
+        return False
 
 
 class Mpg(Video):
     file_ext = "mpg"
 
-    def sniff(self, filename):
-        if which('ffprobe'):
+    def sniff(self, filename: str) -> bool:
+        if which("ffprobe"):
             metadata, streams = ffprobe(filename)
-            return 'mpegvideo' in metadata['format_name'].split(',')
+            return "mpegvideo" in metadata["format_name"].split(",")
+        return False
 
 
 class Mp3(Audio):
     """
     Class that reads MP3 audio file.
     >>> from galaxy.datatypes.sniff import sniff_with_cls
     >>> sniff_with_cls(Mp3, 'audio_2.mp3')
     True
     >>> sniff_with_cls(Mp3, 'audio_1.wav')
     False
     """
+
     file_ext = "mp3"
 
-    def sniff(self, filename):
-        if which('ffprobe'):
+    def sniff(self, filename: str) -> bool:
+        if which("ffprobe"):
             metadata, streams = ffprobe(filename)
-            return 'mp3' in metadata['format_name'].split(',')
+            return "mp3" in metadata["format_name"].split(",")
+        return False
 
 
 class Wav(Audio):
     """Class that reads WAV audio file
     >>> from galaxy.datatypes.sniff import sniff_with_cls
     >>> sniff_with_cls(Wav, 'hello.wav')
     True
     >>> sniff_with_cls(Wav, 'audio_2.mp3')
     False
     >>> sniff_with_cls(Wav, 'drugbank_drugs.cml')
     False
     """
+
     file_ext = "wav"
     blurb = "RIFF WAV Audio file"
     is_binary = True
 
     MetadataElement(name="rate", desc="Sample Rate", default=0, no_value=0, readonly=True, visible=True, optional=True)
-    MetadataElement(name="nframes", desc="Number of Samples", default=0, no_value=0, readonly=True, visible=True, optional=True)
-    MetadataElement(name="nchannels", desc="Number of Channels", default=0, no_value=0, readonly=True, visible=True, optional=True)
-    MetadataElement(name="sampwidth", desc="Sample Width", default=0, no_value=0, readonly=True, visible=True, optional=True)
+    MetadataElement(
+        name="nframes", desc="Number of Samples", default=0, no_value=0, readonly=True, visible=True, optional=True
+    )
+    MetadataElement(
+        name="nchannels", desc="Number of Channels", default=0, no_value=0, readonly=True, visible=True, optional=True
+    )
+    MetadataElement(
+        name="sampwidth", desc="Sample Width", default=0, no_value=0, readonly=True, visible=True, optional=True
+    )
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype."""
-        return 'audio/wav'
+        return "audio/wav"
 
-    def sniff(self, filename):
-        with wave.open(filename, 'rb'):
+    def sniff(self, filename: str) -> bool:
+        with wave.open(filename, "rb"):
             return True
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """Set the metadata for this dataset from the file contents."""
         try:
-            with wave.open(dataset.dataset.file_name, 'rb') as fd:
+            with wave.open(dataset.dataset.file_name, "rb") as fd:
                 dataset.metadata.rate = fd.getframerate()
                 dataset.metadata.nframes = fd.getnframes()
                 dataset.metadata.sampwidth = fd.getsampwidth()
                 dataset.metadata.nchannels = fd.getnchannels()
         except wave.Error:
             pass
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/metacyto.py` & `galaxy-data-23.0.1/galaxy/datatypes/metacyto.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,26 +8,28 @@
 from galaxy.datatypes.tabular import Tabular
 
 log = logging.getLogger(__name__)
 
 
 class mStats(Tabular):
     """Class describing the table of cluster statistics output from MetaCyto"""
+
     file_ext = "metacyto_stats.txt"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """Quick test on file headings"""
         if file_prefix.startswith("fcs_files\tcluster_id\tlabel\tfcs_names"):
             header_line = file_prefix.string_io().readline()
-            if header_line.strip().split("\t")[-1] == 'fraction':
+            if header_line.strip().split("\t")[-1] == "fraction":
                 return True
             elif file_prefix.truncated and file_prefix.string_io().read() == header_line:
                 return True
         return False
 
 
 class mSummary(Tabular):
     """Class describing the summary table output by MetaCyto after FCS preprocessing"""
+
     file_ext = "metacyto_summary.txt"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        return file_prefix.startswith('study_id\tantibodies\tfilenames')
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith("study_id\tantibodies\tfilenames")
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/metadata.py` & `galaxy-data-23.0.1/galaxy/datatypes/metadata.py`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/microarrays.py` & `galaxy-data-23.0.1/galaxy/datatypes/microarrays.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,131 +1,175 @@
 import logging
+from typing import TYPE_CHECKING
 
 from galaxy.datatypes import data
 from galaxy.datatypes.binary import Cel  # noqa: F401
 from galaxy.datatypes.data import get_file_peek
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
-    get_headers
+    get_headers,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 class GenericMicroarrayFile(data.Text):
     """
     Abstract class for most of the microarray files.
     """
-    MetadataElement(name="version_number", default="1.0", desc="Version number", readonly=True, visible=True,
-                    optional=True, no_value="1.0")
-    MetadataElement(name="file_format", default="ATF", desc="File format", readonly=True, visible=True,
-                    optional=True, no_value="ATF")
-    MetadataElement(name="number_of_optional_header_records", default=1, desc="Number of optional header records",
-                    readonly=True, visible=True, optional=True, no_value=1)
-    MetadataElement(name="number_of_data_columns", default=1, desc="Number of data columns",
-                    readonly=True, visible=True,
-                    optional=True, no_value=1)
-    MetadataElement(name="file_type", default="GenePix", desc="File type",
-                    readonly=True, visible=True,
-                    optional=True, no_value="GenePix")
-    MetadataElement(name="block_count", default=1, desc="Number of blocks described in the file",
-                    readonly=True, visible=True,
-                    optional=True, no_value=1)
-    MetadataElement(name="block_type", default=0, desc="Type of block",
-                    readonly=True, visible=True,
-                    optional=True, no_value=0)
 
-    def set_peek(self, dataset):
+    MetadataElement(
+        name="version_number",
+        default="1.0",
+        desc="Version number",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="1.0",
+    )
+    MetadataElement(
+        name="file_format",
+        default="ATF",
+        desc="File format",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="ATF",
+    )
+    MetadataElement(
+        name="number_of_optional_header_records",
+        default=1,
+        desc="Number of optional header records",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=1,
+    )
+    MetadataElement(
+        name="number_of_data_columns",
+        default=1,
+        desc="Number of data columns",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=1,
+    )
+    MetadataElement(
+        name="file_type",
+        default="GenePix",
+        desc="File type",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="GenePix",
+    )
+    MetadataElement(
+        name="block_count",
+        default=1,
+        desc="Number of blocks described in the file",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=1,
+    )
+    MetadataElement(
+        name="block_type", default=0, desc="Type of block", readonly=True, visible=True, optional=True, no_value=0
+    )
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             if dataset.metadata.block_count == 1:
                 dataset.blurb = f"{dataset.metadata.file_type} {dataset.metadata.version_number}: Format {dataset.metadata.file_format}, 1 block, {dataset.metadata.number_of_optional_header_records} headers and {dataset.metadata.number_of_data_columns} columns"
             else:
                 dataset.blurb = f"{dataset.metadata.file_type} {dataset.metadata.version_number}: Format {dataset.metadata.file_format}, {dataset.metadata.block_count} blocks, {dataset.metadata.number_of_optional_header_records} headers and {dataset.metadata.number_of_data_columns} columns"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def get_mime(self):
-        return 'text/plain'
+    def get_mime(self) -> str:
+        return "text/plain"
 
 
 @build_sniff_from_prefix
 class Gal(GenericMicroarrayFile):
-    """ Gal File format described at:
-            http://mdc.custhelp.com/app/answers/detail/a_id/18883/#gal
+    """Gal File format described at:
+    http://mdc.custhelp.com/app/answers/detail/a_id/18883/#gal
     """
 
     edam_format = "format_3829"
     edam_data = "data_3110"
     file_ext = "gal"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a Gal file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('test.gal')
         >>> Gal().sniff(fname)
         True
         >>> fname = get_test_fname('test.gpr')
         >>> Gal().sniff(fname)
         False
         """
         headers = get_headers(file_prefix, sep="\t", count=3)
         return "ATF" in headers[0][0] and "GenePix ArrayList" in headers[2][0]
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set metadata for Gal file.
         """
-        super().set_meta(dataset, **kwd)
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         headers = get_headers(dataset.file_name, sep="\t", count=5)
         dataset.metadata.file_format = headers[0][0]
         dataset.metadata.version_number = headers[0][1]
         dataset.metadata.number_of_optional_header_records = int(headers[1][0])
         dataset.metadata.number_of_data_columns = int(headers[1][1])
         dataset.metadata.file_type = headers[2][0].strip().strip('"').split("=")[1]
         if "BlockCount" in headers[3][0]:
             dataset.metadata.block_count = int(headers[3][0].strip().strip('"').split("=")[1])
         if "BlockType" in headers[4][0]:
             dataset.metadata.block_type = int(headers[4][0].strip().strip('"').split("=")[1])
 
 
 @build_sniff_from_prefix
 class Gpr(GenericMicroarrayFile):
-    """ Gpr File format described at:
-            http://mdc.custhelp.com/app/answers/detail/a_id/18883/#gpr
+    """Gpr File format described at:
+    http://mdc.custhelp.com/app/answers/detail/a_id/18883/#gpr
     """
 
     edam_format = "format_3829"
     edam_data = "data_3110"
     file_ext = "gpr"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a Gpr file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('test.gpr')
         >>> Gpr().sniff(fname)
         True
         >>> fname = get_test_fname('test.gal')
         >>> Gpr().sniff(fname)
         False
         """
         headers = get_headers(file_prefix, sep="\t", count=3)
         return "ATF" in headers[0][0] and "GenePix Results" in headers[2][0]
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set metadata for Gpr file.
         """
-        super().set_meta(dataset, **kwd)
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         headers = get_headers(dataset.file_name, sep="\t", count=5)
         dataset.metadata.file_format = headers[0][0]
         dataset.metadata.version_number = headers[0][1]
         dataset.metadata.number_of_optional_header_records = int(headers[1][0])
         dataset.metadata.number_of_data_columns = int(headers[1][1])
         dataset.metadata.file_type = headers[2][0].strip().strip('"').split("=")[1]
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/molecules.py` & `galaxy-data-23.0.1/galaxy/datatypes/molecules.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,76 +1,94 @@
 import logging
 import os
 import re
+from typing import (
+    Callable,
+    Dict,
+    List,
+    TYPE_CHECKING,
+)
 
 from galaxy.datatypes import metadata
 from galaxy.datatypes.binary import Binary
 from galaxy.datatypes.data import (
     get_file_peek,
     Text,
 )
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     get_headers,
-    iter_headers
+    iter_headers,
 )
 from galaxy.datatypes.tabular import Tabular
 from galaxy.datatypes.util.generic_util import count_special_lines
 from galaxy.datatypes.xml import GenericXml
 from galaxy.util import (
     commands,
-    unicodify
+    unicodify,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 # optional import to enhance metadata
 try:
     from ase import io as ase_io
 except ImportError:
     ase_io = None
 
 log = logging.getLogger(__name__)
 
 
 def count_lines(filename, non_empty=False):
     """
     counting the number of lines from the 'filename' file
     """
     if non_empty:
-        cmd = ['grep', '-cve', r'^\s*$', filename]
+        cmd = ["grep", "-cve", r"^\s*$", filename]
     else:
-        cmd = ['wc', '-l', filename]
+        cmd = ["wc", "-l", filename]
     try:
         out = commands.execute(cmd)
     except commands.CommandLineException as e:
         log.error(unicodify(e))
         return 0
     return int(out.split()[0])
 
 
 class GenericMolFile(Text):
     """
     Abstract class for most of the molecule files.
     """
-    MetadataElement(name="number_of_molecules", default=0, desc="Number of molecules", readonly=True, visible=True, optional=True, no_value=0)
 
-    def set_peek(self, dataset):
+    MetadataElement(
+        name="number_of_molecules",
+        default=0,
+        desc="Number of molecules",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            if (dataset.metadata.number_of_molecules == 1):
+            if dataset.metadata.number_of_molecules == 1:
                 dataset.blurb = "1 molecule"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_molecules} molecules"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def get_mime(self):
-        return 'text/plain'
+    def get_mime(self) -> str:
+        return "text/plain"
 
     element_symbols = [
         "Ac",
         "Ag",
         "Al",
         "Am",
         "Ar",
@@ -182,26 +200,26 @@
         "Zr",
     ]
 
 
 class MOL(GenericMolFile):
     file_ext = "mol"
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number molecules, in the case of MOL its always one.
         """
         dataset.metadata.number_of_molecules = 1
 
 
 @build_sniff_from_prefix
 class SDF(GenericMolFile):
     file_ext = "sdf"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a SDF2 file.
 
         An SDfile (structure-data file) can contain multiple compounds.
 
         Each compound starts with a block in V2000 or V3000 molfile format,
         which ends with a line equal to 'M  END'.
@@ -220,52 +238,52 @@
         False
         """
         m_end_found = False
         limit = 10000
         idx = 0
         for line in file_prefix.line_iterator():
             idx += 1
-            line = line.rstrip('\n\r')
+            line = line.rstrip("\n\r")
             if idx < 4:
                 continue
             elif idx == 4:
                 if len(line) != 39 or not (line.endswith(" V2000") or line.endswith(" V3000")):
                     return False
             elif not m_end_found:
-                if line == 'M  END':
+                if line == "M  END":
                     m_end_found = True
-            elif line == '$$$$':
+            elif line == "$$$$":
                 return True
             if idx == limit:
                 break
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of molecules in dataset.
         """
         dataset.metadata.number_of_molecules = count_special_lines(r"^\$\$\$\$$", dataset.file_name)
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
         Split the input files by molecule records.
         """
         if split_params is None:
             return None
 
         if len(input_datasets) > 1:
             raise Exception("SD-file splitting does not support multiple files")
         input_files = [ds.file_name for ds in input_datasets]
 
         chunk_size = None
-        if split_params['split_mode'] == 'number_of_parts':
+        if split_params["split_mode"] == "number_of_parts":
             raise Exception(f"Split mode \"{split_params['split_mode']}\" is currently not implemented for SD-files.")
-        elif split_params['split_mode'] == 'to_size':
-            chunk_size = int(split_params['split_size'])
+        elif split_params["split_mode"] == "to_size":
+            chunk_size = int(split_params["split_size"])
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
         def _read_sdf_records(filename):
             lines = []
             with open(filename) as handle:
                 for line in handle:
@@ -273,82 +291,82 @@
                     if line.startswith("$$$$"):
                         yield lines
                         lines = []
 
         def _write_part_sdf_file(accumulated_lines):
             part_dir = subdir_generator_function()
             part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
-            with open(part_path, 'w') as part_file:
+            with open(part_path, "w") as part_file:
                 part_file.writelines(accumulated_lines)
 
         try:
             sdf_records = _read_sdf_records(input_files[0])
             sdf_lines_accumulated = []
             for counter, sdf_record in enumerate(sdf_records, start=1):
                 sdf_lines_accumulated.extend(sdf_record)
                 if counter % chunk_size == 0:
                     _write_part_sdf_file(sdf_lines_accumulated)
                     sdf_lines_accumulated = []
             if sdf_lines_accumulated:
                 _write_part_sdf_file(sdf_lines_accumulated)
         except Exception as e:
-            log.error('Unable to split files: %s', unicodify(e))
+            log.error("Unable to split files: %s", unicodify(e))
             raise
 
 
 @build_sniff_from_prefix
 class MOL2(GenericMolFile):
     file_ext = "mol2"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a MOL2 file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('drugbank_drugs.mol2')
         >>> MOL2().sniff(fname)
         True
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> MOL2().sniff(fname)
         False
         """
         limit = 60
         idx = 0
         for line in file_prefix.line_iterator():
-            line = line.rstrip('\n\r')
-            if line == '@<TRIPOS>MOLECULE':
+            line = line.rstrip("\n\r")
+            if line == "@<TRIPOS>MOLECULE":
                 return True
             idx += 1
             if idx == limit:
                 break
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
         dataset.metadata.number_of_molecules = count_special_lines("@<TRIPOS>MOLECULE", dataset.file_name)
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
         Split the input files by molecule records.
         """
         if split_params is None:
             return None
 
         if len(input_datasets) > 1:
             raise Exception("MOL2-file splitting does not support multiple files")
         input_files = [ds.file_name for ds in input_datasets]
 
         chunk_size = None
-        if split_params['split_mode'] == 'number_of_parts':
+        if split_params["split_mode"] == "number_of_parts":
             raise Exception(f"Split mode \"{split_params['split_mode']}\" is currently not implemented for MOL2-files.")
-        elif split_params['split_mode'] == 'to_size':
-            chunk_size = int(split_params['split_size'])
+        elif split_params["split_mode"] == "to_size":
+            chunk_size = int(split_params["split_size"])
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
         def _read_mol2_records(filename):
             lines = []
             start = True
             with open(filename) as handle:
@@ -360,351 +378,357 @@
                             yield lines
                             lines = []
                     lines.append(line)
 
         def _write_part_mol2_file(accumulated_lines):
             part_dir = subdir_generator_function()
             part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
-            with open(part_path, 'w') as part_file:
+            with open(part_path, "w") as part_file:
                 part_file.writelines(accumulated_lines)
 
         try:
             mol2_records = _read_mol2_records(input_files[0])
             mol2_lines_accumulated = []
             for counter, mol2_record in enumerate(mol2_records, start=1):
                 mol2_lines_accumulated.extend(mol2_record)
                 if counter % chunk_size == 0:
                     _write_part_mol2_file(mol2_lines_accumulated)
                     mol2_lines_accumulated = []
             if mol2_lines_accumulated:
                 _write_part_mol2_file(mol2_lines_accumulated)
         except Exception as e:
-            log.error('Unable to split files: %s', unicodify(e))
+            log.error("Unable to split files: %s", unicodify(e))
             raise
 
 
 @build_sniff_from_prefix
 class FPS(GenericMolFile):
     """
     chemfp fingerprint file: http://code.google.com/p/chem-fingerprints/wiki/FPS
     """
+
     file_ext = "fps"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a FPS file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('q.fps')
         >>> FPS().sniff(fname)
         True
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> FPS().sniff(fname)
         False
         """
-        header = get_headers(file_prefix, sep='\t', count=1)
-        if header[0][0].strip() == '#FPS1':
+        header = get_headers(file_prefix, sep="\t", count=1)
+        if header[0][0].strip() == "#FPS1":
             return True
         else:
             return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
-        dataset.metadata.number_of_molecules = count_special_lines('^#', dataset.file_name, invert=True)
+        dataset.metadata.number_of_molecules = count_special_lines("^#", dataset.file_name, invert=True)
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
         Split the input files by fingerprint records.
         """
         if split_params is None:
             return None
 
         if len(input_datasets) > 1:
             raise Exception("FPS-file splitting does not support multiple files")
         input_files = [ds.file_name for ds in input_datasets]
 
         chunk_size = None
-        if split_params['split_mode'] == 'number_of_parts':
+        if split_params["split_mode"] == "number_of_parts":
             raise Exception(f"Split mode \"{split_params['split_mode']}\" is currently not implemented for MOL2-files.")
-        elif split_params['split_mode'] == 'to_size':
-            chunk_size = int(split_params['split_size'])
+        elif split_params["split_mode"] == "to_size":
+            chunk_size = int(split_params["split_size"])
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
         def _write_part_fingerprint_file(accumulated_lines):
             part_dir = subdir_generator_function()
             part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
-            with open(part_path, 'w') as part_file:
+            with open(part_path, "w") as part_file:
                 part_file.writelines(accumulated_lines)
 
         try:
             header_lines = []
             lines_accumulated = []
             fingerprint_counter = 0
             for line in open(input_files[0]):
                 if not line.strip():
                     continue
-                if line.startswith('#'):
+                if line.startswith("#"):
                     header_lines.append(line)
                 else:
                     fingerprint_counter += 1
                     lines_accumulated.append(line)
                 if fingerprint_counter != 0 and fingerprint_counter % chunk_size == 0:
                     _write_part_fingerprint_file(header_lines + lines_accumulated)
                     lines_accumulated = []
             if lines_accumulated:
                 _write_part_fingerprint_file(header_lines + lines_accumulated)
         except Exception as e:
-            log.error('Unable to split files: %s', unicodify(e))
+            log.error("Unable to split files: %s", unicodify(e))
             raise
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """
         Merging fps files requires merging the header manually.
         We take the header from the first file.
         """
         if len(split_files) == 1:
             # For one file only, use base class method (move/copy)
             return Text.merge(split_files, output_file)
         if not split_files:
-            raise ValueError("No fps files given, %r, to merge into %s"
-                             % (split_files, output_file))
+            raise ValueError(f"No fps files given, {split_files!r}, to merge into {output_file}")
         with open(output_file, "w") as out:
             first = True
             for filename in split_files:
                 with open(filename) as handle:
                     for line in handle:
-                        if line.startswith('#'):
+                        if line.startswith("#"):
                             if first:
                                 out.write(line)
                         else:
                             # line is no header and not a comment, we assume the first header is written to out and we set 'first' to False
                             first = False
                             out.write(line)
 
 
 class OBFS(Binary):
     """OpenBabel Fastsearch format (fs)."""
-    file_ext = 'obfs'
-    composite_type = 'basic'
 
-    MetadataElement(name="base_name", default='OpenBabel Fastsearch Index',
-                    readonly=True, visible=True, optional=True,)
+    file_ext = "obfs"
+    composite_type = "basic"
+
+    MetadataElement(
+        name="base_name",
+        default="OpenBabel Fastsearch Index",
+        readonly=True,
+        visible=True,
+        optional=True,
+    )
 
     def __init__(self, **kwd):
         """
-            A Fastsearch Index consists of a binary file with the fingerprints
-            and a pointer the actual molecule file.
+        A Fastsearch Index consists of a binary file with the fingerprints
+        and a pointer the actual molecule file.
         """
         super().__init__(**kwd)
-        self.add_composite_file('molecule.fs', is_binary=True,
-                                description='OpenBabel Fastsearch Index')
-        self.add_composite_file('molecule.sdf', optional=True,
-                                is_binary=False, description='Molecule File')
-        self.add_composite_file('molecule.smi', optional=True,
-                                is_binary=False, description='Molecule File')
-        self.add_composite_file('molecule.inchi', optional=True,
-                                is_binary=False, description='Molecule File')
-        self.add_composite_file('molecule.mol2', optional=True,
-                                is_binary=False, description='Molecule File')
-        self.add_composite_file('molecule.cml', optional=True,
-                                is_binary=False, description='Molecule File')
+        self.add_composite_file("molecule.fs", is_binary=True, description="OpenBabel Fastsearch Index")
+        self.add_composite_file("molecule.sdf", optional=True, is_binary=False, description="Molecule File")
+        self.add_composite_file("molecule.smi", optional=True, is_binary=False, description="Molecule File")
+        self.add_composite_file("molecule.inchi", optional=True, is_binary=False, description="Molecule File")
+        self.add_composite_file("molecule.mol2", optional=True, is_binary=False, description="Molecule File")
+        self.add_composite_file("molecule.cml", optional=True, is_binary=False, description="Molecule File")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text."""
         if not dataset.dataset.purged:
             dataset.peek = "OpenBabel Fastsearch Index"
             dataset.blurb = "OpenBabel Fastsearch Index"
         else:
             dataset.peek = "file does not exist"
             dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "OpenBabel Fastsearch Index"
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype (pretend it is text for peek)"""
-        return 'text/plain'
+        return "text/plain"
 
-    def merge(split_files, output_file, extra_merge_args):
+    @staticmethod
+    def merge(split_files: List[str], output_file: str) -> None:
         """Merging Fastsearch indices is not supported."""
         raise NotImplementedError("Merging Fastsearch indices is not supported.")
 
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    @classmethod
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """Splitting Fastsearch indices is not supported."""
         if split_params is None:
             return None
         raise NotImplementedError("Splitting Fastsearch indices is not possible.")
 
 
 class DRF(GenericMolFile):
     file_ext = "drf"
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
-        dataset.metadata.number_of_molecules = count_special_lines('\"ligand id\"', dataset.file_name, invert=True)
+        dataset.metadata.number_of_molecules = count_special_lines('"ligand id"', dataset.file_name, invert=True)
 
 
 class PHAR(GenericMolFile):
     """
     Pharmacophore database format from silicos-it.
     """
+
     file_ext = "phar"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "pharmacophore"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class PDB(GenericMolFile):
     """
     Protein Databank format.
     http://www.wwpdb.org/documentation/format33/v3.3.html
     """
+
     file_ext = "pdb"
     MetadataElement(name="chain_ids", default=[], desc="Chain IDs", readonly=False, visible=True)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a PDB file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('5e5z.pdb')
         >>> PDB().sniff(fname)
         True
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> PDB().sniff(fname)
         False
         """
-        headers = iter_headers(file_prefix, sep=' ', count=300)
+        headers = iter_headers(file_prefix, sep=" ", count=300)
         h = t = c = s = k = e = False
         for line in headers:
             section_name = line[0].strip()
-            if section_name == 'HEADER':
+            if section_name == "HEADER":
                 h = True
-            elif section_name == 'TITLE':
+            elif section_name == "TITLE":
                 t = True
-            elif section_name == 'COMPND':
+            elif section_name == "COMPND":
                 c = True
-            elif section_name == 'SOURCE':
+            elif section_name == "SOURCE":
                 s = True
-            elif section_name == 'KEYWDS':
+            elif section_name == "KEYWDS":
                 k = True
-            elif section_name == 'EXPDTA':
+            elif section_name == "EXPDTA":
                 e = True
 
         if h * t * c * s * k * e:
             return True
         else:
             return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Find Chain_IDs for metadata.
         """
         try:
             chain_ids = set()
             with open(dataset.file_name) as fh:
                 for line in fh:
-                    if line.startswith('ATOM  ') or line.startswith('HETATM'):
-                        if line[21] != ' ':
+                    if line.startswith("ATOM  ") or line.startswith("HETATM"):
+                        if line[21] != " ":
                             chain_ids.add(line[21])
             dataset.metadata.chain_ids = list(chain_ids)
         except Exception as e:
-            log.error('Error finding chain_ids: %s', unicodify(e))
+            log.error("Error finding chain_ids: %s", unicodify(e))
             raise
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             atom_numbers = count_special_lines("^ATOM", dataset.file_name)
             hetatm_numbers = count_special_lines("^HETATM", dataset.file_name)
-            chain_ids = ','.join(dataset.metadata.chain_ids) if len(dataset.metadata.chain_ids) > 0 else 'None'
+            chain_ids = ",".join(dataset.metadata.chain_ids) if len(dataset.metadata.chain_ids) > 0 else "None"
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = f"{atom_numbers} atoms and {hetatm_numbers} HET-atoms\nchain_ids: {chain_ids}"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class PDBQT(GenericMolFile):
     """
     PDBQT Autodock and Autodock Vina format
     http://autodock.scripps.edu/faqs-help/faq/what-is-the-format-of-a-pdbqt-file
     """
+
     file_ext = "pdbqt"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a PDBQT file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('NuBBE_1_obabel_3D.pdbqt')
         >>> PDBQT().sniff(fname)
         True
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> PDBQT().sniff(fname)
         False
         """
-        headers = iter_headers(file_prefix, sep=' ', count=300)
+        headers = iter_headers(file_prefix, sep=" ", count=300)
         h = t = c = s = k = False
         for line in headers:
             section_name = line[0].strip()
-            if section_name == 'REMARK':
+            if section_name == "REMARK":
                 h = True
-            elif section_name == 'ROOT':
+            elif section_name == "ROOT":
                 t = True
-            elif section_name == 'ENDROOT':
+            elif section_name == "ENDROOT":
                 c = True
-            elif section_name == 'BRANCH':
+            elif section_name == "BRANCH":
                 s = True
-            elif section_name == 'TORSDOF':
+            elif section_name == "TORSDOF":
                 k = True
 
         if h * t * c * s * k:
             return True
         else:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             root_numbers = count_special_lines("^ROOT", dataset.file_name)
             branch_numbers = count_special_lines("^BRANCH", dataset.file_name)
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = f"{root_numbers} roots and {branch_numbers} branches"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class PQR(GenericMolFile):
     """
     Protein Databank format.
     https://apbs-pdb2pqr.readthedocs.io/en/latest/formats/pqr.html
     """
+
     file_ext = "pqr"
     MetadataElement(name="chain_ids", default=[], desc="Chain IDs", readonly=False, visible=True)
 
-    def get_matcher(self):
+    def get_matcher(self) -> re.Pattern:
         """
         Atom and HETATM line fields are space separated, match group:
           0: Field_name
               A string which specifies the type of PQR entry: ATOM or HETATM.
           1: Atom_number
               An integer which provides the atom index.
           2: Atom_name
@@ -719,82 +743,84 @@
           7: X 8: Y 9: Z
               3 floats which provide the atomic coordinates (in angstroms)
           10: Charge
               A float which provides the atomic charge (in electrons).
           11: Radius
               A float which provides the atomic radius (in angstroms).
         """
-        pat = r'(ATOM|HETATM)\s+' +\
-              r'(\d+)\s+' +\
-              r'([A-Z0-9]+)\s+' +\
-              r'([A-Z0-9]+)\s+' +\
-              r'(([A-Z]?)\s+)?' +\
-              r'([-+]?\d*\.\d+|\d+)\s+' +\
-              r'([-+]?\d*\.\d+|\d+)\s+' +\
-              r'([-+]?\d*\.\d+|\d+)\s+' +\
-              r'([-+]?\d*\.\d+|\d+)\s+' +\
-              r'([-+]?\d*\.\d+|\d+)\s+'
+        pat = (
+            r"(ATOM|HETATM)\s+"
+            + r"(\d+)\s+"
+            + r"([A-Z0-9]+)\s+"
+            + r"([A-Z0-9]+)\s+"
+            + r"(([A-Z]?)\s+)?"
+            + r"([-+]?\d*\.\d+|\d+)\s+"
+            + r"([-+]?\d*\.\d+|\d+)\s+"
+            + r"([-+]?\d*\.\d+|\d+)\s+"
+            + r"([-+]?\d*\.\d+|\d+)\s+"
+            + r"([-+]?\d*\.\d+|\d+)\s+"
+        )
         return re.compile(pat)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a PQR file.
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('5e5z.pqr')
         >>> PQR().sniff(fname)
         True
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> PQR().sniff(fname)
         False
         """
         prog = self.get_matcher()
-        headers = iter_headers(file_prefix, sep=None, comment_designator='REMARK   5', count=3000)
+        headers = iter_headers(file_prefix, sep=None, comment_designator="REMARK   5", count=3000)
         h = a = False
         for line in headers:
             section_name = line[0].strip()
-            if section_name == 'REMARK':
+            if section_name == "REMARK":
                 h = True
-            elif section_name == 'ATOM' or section_name == 'HETATM':
-                if prog.match(' '.join(line)):
+            elif section_name == "ATOM" or section_name == "HETATM":
+                if prog.match(" ".join(line)):
                     a = True
                     break
         if h * a:
             return True
         else:
             return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Find Optional Chain_IDs for metadata.
         """
         try:
             prog = self.get_matcher()
             chain_ids = set()
             with open(dataset.file_name) as fh:
                 for line in fh:
-                    if line.startswith('REMARK'):
+                    if line.startswith("REMARK"):
                         continue
                     match = prog.match(line.rstrip())
                     if match and match.groups()[5]:
                         chain_ids.add(match.groups()[5])
             dataset.metadata.chain_ids = list(chain_ids)
         except Exception as e:
-            log.error('Error finding chain_ids: %s', unicodify(e))
+            log.error("Error finding chain_ids: %s", unicodify(e))
             raise
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             atom_numbers = count_special_lines("^ATOM", dataset.file_name)
             hetatm_numbers = count_special_lines("^HETATM", dataset.file_name)
-            chain_ids = ','.join(dataset.metadata.chain_ids) if len(dataset.metadata.chain_ids) > 0 else 'None'
+            chain_ids = ",".join(dataset.metadata.chain_ids) if len(dataset.metadata.chain_ids) > 0 else "None"
             dataset.peek = get_file_peek(dataset.file_name)
-            dataset.blurb = f"{atom_numbers} atoms and {hetatm_numbers} HET-atoms\nchain_ids: {str(chain_ids)}"
+            dataset.blurb = f"{atom_numbers} atoms and {hetatm_numbers} HET-atoms\nchain_ids: {chain_ids}"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class Cell(GenericMolFile):
     """
     CASTEP CELL format.
     """
@@ -829,15 +855,15 @@
     MetadataElement(
         name="lattice_parameters",
         desc="Lattice parameters",
         readonly=True,
         visible=True,
     )
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a CASTEP CELL file.
 
         A fingerprint for CELL files is the use of %BLOCK and %ENDBLOCK to
         denote data blocks (not case sensitive).
 
         >>> from galaxy.datatypes.sniff import get_test_fname
@@ -849,61 +875,61 @@
         True
         >>> fname = get_test_fname('Si.cif')
         >>> Cell().sniff(fname)
         False
         """
         start_found = False
         for line in file_prefix.line_iterator():
-            if not start_found and line.lower().startswith('%block'):
+            if not start_found and line.lower().startswith("%block"):
                 start_found = True
-            elif start_found and line.lower().startswith('%endblock'):
+            elif start_found and line.lower().startswith("%endblock"):
                 return True
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Find Atom IDs for metadata.
         """
         self.meta_error = False
         if ase_io is None:
             # Don't have optional dependency, can't set advanced values
             return
         else:
             # enhanced metadata
             try:
                 ase_data = ase_io.read(dataset.file_name, format="castep-cell")
             except Exception as e:
-                log.warning('%s, set_meta Exception during ASE read: %s', self, unicodify(e))
+                log.warning("%s, set_meta Exception during ASE read: %s", self, unicodify(e))
                 self.meta_error = True
                 return
 
             try:
                 atom_data = [
                     str(sym) + str(pos) for sym, pos in zip(ase_data.get_chemical_symbols(), ase_data.get_positions())
                 ]
                 chemical_formula = ase_data.get_chemical_formula()
                 pbc = ase_data.get_pbc()
                 lattice_parameters = ase_data.get_cell().cellpar()
             except Exception as e:
-                log.warning('%s, set_meta Exception during ASE metadata collection: %s', self, unicodify(e))
+                log.warning("%s, set_meta Exception during ASE metadata collection: %s", self, unicodify(e))
                 self.meta_error = True
                 return
 
             # CELL file can only have one molecule
             dataset.metadata.number_of_molecules = 1
             dataset.metadata.atom_data = atom_data
             dataset.metadata.number_of_atoms = len(dataset.metadata.atom_data)
             dataset.metadata.chemical_formula = chemical_formula
             try:
                 dataset.metadata.is_periodic = bool(pbc)
             except ValueError:  # pbc is an array
                 dataset.metadata.is_periodic = bool(pbc.any())
             dataset.metadata.lattice_parameters = list(lattice_parameters)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.info = self.get_dataset_info(dataset.metadata)
             structure_string = "structure" if dataset.metadata.number_of_molecules == 1 else "structures"
             dataset.blurb = f"CASTEP CELL file containing {dataset.metadata.number_of_molecules} {structure_string}"
 
         else:
@@ -978,15 +1004,15 @@
     MetadataElement(
         name="lattice_parameters",
         desc="Lattice parameters",
         readonly=True,
         visible=True,
     )
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a CIF file.
 
         The CIF format and the Relion STAR format have a shared origin.
         Note therefore that STAR files and the STAR sniffer also use ``data_`` blocks.
         STAR files will not pass the CIF sniffer, but CIF files can pass the STAR sniffer.
 
@@ -1014,29 +1040,30 @@
                 continue
             elif line[0] == "#":  # comment so skip
                 continue
             elif line.startswith("data_"):
                 return file_prefix.search_str("_atom_site_fract_")
             else:  # line has some other content
                 return False
+        return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Find Atom IDs for metadata.
         """
         self.meta_error = False
         if ase_io is None:
             # Don't have optional dependency, can't set advanced values
             return
         else:
             # enhanced metadata
             try:
                 ase_data = ase_io.read(dataset.file_name, index=":", format="cif")
             except Exception as e:
-                log.warning('%s, set_meta Exception during ASE read: %s', self, unicodify(e))
+                log.warning("%s, set_meta Exception during ASE read: %s", self, unicodify(e))
                 self.meta_error = True
                 return
 
             atom_data = []
             chemical_formula = []
             is_periodic = []
             lattice_parameters = []
@@ -1050,26 +1077,26 @@
                     try:
                         p = bool(pbc)
                     except ValueError:  # pbc is an array
                         p = bool(pbc.any())
                     is_periodic.append(p)
                     lattice_parameters.append(list(block.get_cell().cellpar()))
             except Exception as e:
-                log.warning('%s, set_meta Exception during ASE metadata collection: %s', self, unicodify(e))
+                log.warning("%s, set_meta Exception during ASE metadata collection: %s", self, unicodify(e))
                 self.meta_error = True
                 return
 
             dataset.metadata.number_of_molecules = len(ase_data)
             dataset.metadata.atom_data = atom_data
             dataset.metadata.number_of_atoms = [len(atoms) for atoms in dataset.metadata.atom_data]
             dataset.metadata.chemical_formula = chemical_formula
             dataset.metadata.is_periodic = is_periodic
             dataset.metadata.lattice_parameters = list(lattice_parameters)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.info = self.get_dataset_info(dataset.metadata)
             structure_string = "structure" if dataset.metadata.number_of_molecules == 1 else "structures"
             dataset.blurb = f"CIF file containing {dataset.metadata.number_of_molecules} {structure_string}"
 
         else:
@@ -1142,15 +1169,15 @@
     MetadataElement(
         name="lattice_parameters",
         desc="Lattice parameters",
         readonly=True,
         visible=True,
     )
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a XYZ file.
 
         XYZ has no fingerprint phrases, so the whole prefix must be checked
         for the correct structure. If the prefix passes, assume the whole file
         passes.
 
@@ -1172,15 +1199,15 @@
 
         try:
             self.read_blocks(list(file_prefix.line_iterator()))
             return True
         except (TypeError, ValueError, IndexError):
             return False
 
-    def read_blocks(self, lines):
+    def read_blocks(self, lines: List) -> List:
         """
         Parses and returns a list of dictionaries representing XYZ structure blocks (aka frames).
 
         Raises IndexError, TypeError, ValueError
         """
         # remove trailing blank lines
         # blank lines not permitted elsewhere in file except for designated comment lines
@@ -1212,29 +1239,29 @@
                 if "pop from empty list" in str(e) and blocks:
                     # we'll require at least one valid block
                     pass
                 else:
                     raise
         return blocks
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Find Atom IDs for metadata.
         """
         self.meta_error = False
         if ase_io is None:
             # Don't have optional dependency, can't set advanced values
             return
         else:
             # enhanced metadata
             try:
                 # ASE recommend always parsing as extended xyz
                 ase_data = ase_io.read(dataset.file_name, index=":", format="extxyz")
             except Exception as e:
-                log.warning('%s, set_meta Exception during ASE read: %s', self, unicodify(e))
+                log.warning("%s, set_meta Exception during ASE read: %s", self, unicodify(e))
                 self.meta_error = True
                 return
 
             atom_data = []
             chemical_formula = []
             is_periodic = []
             lattice_parameters = []
@@ -1248,26 +1275,26 @@
                     try:
                         p = bool(pbc)
                     except ValueError:  # pbc is an array
                         p = bool(pbc.any())
                     is_periodic.append(p)
                     lattice_parameters.append(list(block.get_cell().cellpar()))
             except Exception as e:
-                log.warning('%s, set_meta Exception during ASE metadata collection: %s', self, unicodify(e))
+                log.warning("%s, set_meta Exception during ASE metadata collection: %s", self, unicodify(e))
                 self.meta_error = True
                 return
 
             dataset.metadata.number_of_molecules = len(ase_data)
             dataset.metadata.atom_data = atom_data
             dataset.metadata.number_of_atoms = [len(atoms) for atoms in dataset.metadata.atom_data]
             dataset.metadata.chemical_formula = chemical_formula
             dataset.metadata.is_periodic = is_periodic
             dataset.metadata.lattice_parameters = list(lattice_parameters)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.info = self.get_dataset_info(dataset.metadata)
             structure_string = "structure" if dataset.metadata.number_of_molecules == 1 else "structures"
             dataset.blurb = f"XYZ file containing {dataset.metadata.number_of_molecules} {structure_string}"
         else:
             dataset.peek = "file does not exist"
@@ -1309,15 +1336,15 @@
     Extended XYZ format.
 
     Uses specification from https://github.com/libAtoms/extxyz.
     """
 
     file_ext = "extxyz"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is an Extended XYZ file.
 
         XYZ files will not pass the ExtendedXYZ sniffer, but ExtendedXYZ files can pass the XYZ sniffer.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('Si.extxyz')
@@ -1336,15 +1363,15 @@
             comment = next(line_iterator)
             properties = re.search(r"Properties=\"?([a-zA-Z0-9:]+)\"?", comment)  # returns None if no match
             return bool(properties)
         except StopIteration:
             # insufficient lines
             return False
 
-    def read_blocks(self, lines):
+    def read_blocks(self, lines: List) -> List:
         """
         Parses and returns a list of XYZ structure blocks (aka frames).
 
         Raises IndexError, TypeError, ValueError
         """
         # remove trailing blank lines
         # blank lines not permitted elsewhere in file except for designated comment lines
@@ -1359,31 +1386,31 @@
             atoms = []
 
             n_atoms = int(lines.pop(0))
             comment = lines.pop(0)
             # extract column properties
             # these have the format 'Properties="<name>:<type>:<number>:..."' in the comment line
             # e.g. Properties="species:S:1:pos:R:3:vel:R:3:select:I:1"
-            properties = re.search(r"Properties=\"?([a-zA-Z0-9:]+)\"?", comment)
-            if properties is None:  # re.search returned None
+            properties_matches = re.search(r"Properties=\"?([a-zA-Z0-9:]+)\"?", comment)
+            if properties_matches is None:  # re.search returned None
                 raise ValueError(f"Could not find column properties in line: {comment}")
-            properties = [s.split(":") for s in re.findall(r"[a-zA-Z]+:[SIRL]:[0-9]+", properties.group(1))]
-            total_columns = sum([int(s[2]) for s in properties])
+            properties = [s.split(":") for s in re.findall(r"[a-zA-Z]+:[SIRL]:[0-9]+", properties_matches.group(1))]
+            total_columns = sum(int(s[2]) for s in properties)
 
             for _ in range(n_atoms):
                 atom_dict = {}
                 atom = lines.pop(0)
                 atom = atom.split()
                 if len(atom) != total_columns:
                     raise ValueError(f"Expected {total_columns} columns but found {len(atom)}: {atom}")
                 index = 0
                 # parse atom data according to column format specified by the properties
                 # this processing will raise errors if the format is incorrect
                 for property in properties:
-                    to_process = atom[index:index + int(property[2])]
+                    to_process = atom[index : index + int(property[2])]
                     property_name = property[0]
                     property_type = property[1]
                     for i in to_process:
                         if property_type == "S":  # string
                             # check that any element symbols are correct, otherwise ignore strings
                             if property_name.lower() == "species":
                                 symbol = i.lower().capitalize()
@@ -1414,203 +1441,244 @@
                     index += int(property[2])
 
                 atoms.append(atom_dict)
 
             blocks.append({"number_of_atoms": n_atoms, "comment": comment, "atom_data": atoms})
         return blocks
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         super().set_peek(dataset)
         dataset.blurb = "Extended " + dataset.blurb
 
 
 class grd(Text):
     file_ext = "grd"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "grids for docking"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class grdtgz(Binary):
     file_ext = "grd.tgz"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = 'binary data'
+            dataset.peek = "binary data"
             dataset.blurb = "compressed grids for docking"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class InChI(Tabular):
     file_ext = "inchi"
-    column_names = ['InChI']
+    column_names = ["InChI"]
     MetadataElement(name="columns", default=2, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=['str'], param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False)
-    MetadataElement(name="number_of_molecules", default=0, desc="Number of molecules", readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="column_types",
+        default=["str"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+    )
+    MetadataElement(
+        name="number_of_molecules",
+        default=0,
+        desc="Number of molecules",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
         dataset.metadata.number_of_molecules = self.count_data_lines(dataset)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            if (dataset.metadata.number_of_molecules == 1):
+            if dataset.metadata.number_of_molecules == 1:
                 dataset.blurb = "1 molecule"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_molecules} molecules"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a InChI file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('drugbank_drugs.inchi')
         >>> InChI().sniff(fname)
         True
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> InChI().sniff(fname)
         False
         """
-        inchi_lines = iter_headers(file_prefix, sep=' ', count=10)
+        inchi_lines = iter_headers(file_prefix, sep=" ", count=10)
         found_lines = False
         for inchi in inchi_lines:
-            if not inchi[0].startswith('InChI='):
+            if not inchi[0].startswith("InChI="):
                 return False
             found_lines = True
         return found_lines
 
 
 class SMILES(Tabular):
     # It is hard or impossible to sniff a SMILES File. We can try to import the
     # first SMILES and check if it is a molecule, but currently it is not
     # possible to use external libraries in datatype definition files.
     # Moreover it seems impossible to include OpenBabel as Python library
     # because OpenBabel is GPL licensed.
     file_ext = "smi"
-    column_names = ['SMILES', 'TITLE']
+    column_names = ["SMILES", "TITLE"]
     MetadataElement(name="columns", default=2, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=['str', 'str'], param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False)
-    MetadataElement(name="number_of_molecules", default=0, desc="Number of molecules", readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="column_types",
+        default=["str", "str"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+    )
+    MetadataElement(
+        name="number_of_molecules",
+        default=0,
+        desc="Number of molecules",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
         dataset.metadata.number_of_molecules = self.count_data_lines(dataset)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             if dataset.metadata.number_of_molecules == 1:
                 dataset.blurb = "1 molecule"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_molecules} molecules"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class CML(GenericXml):
     """
     Chemical Markup Language
     http://cml.sourceforge.net/
     """
+
     file_ext = "cml"
-    MetadataElement(name="number_of_molecules", default=0, desc="Number of molecules", readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="number_of_molecules",
+        default=0,
+        desc="Number of molecules",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of lines of data in dataset.
         """
-        dataset.metadata.number_of_molecules = count_special_lines(r'^\s*<molecule', dataset.file_name)
+        dataset.metadata.number_of_molecules = count_special_lines(r"^\s*<molecule", dataset.file_name)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            if (dataset.metadata.number_of_molecules == 1):
+            if dataset.metadata.number_of_molecules == 1:
                 dataset.blurb = "1 molecule"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_molecules} molecules"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a CML file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('interval.interval')
         >>> CML().sniff(fname)
         False
         >>> fname = get_test_fname('drugbank_drugs.cml')
         >>> CML().sniff(fname)
         True
         """
-        for expected_string in ['<?xml version="1.0"?>', 'http://www.xml-cml.org/schema']:
+        for expected_string in ['<?xml version="1.0"?>', "http://www.xml-cml.org/schema"]:
             if expected_string not in file_prefix.contents_header:
                 return False
 
         return True
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
         Split the input files by molecule records.
         """
         if split_params is None:
             return None
 
         if len(input_datasets) > 1:
             raise Exception("CML-file splitting does not support multiple files")
         input_files = [ds.file_name for ds in input_datasets]
 
         chunk_size = None
-        if split_params['split_mode'] == 'number_of_parts':
+        if split_params["split_mode"] == "number_of_parts":
             raise Exception(f"Split mode \"{split_params['split_mode']}\" is currently not implemented for CML-files.")
-        elif split_params['split_mode'] == 'to_size':
-            chunk_size = int(split_params['split_size'])
+        elif split_params["split_mode"] == "to_size":
+            chunk_size = int(split_params["split_size"])
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
         def _read_cml_records(filename):
             lines = []
             with open(filename) as handle:
                 for line in handle:
-                    if line.lstrip().startswith('<?xml version="1.0"?>') or \
-                       line.lstrip().startswith('<cml xmlns="http://www.xml-cml.org/schema') or \
-                       line.lstrip().startswith('</cml>'):
+                    if (
+                        line.lstrip().startswith('<?xml version="1.0"?>')
+                        or line.lstrip().startswith('<cml xmlns="http://www.xml-cml.org/schema')
+                        or line.lstrip().startswith("</cml>")
+                    ):
                         continue
                     lines.append(line)
-                    if line.lstrip().startswith('</molecule>'):
+                    if line.lstrip().startswith("</molecule>"):
                         yield lines
                         lines = []
 
         header_lines = ['<?xml version="1.0"?>\n', '<cml xmlns="http://www.xml-cml.org/schema">\n']
-        footer_line = ['</cml>\n']
+        footer_line = ["</cml>\n"]
 
         def _write_part_cml_file(accumulated_lines):
             part_dir = subdir_generator_function()
             part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
-            with open(part_path, 'w') as part_file:
+            with open(part_path, "w") as part_file:
                 part_file.writelines(header_lines)
                 part_file.writelines(accumulated_lines)
                 part_file.writelines(footer_line)
 
         try:
             cml_records = _read_cml_records(input_files[0])
             cml_lines_accumulated = []
@@ -1618,28 +1686,27 @@
                 cml_lines_accumulated.extend(cml_record)
                 if counter % chunk_size == 0:
                     _write_part_cml_file(cml_lines_accumulated)
                     cml_lines_accumulated = []
             if cml_lines_accumulated:
                 _write_part_cml_file(cml_lines_accumulated)
         except Exception as e:
-            log.error('Unable to split files: %s', unicodify(e))
+            log.error("Unable to split files: %s", unicodify(e))
             raise
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """
         Merging CML files.
         """
         if len(split_files) == 1:
             # For one file only, use base class method (move/copy)
             return Text.merge(split_files, output_file)
         if not split_files:
-            raise ValueError("Given no CML files, %r, to merge into %s"
-                             % (split_files, output_file))
+            raise ValueError(f"Given no CML files, {split_files!r}, to merge into {output_file}")
         with open(output_file, "w") as out:
             for filename in split_files:
                 with open(filename) as handle:
                     header = handle.readline()
                     if not header:
                         raise ValueError(f"CML file {filename} was empty")
                     if not header.lstrip().startswith('<?xml version="1.0"?>'):
@@ -1649,53 +1716,57 @@
                     header += line
                     if not line.lstrip().startswith('<cml xmlns="http://www.xml-cml.org/schema'):
                         out.write(header)
                         raise ValueError(f"{filename} is not a CML file!")
                     molecule_found = False
                     for line in handle.readlines():
                         # We found two required header lines, the next line should start with <molecule >
-                        if line.lstrip().startswith('</cml>'):
+                        if line.lstrip().startswith("</cml>"):
                             continue
-                        if line.lstrip().startswith('<molecule'):
+                        if line.lstrip().startswith("<molecule"):
                             molecule_found = True
                         if molecule_found:
                             out.write(line)
             out.write("</cml>\n")
 
 
 class GRO(GenericMolFile):
     """
     GROMACS structure format.
     https://manual.gromacs.org/current/reference-manual/file-formats.html#gro
     """
+
     file_ext = "gro"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Try to guess if the file is a GRO file.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('5e5z.gro')
         >>> GRO().sniff_prefix(fname)
         True
         >>> fname = get_test_fname('5e5z.pdb')
         >>> GRO().sniff_prefix(fname)
         False
         """
-        headers = get_headers(file_prefix, sep='\n', count=300)
+        headers = get_headers(file_prefix, sep="\n", count=300)
         try:
             int(headers[1][0])  # the second line should just be the number of atoms
         except ValueError:
             return False
         for line in headers[2:-1]:  # skip the first, second and last lines
-            if not re.search(r'^[0-9 ]{5}[a-zA-Z0-9 ]{10}[0-9 ]{5}[0-9 -]{4}\.[0-9]{3}[0-9 -]{4}\.[0-9]{3}[0-9 -]{4}\.[0-9]{3}', line[0]):
+            if not re.search(
+                r"^[0-9 ]{5}[a-zA-Z0-9 ]{10}[0-9 ]{5}[0-9 -]{4}\.[0-9]{3}[0-9 -]{4}\.[0-9]{3}[0-9 -]{4}\.[0-9]{3}",
+                line[0],
+            ):
                 return False
         return True
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
-            atom_number = int(dataset.peek.split('\n')[1])
+            atom_number = int(dataset.peek.split("\n")[1])
             dataset.blurb = f"{atom_number} atoms"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/mothur.py` & `galaxy-data-23.0.1/galaxy/datatypes/mothur.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,46 @@
 """
 Mothur Metagenomics Datatypes
 """
 import logging
 import re
-import sys
+from typing import (
+    List,
+    Optional,
+    TYPE_CHECKING,
+)
 
 from galaxy.datatypes.data import Text
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     get_headers,
-    iter_headers
+    iter_headers,
 )
 from galaxy.datatypes.tabular import Tabular
 from galaxy.util import unicodify
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class Otu(Text):
-    file_ext = 'mothur.otu'
+    file_ext = "mothur.otu"
     MetadataElement(name="columns", default=0, desc="Number of columns", readonly=True, visible=True, no_value=0)
     MetadataElement(name="labels", default=[], desc="Label Names", readonly=True, visible=True, no_value=[])
     MetadataElement(name="otulabels", default=[], desc="OTU Names", readonly=True, visible=True, no_value=[])
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set metadata for Otu files.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> from galaxy.util.bunch import Bunch
         >>> dataset = Bunch()
         >>> dataset.metadata = Bunch
@@ -53,51 +60,51 @@
         if dataset.has_data():
             label_names = set()
             otulabel_names = set()
             ncols = 0
             data_lines = 0
             comment_lines = 0
 
-            headers = iter_headers(dataset.file_name, sep='\t', count=-1)
-            first_line = get_headers(dataset.file_name, sep='\t', count=1)
+            headers = iter_headers(dataset.file_name, sep="\t", count=-1)
+            first_line = get_headers(dataset.file_name, sep="\t", count=1)
             if first_line:
                 first_line = first_line[0]
             # set otulabels
             if len(first_line) > 2:
                 otulabel_names = first_line[2:]
             # set label names and number of lines
             for line in headers:
-                if len(line) >= 2 and not line[0].startswith('@'):
+                if len(line) >= 2 and not line[0].startswith("@"):
                     data_lines += 1
                     ncols = max(ncols, len(line))
                     label_names.add(line[0])
                 else:
                     comment_lines += 1
             # Set the discovered metadata values for the dataset
             dataset.metadata.data_lines = data_lines
             dataset.metadata.columns = ncols
             dataset.metadata.labels = sorted(label_names)
             dataset.metadata.otulabels = sorted(otulabel_names)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is otu (operational taxonomic unit) format
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.otu' )
         >>> Otu().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.otu' )
         >>> Otu().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 if len(line) < 2:
                     return False
                 if count >= 1:
                     try:
                         check = int(line[1])
                         if check + 2 != len(line):
                             return False
@@ -107,42 +114,42 @@
         if count > 2:
             return True
 
         return False
 
 
 class Sabund(Otu):
-    file_ext = 'mothur.sabund'
+    file_ext = "mothur.sabund"
 
     def __init__(self, **kwd):
         """
         http://www.mothur.org/wiki/Sabund_file
         """
         super().__init__(**kwd)
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         super().init_meta(dataset, copy_from=copy_from)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is otu (operational taxonomic unit) format
         label<TAB>count[<TAB>value(1..n)]
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.sabund' )
         >>> Sabund().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.sabund' )
         >>> Sabund().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 if len(line) < 2:
                     return False
                 try:
                     check = int(line[1])
                     if check + 2 != len(line):
                         return False
                     for i in range(2, len(line)):
@@ -153,37 +160,37 @@
         if count > 0:
             return True
 
         return False
 
 
 class GroupAbund(Otu):
-    file_ext = 'mothur.shared'
+    file_ext = "mothur.shared"
     MetadataElement(name="groups", default=[], desc="Group Names", readonly=True, visible=True, no_value=[])
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         super().init_meta(dataset, copy_from=copy_from)
 
-    def set_meta(self, dataset, overwrite=True, skip=1, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, skip: Optional[int] = 1, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
         # See if file starts with header line
         if dataset.has_data():
             label_names = set()
             group_names = set()
             data_lines = 0
             comment_lines = 0
             ncols = 0
 
-            headers = iter_headers(dataset.file_name, sep='\t', count=-1)
+            headers = iter_headers(dataset.file_name, sep="\t", count=-1)
             for line in headers:
-                if line[0] == 'label' and line[1] == 'Group':
+                if line[0] == "label" and line[1] == "Group":
                     skip = 1
                     comment_lines += 1
                 else:
                     skip = 0
                     data_lines += 1
                     ncols = max(ncols, len(line))
                     label_names.add(line[0])
@@ -192,36 +199,36 @@
             # Set the discovered metadata values for the dataset
             dataset.metadata.data_lines = data_lines
             dataset.metadata.columns = ncols
             dataset.metadata.labels = sorted(label_names)
             dataset.metadata.groups = sorted(group_names)
             dataset.metadata.skip = skip
 
-    def sniff_prefix(self, file_prefix: FilePrefix, vals_are_int=False):
+    def sniff_prefix(self, file_prefix: FilePrefix, vals_are_int=False) -> bool:
         """
         Determines whether the file is a otu (operational taxonomic unit)
         Shared format
         label<TAB>group<TAB>count[<TAB>value(1..n)]
         The first line is column headings as of Mothur v 1.2
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.shared' )
         >>> GroupAbund().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.shared' )
         >>> GroupAbund().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 if len(line) < 3:
                     return False
-                if count > 0 or line[0] != 'label':
+                if count > 0 or line[0] != "label":
                     try:
                         check = int(line[2])
                         if check + 3 != len(line):
                             return False
                         for i in range(3, len(line)):
                             if vals_are_int:
                                 int(line[i])
@@ -233,37 +240,37 @@
         if count > 1:
             return True
         return False
 
 
 @build_sniff_from_prefix
 class SecondaryStructureMap(Tabular):
-    file_ext = 'mothur.map'
+    file_ext = "mothur.map"
 
     def __init__(self, **kwd):
         """Initialize secondary structure map datatype"""
         super().__init__(**kwd)
-        self.column_names = ['Map']
+        self.column_names = ["Map"]
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a secondary structure map format
         A single column with an integer value which indicates the row that this
         row maps to. Check to make sure if structMap[10] = 380 then
         structMap[380] = 10 and vice versa.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.map' )
         >>> SecondaryStructureMap().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.map' )
         >>> SecondaryStructureMap().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         line_num = 0
         rowidxmap = {}
         for line in headers:
             line_num += 1
             if len(line) > 1:
                 return False
             try:
@@ -277,83 +284,106 @@
                 return False
         if line_num < 3:
             return False
         return True
 
 
 class AlignCheck(Tabular):
-    file_ext = 'mothur.align.check'
+    file_ext = "mothur.align.check"
 
     def __init__(self, **kwd):
         """Initialize AlignCheck datatype"""
         super().__init__(**kwd)
-        self.column_names = ['name', 'pound', 'dash', 'plus', 'equal', 'loop', 'tilde', 'total']
-        self.column_types = ['str', 'int', 'int', 'int', 'int', 'int', 'int', 'int']
+        self.column_names = ["name", "pound", "dash", "plus", "equal", "loop", "tilde", "total"]
+        self.column_types = ["str", "int", "int", "int", "int", "int", "int", "int"]
         self.comment_lines = 1
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
         dataset.metadata.column_names = self.column_names
         dataset.metadata.column_types = self.column_types
         dataset.metadata.comment_lines = self.comment_lines
         if isinstance(dataset.metadata.data_lines, int):
             dataset.metadata.data_lines -= self.comment_lines
 
 
 class AlignReport(Tabular):
     """
     QueryName	QueryLength	TemplateName	TemplateLength	SearchMethod	SearchScore	AlignmentMethod	QueryStart	QueryEnd	TemplateStart	TemplateEnd	PairwiseAlignmentLength	GapsInQuery	GapsInTemplate	LongestInsert	SimBtwnQuery&Template
     AY457915	501		82283		1525		kmer		89.07		needleman	5		501		1		499		499			2		0		0		97.6
     """
-    file_ext = 'mothur.align.report'
+
+    file_ext = "mothur.align.report"
 
     def __init__(self, **kwd):
         """Initialize AlignCheck datatype"""
         super().__init__(**kwd)
-        self.column_names = ['QueryName', 'QueryLength', 'TemplateName', 'TemplateLength', 'SearchMethod', 'SearchScore',
-                             'AlignmentMethod', 'QueryStart', 'QueryEnd', 'TemplateStart', 'TemplateEnd',
-                             'PairwiseAlignmentLength', 'GapsInQuery', 'GapsInTemplate', 'LongestInsert', 'SimBtwnQuery&Template'
-                             ]
+        self.column_names = [
+            "QueryName",
+            "QueryLength",
+            "TemplateName",
+            "TemplateLength",
+            "SearchMethod",
+            "SearchScore",
+            "AlignmentMethod",
+            "QueryStart",
+            "QueryEnd",
+            "TemplateStart",
+            "TemplateEnd",
+            "PairwiseAlignmentLength",
+            "GapsInQuery",
+            "GapsInTemplate",
+            "LongestInsert",
+            "SimBtwnQuery&Template",
+        ]
 
 
 class DistanceMatrix(Text):
-    file_ext = 'mothur.dist'
+    file_ext = "mothur.dist"
 
-    MetadataElement(name="sequence_count", default=0, desc="Number of sequences", readonly=True, visible=True, optional=True, no_value='?')
+    MetadataElement(
+        name="sequence_count",
+        default=0,
+        desc="Number of sequences",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="?",
+    )
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         super().init_meta(dataset, copy_from=copy_from)
 
-    def set_meta(self, dataset, overwrite=True, skip=0, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, skip: Optional[int] = 0, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, skip=skip, **kwd)
 
-        headers = iter_headers(dataset.file_name, sep='\t')
+        headers = iter_headers(dataset.file_name, sep="\t")
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 try:
-                    dataset.metadata.sequence_count = int(''.join(line))  # seq count sometimes preceded by tab
+                    dataset.metadata.sequence_count = int("".join(line))  # seq count sometimes preceded by tab
                     break
                 except Exception as e:
                     if not isinstance(self, PairwiseDistanceMatrix):
                         log.warning(f"DistanceMatrix set_meta {e}")
 
 
 @build_sniff_from_prefix
 class LowerTriangleDistanceMatrix(DistanceMatrix):
-    file_ext = 'mothur.lower.dist'
+    file_ext = "mothur.lower.dist"
 
     def __init__(self, **kwd):
         """Initialize secondary structure map datatype"""
         super().__init__(**kwd)
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         super().init_meta(dataset, copy_from=copy_from)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a lower-triangle distance matrix (phylip) format
         The first line has the number of sequences in the matrix.
         The remaining lines have the sequence name followed by a list of distances from all preceeding sequences
 
                 5  # possibly but not always preceded by a tab :/
                 U68589
@@ -367,25 +397,25 @@
         >>> LowerTriangleDistanceMatrix().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.lower.dist' )
         >>> LowerTriangleDistanceMatrix().sniff( fname )
         False
         """
         numlines = 300
-        headers = iter_headers(file_prefix, sep='\t', count=numlines)
+        headers = iter_headers(file_prefix, sep="\t", count=numlines)
         line_num = 0
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 # first line should contain the number of sequences in the file
                 if line_num == 0:
                     if len(line) > 2:
                         return False
                     else:
                         try:
-                            sequence_count = int(''.join(line))
+                            sequence_count = int("".join(line))
                             assert sequence_count > 0
                         except ValueError:
                             return False
                 else:
                     # number of fields should equal the line number
                     if len(line) != (line_num):
                         return False
@@ -402,23 +432,23 @@
             return True
 
         return False
 
 
 @build_sniff_from_prefix
 class SquareDistanceMatrix(DistanceMatrix):
-    file_ext = 'mothur.square.dist'
+    file_ext = "mothur.square.dist"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         super().init_meta(dataset, copy_from=copy_from)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a square distance matrix (Column-formatted distance matrix) format
         The first line has the number of sequences in the matrix.
         The following lines have the sequence name in the first column plus a column for the distance to each sequence
         in the row order in which they appear in the matrix.
 
                3
@@ -431,24 +461,24 @@
         >>> SquareDistanceMatrix().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.square.dist' )
         >>> SquareDistanceMatrix().sniff( fname )
         False
         """
         numlines = 300
-        headers = iter_headers(file_prefix, sep='\t', count=numlines)
+        headers = iter_headers(file_prefix, sep="\t", count=numlines)
         line_num = 0
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 if line_num == 0:
                     if len(line) > 2:
                         return False
                     else:
                         try:
-                            sequence_count = int(''.join(line))
+                            sequence_count = int("".join(line))
                             assert sequence_count > 0
                         except ValueError:
                             return False
                 else:
                     # number of fields should equal the number of sequences
                     if len(line) != sequence_count + 1:
                         return False
@@ -465,43 +495,43 @@
             return True
 
         return False
 
 
 @build_sniff_from_prefix
 class PairwiseDistanceMatrix(DistanceMatrix, Tabular):
-    file_ext = 'mothur.pair.dist'
+    file_ext = "mothur.pair.dist"
 
     def __init__(self, **kwd):
         """Initialize secondary structure map datatype"""
         super().__init__(**kwd)
-        self.column_names = ['Sequence', 'Sequence', 'Distance']
-        self.column_types = ['str', 'str', 'float']
+        self.column_names = ["Sequence", "Sequence", "Distance"]
+        self.column_types = ["str", "str", "float"]
 
-    def set_meta(self, dataset, overwrite=True, skip=None, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, skip: Optional[int] = None, **kwd) -> None:
         super().set_meta(dataset, overwrite=overwrite, skip=skip, **kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a pairwise distance matrix (Column-formatted distance matrix) format
         The first and second columns have the sequence names and the third column is the distance between those sequences.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.pair.dist' )
         >>> PairwiseDistanceMatrix().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.pair.dist' )
         >>> PairwiseDistanceMatrix().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         names = [False, False]
         for line in headers:
-            if line[0].startswith('@'):
+            if line[0].startswith("@"):
                 continue
             if len(line) != 3:
                 return False
             # check if col3 contains distances (floats)
             try:
                 float(line[2])
                 try:
@@ -526,116 +556,123 @@
         if count > 2:
             return not all_ints
 
         return False
 
 
 class Names(Tabular):
-    file_ext = 'mothur.names'
+    file_ext = "mothur.names"
 
     def __init__(self, **kwd):
         """
         http://www.mothur.org/wiki/Name_file
         Name file shows the relationship between a representative sequence(col 1)  and the sequences(comma-separated) it represents(col 2)
         """
         super().__init__(**kwd)
-        self.column_names = ['name', 'representatives']
+        self.column_names = ["name", "representatives"]
         self.columns = 2
 
 
 class Summary(Tabular):
-    file_ext = 'mothur.summary'
+    file_ext = "mothur.summary"
 
     def __init__(self, **kwd):
         """summarizes the quality of sequences in an unaligned or aligned fasta-formatted sequence file"""
         super().__init__(**kwd)
-        self.column_names = ['seqname', 'start', 'end', 'nbases', 'ambigs', 'polymer']
+        self.column_names = ["seqname", "start", "end", "nbases", "ambigs", "polymer"]
         self.columns = 6
 
 
 class Group(Tabular):
-    file_ext = 'mothur.groups'
+    file_ext = "mothur.groups"
     MetadataElement(name="groups", default=[], desc="Group Names", readonly=True, visible=True, no_value=[])
 
     def __init__(self, **kwd):
         """
         http://www.mothur.org/wiki/Groups_file
         Group file assigns sequence (col 1)  to a group (col 2)
         """
         super().__init__(**kwd)
-        self.column_names = ['name', 'group']
+        self.column_names = ["name", "group"]
         self.columns = 2
 
-    def set_meta(self, dataset, overwrite=True, skip=None, max_data_lines=None, **kwd):
-        super().set_meta(dataset, overwrite, skip, max_data_lines)
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = None,
+        max_data_lines: Optional[int] = None,
+        **kwd,
+    ) -> None:
+        super().set_meta(dataset, overwrite=overwrite, skip=skip, max_data_lines=max_data_lines, **kwd)
 
         group_names = set()
-        headers = iter_headers(dataset.file_name, sep='\t', count=-1)
+        headers = iter_headers(dataset.file_name, sep="\t", count=-1)
         for line in headers:
             if len(line) > 1:
                 group_names.add(line[1])
         dataset.metadata.groups = list(group_names)
 
 
 class AccNos(Tabular):
-    file_ext = 'mothur.accnos'
+    file_ext = "mothur.accnos"
 
     def __init__(self, **kwd):
         """A list of names"""
         super().__init__(**kwd)
-        self.column_names = ['name']
+        self.column_names = ["name"]
         self.columns = 1
 
 
 @build_sniff_from_prefix
 class Oligos(Text):
-    file_ext = 'mothur.oligos'
+    file_ext = "mothur.oligos"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         http://www.mothur.org/wiki/Oligos_File
         Determines whether the file is a otu (operational taxonomic unit) format
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.oligos' )
         >>> Oligos().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.oligos' )
         >>> Oligos().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         for line in headers:
-            if not line[0].startswith('@') and not line[0].startswith('#'):
-                if len(line) == 2 and line[0] in ['forward', 'reverse']:
+            if not line[0].startswith("@") and not line[0].startswith("#"):
+                if len(line) == 2 and line[0] in ["forward", "reverse"]:
                     count += 1
                     continue
-                elif len(line) == 3 and line[0] == 'barcode':
+                elif len(line) == 3 and line[0] == "barcode":
                     count += 1
                     continue
                 else:
                     return False
         if count > 0:
             return True
 
         return False
 
 
 @build_sniff_from_prefix
 class Frequency(Tabular):
-    file_ext = 'mothur.freq'
+    file_ext = "mothur.freq"
 
     def __init__(self, **kwd):
         """A list of names"""
         super().__init__(**kwd)
-        self.column_names = ['position', 'frequency']
-        self.column_types = ['int', 'float']
+        self.column_names = ["position", "frequency"]
+        self.column_types = ["int", "float"]
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a frequency tabular format for chimera analysis
 
         .. code-block::
 
             #1.14.0
             0	0.000
@@ -651,56 +688,70 @@
         >>> Frequency().sniff( fname )
         False
         >>> # Expression count matrix (EdgeR wrapper)
         >>> fname = get_test_fname( 'mothur_datatypetest_false_2.mothur.freq' )
         >>> Frequency().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         for line in headers:
-            if not line[0].startswith('@'):
+            if not line[0].startswith("@"):
                 # first line should be #<version string>
                 if count == 0:
-                    if not line[0].startswith('#') or len(line) != 1:
+                    if not line[0].startswith("#") or len(line) != 1:
                         return False
 
                 else:
                     # all other lines should be <int> <float>
                     if len(line) != 2:
                         return False
                     try:
                         int(line[0])
                         float(line[1])
 
-                        if line[1].find('.') == -1:
+                        if line[1].find(".") == -1:
                             return False
                     except Exception:
                         return False
                 count += 1
 
         if count > 1:
             return True
 
         return False
 
 
 @build_sniff_from_prefix
 class Quantile(Tabular):
-    file_ext = 'mothur.quan'
-    MetadataElement(name="filtered", default=False, no_value=False, optional=True, desc="Quantiles calculated using a mask", readonly=True)
-    MetadataElement(name="masked", default=False, no_value=False, optional=True, desc="Quantiles calculated using a frequency filter", readonly=True)
+    file_ext = "mothur.quan"
+    MetadataElement(
+        name="filtered",
+        default=False,
+        no_value=False,
+        optional=True,
+        desc="Quantiles calculated using a mask",
+        readonly=True,
+    )
+    MetadataElement(
+        name="masked",
+        default=False,
+        no_value=False,
+        optional=True,
+        desc="Quantiles calculated using a frequency filter",
+        readonly=True,
+    )
 
     def __init__(self, **kwd):
         """Quantiles for chimera analysis"""
         super().__init__(**kwd)
-        self.column_names = ['num', 'ten', 'twentyfive', 'fifty', 'seventyfive', 'ninetyfive', 'ninetynine']
-        self.column_types = ['int', 'float', 'float', 'float', 'float', 'float', 'float']
+        self.column_names = ["num", "ten", "twentyfive", "fifty", "seventyfive", "ninetyfive", "ninetynine"]
+        self.column_types = ["int", "float", "float", "float", "float", "float", "float"]
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a quantiles tabular format for chimera analysis
 
         .. code-block::
 
             1	0	0	0	0	0	0
             2       0.309198        0.309198        0.37161 0.37161 0.37161 0.37161
@@ -711,18 +762,18 @@
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.quan' )
         >>> Quantile().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.quan' )
         >>> Quantile().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         for line in headers:
-            if not line[0].startswith('@') and not line[0].startswith('#'):
+            if not line[0].startswith("@") and not line[0].startswith("#"):
                 if len(line) != 7:
                     return False
                 try:
                     int(line[0])
                     float(line[1])
                     float(line[2])
                     float(line[3])
@@ -736,45 +787,45 @@
             return True
 
         return False
 
 
 @build_sniff_from_prefix
 class LaneMask(Text):
-    file_ext = 'mothur.filter'
+    file_ext = "mothur.filter"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a lane mask filter:  1 line consisting of zeros and ones.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.filter' )
         >>> LaneMask().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.filter' )
         >>> LaneMask().sniff( fname )
         False
         """
-        headers = get_headers(file_prefix, sep='\t', count=2)
+        headers = get_headers(file_prefix, sep="\t", count=2)
         if len(headers) != 1 or len(headers[0]) != 1:
             return False
 
         if len(headers[0][0]) < 1000:
             # these filter files should be relatively big
             return False
 
-        if not re.match('^[01]+$', headers[0][0]):
+        if not re.match("^[01]+$", headers[0][0]):
             return False
 
         return True
 
 
 class CountTable(Tabular):
     MetadataElement(name="groups", default=[], desc="Group Names", readonly=True, visible=True, no_value=[])
-    file_ext = 'mothur.count_table'
+    file_ext = "mothur.count_table"
 
     def __init__(self, **kwd):
         """
         http://www.mothur.org/wiki/Count_File
         A table with first column names and following columns integer counts
         # Example 1:
         Representative_Sequence total
@@ -786,41 +837,48 @@
         U68630  1       1       0
         U68595  1       1       0
         U68600  1       1       0
         U68591  1       1       0
         U68647  1       0       1
         """
         super().__init__(**kwd)
-        self.column_names = ['name', 'total']
+        self.column_names = ["name", "total"]
 
-    def set_meta(self, dataset, overwrite=True, skip=1, max_data_lines=None, **kwd):
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = 1,
+        max_data_lines: Optional[int] = None,
+        **kwd,
+    ) -> None:
         super().set_meta(dataset, overwrite=overwrite, **kwd)
 
-        headers = get_headers(dataset.file_name, sep='\t', count=1)
+        headers = get_headers(dataset.file_name, sep="\t", count=1)
         colnames = headers[0]
-        dataset.metadata.column_types = ['str'] + (['int'] * (len(headers[0]) - 1))
+        dataset.metadata.column_types = ["str"] + (["int"] * (len(headers[0]) - 1))
         if len(colnames) > 1:
             dataset.metadata.columns = len(colnames)
         if len(colnames) > 2:
             dataset.metadata.groups = colnames[2:]
 
         dataset.metadata.comment_lines = 1
         if isinstance(dataset.metadata.data_lines, int):
             dataset.metadata.data_lines -= 1
 
 
 @build_sniff_from_prefix
 class RefTaxonomy(Tabular):
-    file_ext = 'mothur.ref.taxonomy'
+    file_ext = "mothur.ref.taxonomy"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.column_names = ['name', 'taxonomy']
+        self.column_names = ["name", "taxonomy"]
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is a Reference Taxonomy
 
         http://www.mothur.org/wiki/Taxonomy_outline
         A table with 2 or 3 columns:
 
         - SequenceName
@@ -847,25 +905,25 @@
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.ref.taxonomy' )
         >>> RefTaxonomy().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.ref.taxonomy' )
         >>> RefTaxonomy().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t', count=300)
+        headers = iter_headers(file_prefix, sep="\t", count=300)
         count = 0
-        pat_prog = re.compile('^([^ \t\n\r\x0c\x0b;]+([(]\\d+[)])?(;[^ \t\n\r\x0c\x0b;]+([(]\\d+[)])?)*(;)?)$')
+        pat_prog = re.compile("^([^ \t\n\r\x0c\x0b;]+([(]\\d+[)])?(;[^ \t\n\r\x0c\x0b;]+([(]\\d+[)])?)*(;)?)$")
         found_semicolons = False
         for line in headers:
-            if not line[0].startswith('@') and not line[0].startswith('#'):
+            if not line[0].startswith("@") and not line[0].startswith("#"):
                 if not (2 <= len(line) <= 3):
                     return False
                 if not pat_prog.match(line[1]):
                     return False
-                if not found_semicolons and line[1].find(';') > -1:
+                if not found_semicolons and line[1].find(";") > -1:
                     found_semicolons = True
                 if len(line) == 3:
                     try:
                         int(line[2])
                     except Exception:
                         return False
                 count += 1
@@ -874,40 +932,40 @@
             # Require that at least one entry has semicolons in the 2nd column
             return found_semicolons
 
         return False
 
 
 class ConsensusTaxonomy(Tabular):
-    file_ext = 'mothur.cons.taxonomy'
+    file_ext = "mothur.cons.taxonomy"
 
     def __init__(self, **kwd):
         """A list of names"""
         super().__init__(**kwd)
-        self.column_names = ['OTU', 'count', 'taxonomy']
+        self.column_names = ["OTU", "count", "taxonomy"]
 
 
 class TaxonomySummary(Tabular):
-    file_ext = 'mothur.tax.summary'
+    file_ext = "mothur.tax.summary"
 
     def __init__(self, **kwd):
         """A Summary of taxon classification"""
         super().__init__(**kwd)
-        self.column_names = ['taxlevel', 'rankID', 'taxon', 'daughterlevels', 'total']
+        self.column_names = ["taxlevel", "rankID", "taxon", "daughterlevels", "total"]
 
 
 @build_sniff_from_prefix
 class Axes(Tabular):
-    file_ext = 'mothur.axes'
+    file_ext = "mothur.axes"
 
     def __init__(self, **kwd):
         """Initialize axes datatype"""
         super().__init__(**kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is an axes format
         The first line may have column headings.
         The following lines have the name in the first column plus float columns for each axis.
 
         .. code-block::
 
@@ -926,15 +984,15 @@
         >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.axes' )
         >>> Axes().sniff( fname )
         True
         >>> fname = get_test_fname( 'mothur_datatypetest_false.mothur.axes' )
         >>> Axes().sniff( fname )
         False
         """
-        headers = iter_headers(file_prefix, sep='\t')
+        headers = iter_headers(file_prefix, sep="\t")
         count = 0
         col_cnt = None
         all_integers = True
         for line in headers:
             if count != 0:
                 if col_cnt is None:
                     col_cnt = len(line)
@@ -980,50 +1038,56 @@
 
         800
         GQY1XT001CQL4K 85 1.04 0.00 1.00 0.02 0.03 1.02 0.05 ...
         GQY1XT001CQIRF 84 1.02 0.06 0.98 0.06 0.09 1.05 0.07 ...
         GQY1XT001CF5YW 88 1.02 0.02 1.01 0.04 0.06 1.02 0.03 ...
 
     """
-    file_ext = 'mothur.sff.flow'
 
-    MetadataElement(name="flow_values", default="", no_value="", optional=True, desc="Total number of flow values", readonly=True)
-    MetadataElement(name="flow_order", default="TACG", no_value="TACG", desc="Total number of flow values", readonly=False)
+    file_ext = "mothur.sff.flow"
+
+    MetadataElement(
+        name="flow_values", default="", no_value="", optional=True, desc="Total number of flow values", readonly=True
+    )
+    MetadataElement(
+        name="flow_order", default="TACG", no_value="TACG", desc="Total number of flow values", readonly=False
+    )
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def set_meta(self, dataset, overwrite=True, skip=1, max_data_lines=None, **kwd):
-        super().set_meta(dataset, overwrite, 1, max_data_lines)
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = 1,
+        max_data_lines: Optional[int] = None,
+        **kwd,
+    ) -> None:
+        super().set_meta(dataset, overwrite=overwrite, skip=1, max_data_lines=max_data_lines, **kwd)
 
-        headers = get_headers(dataset.file_name, sep='\t', count=1)
+        headers = get_headers(dataset.file_name, sep="\t", count=1)
         try:
             flow_values = int(headers[0][0])
             dataset.metadata.flow_values = flow_values
         except Exception as e:
             log.warning(f"SffFlow set_meta {e}")
 
-    def make_html_table(self, dataset, skipchars=None):
+    def make_html_table(self, dataset: "DatasetInstance", skipchars: Optional[List] = None, **kwargs) -> str:
         """Create HTML table, used for displaying peek"""
-        if skipchars is None:
-            skipchars = []
+        skipchars = skipchars or []
         try:
             out = '<table cellspacing="0" cellpadding="3">'
 
             # Generate column header
-            out += '<tr>'
-            out += '<th>1. Name</th>'
-            out += '<th>2. Flows</th>'
+            out += "<tr>"
+            out += "<th>1. Name</th>"
+            out += "<th>2. Flows</th>"
             for i in range(3, dataset.metadata.columns + 1):
                 base = dataset.metadata.flow_order[(i + 1) % 4]
-                out += '<th>%d. %s</th>' % (i - 2, base)
-            out += '</tr>'
+                out += "<th>%d. %s</th>" % (i - 2, base)
+            out += "</tr>"
             out += self.make_html_peek_rows(dataset, skipchars=skipchars)
-            out += '</table>'
+            out += "</table>"
         except Exception as exc:
             out = f"Can't create peek: {unicodify(exc)}"
         return out
-
-
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/msa.py` & `galaxy-data-23.0.1/galaxy/datatypes/msa.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,258 +1,305 @@
 import abc
 import logging
 import os
 import re
+from typing import (
+    Callable,
+    Dict,
+    List,
+    TYPE_CHECKING,
+)
 
 from galaxy.datatypes.binary import Binary
-from galaxy.datatypes.data import get_file_peek, Text
+from galaxy.datatypes.data import (
+    get_file_peek,
+    Text,
+)
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.datatypes.util import generic_util
 from galaxy.util import (
     nice_size,
     unicodify,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
-STOCKHOLM_SEARCH_PATTERN = re.compile(r'#\s+STOCKHOLM\s+1\.0')
+STOCKHOLM_SEARCH_PATTERN = re.compile(r"#\s+STOCKHOLM\s+1\.0")
 
 
 @build_sniff_from_prefix
 class InfernalCM(Text):
     file_ext = "cm"
 
-    MetadataElement(name="number_of_models", default=0, desc="Number of covariance models",
-                    readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="number_of_models",
+        default=0,
+        desc="Number of covariance models",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
+
+    MetadataElement(
+        name="cm_version",
+        default="1/a",
+        desc="Infernal Covariance Model version",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    MetadataElement(name="cm_version", default="1/a", desc="Infernal Covariance Model version",
-                    readonly=True, visible=True, optional=True, no_value=0)
-
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             if dataset.metadata.number_of_models == 1:
                 dataset.blurb = "1 model"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_models} models"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'infernal_model.cm' )
         >>> InfernalCM().sniff( fname )
         True
         >>> fname = get_test_fname( '2.txt' )
         >>> InfernalCM().sniff( fname )
         False
         """
         return file_prefix.startswith("INFERNAL")
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of models and the version of CM file in dataset.
         """
-        dataset.metadata.number_of_models = generic_util.count_special_lines('^INFERNAL', dataset.file_name)
+        dataset.metadata.number_of_models = generic_util.count_special_lines("^INFERNAL", dataset.file_name)
         with open(dataset.file_name) as f:
             first_line = f.readline()
             if first_line.startswith("INFERNAL"):
-                dataset.metadata.cm_version = (first_line.split()[0]).replace('INFERNAL', '')
+                dataset.metadata.cm_version = (first_line.split()[0]).replace("INFERNAL", "")
 
 
 @build_sniff_from_prefix
 class Hmmer(Text):
     edam_data = "data_1364"
     edam_format = "format_1370"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "HMMER Database"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"HMMER database ({nice_size(dataset.get_size())})"
 
     @abc.abstractmethod
-    def sniff_prefix(self, filename):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         raise NotImplementedError
 
 
 class Hmmer2(Hmmer):
     edam_format = "format_3328"
     file_ext = "hmm2"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """HMMER2 files start with HMMER2.0
-        """
-        return file_prefix.startswith('HMMER2.0')
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """HMMER2 files start with HMMER2.0"""
+        return file_prefix.startswith("HMMER2.0")
 
 
 class Hmmer3(Hmmer):
     edam_format = "format_3329"
     file_ext = "hmm3"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """HMMER3 files start with HMMER3/f
-        """
-        return file_prefix.startswith('HMMER3/f')
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """HMMER3 files start with HMMER3/f"""
+        return file_prefix.startswith("HMMER3/f")
 
 
 class HmmerPress(Binary):
     """Class for hmmpress database files."""
-    file_ext = 'hmmpress'
-    composite_type = 'basic'
 
-    def set_peek(self, dataset):
+    file_ext = "hmmpress"
+    composite_type = "basic"
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text."""
         if not dataset.dataset.purged:
             dataset.peek = "HMMER Binary database"
             dataset.blurb = "HMMER Binary database"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "HMMER3 database (multiple files)"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         # Binary model
-        self.add_composite_file('model.hmm.h3m', is_binary=True)
+        self.add_composite_file("model.hmm.h3m", is_binary=True)
         # SSI index for binary model
-        self.add_composite_file('model.hmm.h3i', is_binary=True)
+        self.add_composite_file("model.hmm.h3i", is_binary=True)
         # Profiles (MSV part)
-        self.add_composite_file('model.hmm.h3f', is_binary=True)
+        self.add_composite_file("model.hmm.h3f", is_binary=True)
         # Profiles (remained)
-        self.add_composite_file('model.hmm.h3p', is_binary=True)
+        self.add_composite_file("model.hmm.h3p", is_binary=True)
 
 
 @build_sniff_from_prefix
 class Stockholm_1_0(Text):
     edam_data = "data_0863"
     edam_format = "format_1961"
     file_ext = "stockholm"
 
-    MetadataElement(name="number_of_models", default=0, desc="Number of multiple alignments", readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="number_of_models",
+        default=0,
+        desc="Number of multiple alignments",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            if (dataset.metadata.number_of_models == 1):
+            if dataset.metadata.number_of_models == 1:
                 dataset.blurb = "1 alignment"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_models} alignments"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         return file_prefix.search(STOCKHOLM_SEARCH_PATTERN)
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
 
         Set the number of models in dataset.
         """
-        dataset.metadata.number_of_models = generic_util.count_special_lines('^#[[:space:]+]STOCKHOLM[[:space:]+]1.0', dataset.file_name)
+        dataset.metadata.number_of_models = generic_util.count_special_lines(
+            "^#[[:space:]+]STOCKHOLM[[:space:]+]1.0", dataset.file_name
+        )
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
 
         Split the input files by model records.
         """
         if split_params is None:
             return None
 
         if len(input_datasets) > 1:
             raise Exception("STOCKHOLM-file splitting does not support multiple files")
         input_files = [ds.file_name for ds in input_datasets]
 
         chunk_size = None
-        if split_params['split_mode'] == 'number_of_parts':
-            raise Exception(f"Split mode \"{split_params['split_mode']}\" is currently not implemented for STOCKHOLM-files.")
-        elif split_params['split_mode'] == 'to_size':
-            chunk_size = int(split_params['split_size'])
+        if split_params["split_mode"] == "number_of_parts":
+            raise Exception(
+                f"Split mode \"{split_params['split_mode']}\" is currently not implemented for STOCKHOLM-files."
+            )
+        elif split_params["split_mode"] == "to_size":
+            chunk_size = int(split_params["split_size"])
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
         def _read_stockholm_records(filename):
             lines = []
             with open(filename) as handle:
                 for line in handle:
                     lines.append(line)
-                    if line.strip() == '//':
+                    if line.strip() == "//":
                         yield lines
                         lines = []
 
         def _write_part_stockholm_file(accumulated_lines):
             part_dir = subdir_generator_function()
             part_path = os.path.join(part_dir, os.path.basename(input_files[0]))
-            with open(part_path, 'w') as part_file:
+            with open(part_path, "w") as part_file:
                 part_file.writelines(accumulated_lines)
 
         try:
-
             stockholm_records = _read_stockholm_records(input_files[0])
             stockholm_lines_accumulated = []
             for counter, stockholm_record in enumerate(stockholm_records, start=1):
                 stockholm_lines_accumulated.extend(stockholm_record)
                 if counter % chunk_size == 0:
                     _write_part_stockholm_file(stockholm_lines_accumulated)
                     stockholm_lines_accumulated = []
             if stockholm_lines_accumulated:
                 _write_part_stockholm_file(stockholm_lines_accumulated)
         except Exception as e:
-            log.error('Unable to split files: %s', unicodify(e))
+            log.error("Unable to split files: %s", unicodify(e))
             raise
 
 
 @build_sniff_from_prefix
 class MauveXmfa(Text):
     file_ext = "xmfa"
 
-    MetadataElement(name="number_of_models", default=0, desc="Number of alignmened sequences", readonly=True, visible=True, optional=True, no_value=0)
+    MetadataElement(
+        name="number_of_models",
+        default=0,
+        desc="Number of alignmened sequences",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=0,
+    )
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            if (dataset.metadata.number_of_models == 1):
+            if dataset.metadata.number_of_models == 1:
                 dataset.blurb = "1 alignment"
             else:
                 dataset.blurb = f"{dataset.metadata.number_of_models} alignments"
             dataset.peek = get_file_peek(dataset.file_name)
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        return file_prefix.startswith('#FormatVersion Mauve1')
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith("#FormatVersion Mauve1")
 
-    def set_meta(self, dataset, **kwd):
-        dataset.metadata.number_of_models = generic_util.count_special_lines('^#Sequence([[:digit:]]+)Entry', dataset.file_name)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        dataset.metadata.number_of_models = generic_util.count_special_lines(
+            "^#Sequence([[:digit:]]+)Entry", dataset.file_name
+        )
 
 
 class Msf(Text):
     """
     Multiple sequence alignment format produced by the Accelrys GCG suite and
     other programs.
     """
+
     edam_data = "data_0863"
     edam_format = "format_1947"
-    file_ext = 'msf'
+    file_ext = "msf"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/neo4j.py` & `galaxy-data-23.0.1/galaxy/datatypes/neo4j.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,140 +1,140 @@
 """
 Neo4j Composite Dataset
 """
 import logging
-import sys
+from typing import TYPE_CHECKING
 
-from galaxy.datatypes.data import Data
+from galaxy.datatypes.data import (
+    Data,
+    GeneratePrimaryFileDataset,
+)
 from galaxy.datatypes.images import Html
 from galaxy.datatypes.metadata import MetadataElement
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 gal_Log = logging.getLogger(__name__)
 verbose = True
 
 
 class Neo4j(Html):
     """
     base class to use for neostore datatypes
     derived from html - composite datatype elements
     stored in extra files path
     """
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """
         This is called only at upload to write the html file
         cannot rename the datasets here - they come with the default unfortunately
         """
         # self.regenerate_primary_file(dataset)
         rval = [
-            '<html><head><title>Files for Composite Dataset (%s)</title></head><p/>\
-            This composite dataset is composed of the following files:<p/><ul>' % (
-                self.file_ext)]
+            "<html><head><title>Files for Composite Dataset (%s)</title></head><p/>\
+            This composite dataset is composed of the following files:<p/><ul>"
+            % (self.file_ext)
+        ]
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            rval.append('<li><a href="%s">%s</a>%s' %
-                        (composite_name, composite_name, opt_text))
-        rval.append('</ul></html>')
+                opt_text = " (optional)"
+            rval.append(f'<li><a href="{composite_name}">{composite_name}</a>{opt_text}')
+        rval.append("</ul></html>")
         return "\n".join(rval)
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/html'
+        return "text/html"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
-            dataset.peek = 'Neo4j database (multiple files)'
-            dataset.blurb = 'Neo4j database (multiple files)'
+            dataset.peek = "Neo4j database (multiple files)"
+            dataset.blurb = "Neo4j database (multiple files)"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "NEO4J database (multiple files)"
 
 
 class Neo4jDB(Neo4j, Data):
     """Class for neo4jDB database files."""
-    file_ext = 'neostore'
-    composite_type = 'auto_primary_file'
+
+    file_ext = "neostore"
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         Data.__init__(self, **kwd)
-        self.add_composite_file('neostore', is_binary=True)
-        self.add_composite_file('neostore.id', is_binary=True)
-        self.add_composite_file('neostore.counts.db.a', optional=True, is_binary=True)
-        self.add_composite_file('neostore.counts.db.b', optional=True, is_binary=True)
-        self.add_composite_file('neostore.labeltokenstore.db', is_binary=True)
-        self.add_composite_file(
-            'neostore.labeltokenstore.db.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.labeltokenstore.db.names', is_binary=True)
-        self.add_composite_file(
-            'neostore.labeltokenstore.db.names.id', is_binary=True)
-        self.add_composite_file('neostore.nodestore.db', is_binary=True)
-        self.add_composite_file('neostore.nodestore.db.id', is_binary=True)
-        self.add_composite_file('neostore.nodestore.db.labels', is_binary=True)
-        self.add_composite_file(
-            'neostore.nodestore.db.labels.id', is_binary=True)
-
-        self.add_composite_file('neostore.propertystore.db', is_binary=True)
-        self.add_composite_file('neostore.propertystore.db.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.arrays', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.arrays.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.index', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.index.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.index.keys', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.index.keys.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.strings', is_binary=True)
-        self.add_composite_file(
-            'neostore.propertystore.db.strings.id', is_binary=True)
-
-        self.add_composite_file(
-            'neostore.relationshipgroupstore.db', is_binary=True)
-        self.add_composite_file(
-            'neostore.relationshipgroupstore.db.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.relationshipstore.db', is_binary=True)
-        self.add_composite_file(
-            'neostore.relationshipstore.db.id', is_binary=True)
-        self.add_composite_file(
-            'neostore.relationshiptypestore.db.names', is_binary=True)
-        self.add_composite_file(
-            'neostore.relationshiptypestore.db.names.id', is_binary=True)
-        self.add_composite_file('neostore.schemastore.db', is_binary=True)
-        self.add_composite_file('neostore.schemastore.db.id', is_binary=True)
-        self.add_composite_file('neostore.transaction.db.0', is_binary=True)
+        self.add_composite_file("neostore", is_binary=True)
+        self.add_composite_file("neostore.id", is_binary=True)
+        self.add_composite_file("neostore.counts.db.a", optional=True, is_binary=True)
+        self.add_composite_file("neostore.counts.db.b", optional=True, is_binary=True)
+        self.add_composite_file("neostore.labeltokenstore.db", is_binary=True)
+        self.add_composite_file("neostore.labeltokenstore.db.id", is_binary=True)
+        self.add_composite_file("neostore.labeltokenstore.db.names", is_binary=True)
+        self.add_composite_file("neostore.labeltokenstore.db.names.id", is_binary=True)
+        self.add_composite_file("neostore.nodestore.db", is_binary=True)
+        self.add_composite_file("neostore.nodestore.db.id", is_binary=True)
+        self.add_composite_file("neostore.nodestore.db.labels", is_binary=True)
+        self.add_composite_file("neostore.nodestore.db.labels.id", is_binary=True)
+
+        self.add_composite_file("neostore.propertystore.db", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.id", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.arrays", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.arrays.id", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.index", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.index.id", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.index.keys", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.index.keys.id", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.strings", is_binary=True)
+        self.add_composite_file("neostore.propertystore.db.strings.id", is_binary=True)
+
+        self.add_composite_file("neostore.relationshipgroupstore.db", is_binary=True)
+        self.add_composite_file("neostore.relationshipgroupstore.db.id", is_binary=True)
+        self.add_composite_file("neostore.relationshipstore.db", is_binary=True)
+        self.add_composite_file("neostore.relationshipstore.db.id", is_binary=True)
+        self.add_composite_file("neostore.relationshiptypestore.db.names", is_binary=True)
+        self.add_composite_file("neostore.relationshiptypestore.db.names.id", is_binary=True)
+        self.add_composite_file("neostore.schemastore.db", is_binary=True)
+        self.add_composite_file("neostore.schemastore.db.id", is_binary=True)
+        self.add_composite_file("neostore.transaction.db.0", is_binary=True)
 
 
 class Neo4jDBzip(Neo4j, Data):
     """Class for neo4jDB database files."""
-    MetadataElement(name='reference_name', default='neostore_file', desc='Reference Name',
-                    readonly=True, visible=True, set_in_upload=True, no_value='neostore')
-    MetadataElement(name="neostore_zip", default=None, desc="Neostore zip",
-                    readonly=True, visible=True, set_in_upload=True, optional=True)
+
+    MetadataElement(
+        name="reference_name",
+        default="neostore_file",
+        desc="Reference Name",
+        readonly=True,
+        visible=True,
+        set_in_upload=True,
+        no_value="neostore",
+    )
+    MetadataElement(
+        name="neostore_zip",
+        default=None,
+        desc="Neostore zip",
+        readonly=True,
+        visible=True,
+        set_in_upload=True,
+        optional=True,
+    )
 
     file_ext = "neostore.zip"
-    composite_type = 'auto_primary_file'
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         Data.__init__(self, **kwd)
-        self.add_composite_file('%s.zip', description='neostore zip', substitute_name_with_metadata='reference_name',
-                                is_binary=True)
-
-
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
+        self.add_composite_file(
+            "%s.zip", description="neostore zip", substitute_name_with_metadata="reference_name", is_binary=True
+        )
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/ngsindex.py` & `galaxy-data-23.0.1/galaxy/datatypes/ngsindex.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,75 +1,109 @@
 """
 NGS indexes
 """
 import logging
 import os
+from typing import TYPE_CHECKING
 
+from galaxy.datatypes.data import GeneratePrimaryFileDataset
 from .metadata import MetadataElement
 from .text import Html
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
 
 class BowtieIndex(Html):
     """
     base class for BowtieIndex
     is subclassed by BowtieColorIndex and BowtieBaseIndex
     """
-    MetadataElement(name="base_name", desc="base name for this index set", default='galaxy_generated_bowtie_index', set_in_upload=True, readonly=True)
-    MetadataElement(name="sequence_space", desc="sequence_space for this index set", default='unknown', set_in_upload=True, readonly=True)
 
-    composite_type = 'auto_primary_file'
+    MetadataElement(
+        name="base_name",
+        desc="base name for this index set",
+        default="galaxy_generated_bowtie_index",
+        set_in_upload=True,
+        readonly=True,
+    )
+    MetadataElement(
+        name="sequence_space",
+        desc="sequence_space for this index set",
+        default="unknown",
+        set_in_upload=True,
+        readonly=True,
+    )
+
+    composite_type = "auto_primary_file"
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """
         This is called only at upload to write the html file
         cannot rename the datasets here - they come with the default unfortunately
         """
-        return '<html><head></head><body>AutoGenerated Primary File for Composite Dataset</body></html>'
+        return "<html><head></head><body>AutoGenerated Primary File for Composite Dataset</body></html>"
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         """
         cannot do this until we are setting metadata
         """
         bn = dataset.metadata.base_name
         flist = os.listdir(dataset.extra_files_path)
-        rval = [f'<html><head><title>Files for Composite Dataset {bn}</title></head><p/>Comprises the following files:<p/><ul>']
+        rval = [
+            f"<html><head><title>Files for Composite Dataset {bn}</title></head><p/>Comprises the following files:<p/><ul>"
+        ]
         for fname in flist:
             sfname = os.path.split(fname)[-1]
             rval.append(f'<li><a href="{sfname}">{sfname}</a>')
-        rval.append('</ul></html>')
-        with open(dataset.file_name, 'w') as f:
+        rval.append("</ul></html>")
+        with open(dataset.file_name, "w") as f:
             f.write("\n".join(rval))
-            f.write('\n')
+            f.write("\n")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = f"Bowtie index file ({dataset.metadata.sequence_space})"
             dataset.blurb = f"{dataset.metadata.sequence_space} space"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return "Bowtie index file"
 
 
 class BowtieColorIndex(BowtieIndex):
     """
     Bowtie color space index
     """
-    MetadataElement(name="sequence_space", desc="sequence_space for this index set", default='color', set_in_upload=True, readonly=True)
 
-    file_ext = 'bowtie_color_index'
+    MetadataElement(
+        name="sequence_space",
+        desc="sequence_space for this index set",
+        default="color",
+        set_in_upload=True,
+        readonly=True,
+    )
+
+    file_ext = "bowtie_color_index"
 
 
 class BowtieBaseIndex(BowtieIndex):
     """
     Bowtie base space index
     """
-    MetadataElement(name="sequence_space", desc="sequence_space for this index set", default='base', set_in_upload=True, readonly=True)
 
-    file_ext = 'bowtie_base_index'
+    MetadataElement(
+        name="sequence_space",
+        desc="sequence_space for this index set",
+        default="base",
+        set_in_upload=True,
+        readonly=True,
+    )
+
+    file_ext = "bowtie_base_index"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/phylip.py` & `galaxy-data-23.0.1/galaxy/datatypes/phylip.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,56 +3,68 @@
 
 @authors: Kenzo-Hugo Hillion and Fabien Mareuil, Institut Pasteur, Paris
 @contacts: kehillio@pasteur.fr and fabien.mareuil@pasteur.fr
 @project: galaxy
 @githuborganization: C3BI
 Phylip datatype sniffer
 """
+from typing import TYPE_CHECKING
+
 from galaxy import util
-from galaxy.datatypes.data import get_file_peek, Text
+from galaxy.datatypes.data import (
+    get_file_peek,
+    Text,
+)
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from galaxy.util import nice_size
 from .metadata import MetadataElement
 
+if TYPE_CHECKING:
+    from io import StringIO
+
+    from galaxy.model import DatasetInstance
+
 
 @build_sniff_from_prefix
 class Phylip(Text):
     """Phylip format stores a multiple sequence alignment"""
+
     edam_data = "data_0863"
     edam_format = "format_1997"
     file_ext = "phylip"
 
-    MetadataElement(name="sequences", default=0, desc="Number of sequences", readonly=True,
-                    visible=False, optional=True, no_value=0)
+    MetadataElement(
+        name="sequences", default=0, desc="Number of sequences", readonly=True, visible=False, optional=True, no_value=0
+    )
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of sequences and the number of data lines in dataset.
         """
         dataset.metadata.data_lines = self.count_data_lines(dataset)
         try:
             dataset.metadata.sequences = int(open(dataset.file_name).readline().split()[0])
         except Exception:
             raise Exception("Header does not correspond to PHYLIP header.")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             if dataset.metadata.sequences:
                 dataset.blurb = f"{util.commaify(str(dataset.metadata.sequences))} sequences"
             else:
                 dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_strict_interleaved(self, nb_seq, seq_length, alignment_prefix):
+    def sniff_strict_interleaved(self, nb_seq: int, seq_length: int, alignment_prefix: "StringIO") -> bool:
         found_seq_length = None
         for _ in range(nb_seq):
             line = alignment_prefix.readline()
             if not line:
                 # Not enough lines, either the prefix is too short or this is not PHYLIP
                 return False
             line = line.rstrip("\n")
@@ -66,25 +78,25 @@
                 return False
             if found_seq_length is None:
                 found_seq_length = this_seq_length
             elif this_seq_length != found_seq_length:
                 # All sequence parts should have the same length
                 return False
             # Fail if sequence is not ascii
-            seq.encode('ascii')
+            seq.encode("ascii")
             if any(str.isdigit(c) for c in seq):
                 # Could tighten up further by requiring IUPAC strings chars
                 return False
         # There may be more lines with the remaining parts of the sequences
         return True
 
-    def sniff_strict_sequential(self, nb_seq, seq_length, alignment_prefix):
+    def sniff_strict_sequential(self, nb_seq: int, seq_length: int, alignment_prefix: "StringIO") -> bool:
         raise NotImplementedError
 
-    def sniff_relaxed_interleaved(self, nb_seq, seq_length, alignment_prefix):
+    def sniff_relaxed_interleaved(self, nb_seq: int, seq_length: int, alignment_prefix: "StringIO") -> bool:
         found_seq_length = None
         for _ in range(nb_seq):
             line = alignment_prefix.readline()
             if not line:
                 # Not enough lines, either the prefix is too short or this is not PHYLIP
                 return False
             line = line.rstrip("\n")
@@ -96,27 +108,27 @@
                 return False
             if found_seq_length is None:
                 found_seq_length = this_seq_length
             elif this_seq_length != found_seq_length:
                 # All sequence parts should have the same length
                 return False
             # Fail if sequence is not ascii
-            seq.encode('ascii')
+            seq.encode("ascii")
             if any(str.isdigit(c) for c in seq):
                 # Could tighten up further by requiring IUPAC strings chars
                 return False
         line = alignment_prefix.readline()
         if line.strip():
             # There should be a newline separating alignments.
             # If we got more content this is probably not a phylip file
             return False
         # There may be more lines with the remaining parts of the sequences
         return True
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         All Phylip files starts with the number of sequences so we can use this
         to count the following number of sequences in the first 'stack'
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('test_strict_interleaved.phylip')
         >>> Phylip().sniff(fname)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/proteomics.py` & `galaxy-data-23.0.1/galaxy/datatypes/proteomics.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,62 +1,124 @@
 """
 Proteomics Datatypes
 """
 import logging
 import re
+from typing import (
+    IO,
+    List,
+    Optional,
+    TYPE_CHECKING,
+)
 
 from galaxy.datatypes import data
 from galaxy.datatypes.binary import Binary
-from galaxy.datatypes.data import Text
+from galaxy.datatypes.data import (
+    GeneratePrimaryFileDataset,
+    Text,
+)
 from galaxy.datatypes.sequence import Sequence
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
-from galaxy.datatypes.tabular import Tabular, TabularData
+from galaxy.datatypes.tabular import (
+    Tabular,
+    TabularData,
+)
 from galaxy.datatypes.xml import GenericXml
 from galaxy.util import nice_size
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
 
 log = logging.getLogger(__name__)
 
 
 class Wiff(Binary):
     """Class for wiff files."""
+
+    edam_data = "data_2536"
+    edam_format = "format_3710"
+    file_ext = "wiff"
+    composite_type = "auto_primary_file"
+
+    def __init__(self, **kwd):
+        super().__init__(**kwd)
+
+        self.add_composite_file(
+            "wiff",
+            description="AB SCIEX files in .wiff format. This can contain all needed information or only metadata.",
+            is_binary=True,
+        )
+
+        self.add_composite_file(
+            "wiff_scan",
+            description="AB SCIEX spectra file (wiff.scan), if the corresponding .wiff file only contains metadata.",
+            optional="True",
+            is_binary=True,
+        )
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Wiff Composite Dataset </title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
+        for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
+            fn = composite_name
+            opt_text = ""
+            if composite_file.optional:
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
+            else:
+                rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
+        rval.append("</ul></div></html>")
+        return "\n".join(rval)
+
+
+class Wiff2(Binary):
+    """Class for wiff2 files."""
+
     edam_data = "data_2536"
     edam_format = "format_3710"
-    file_ext = 'wiff'
-    composite_type = 'auto_primary_file'
+    file_ext = "wiff2"
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
         self.add_composite_file(
-            'wiff',
-            description='AB SCIEX files in .wiff format. This can contain all needed information or only metadata.',
-            is_binary=True)
+            "wiff2",
+            description="AB SCIEX files in .wiff2 format. This can contain all needed information or only metadata.",
+            is_binary=True,
+        )
 
         self.add_composite_file(
-            'wiff_scan',
-            description='AB SCIEX spectra file (wiff.scan), if the corresponding .wiff file only contains metadata.',
-            optional='True', is_binary=True)
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>Wiff Composite Dataset </title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+            "wiff_scan",
+            description="AB SCIEX spectra file (wiff.scan), if the corresponding .wiff2 file only contains metadata.",
+            optional="True",
+            is_binary=True,
+        )
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Wiff2 Composite Dataset </title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
 
 
 @build_sniff_from_prefix
 class MzTab(Text):
     """
     exchange format for proteomics and metabolomics results
@@ -65,54 +127,57 @@
     >>> fname = get_test_fname('test.mztab')
     >>> MzTab().sniff(fname)
     True
     >>> fname = get_test_fname('test.mztab2')
     >>> MzTab().sniff(fname)
     False
     """
+
     edam_data = "data_3681"
     file_ext = "mztab"
     # section names (except MTD)
     _sections = ["PRH", "PRT", "PEH", "PEP", "PSH", "PSM", "SMH", "SML", "COM"]
     # mandatory metadata fields and list of allowed entries (in lower case)
     # (or None if everything is allowed)
-    _man_mtd = {"mzTab-mode": ["complete", "summary"],
-                "mzTab-type": ['quantification', 'identification'],
-                "description": None}
+    _man_mtd = {
+        "mzTab-mode": ["complete", "summary"],
+        "mzTab-type": ["quantification", "identification"],
+        "description": None,
+    }
     _version_re = r"(1)(\.[0-9])?(\.[0-9])?"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'mzTab Format'
+            dataset.blurb = "mzTab Format"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """ Determines whether the file is the correct type. """
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file is the correct type."""
         has_version = False
         found_man_mtd = set()
         contents = file_prefix.string_io()
         for line in contents:
             if re.match(r"^\s*$", line):
                 continue
             columns = line.strip("\r\n").split("\t")
             if columns[0] == "MTD":
                 if columns[1] == "mzTab-version" and re.match(self._version_re, columns[2]) is not None:
                     has_version = True
                 elif columns[1] in self._man_mtd:
                     mandatory_field = self._man_mtd[columns[1]]
                     if mandatory_field is None or columns[2].lower() in mandatory_field:
                         found_man_mtd.add(columns[1])
-            elif not columns[0] in self._sections:
+            elif columns[0] not in self._sections:
                 return False
         return has_version and found_man_mtd == set(self._man_mtd.keys())
 
 
 class MzTab2(MzTab):
     """
     exchange format for proteomics and metabolomics results
@@ -121,30 +186,31 @@
     >>> fname = get_test_fname('test.mztab2')
     >>> MzTab2().sniff(fname)
     True
     >>> fname = get_test_fname('test.mztab')
     >>> MzTab2().sniff(fname)
     False
     """
+
     file_ext = "mztab2"
     _sections = ["SMH", "SML", "SFH", "SMF", "SEH", "SME", "COM"]
     _version_re = r"(2)(\.[0-9])?(\.[0-9])?-M$"
     _man_mtd = {"mzTab-ID": None}
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'mzTab2 Format'
+            dataset.blurb = "mzTab2 Format"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class Kroenik(Tabular):
     """
     Kroenik (HardKloer sibling) files
 
@@ -152,25 +218,41 @@
     >>> fname = get_test_fname('test.kroenik')
     >>> Kroenik().sniff(fname)
     True
     >>> fname = get_test_fname('test.peplist')
     >>> Kroenik().sniff(fname)
     False
     """
+
     file_ext = "kroenik"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.column_names = ["File", "First Scan", "Last Scan", "Num of Scans", "Charge", "Monoisotopic Mass", "Base Isotope Peak", "Best Intensity", "Summed Intensity", "First RTime", "Last RTime", "Best RTime", "Best Correlation", "Modifications"]
+        self.column_names = [
+            "File",
+            "First Scan",
+            "Last Scan",
+            "Num of Scans",
+            "Charge",
+            "Monoisotopic Mass",
+            "Base Isotope Peak",
+            "Best Intensity",
+            "Summed Intensity",
+            "First RTime",
+            "Last RTime",
+            "Best RTime",
+            "Best Correlation",
+            "Modifications",
+        ]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         fh = file_prefix.string_io()
         line = [_.strip() for _ in fh.readline().split("\t")]
         if line != self.column_names:
             return False
         line = fh.readline().split("\t")
         try:
             [int(_) for _ in line[1:5]]
@@ -190,25 +272,26 @@
     >>> fname = get_test_fname('test.peplist')
     >>> PepList().sniff(fname)
     True
     >>> fname = get_test_fname('test.psms')
     >>> PepList().sniff(fname)
     False
     """
+
     file_ext = "peplist"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self.column_names = ["m/z", "rt(min)", "snr", "charge", "intensity"]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         fh = file_prefix.string_io()
         line = [_.strip() for _ in fh.readline().split("\t")]
         if line == self.column_names:
             return True
         return False
 
 
@@ -226,41 +309,43 @@
     >>> fname = get_test_fname('test.psms')
     >>> PSMS().sniff(fname)
     True
     >>> fname = get_test_fname('test.kroenik')
     >>> PSMS().sniff(fname)
     False
     """
+
     file_ext = "psms"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self.column_names = ["PSMId", "score", "q-value", "posterior_error_prob", "peptide", "proteinIds"]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         fh = file_prefix.string_io()
         line = [_.strip() for _ in fh.readline().split("\t")]
         if line == self.column_names:
             return True
         return False
 
 
 @build_sniff_from_prefix
 class PEFF(Sequence):
     """
     PSI Extended FASTA Format
     https://github.com/HUPO-PSI/PEFF
     """
+
     file_ext = "peff"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'test.peff' )
         >>> PEFF().sniff( fname )
         True
         >>> fname = get_test_fname( 'sequence.fasta' )
         >>> PEFF().sniff( fname )
@@ -271,79 +356,109 @@
             return True
         else:
             return False
 
 
 class PepXmlReport(Tabular):
     """pepxml converted to tabular report"""
+
     edam_data = "data_2536"
     file_ext = "pepxml.tsv"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.column_names = ['Protein', 'Peptide', 'Assumed Charge', 'Neutral Pep Mass (calculated)', 'Neutral Mass', 'Retention Time', 'Start Scan', 'End Scan', 'Search Engine', 'PeptideProphet Probability', 'Interprophet Probability']
+        self.column_names = [
+            "Protein",
+            "Peptide",
+            "Assumed Charge",
+            "Neutral Pep Mass (calculated)",
+            "Neutral Mass",
+            "Retention Time",
+            "Start Scan",
+            "End Scan",
+            "Search Engine",
+            "PeptideProphet Probability",
+            "Interprophet Probability",
+        ]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
 
 class ProtXmlReport(Tabular):
     """protxml converted to tabular report"""
+
     edam_data = "data_2536"
     file_ext = "protxml.tsv"
     comment_lines = 1
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self.column_names = [
-            "Entry Number", "Group Probability",
-            "Protein", "Protein Link", "Protein Probability",
-            "Percent Coverage", "Number of Unique Peptides",
-            "Total Independent Spectra", "Percent Share of Spectrum ID's",
-            "Description", "Protein Molecular Weight", "Protein Length",
-            "Is Nondegenerate Evidence", "Weight", "Precursor Ion Charge",
-            "Peptide sequence", "Peptide Link", "NSP Adjusted Probability",
-            "Initial Probability", "Number of Total Termini",
-            "Number of Sibling Peptides Bin", "Number of Instances",
-            "Peptide Group Designator", "Is Evidence?"]
+            "Entry Number",
+            "Group Probability",
+            "Protein",
+            "Protein Link",
+            "Protein Probability",
+            "Percent Coverage",
+            "Number of Unique Peptides",
+            "Total Independent Spectra",
+            "Percent Share of Spectrum ID's",
+            "Description",
+            "Protein Molecular Weight",
+            "Protein Length",
+            "Is Nondegenerate Evidence",
+            "Weight",
+            "Precursor Ion Charge",
+            "Peptide sequence",
+            "Peptide Link",
+            "NSP Adjusted Probability",
+            "Initial Probability",
+            "Number of Total Termini",
+            "Number of Sibling Peptides Bin",
+            "Number of Instances",
+            "Peptide Group Designator",
+            "Is Evidence?",
+        ]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
 
 class Dta(TabularData):
     """dta
     The first line contains the singly protonated peptide mass (MH+) and the
     peptide charge state separated by a space. Subsequent lines contain space
     separated pairs of fragment ion m/z and intensity values.
     """
+
     file_ext = "dta"
     comment_lines = 0
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         column_types = []
-        data_row = []
+        data_row: List = []
         data_lines = 0
         if dataset.has_data():
             with open(dataset.file_name) as dtafile:
                 for _ in dtafile:
                     data_lines += 1
 
         # Guess column types
         for cell in data_row:
             column_types.append(self.guess_type(cell))
 
         # Set metadata
         dataset.metadata.data_lines = data_lines
         dataset.metadata.comment_lines = 0
-        dataset.metadata.column_types = ['float', 'float']
+        dataset.metadata.column_types = ["float", "float"]
         dataset.metadata.columns = 2
-        dataset.metadata.column_names = ['m/z', 'intensity']
+        dataset.metadata.column_names = ["m/z", "intensity"]
         dataset.metadata.delimiter = " "
 
 
 @build_sniff_from_prefix
 class Dta2d(TabularData):
     """
     dta2d: files with three tab/space-separated columns.
@@ -361,43 +476,44 @@
     >>> fname = get_test_fname('test.dta2d')
     >>> Dta2d().sniff(fname)
     True
     >>> fname = get_test_fname('test.edta')
     >>> Dta2d().sniff(fname)
     False
     """
+
     file_ext = "dta2d"
     comment_lines = 0
 
-    def _parse_header(self, line):
+    def _parse_header(self, line: List) -> Optional[List]:
         if len(line) != 3 or len(line[0]) < 3 or not line[0].startswith("#"):
             return None
         line[0] = line[0].lstrip("#")
         line = [_.strip() for _ in line]
-        if 'MZ' not in line or 'INT' not in line or ('MIN' not in line and 'SEC' not in line):
+        if "MZ" not in line or "INT" not in line or ("MIN" not in line and "SEC" not in line):
             return None
         return line
 
-    def _parse_delimiter(self, line):
+    def _parse_delimiter(self, line: str) -> Optional[str]:
         if len(line.split(" ")) == 3:
             return " "
         elif len(line.split("\t")) == 3:
             return "\t"
         return None
 
-    def _parse_dataline(self, line):
+    def _parse_dataline(self, line: List) -> bool:
         try:
             line = [float(_) for _ in line]
         except ValueError:
             return False
         if not all(_ >= 0 for _ in line):
             return False
         return True
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         data_lines = 0
         delim = None
         if dataset.has_data():
             with open(dataset.file_name) as dtafile:
                 for line in dtafile:
                     if delim is None:
                         delim = self._parse_delimiter(line)
@@ -406,22 +522,22 @@
 
         # Set metadata
         if delim is not None:
             dataset.metadata.delimiter = delim
 
         dataset.metadata.data_lines = data_lines
         dataset.metadata.comment_lines = 0
-        dataset.metadata.column_types = ['float', 'float', 'float']
+        dataset.metadata.column_types = ["float", "float", "float"]
         dataset.metadata.columns = 3
         if dataset.metadata.column_names is None or dataset.metadata.column_names == []:
             dataset.metadata.comment_lines += 1
             dataset.metadata.data_lines -= 1
-            dataset.metadata.column_names = ['SEC', 'MZ', 'INT']
+            dataset.metadata.column_names = ["SEC", "MZ", "INT"]
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         sep = None
         header = None
         for idx, line in enumerate(file_prefix.line_iterator()):
             line = line.strip()
             if sep is None:
                 sep = self._parse_delimiter(line)
                 if sep is None:
@@ -462,27 +578,28 @@
     >>> fname = get_test_fname('test.edta')
     >>> Edta().sniff(fname)
     True
     >>> fname = get_test_fname('test.dta2d')
     >>> Edta().sniff(fname)
     False
     """
+
     file_ext = "edta"
     comment_lines = 0
 
-    def _parse_delimiter(self, line):
+    def _parse_delimiter(self, line: str) -> Optional[str]:
         if len(line.split(" ")) >= 3:
             return " "
         elif len(line.split("\t")) >= 3:
             return "\t"
         elif len(line.split(",")) >= 3:
             return "\t"
         return None
 
-    def _parse_type(self, line):
+    def _parse_type(self, line: List) -> Optional[int]:
         """
         parse the type from the header line
         types 1-3 as in the class docs, 0: type 1 wo/wrong header
         """
         if len(line) < 3:
             return None
         line = [_.lower().replace("/", "") for _ in line]
@@ -494,28 +611,28 @@
         if line[0] != "rt" or line[1] != "mz" or (line[2] != "int" and line[2] != "intensity") or line[3] != "charge":
             return None
         if not line[4].startswith("rt"):
             return 2
         else:
             return 3
 
-    def _parse_dataline(self, line, tpe):
+    def _parse_dataline(self, line: List, tpe: Optional[int]) -> bool:
         if tpe == 2 or tpe == 3:
             idx = 4
         else:
             idx = 3
         try:
             line = [float(_) for _ in line[:idx]]
         except ValueError:
             return False
         if not all(_ >= 0 for _ in line[:idx]):
             return False
         return True
 
-    def _clean_header(self, line):
+    def _clean_header(self, line: List) -> List:
         for idx, el in enumerate(line):
             el = el.lower()
             if el.startswith("rt"):
                 line[idx] = "RT"
             elif el.startswith("int"):
                 line[idx] = "intensity"
             elif el.startswith("mz"):
@@ -524,15 +641,15 @@
                 line[idx] = "charge"
             else:
                 break
             if idx // 4 > 0:
                 line[idx] += str(idx // 4)
         return line
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         data_lines = 0
         delim = None
         tpe = None
         if dataset.has_data():
             with open(dataset.file_name) as dtafile:
                 for idx, line in enumerate(dtafile):
                     if idx == 0:
@@ -556,15 +673,15 @@
         dataset.metadata.data_lines = data_lines
         dataset.metadata.comment_lines = 0
         dataset.metadata.columns = len(dataset.metadata.column_names)
         if tpe is not None and tpe > 0:
             dataset.metadata.comment_lines += 1
             dataset.metadata.data_lines -= 1
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         sep = None
         tpe = None
         for idx, line in enumerate(file_prefix.line_iterator()):
             line = line.strip("\r\n")
             if sep is None:
                 sep = self._parse_delimiter(line)
                 if sep is None:
@@ -581,95 +698,104 @@
                 return False
         if tpe is None:
             return False
         return True
 
 
 class ProteomicsXml(GenericXml):
-    """ An enhanced XML datatype used to reuse code across several
-    proteomic/mass-spec datatypes. """
+    """An enhanced XML datatype used to reuse code across several
+    proteomic/mass-spec datatypes."""
+
     edam_data = "data_2536"
     edam_format = "format_2032"
     root: str
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """ Determines whether the file is the correct XML type. """
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file is the correct XML type."""
         for line in file_prefix.line_iterator():
             line = line.strip()
-            if not line.startswith('<?'):
+            if not line.startswith("<?"):
                 break
         # pattern match <root or <ns:root for any ns string
-        pattern = r'<(\w*:)?%s' % self.root
+        pattern = r"<(\w*:)?%s" % self.root
         return re.search(pattern, line) is not None
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = self.blurb
+            dataset.blurb = "ProteomicsXML data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class ParamXml(ProteomicsXml):
     """store Parameters in XML formal"""
+
     file_ext = "paramxml"
     blurb = "parameters in xmls"
     root = "parameters|PARAMETERS"
 
 
 class PepXml(ProteomicsXml):
     """pepXML data"""
+
     edam_format = "format_3655"
     file_ext = "pepxml"
-    blurb = 'pepXML data'
+    blurb = "pepXML data"
     root = "msms_pipeline_analysis"
 
 
 class MascotXML(ProteomicsXml):
     """mzXML data"""
+
     file_ext = "mascotxml"
     blurb = "mascot Mass Spectrometry data"
     root = "mascot_search_results"
 
 
 class MzML(ProteomicsXml):
     """mzML data"""
+
     edam_format = "format_3244"
     file_ext = "mzml"
-    blurb = 'mzML Mass Spectrometry data'
+    blurb = "mzML Mass Spectrometry data"
     root = "(mzML|indexedmzML)"
 
 
 class NmrML(ProteomicsXml):
     """nmrML data"""
+
     # No edam format number yet.
     file_ext = "nmrml"
-    blurb = 'nmrML NMR data'
+    blurb = "nmrML NMR data"
     root = "nmrML"
 
 
 class ProtXML(ProteomicsXml):
     """protXML data"""
+
     file_ext = "protxml"
-    blurb = 'prot XML Search Results'
+    blurb = "prot XML Search Results"
     root = "protein_summary"
 
 
 class MzXML(ProteomicsXml):
     """mzXML data"""
+
     edam_format = "format_3654"
     file_ext = "mzxml"
     blurb = "mzXML Mass Spectrometry data"
     root = "mzXML"
 
 
 class MzData(ProteomicsXml):
     """mzData data"""
+
     edam_format = "format_3245"
     file_ext = "mzdata"
     blurb = "mzData Mass Spectrometry data"
     root = "mzData"
 
 
 class MzIdentML(ProteomicsXml):
@@ -734,267 +860,275 @@
     file_ext = "xquest.xml"
     blurb = "XQuest XML file"
     root = "xquest_results"
 
 
 class XquestSpecXML(ProteomicsXml):
     """spec.xml"""
+
     file_ext = "spec.xml"
-    blurb = 'xquest_spectra'
+    blurb = "xquest_spectra"
     root = "xquest_spectra"
 
 
 class QCML(ProteomicsXml):
     """qcml
     https://github.com/OpenMS/OpenMS/blob/113c49d01677f7f03343ce7cd542d83c99b351ee/share/OpenMS/SCHEMAS/mzQCML_0_0_5.xsd
     https://github.com/OpenMS/OpenMS/blob/3cfc57ad1788e7ab2bd6dd9862818b2855234c3f/share/OpenMS/SCHEMAS/qcML_0.0.7.xsd
     """
+
     file_ext = "qcml"
-    blurb = 'QualityAssessments to runs'
+    blurb = "QualityAssessments to runs"
     root = "qcML|MzQualityML)"
 
 
 class Mgf(Text):
     """Mascot Generic Format data"""
+
     edam_data = "data_2536"
     edam_format = "format_3651"
     file_ext = "mgf"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'mgf Mascot Generic Format'
+            dataset.blurb = "mgf Mascot Generic Format"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         mgf_begin_ions = "BEGIN IONS"
         max_lines = 100
 
         with open(filename) as handle:
             for i, line in enumerate(handle):
                 line = line.rstrip()
                 if line == mgf_begin_ions:
                     return True
                 if i > max_lines:
                     return False
+        return False
 
 
 class MascotDat(Text):
-    """Mascot search results """
+    """Mascot search results"""
+
     edam_data = "data_2536"
     edam_format = "format_3713"
     file_ext = "mascotdat"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'mascotdat Mascot Search Results'
+            dataset.blurb = "mascotdat Mascot Search Results"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         mime_version = "MIME-Version: 1.0 (Generated by Mascot version 1.0)"
         max_lines = 10
 
         with open(filename) as handle:
             for i, line in enumerate(handle):
                 line = line.rstrip()
                 if line == mime_version:
                     return True
                 if i > max_lines:
                     return False
+        return False
 
 
 class ThermoRAW(Binary):
     """Class describing a Thermo Finnigan binary RAW file"""
+
     edam_data = "data_2536"
     edam_format = "format_3712"
     file_ext = "thermo.raw"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # Thermo Finnigan RAW format is proprietary and hence not well documented.
         # Files start with 2 bytes that seem to differ followed by F\0i\0n\0n\0i\0g\0a\0n
         # This combination represents 17 bytes, but to play safe we read 20 bytes from
         # the start of the file.
         try:
-            header = open(filename, 'rb').read(20)
-            finnigan = b'F\0i\0n\0n\0i\0g\0a\0n'
+            header = open(filename, "rb").read(20)
+            finnigan = b"F\0i\0n\0n\0i\0g\0a\0n"
             if header.find(finnigan) != -1:
                 return True
             return False
         except Exception:
             return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "Thermo Finnigan RAW file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"Thermo Finnigan RAW file ({nice_size(dataset.get_size())})"
 
 
 @build_sniff_from_prefix
 class Msp(Text):
-    """ Output of NIST MS Search Program chemdata.nist.gov/mass-spc/ftp/mass-spc/PepLib.pdf """
+    """Output of NIST MS Search Program chemdata.nist.gov/mass-spc/ftp/mass-spc/PepLib.pdf"""
+
     file_ext = "msp"
 
     @staticmethod
-    def next_line_starts_with(contents, prefix):
+    def next_line_starts_with(contents: IO, prefix: str) -> bool:
         next_line = contents.readline()
         return next_line is not None and next_line.startswith(prefix)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """ Determines whether the file is a NIST MSP output file."""
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file is a NIST MSP output file."""
         begin_contents = file_prefix.contents_header
         if "\n" not in begin_contents:
             return False
         lines = begin_contents.splitlines()
         if len(lines) < 2:
             return False
         return lines[0].startswith("Name:") and lines[1].startswith("MW:")
 
 
 class SPLibNoIndex(Text):
-    """SPlib without index file """
+    """SPlib without index file"""
+
     file_ext = "splib_noindex"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'Spectral Library without index files'
+            dataset.blurb = "Spectral Library without index files"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class SPLib(Msp):
     """SpectraST Spectral Library. Closely related to msp format"""
+
     file_ext = "splib"
-    composite_type = 'auto_primary_file'
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('library.splib',
-                                description='Spectral Library. Contains actual library spectra',
-                                is_binary=False)
-        self.add_composite_file('library.spidx',
-                                description='Spectrum index', is_binary=False)
-        self.add_composite_file('library.pepidx',
-                                description='Peptide index', is_binary=False)
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>Spectral Library Composite Dataset </title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+        self.add_composite_file(
+            "library.splib", description="Spectral Library. Contains actual library spectra", is_binary=False
+        )
+        self.add_composite_file("library.spidx", description="Spectrum index", is_binary=False)
+        self.add_composite_file("library.pepidx", description="Peptide index", is_binary=False)
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>Spectral Library Composite Dataset </title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
-            opt_text = ''
+            opt_text = ""
             if composite_file.optional:
-                opt_text = ' (optional)'
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+                opt_text = " (optional)"
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'splib Spectral Library Format'
+            dataset.blurb = "splib Spectral Library Format"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """ Determines whether the file is a SpectraST generated file.
-        """
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file is a SpectraST generated file."""
         contents = file_prefix.string_io()
         return Msp.next_line_starts_with(contents, "Name:") and Msp.next_line_starts_with(contents, "LibID:")
 
 
 @build_sniff_from_prefix
 class Ms2(Text):
     file_ext = "ms2"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """ Determines whether the file is a valid ms2 file."""
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file is a valid ms2 file."""
         header_lines = []
         for line in file_prefix.line_iterator():
             if line.strip() == "":
                 continue
-            elif line.startswith('H\t'):
+            elif line.startswith("H\t"):
                 header_lines.append(line)
             else:
                 break
 
-        for header_field in ['CreationDate', 'Extractor', 'ExtractorVersion', 'ExtractorOptions']:
+        for header_field in ["CreationDate", "Extractor", "ExtractorVersion", "ExtractorOptions"]:
             found_header = False
             for header_line in header_lines:
-                if header_line.startswith(f'H\t{header_field}'):
+                if header_line.startswith(f"H\t{header_field}"):
                     found_header = True
                     break
             if not found_header:
                 return False
 
         return True
 
 
 # unsniffable binary format, should do something about this
 class XHunterAslFormat(Binary):
-    """ Annotated Spectra in the HLF format http://www.thegpm.org/HUNTER/format_2006_09_15.html """
+    """Annotated Spectra in the HLF format http://www.thegpm.org/HUNTER/format_2006_09_15.html"""
+
     file_ext = "hlf"
 
 
 class Sf3(Binary):
     """Class describing a Scaffold SF3 files"""
+
     file_ext = "sf3"
 
 
 class ImzML(Binary):
     """
-        Class for imzML files.
-        http://www.imzml.org
+    Class for imzML files.
+    http://www.imzml.org
     """
+
     edam_format = "format_3682"
-    file_ext = 'imzml'
-    composite_type = 'auto_primary_file'
+    file_ext = "imzml"
+    composite_type = "auto_primary_file"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-        self.add_composite_file(
-            'imzml',
-            description='The imzML metadata component.',
-            is_binary=False)
+        self.add_composite_file("imzml", description="The imzML metadata component.", is_binary=False)
 
-        self.add_composite_file(
-            'ibd',
-            description='The mass spectral data component.',
-            is_binary=True)
-
-    def generate_primary_file(self, dataset=None):
-        rval = ['<html><head><title>imzML Composite Dataset </title></head><p/>']
-        rval.append('<div>This composite dataset is composed of the following files:<p/><ul>')
+        self.add_composite_file("ibd", description="The mass spectral data component.", is_binary=True)
+
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
+        rval = ["<html><head><title>imzML Composite Dataset </title></head><p/>"]
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
         for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
-            opt_text = ''
-            if composite_file.get('description'):
-                rval.append(f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>")
+            opt_text = ""
+            if composite_file.get("description"):
+                rval.append(
+                    f"<li><a href=\"{fn}\" type=\"text/plain\">{fn} ({composite_file.get('description')})</a>{opt_text}</li>"
+                )
             else:
                 rval.append(f'<li><a href="{fn}" type="text/plain">{fn}</a>{opt_text}</li>')
-        rval.append('</ul></div></html>')
+        rval.append("</ul></div></html>")
         return "\n".join(rval)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/qualityscore.py` & `galaxy-data-23.0.1/galaxy/datatypes/qualityscore.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,59 +1,63 @@
 """
 Qualityscore class
 """
 import logging
+from typing import TYPE_CHECKING
 
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
-from . import (
-    data,
-)
+from . import data
+
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
 
 log = logging.getLogger(__name__)
 
 
 class QualityScore(data.Text):
     """
     until we know more about quality score formats
     """
+
     edam_data = "data_2048"
     edam_format = "format_3606"
     file_ext = "qual"
 
 
 @build_sniff_from_prefix
 class QualityScoreSOLiD(QualityScore):
     """
     until we know more about quality score formats
     """
+
     edam_format = "format_3610"
     file_ext = "qualsolid"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'sequence.fasta' )
         >>> QualityScoreSOLiD().sniff( fname )
         False
         >>> fname = get_test_fname( 'sequence.qualsolid' )
         >>> QualityScoreSOLiD().sniff( fname )
         True
         """
         fh = file_prefix.string_io()
         readlen = None
         goodblock = 0
         for line in fh:
             line = line.strip()
-            if not line.startswith('#'):  # first non-empty non-comment line
-                if line.startswith('>'):
+            if not line.startswith("#"):  # first non-empty non-comment line
+                if line.startswith(">"):
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         return False
                     try:
                         [int(x) for x in line.split()]
                         if not readlen:
                             readlen = len(line.split())
                         assert len(line.split()) == readlen  # SOLiD reads should be of the same length
                     except Exception:
@@ -61,46 +65,47 @@
                     goodblock += 1
                     if goodblock > 10:
                         return True
                 else:
                     return False
         return goodblock > 0
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
             dataset.metadata.data_lines = None
             return
-        return QualityScore.set_meta(self, dataset, **kwd)
+        return QualityScore.set_meta(self, dataset, overwrite=overwrite, **kwd)
 
 
 @build_sniff_from_prefix
 class QualityScore454(QualityScore):
     """
     until we know more about quality score formats
     """
+
     edam_format = "format_3611"
     file_ext = "qual454"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'sequence.fasta' )
         >>> QualityScore454().sniff( fname )
         False
         >>> fname = get_test_fname( 'sequence.qual454' )
         >>> QualityScore454().sniff( fname )
         True
         """
         fh = file_prefix.string_io()
         for line in fh:
             line = line.strip()
-            if line and not line.startswith('#'):  # first non-empty non-comment line
-                if line.startswith('>'):
+            if line and not line.startswith("#"):  # first non-empty non-comment line
+                if line.startswith(">"):
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         break
                     try:
                         [int(x) for x in line.split()]
                     except Exception:
                         return False
                     return True
                 else:
@@ -108,17 +113,19 @@
         return False
 
 
 class QualityScoreSolexa(QualityScore):
     """
     until we know more about quality score formats
     """
+
     edam_format = "format_3608"
     file_ext = "qualsolexa"
 
 
 class QualityScoreIllumina(QualityScore):
     """
     until we know more about quality score formats
     """
+
     edam_format = "format_3609"
     file_ext = "qualillumina"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/registry.py` & `galaxy-data-23.0.1/galaxy/datatypes/registry.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,444 +1,451 @@
 """
 Provides mapping between extensions and datatypes, mime-types, etc.
 """
 
-import imp
+import importlib.util
 import logging
 import os
+import pkgutil
+from pathlib import Path
 from string import Template
-from typing import Dict, List, Optional, Tuple, TYPE_CHECKING
+from typing import (
+    Dict,
+    List,
+    Optional,
+    Tuple,
+    TYPE_CHECKING,
+)
 
 import yaml
 
 import galaxy.util
+from galaxy.tool_util.edam_util import load_edam_tree
 from galaxy.util import RW_R__R__
 from galaxy.util.bunch import Bunch
 from . import (
     binary,
     coverage,
     data,
     images,
     interval,
     qualityscore,
     sequence,
     tabular,
     text,
     tracks,
-    xml
+    xml,
 )
 from .display_applications.application import DisplayApplication
 
 if TYPE_CHECKING:
     from galaxy.model import DatasetInstance
 
 
 class ConfigurationError(Exception):
     pass
 
 
 class Registry:
-
     def __init__(self, config=None):
+        edam_ontology_path = config.get("edam_toolbox_ontology_path", None) if config is not None else None
+
+        edam = load_edam_tree(
+            None if not edam_ontology_path or not os.path.exists(edam_ontology_path) else edam_ontology_path,
+            "format_",
+            "data_",
+            "operation_",
+            "topic_",
+        )
+
         self.log = logging.getLogger(__name__)
         self.log.addHandler(logging.NullHandler())
         self.config = config
+        self.edam = edam
         self.datatypes_by_extension = {}
+        self.datatypes_by_suffix_inferences = {}
         self.mimetypes_by_extension = {}
         self.datatype_converters = {}
         # Converters defined in local datatypes_conf.xml
         self.converters = []
         self.converter_tools = set()
-        # Converters defined in datatypes_conf.xml included in installed tool shed repositories.
-        self.proprietary_converters = []
         self.converter_deps = {}
         self.available_tracks = []
         self.set_external_metadata_tool = None
         self.sniff_order = []
         self.upload_file_formats = []
         # Datatype elements defined in local datatypes_conf.xml that contain display applications.
         self.display_app_containers = []
-        # Datatype elements in datatypes_conf.xml included in installed
-        # tool shed repositories that contain display applications.
-        self.proprietary_display_app_containers = []
         # Map a display application id to a display application
         self.display_applications: Dict[str, DisplayApplication] = {}
         # The following 2 attributes are used in the to_xml_file()
         # method to persist the current state into an xml file.
         self.display_path_attr = None
         self.converters_path_attr = None
         # The 'default' converters_path defined in local datatypes_conf.xml
         self.converters_path = None
         # The 'default' display_path defined in local datatypes_conf.xml
         self.display_applications_path = None
         self.inherit_display_application_by_class = []
-        # Keep a list of imported proprietary datatype class modules.
-        self.imported_modules = []
         self.datatype_elems = []
         self.datatype_info_dicts = []
         self.sniffer_elems = []
         self._registry_xml_string = None
         self._edam_formats_mapping = None
         self._edam_data_mapping = None
         self._converters_by_datatype = {}
         # Build sites
         self.build_sites = {}
         self.display_sites = {}
         self.legacy_build_sites = {}
 
-    def load_datatypes(self, root_dir=None, config=None, deactivate=False, override=True, use_converters=True, use_display_applications=True, use_build_sites=True):
+    def load_datatypes(
+        self,
+        root_dir=None,
+        config=None,
+        override=True,
+        use_converters=True,
+        use_display_applications=True,
+        use_build_sites=True,
+    ):
         """
         Parse a datatypes XML file located at root_dir/config (if processing the Galaxy distributed config) or contained within
-        an installed Tool Shed repository.  If deactivate is True, an installed Tool Shed repository that includes custom datatypes
-        is being deactivated or uninstalled, so appropriate loaded datatypes will be removed from the registry.  The value of
-        override will be False when a Tool Shed repository is being installed.  Since installation is occurring after the datatypes
-        registry has been initialized at server startup, its contents cannot be overridden by newly introduced conflicting data types.
+        an installed Tool Shed repository.
         """
 
-        def __import_module(full_path, datatype_module, datatype_class_name):
-            open_file_obj, file_name, description = imp.find_module(datatype_module, [full_path])
-            imported_module = imp.load_module(datatype_class_name, open_file_obj, file_name, description)
-            return imported_module
+        def __import_module(full_path: str, datatype_module: str):
+            path_entry_finder = pkgutil.get_importer(full_path)
+            assert path_entry_finder, "path_entry_finder is None"
+            spec = path_entry_finder.find_spec(datatype_module)
+            assert spec, "spec is None"
+            module = importlib.util.module_from_spec(spec)
+            assert spec.loader, "spec.loader is None"
+            spec.loader.exec_module(module)
+            return module
 
         if root_dir and config:
-            # If handling_proprietary_datatypes is determined as True below, we'll have an elem that looks something like this:
-            # <datatype display_in_upload="true"
-            #           extension="blastxml"
-            #           mimetype="application/xml"
-            #           proprietary_datatype_module="blast"
-            #           proprietary_path="[cloned repository path]"
-            #           type="galaxy.datatypes.blast:BlastXml" />
             compressed_sniffers = {}
-            handling_proprietary_datatypes = False
-            if isinstance(config, str):
+            if isinstance(config, (str, Path)):
                 # Parse datatypes_conf.xml
                 tree = galaxy.util.parse_xml(config)
                 root = tree.getroot()
                 # Load datatypes and converters from config
-                if deactivate:
-                    self.log.debug(f'Deactivating datatypes from {config}')
-                else:
-                    self.log.debug(f'Loading datatypes from {config}')
+                self.log.debug(f"Loading datatypes from {config}")
             else:
                 root = config
-            registration = root.find('registration')
+            registration = root.find("registration")
             # Set default paths defined in local datatypes_conf.xml.
             if use_converters:
                 if not self.converters_path:
-                    self.converters_path_attr = registration.get('converters_path', 'lib/galaxy/datatypes/converters')
+                    self.converters_path_attr = registration.get("converters_path", "lib/galaxy/datatypes/converters")
                     self.converters_path = os.path.join(root_dir, self.converters_path_attr)
-                    if self.converters_path_attr == 'lib/galaxy/datatypes/converters' \
-                            and not os.path.isdir(self.converters_path):
+                    if self.converters_path_attr == "lib/galaxy/datatypes/converters" and not os.path.isdir(
+                        self.converters_path
+                    ):
                         # Deal with the old default of this path being set in
                         # datatypes_conf.xml.sample (this path is not useful in an
                         # "installed Galaxy" world)
-                        self.converters_path_attr = os.path.abspath(os.path.join(os.path.dirname(__file__), 'converters'))
+                        self.converters_path_attr = os.path.abspath(
+                            os.path.join(os.path.dirname(__file__), "converters")
+                        )
                         self.converters_path = self.converters_path_attr
                     if not os.path.isdir(self.converters_path):
                         raise ConfigurationError(f"Directory does not exist: {self.converters_path}")
             if use_display_applications:
                 if not self.display_applications_path:
-                    self.display_path_attr = registration.get('display_path', 'display_applications')
+                    self.display_path_attr = registration.get("display_path", "display_applications")
                     self.display_applications_path = os.path.join(root_dir, self.display_path_attr)
-                    if self.display_path_attr == 'display_applications' \
-                            and not os.path.isdir('display_applications'):
+                    if self.display_path_attr == "display_applications" and not os.path.isdir("display_applications"):
                         # Ditto as with converters_path
-                        self.display_path_attr = os.path.abspath(os.path.join(os.path.dirname(__file__), 'display_applications', 'configs'))
+                        self.display_path_attr = os.path.abspath(
+                            os.path.join(os.path.dirname(__file__), "display_applications", "configs")
+                        )
                         self.display_applications_path = self.display_path_attr
-            # Proprietary datatype's <registration> tag may have special attributes, proprietary_converter_path and proprietary_display_path.
-            proprietary_converter_path = registration.get('proprietary_converter_path', None)
-            proprietary_display_path = registration.get('proprietary_display_path', None)
-            if proprietary_converter_path is not None or proprietary_display_path is not None and not handling_proprietary_datatypes:
-                handling_proprietary_datatypes = True
-            for elem in registration.findall('datatype'):
+
+            for elem in registration.findall("datatype"):
                 # Keep a status of the process steps to enable stopping the process of handling the datatype if necessary.
                 ok = True
                 extension = self.get_extension(elem)
-                dtype = elem.get('type', None)
-                type_extension = elem.get('type_extension', None)
-                auto_compressed_types = galaxy.util.listify(elem.get('auto_compressed_types', ''))
+                dtype = elem.get("type", None)
+                type_extension = elem.get("type_extension", None)
+                auto_compressed_types = galaxy.util.listify(elem.get("auto_compressed_types", ""))
                 sniff_compressed_types = galaxy.util.string_as_bool_or_none(elem.get("sniff_compressed_types", "None"))
                 if sniff_compressed_types is None:
                     sniff_compressed_types = getattr(self.config, "sniff_compressed_dynamic_datatypes_default", True)
                     # Make sure this is set in the elems we write out so the config option is passed to the upload
                     # tool which does not have a config object.
                     elem.set("sniff_compressed_types", str(sniff_compressed_types))
-                mimetype = elem.get('mimetype', None)
-                display_in_upload = galaxy.util.string_as_bool(elem.get('display_in_upload', False))
+                mimetype = elem.get("mimetype", None)
+                display_in_upload = galaxy.util.string_as_bool(elem.get("display_in_upload", False))
                 # If make_subclass is True, it does not necessarily imply that we are subclassing a datatype that is contained
                 # in the distribution.
-                make_subclass = galaxy.util.string_as_bool(elem.get('subclass', False))
-                edam_format = elem.get('edam_format', None)
+                make_subclass = galaxy.util.string_as_bool(elem.get("subclass", False))
+                edam_format = elem.get("edam_format", None)
                 if edam_format and not make_subclass:
                     self.log.warning("Cannot specify edam_format without setting subclass to True, skipping datatype.")
                     continue
-                edam_data = elem.get('edam_data', None)
+                edam_data = elem.get("edam_data", None)
                 if edam_data and not make_subclass:
                     self.log.warning("Cannot specify edam_data without setting subclass to True, skipping datatype.")
                     continue
-                # Proprietary datatypes included in installed tool shed repositories will include two special attributes
-                # (proprietary_path and proprietary_datatype_module) if they depend on proprietary datatypes classes.
-                # The value of proprietary_path is the path to the cloned location of the tool shed repository's contained
-                # datatypes_conf.xml file.
-                proprietary_path = elem.get('proprietary_path', None)
-                proprietary_datatype_module = elem.get('proprietary_datatype_module', None)
-                if proprietary_path is not None or proprietary_datatype_module is not None and not handling_proprietary_datatypes:
-                    handling_proprietary_datatypes = True
-                if deactivate:
-                    # We are deactivating or uninstalling an installed tool shed repository, so eliminate the datatype
-                    # elem from the in-memory list of datatype elems.
-                    for in_memory_elem in self.datatype_elems:
-                        in_memory_extension = in_memory_elem.get('extension', None)
-                        if in_memory_extension == extension:
-                            in_memory_dtype = elem.get('type', None)
-                            in_memory_type_extension = elem.get('type_extension', None)
-                            in_memory_mimetype = elem.get('mimetype', None)
-                            in_memory_display_in_upload = galaxy.util.string_as_bool(elem.get('display_in_upload', False))
-                            in_memory_make_subclass = galaxy.util.string_as_bool(elem.get('subclass', False))
-                            if in_memory_dtype == dtype and \
-                                    in_memory_type_extension == type_extension and \
-                                    in_memory_mimetype == mimetype and \
-                                    in_memory_display_in_upload == display_in_upload and \
-                                    in_memory_make_subclass == make_subclass:
-                                self.datatype_elems.remove(in_memory_elem)
-                    if extension is not None and extension in self.datatypes_by_extension:
-                        # We are deactivating or uninstalling an installed tool shed repository, so eliminate the datatype
-                        # from the registry.  TODO: Handle deactivating datatype converters, etc before removing from
-                        # self.datatypes_by_extension.
-                        del self.datatypes_by_extension[extension]
-                        if extension in self.upload_file_formats:
-                            self.upload_file_formats.remove(extension)
-                        self.log.debug(f"Removed datatype with extension '{extension}' from the registry.")
-                else:
-                    # We are loading new datatype, so we'll make sure it is correctly defined before proceeding.
-                    can_process_datatype = False
-                    if extension is not None:
-                        if dtype is not None or type_extension is not None:
-                            if override or extension not in self.datatypes_by_extension:
-                                can_process_datatype = True
-                    if can_process_datatype:
-                        if dtype is not None:
-                            try:
-                                fields = dtype.split(':')
-                                datatype_module = fields[0]
-                                datatype_class_name = fields[1]
-                            except Exception:
-                                self.log.exception('Error parsing datatype definition for dtype %s', str(dtype))
-                                ok = False
-                            if ok:
-                                datatype_class = None
-                                if proprietary_path and proprietary_datatype_module and datatype_class_name:
-                                    # TODO: previously comments suggested this needs to be locked because it modifies
-                                    # the sys.path, probably true but the previous lock wasn't doing that.
-                                    try:
-                                        imported_module = __import_module(proprietary_path,
-                                                                          proprietary_datatype_module,
-                                                                          datatype_class_name)
-                                        if imported_module not in self.imported_modules:
-                                            self.imported_modules.append(imported_module)
-                                        if hasattr(imported_module, datatype_class_name):
-                                            datatype_class = getattr(imported_module, datatype_class_name)
-                                    except Exception as e:
-                                        full_path = os.path.join(proprietary_path, proprietary_datatype_module)
-                                        self.log.debug("Exception importing proprietary code file %s: %s", full_path, galaxy.util.unicodify(e))
-                                # Either the above exception was thrown because the proprietary_datatype_module is not derived from a class
-                                # in the repository, or we are loading Galaxy's datatypes. In either case we'll look in the registry.
-                                if datatype_class is None:
-                                    try:
-                                        # The datatype class name must be contained in one of the datatype modules in the Galaxy distribution.
-                                        fields = datatype_module.split('.')[1:]
-                                        module = __import__(datatype_module)
-                                        for mod in fields:
-                                            module = getattr(module, mod)
-                                        datatype_class = getattr(module, datatype_class_name)
-                                        self.log.debug(f'Retrieved datatype module {str(datatype_module)}:{datatype_class_name} from the datatype registry for extension {extension}.')
-                                    except Exception:
-                                        self.log.exception('Error importing datatype module %s', str(datatype_module))
-                                        ok = False
-                        elif type_extension is not None:
-                            try:
-                                datatype_class = self.datatypes_by_extension[type_extension].__class__
-                                self.log.debug(f'Retrieved datatype module {str(datatype_class.__name__)} from type_extension {type_extension} for extension {extension}.')
-                            except Exception:
-                                self.log.exception('Error determining datatype_class for type_extension %s', str(type_extension))
-                                ok = False
+
+                # We are loading new datatype, so we'll make sure it is correctly defined before proceeding.
+                can_process_datatype = False
+                if extension is not None:
+                    if dtype is not None or type_extension is not None:
+                        if override or extension not in self.datatypes_by_extension:
+                            can_process_datatype = True
+                if can_process_datatype:
+                    if dtype is not None:
+                        try:
+                            fields = dtype.split(":")
+                            datatype_module = fields[0]
+                            datatype_class_name = fields[1]
+                        except Exception:
+                            self.log.exception("Error parsing datatype definition for dtype %s", str(dtype))
+                            ok = False
                         if ok:
-                            if not deactivate:
-                                # A new tool shed repository that contains custom datatypes is being installed, and since installation is
-                                # occurring after the datatypes registry has been initialized at server startup, its contents cannot be
-                                # overridden by new introduced conflicting data types unless the value of override is True.
-                                if extension in self.datatypes_by_extension:
-                                    # Because of the way that the value of can_process_datatype was set above, we know that the value of
-                                    # override is True.
-                                    self.log.debug("Overriding conflicting datatype with extension '%s', using datatype from %s." %
-                                                   (str(extension), str(config)))
-                                if make_subclass:
-                                    datatype_class = type(datatype_class_name, (datatype_class, ), {})
-                                    if edam_format:
-                                        datatype_class.edam_format = edam_format
-                                    if edam_data:
-                                        datatype_class.edam_data = edam_data
-                                datatype_class.is_subclass = make_subclass
-                                description = elem.get("description", None)
-                                description_url = elem.get("description_url", None)
-                                datatype_instance = datatype_class()
-                                self.datatypes_by_extension[extension] = datatype_instance
-                                if mimetype is None:
-                                    # Use default mimetype per datatype specification.
-                                    mimetype = self.datatypes_by_extension[extension].get_mime()
-                                self.mimetypes_by_extension[extension] = mimetype
-                                if datatype_class.track_type:
-                                    self.available_tracks.append(extension)
-                                if display_in_upload and extension not in self.upload_file_formats:
-                                    self.upload_file_formats.append(extension)
-                                # Max file size cut off for setting optional metadata.
-                                self.datatypes_by_extension[extension].max_optional_metadata_filesize = elem.get('max_optional_metadata_filesize', None)
-                                for converter in elem.findall('converter'):
-                                    # Build the list of datatype converters which will later be loaded into the calling app's toolbox.
-                                    converter_config = converter.get('file', None)
-                                    target_datatype = converter.get('target_datatype', None)
-                                    depends_on = converter.get('depends_on', None)
-                                    if depends_on is not None and target_datatype is not None:
-                                        if extension not in self.converter_deps:
-                                            self.converter_deps[extension] = {}
-                                        self.converter_deps[extension][target_datatype] = depends_on.split(',')
-                                    if converter_config and target_datatype:
-                                        if proprietary_converter_path:
-                                            self.proprietary_converters.append((converter_config, extension, target_datatype))
-                                        else:
-                                            self.converters.append((converter_config, extension, target_datatype))
-                                # Add composite files.
-                                for composite_file in elem.findall('composite_file'):
-                                    name = composite_file.get('name', None)
-                                    if name is None:
-                                        self.log.warning(f"You must provide a name for your composite_file ({composite_file}).")
-                                    optional = composite_file.get('optional', False)
-                                    mimetype = composite_file.get('mimetype', None)
-                                    self.datatypes_by_extension[extension].add_composite_file(name, optional=optional, mimetype=mimetype)
-                                for _display_app in elem.findall('display'):
-                                    if proprietary_display_path:
-                                        if elem not in self.proprietary_display_app_containers:
-                                            self.proprietary_display_app_containers.append(elem)
-                                    else:
-                                        if elem not in self.display_app_containers:
-                                            self.display_app_containers.append(elem)
-                                datatype_info_dict = {
+                            datatype_class = None
+                            if datatype_class is None:
+                                try:
+                                    # The datatype class name must be contained in one of the datatype modules in the Galaxy distribution.
+                                    fields = datatype_module.split(".")[1:]
+                                    module = __import__(datatype_module)
+                                    for mod in fields:
+                                        module = getattr(module, mod)
+                                    datatype_class = getattr(module, datatype_class_name)
+                                    self.log.debug(
+                                        f"Retrieved datatype module {str(datatype_module)}:{datatype_class_name} from the datatype registry for extension {extension}."
+                                    )
+                                except Exception:
+                                    self.log.exception("Error importing datatype module %s", str(datatype_module))
+                                    ok = False
+                    elif type_extension is not None:
+                        try:
+                            datatype_class = self.datatypes_by_extension[type_extension].__class__
+                            self.log.debug(
+                                f"Retrieved datatype module {str(datatype_class.__name__)} from type_extension {type_extension} for extension {extension}."
+                            )
+                        except Exception:
+                            self.log.exception(
+                                "Error determining datatype_class for type_extension %s", str(type_extension)
+                            )
+                            ok = False
+                    if ok:
+                        # A new tool shed repository that contains custom datatypes is being installed, and since installation is
+                        # occurring after the datatypes registry has been initialized at server startup, its contents cannot be
+                        # overridden by new introduced conflicting data types unless the value of override is True.
+                        if extension in self.datatypes_by_extension:
+                            # Because of the way that the value of can_process_datatype was set above, we know that the value of
+                            # override is True.
+                            self.log.debug(
+                                "Overriding conflicting datatype with extension '%s', using datatype from %s."
+                                % (str(extension), str(config))
+                            )
+                        if make_subclass:
+                            datatype_class = type(datatype_class_name, (datatype_class,), {})
+                            if edam_format:
+                                datatype_class.edam_format = edam_format
+                            if edam_data:
+                                datatype_class.edam_data = edam_data
+                        datatype_class.is_subclass = make_subclass
+                        description = elem.get("description", None)
+                        description_url = elem.get("description_url", None)
+                        datatype_instance = datatype_class()
+                        self.datatypes_by_extension[extension] = datatype_instance
+                        if mimetype is None:
+                            # Use default mimetype per datatype specification.
+                            mimetype = self.datatypes_by_extension[extension].get_mime()
+                        self.mimetypes_by_extension[extension] = mimetype
+                        if datatype_class.track_type:
+                            self.available_tracks.append(extension)
+                        if display_in_upload and extension not in self.upload_file_formats:
+                            self.upload_file_formats.append(extension)
+                        # Max file size cut off for setting optional metadata.
+                        self.datatypes_by_extension[extension].max_optional_metadata_filesize = elem.get(
+                            "max_optional_metadata_filesize", None
+                        )
+                        infer_from_suffixes = []
+                        # read from element instead of attribute so we can customize references to
+                        # compressed files in the future (e.g. maybe some day faz will be a compressed fasta
+                        # or something along those lines)
+                        for infer_from in elem.findall("infer_from"):
+                            suffix = infer_from.get("suffix", None)
+                            if suffix is None:
+                                raise Exception("Failed to parse infer_from datatype element")
+                            infer_from_suffixes.append(suffix)
+                            self.datatypes_by_suffix_inferences[suffix] = datatype_instance
+                        for converter in elem.findall("converter"):
+                            # Build the list of datatype converters which will later be loaded into the calling app's toolbox.
+                            converter_config = converter.get("file", None)
+                            target_datatype = converter.get("target_datatype", None)
+                            depends_on = converter.get("depends_on", None)
+                            if depends_on is not None and target_datatype is not None:
+                                if extension not in self.converter_deps:
+                                    self.converter_deps[extension] = {}
+                                self.converter_deps[extension][target_datatype] = depends_on.split(",")
+                            if converter_config and target_datatype:
+                                self.converters.append((converter_config, extension, target_datatype))
+                        # Add composite files.
+                        for composite_file in elem.findall("composite_file"):
+                            name = composite_file.get("name", None)
+                            if name is None:
+                                self.log.warning(f"You must provide a name for your composite_file ({composite_file}).")
+                            optional = composite_file.get("optional", False)
+                            mimetype = composite_file.get("mimetype", None)
+                            self.datatypes_by_extension[extension].add_composite_file(
+                                name, optional=optional, mimetype=mimetype
+                            )
+                        for _display_app in elem.findall("display"):
+                            if elem not in self.display_app_containers:
+                                self.display_app_containers.append(elem)
+                        datatype_info_dict = {
+                            "display_in_upload": display_in_upload,
+                            "extension": extension,
+                            "description": description,
+                            "description_url": description_url,
+                        }
+                        composite_files = datatype_instance.get_composite_files()
+                        if composite_files:
+                            _composite_files = []
+                            for name, composite_file in composite_files.items():
+                                _composite_file = composite_file.dict()
+                                _composite_file["name"] = name
+                                _composite_files.append(_composite_file)
+                            datatype_info_dict["composite_files"] = _composite_files
+                        self.datatype_info_dicts.append(datatype_info_dict)
+
+                        for auto_compressed_type in auto_compressed_types:
+                            compressed_extension = f"{extension}.{auto_compressed_type}"
+                            upper_compressed_type = auto_compressed_type[0].upper() + auto_compressed_type[1:]
+                            auto_compressed_type_name = datatype_class_name + upper_compressed_type
+                            attributes = {}
+                            if auto_compressed_type == "gz":
+                                dynamic_parent = binary.GzDynamicCompressedArchive
+                            elif auto_compressed_type == "bz2":
+                                dynamic_parent = binary.Bz2DynamicCompressedArchive
+                            else:
+                                raise Exception(f"Unknown auto compression type [{auto_compressed_type}]")
+                            attributes["file_ext"] = compressed_extension
+                            attributes["uncompressed_datatype_instance"] = datatype_instance
+                            compressed_datatype_class = type(
+                                auto_compressed_type_name,
+                                (
+                                    datatype_class,
+                                    dynamic_parent,
+                                ),
+                                attributes,
+                            )
+                            if edam_format:
+                                compressed_datatype_class.edam_format = edam_format
+                            if edam_data:
+                                compressed_datatype_class.edam_data = edam_data
+                            compressed_datatype_instance = compressed_datatype_class()
+                            self.datatypes_by_extension[compressed_extension] = compressed_datatype_instance
+                            for suffix in infer_from_suffixes:
+                                self.datatypes_by_suffix_inferences[
+                                    f"{suffix}.{auto_compressed_type}"
+                                ] = compressed_datatype_instance
+                            if display_in_upload and compressed_extension not in self.upload_file_formats:
+                                self.upload_file_formats.append(compressed_extension)
+                            self.datatype_info_dicts.append(
+                                {
                                     "display_in_upload": display_in_upload,
-                                    "extension": extension,
+                                    "extension": compressed_extension,
                                     "description": description,
                                     "description_url": description_url,
                                 }
-                                composite_files = datatype_instance.composite_files
-                                if composite_files:
-                                    datatype_info_dict['composite_files'] = [_.dict() for _ in composite_files.values()]
-                                self.datatype_info_dicts.append(datatype_info_dict)
-
-                                for auto_compressed_type in auto_compressed_types:
-                                    compressed_extension = f"{extension}.{auto_compressed_type}"
-                                    upper_compressed_type = auto_compressed_type[0].upper() + auto_compressed_type[1:]
-                                    auto_compressed_type_name = datatype_class_name + upper_compressed_type
-                                    attributes = {}
-                                    if auto_compressed_type == "gz":
-                                        dynamic_parent = binary.GzDynamicCompressedArchive
-                                    elif auto_compressed_type == "bz2":
-                                        dynamic_parent = binary.Bz2DynamicCompressedArchive
-                                    else:
-                                        raise Exception(f"Unknown auto compression type [{auto_compressed_type}]")
-                                    attributes["file_ext"] = compressed_extension
-                                    attributes["uncompressed_datatype_instance"] = datatype_instance
-                                    compressed_datatype_class = type(auto_compressed_type_name, (datatype_class, dynamic_parent, ), attributes)
-                                    if edam_format:
-                                        compressed_datatype_class.edam_format = edam_format
-                                    if edam_data:
-                                        compressed_datatype_class.edam_data = edam_data
-                                    compressed_datatype_instance = compressed_datatype_class()
-                                    self.datatypes_by_extension[compressed_extension] = compressed_datatype_instance
-                                    if display_in_upload and compressed_extension not in self.upload_file_formats:
-                                        self.upload_file_formats.append(compressed_extension)
-                                    self.datatype_info_dicts.append({
-                                        "display_in_upload": display_in_upload,
-                                        "extension": compressed_extension,
-                                        "description": description,
-                                        "description_url": description_url,
-                                    })
-                                    if auto_compressed_type == 'gz':
-                                        self.converters.append((f"uncompressed_to_{auto_compressed_type}.xml", extension, compressed_extension))
-                                    self.converters.append((f"{auto_compressed_type}_to_uncompressed.xml", compressed_extension, extension))
-                                    if datatype_class not in compressed_sniffers:
-                                        compressed_sniffers[datatype_class] = []
-                                    if sniff_compressed_types:
-                                        compressed_sniffers[datatype_class].append(compressed_datatype_instance)
-                                # Processing the new datatype elem is now complete, so make sure the element defining it is retained by appending
-                                # the new datatype to the in-memory list of datatype elems to enable persistence.
-                                self.datatype_elems.append(elem)
+                            )
+                            if auto_compressed_type == "gz":
+                                self.converters.append(
+                                    (
+                                        f"uncompressed_to_{auto_compressed_type}.xml",
+                                        extension,
+                                        compressed_extension,
+                                    )
+                                )
+                            self.converters.append(
+                                (f"{auto_compressed_type}_to_uncompressed.xml", compressed_extension, extension)
+                            )
+                            if datatype_class not in compressed_sniffers:
+                                compressed_sniffers[datatype_class] = []
+                            if sniff_compressed_types:
+                                compressed_sniffers[datatype_class].append(compressed_datatype_instance)
+                        # Processing the new datatype elem is now complete, so make sure the element defining it is retained by appending
+                        # the new datatype to the in-memory list of datatype elems to enable persistence.
+                        self.datatype_elems.append(elem)
                     else:
                         if extension is not None:
                             if dtype is not None or type_extension is not None:
                                 if extension in self.datatypes_by_extension:
                                     if not override:
                                         # Do not load the datatype since it conflicts with an existing datatype which we are not supposed
                                         # to override.
-                                        self.log.debug(f"Ignoring conflicting datatype with extension '{extension}' from {config}.")
+                                        self.log.debug(
+                                            f"Ignoring conflicting datatype with extension '{extension}' from {config}."
+                                        )
             # Load datatype sniffers from the config - we'll do this even if one or more datatypes were not properly processed in the config
             # since sniffers are not tightly coupled with datatypes.
-            self.load_datatype_sniffers(root,
-                                        deactivate=deactivate,
-                                        handling_proprietary_datatypes=handling_proprietary_datatypes,
-                                        override=override,
-                                        compressed_sniffers=compressed_sniffers)
+            self.load_datatype_sniffers(
+                root,
+                override=override,
+                compressed_sniffers=compressed_sniffers,
+            )
             self.upload_file_formats.sort()
             # Load build sites
             if use_build_sites:
                 self._load_build_sites(root)
         self.set_default_values()
 
         def append_to_sniff_order():
             sniff_order_classes = {type(_) for _ in self.sniff_order}
             for datatype in self.datatypes_by_extension.values():
                 # Add a datatype only if it is not already in sniff_order, it
                 # has a sniff() method and was not defined with subclass="true".
                 # Do not add dynamic compressed types - these were carefully added or not
                 # to the sniff order in the proper position above.
-                if type(datatype) not in sniff_order_classes and \
-                        hasattr(datatype, 'sniff') and not datatype.is_subclass and \
-                        not hasattr(datatype, "uncompressed_datatype_instance"):
+                if (
+                    type(datatype) not in sniff_order_classes
+                    and hasattr(datatype, "sniff")
+                    and not datatype.is_subclass
+                    and not hasattr(datatype, "uncompressed_datatype_instance")
+                ):
                     self.sniff_order.append(datatype)
 
         append_to_sniff_order()
 
     def _load_build_sites(self, root):
-
         def load_build_site(build_site_config):
             # Take in either an XML element or simple dictionary from YAML and add build site for this.
-            if not (build_site_config.get('type') and build_site_config.get('file')):
+            if not (build_site_config.get("type") and build_site_config.get("file")):
                 self.log.exception("Site is missing required 'type' and 'file' attributes")
                 return
 
-            site_type = build_site_config.get('type')
-            path = build_site_config.get('file')
+            site_type = build_site_config.get("type")
+            path = build_site_config.get("file")
             if not os.path.exists(path):
                 sample_path = f"{path}.sample"
                 if os.path.exists(sample_path):
                     self.log.debug(f"Build site file [{path}] not found using sample [{sample_path}].")
                     path = sample_path
 
             self.build_sites[site_type] = path
-            if site_type in ('ucsc', 'gbrowse'):
+            if site_type in ("ucsc", "gbrowse"):
                 self.legacy_build_sites[site_type] = galaxy.util.read_build_sites(path)
-            if build_site_config.get('display', None):
-                display = build_site_config.get('display')
+            if build_site_config.get("display", None):
+                display = build_site_config.get("display")
                 if not isinstance(display, list):
-                    display = [x.strip() for x in display.lower().split(',')]
+                    display = [x.strip() for x in display.lower().split(",")]
                 self.display_sites[site_type] = display
                 self.log.debug("Loaded build site '%s': %s with display sites: %s", site_type, path, display)
             else:
                 self.log.debug("Loaded build site '%s': %s", site_type, path)
 
-        if root.find('build_sites') is not None:
-            for elem in root.find('build_sites').findall('site'):
+        if root.find("build_sites") is not None:
+            for elem in root.find("build_sites").findall("site"):
                 load_build_site(elem)
         else:
             build_sites_config_file = getattr(self.config, "build_sites_config_file", None)
             if build_sites_config_file and os.path.exists(build_sites_config_file):
                 with open(build_sites_config_file) as f:
                     build_sites_config = yaml.safe_load(f)
                 if not isinstance(build_sites_config, list):
@@ -449,140 +456,137 @@
                     load_build_site(build_site_config)
             else:
                 self.log.debug("No build sites source located.")
 
     def get_legacy_sites_by_build(self, site_type, build):
         sites = []
         for site in self.legacy_build_sites.get(site_type, []):
-            if build in site['builds']:
-                sites.append((site['name'], site['url']))
+            if build in site["builds"]:
+                sites.append((site["name"], site["url"]))
         return sites
 
     def get_display_sites(self, site_type):
         return self.display_sites.get(site_type, [])
 
-    def load_datatype_sniffers(self, root, deactivate=False, handling_proprietary_datatypes=False, override=False, compressed_sniffers=None):
+    def load_datatype_sniffers(self, root, override=False, compressed_sniffers=None):
         """
         Process the sniffers element from a parsed a datatypes XML file located at root_dir/config (if processing the Galaxy
-        distributed config) or contained within an installed Tool Shed repository.  If deactivate is True, an installed Tool
-        Shed repository that includes custom sniffers is being deactivated or uninstalled, so appropriate loaded sniffers will
-        be removed from the registry.  The value of override will be False when a Tool Shed repository is being installed.
-        Since installation is occurring after the datatypes registry has been initialized at server startup, its contents
-        cannot be overridden by newly introduced conflicting sniffers.
+        distributed config) or contained within an installed Tool Shed repository.
         """
-        sniffer_elem_classes = [e.attrib['type'] for e in self.sniffer_elems]
-        sniffers = root.find('sniffers')
+        sniffer_elem_classes = [e.attrib["type"] for e in self.sniffer_elems]
+        sniffers = root.find("sniffers")
         if sniffers is not None:
-            for elem in sniffers.findall('sniffer'):
+            for elem in sniffers.findall("sniffer"):
                 # Keep a status of the process steps to enable stopping the process of handling the sniffer if necessary.
                 ok = True
-                dtype = elem.get('type', None)
+                dtype = elem.get("type", None)
                 if dtype is not None:
                     try:
                         fields = dtype.split(":")
                         datatype_module = fields[0]
                         datatype_class_name = fields[1]
                         module = None
                     except Exception:
-                        self.log.exception('Error determining datatype class or module for dtype %s', str(dtype))
+                        self.log.exception("Error determining datatype class or module for dtype %s", str(dtype))
                         ok = False
                     if ok:
-                        if handling_proprietary_datatypes:
-                            # See if one of the imported modules contains the datatype class name.
-                            for imported_module in self.imported_modules:
-                                if hasattr(imported_module, datatype_class_name):
-                                    module = imported_module
-                                    break
                         if module is None:
                             try:
                                 # The datatype class name must be contained in one of the datatype modules in the Galaxy distribution.
                                 module = __import__(datatype_module)
-                                for comp in datatype_module.split('.')[1:]:
+                                for comp in datatype_module.split(".")[1:]:
                                     module = getattr(module, comp)
                             except Exception:
                                 self.log.exception("Error importing datatype class for '%s'", str(dtype))
                                 ok = False
                         if ok:
                             try:
                                 aclass = getattr(module, datatype_class_name)()
                             except Exception:
-                                self.log.exception('Error calling method %s from class %s', str(datatype_class_name), str(module))
+                                self.log.exception(
+                                    "Error calling method %s from class %s", str(datatype_class_name), str(module)
+                                )
                                 ok = False
                             if ok:
-                                if deactivate:
-                                    # We are deactivating or uninstalling an installed Tool Shed repository, so eliminate the appropriate sniffers.
-                                    sniffer_class = elem.get('type', None)
-                                    if sniffer_class is not None:
-                                        for index, s_e_c in enumerate(sniffer_elem_classes):
-                                            if sniffer_class == s_e_c:
-                                                del self.sniffer_elems[index]
-                                                sniffer_elem_classes = [elem.attrib['type'] for elem in self.sniffer_elems]
-                                                self.log.debug(f"Removed sniffer element for datatype '{str(dtype)}'")
-                                                break
-                                        for sniffer_class in self.sniff_order:
-                                            if sniffer_class.__class__ == aclass.__class__:
-                                                self.sniff_order.remove(sniffer_class)
-                                                self.log.debug(f"Removed sniffer class for datatype '{str(dtype)}' from sniff order")
-                                                break
-                                else:
-                                    # We are loading new sniffer, so see if we have a conflicting sniffer already loaded.
-                                    conflict = False
-                                    for conflict_loc, sniffer_class in enumerate(self.sniff_order):
-                                        if sniffer_class.__class__ == aclass.__class__:
-                                            # We have a conflicting sniffer, so replace the one previously loaded.
-                                            conflict = True
-                                            if override:
-                                                del self.sniff_order[conflict_loc]
-                                                self.log.debug(f"Removed conflicting sniffer for datatype '{dtype}'")
-                                            break
-                                    if not conflict or override:
-                                        if compressed_sniffers and aclass.__class__ in compressed_sniffers:
-                                            for compressed_sniffer in compressed_sniffers[aclass.__class__]:
-                                                self.sniff_order.append(compressed_sniffer)
-                                        self.sniff_order.append(aclass)
-                                        self.log.debug(f"Loaded sniffer for datatype '{dtype}'")
-                                    # Processing the new sniffer elem is now complete, so make sure the element defining it is loaded if necessary.
-                                    sniffer_class = elem.get('type', None)
-                                    if sniffer_class is not None:
-                                        if sniffer_class not in sniffer_elem_classes:
-                                            self.sniffer_elems.append(elem)
+                                # We are loading new sniffer, so see if we have a conflicting sniffer already loaded.
+                                conflict = False
+                                for conflict_loc, sniffer_class in enumerate(self.sniff_order):
+                                    if sniffer_class.__class__ == aclass.__class__:
+                                        # We have a conflicting sniffer, so replace the one previously loaded.
+                                        conflict = True
+                                        if override:
+                                            del self.sniff_order[conflict_loc]
+                                            self.log.debug(f"Removed conflicting sniffer for datatype '{dtype}'")
+                                        break
+                                if not conflict or override:
+                                    if compressed_sniffers and aclass.__class__ in compressed_sniffers:
+                                        for compressed_sniffer in compressed_sniffers[aclass.__class__]:
+                                            self.sniff_order.append(compressed_sniffer)
+                                    self.sniff_order.append(aclass)
+                                    self.log.debug(f"Loaded sniffer for datatype '{dtype}'")
+                                # Processing the new sniffer elem is now complete, so make sure the element defining it is loaded if necessary.
+                                sniffer_class = elem.get("type", None)
+                                if sniffer_class is not None:
+                                    if sniffer_class not in sniffer_elem_classes:
+                                        self.sniffer_elems.append(elem)
+
+    def get_datatype_from_filename(self, name):
+        max_extension_parts = 3
+        generic_datatype_instance = self.get_datatype_by_extension("data")
+        if "." not in name:
+            return generic_datatype_instance
+        extension_parts = name.rsplit(".", max_extension_parts)[1:]
+        possible_extensions = []
+        for n, _ in enumerate(extension_parts):
+            possible_extensions.append(".".join(extension_parts[n:]))
+
+        infer_from = self.datatypes_by_suffix_inferences
+        for possible_extension in possible_extensions:
+            if possible_extension in infer_from:
+                return infer_from[possible_extension]
+
+        for possible_extension in possible_extensions:
+            if possible_extension in self.datatypes_by_extension:
+                return self.datatypes_by_extension[possible_extension]
+
+        return generic_datatype_instance
 
     def is_extension_unsniffable_binary(self, ext):
         datatype = self.get_datatype_by_extension(ext)
-        return datatype is not None and isinstance(datatype, binary.Binary) and not hasattr(datatype, 'sniff')
+        return datatype is not None and isinstance(datatype, binary.Binary) and not hasattr(datatype, "sniff")
 
     def get_datatype_class_by_name(self, name):
         """
         Return the datatype class where the datatype's `type` attribute
         (as defined in the datatype_conf.xml file) contains `name`.
         """
         # TODO: obviously not ideal but some of these base classes that are useful for testing datatypes
         # aren't loaded into the datatypes registry, so we'd need to test for them here
-        if name == 'images.Image':
+        if name == "images.Image":
             return images.Image
 
         # TODO: too inefficient - would be better to generate this once as a map and store in this object
         for datatype_obj in self.datatypes_by_extension.values():
             datatype_obj_class = datatype_obj.__class__
             datatype_obj_class_str = str(datatype_obj_class)
             if name in datatype_obj_class_str:
                 return datatype_obj_class
         return None
 
     def get_available_tracks(self):
         return self.available_tracks
 
-    def get_mimetype_by_extension(self, ext, default='application/octet-stream'):
+    def get_mimetype_by_extension(self, ext, default="application/octet-stream"):
         """Returns a mimetype based on an extension"""
         try:
             mimetype = self.mimetypes_by_extension[ext]
         except KeyError:
             # datatype was never declared
             mimetype = default
-            self.log.warning(f'unknown mimetype in data factory {str(ext)}')
+            self.log.warning(f"unknown mimetype in data factory {str(ext)}")
         return mimetype
 
     def get_datatype_by_extension(self, ext):
         """Returns a datatype object based on an extension"""
         return self.datatypes_by_extension.get(ext, None)
 
     def change_datatype(self, data, ext):
@@ -591,139 +595,72 @@
         # being converted *to* will handle any metadata copying and
         # initialization.
         if data.has_data():
             data.set_size()
             data.init_meta(copy_from=data)
         return data
 
-    def load_datatype_converters(self, toolbox, installed_repository_dict=None, deactivate=False, use_cached=False):
+    def load_datatype_converters(self, toolbox, use_cached=False):
         """
-        If deactivate is False, add datatype converters from self.converters or self.proprietary_converters
-        to the calling app's toolbox.  If deactivate is True, eliminates relevant converters from the calling
-        app's toolbox.
-        """
-        if installed_repository_dict:
-            # Load converters defined by datatypes_conf.xml included in installed tool shed repository.
-            converters = self.proprietary_converters
-        else:
-            # Load converters defined by local datatypes_conf.xml.
-            converters = self.converters
+        Add datatype converters from self.converters to the calling app's toolbox.
+        """
+        # Load converters defined by local datatypes_conf.xml.
+        converters = self.converters
         for elem in converters:
             tool_config = elem[0]
             source_datatype = elem[1]
             target_datatype = elem[2]
-            if installed_repository_dict:
-                converter_path = installed_repository_dict['converter_path']
-            else:
-                converter_path = self.converters_path
+            converter_path = self.converters_path
             try:
                 config_path = os.path.join(converter_path, tool_config)
                 converter = toolbox.load_tool(config_path, use_cached=use_cached)
                 self.converter_tools.add(converter)
-                if installed_repository_dict:
-                    # If the converter is included in an installed tool shed repository, set the tool
-                    # shed related tool attributes.
-                    converter.tool_shed = installed_repository_dict['tool_shed']
-                    converter.repository_name = installed_repository_dict['repository_name']
-                    converter.repository_owner = installed_repository_dict['repository_owner']
-                    converter.installed_changeset_revision = installed_repository_dict['installed_changeset_revision']
-                    converter.old_id = converter.id
-                    # The converter should be included in the list of tools defined in tool_dicts.
-                    tool_dicts = installed_repository_dict['tool_dicts']
-                    for tool_dict in tool_dicts:
-                        if tool_dict['id'] == converter.id:
-                            converter.guid = tool_dict['guid']
-                            converter.id = tool_dict['guid']
-                            break
-                if deactivate:
-                    toolbox.remove_tool_by_id(converter.id, remove_from_panel=False)
-                    if source_datatype in self.datatype_converters:
-                        if target_datatype in self.datatype_converters[source_datatype]:
-                            del self.datatype_converters[source_datatype][target_datatype]
-                    self.log.debug("Deactivated converter: %s", converter.id)
-                else:
-                    toolbox.register_tool(converter)
-                    if source_datatype not in self.datatype_converters:
-                        self.datatype_converters[source_datatype] = {}
-                    self.datatype_converters[source_datatype][target_datatype] = converter
-                    if not hasattr(toolbox.app, 'tool_cache') or converter.id in toolbox.app.tool_cache._new_tool_ids:
-                        self.log.debug("Loaded converter: %s", converter.id)
+                toolbox.register_tool(converter)
+                if source_datatype not in self.datatype_converters:
+                    self.datatype_converters[source_datatype] = {}
+                self.datatype_converters[source_datatype][target_datatype] = converter
+                if not hasattr(toolbox.app, "tool_cache") or converter.id in toolbox.app.tool_cache._new_tool_ids:
+                    self.log.debug("Loaded converter: %s", converter.id)
             except Exception:
-                if deactivate:
-                    self.log.exception(f"Error deactivating converter from ({converter_path})")
-                else:
-                    self.log.exception(f"Error loading converter ({converter_path})")
+                self.log.exception(f"Error loading converter ({converter_path})")
 
-    def load_display_applications(self, app, installed_repository_dict=None, deactivate=False):
+    def load_display_applications(self, app):
         """
-        If deactivate is False, add display applications from self.display_app_containers or
-        self.proprietary_display_app_containers to appropriate datatypes.  If deactivate is
-        True, eliminates relevant display applications from appropriate datatypes.
+        Add display applications from self.display_app_containers or to appropriate datatypes.
         """
-        if installed_repository_dict:
-            # Load display applications defined by datatypes_conf.xml included in installed tool shed repository.
-            datatype_elems = self.proprietary_display_app_containers
-        else:
-            # Load display applications defined by local datatypes_conf.xml.
-            datatype_elems = self.display_app_containers
+        # Load display applications defined by local datatypes_conf.xml.
+        datatype_elems = self.display_app_containers
         for elem in datatype_elems:
             extension = self.get_extension(elem)
-            for display_app in elem.findall('display'):
-                display_file = display_app.get('file', None)
-                if installed_repository_dict:
-                    display_path = installed_repository_dict['display_path']
-                    display_file_head, display_file_tail = os.path.split(display_file)
-                    config_path = os.path.join(display_path, display_file_tail)
-                else:
-                    config_path = os.path.join(self.display_applications_path, display_file)
+            for display_app in elem.findall("display"):
+                display_file = display_app.get("file", None)
+                config_path = os.path.join(self.display_applications_path, display_file)
                 try:
-                    inherit = galaxy.util.string_as_bool(display_app.get('inherit', 'False'))
+                    inherit = galaxy.util.string_as_bool(display_app.get("inherit", "False"))
                     display_app = DisplayApplication.from_file(config_path, app)
                     if display_app:
                         if display_app.id in self.display_applications:
-                            if deactivate:
-                                del self.display_applications[display_app.id]
-                            else:
-                                # If we already loaded this display application, we'll use the first one loaded.
-                                display_app = self.display_applications[display_app.id]
-                        elif installed_repository_dict:
-                            # If the display application is included in an installed tool shed repository,
-                            # set the tool shed related tool attributes.
-                            display_app.tool_shed = installed_repository_dict['tool_shed']
-                            display_app.repository_name = installed_repository_dict['repository_name']
-                            display_app.repository_owner = installed_repository_dict['repository_owner']
-                            display_app.installed_changeset_revision = installed_repository_dict['installed_changeset_revision']
-                            display_app.old_id = display_app.id
-                            # The display application should be included in the list of tools defined in tool_dicts.
-                            tool_dicts = installed_repository_dict['tool_dicts']
-                            for tool_dict in tool_dicts:
-                                if tool_dict['id'] == display_app.id:
-                                    display_app.guid = tool_dict['guid']
-                                    display_app.id = tool_dict['guid']
-                                    break
-                        if deactivate:
-                            if display_app.id in self.display_applications:
-                                del self.display_applications[display_app.id]
-                            if extension in self.datatypes_by_extension:
-                                if display_app.id in self.datatypes_by_extension[extension].display_applications:
-                                    del self.datatypes_by_extension[extension].display_applications[display_app.id]
-                            if inherit and (self.datatypes_by_extension[extension], display_app) in self.inherit_display_application_by_class:
-                                self.inherit_display_application_by_class.remove((self.datatypes_by_extension[extension], display_app))
-                            self.log.debug(f"Deactivated display application '{display_app.id}' for datatype '{extension}'.")
-                        else:
-                            self.display_applications[display_app.id] = display_app
-                            self.datatypes_by_extension[extension].add_display_application(display_app)
-                            if inherit and (self.datatypes_by_extension[extension], display_app) not in self.inherit_display_application_by_class:
-                                self.inherit_display_application_by_class.append((self.datatypes_by_extension[extension], display_app))
-                            self.log.debug(f"Loaded display application '{display_app.id}' for datatype '{extension}', inherit={inherit}.")
+                            # If we already loaded this display application, we'll use the first one loaded.
+                            display_app = self.display_applications[display_app.id]
+
+                        self.display_applications[display_app.id] = display_app
+                        self.datatypes_by_extension[extension].add_display_application(display_app)
+                        if (
+                            inherit
+                            and (self.datatypes_by_extension[extension], display_app)
+                            not in self.inherit_display_application_by_class
+                        ):
+                            self.inherit_display_application_by_class.append(
+                                (self.datatypes_by_extension[extension], display_app)
+                            )
+                        self.log.debug(
+                            f"Loaded display application '{display_app.id}' for datatype '{extension}', inherit={inherit}."
+                        )
                 except Exception:
-                    if deactivate:
-                        self.log.exception(f"Error deactivating display application ({config_path})")
-                    else:
-                        self.log.exception(f"Error loading display application ({config_path})")
+                    self.log.exception(f"Error loading display application ({config_path})")
         # Handle display_application subclass inheritance.
         for extension, d_type1 in self.datatypes_by_extension.items():
             for d_type2, display_app in self.inherit_display_application_by_class:
                 current_app = d_type1.get_display_application(display_app.id, None)
                 if current_app is None and isinstance(d_type1, type(d_type2)):
                     self.log.debug(f"Adding inherited display application '{display_app.id}' to datatype '{extension}'")
                     d_type1.add_display_application(display_app)
@@ -740,105 +677,109 @@
         reloaded = []
         failed = []
         for display_application_id in display_application_ids:
             try:
                 self.display_applications[display_application_id].reload()
                 reloaded.append(display_application_id)
             except Exception as e:
-                self.log.debug('Requested to reload display application "%s", but failed: %s.', display_application_id, e)
+                self.log.debug(
+                    'Requested to reload display application "%s", but failed: %s.', display_application_id, e
+                )
                 failed.append(display_application_id)
         return (reloaded, failed)
 
     def load_external_metadata_tool(self, toolbox):
         """Adds a tool which is used to set external metadata"""
         # We need to be able to add a job to the queue to set metadata. The queue will currently only accept jobs with an associated
         # tool.  We'll load a special tool to be used for Auto-Detecting metadata; this is less than ideal, but effective
         # Properly building a tool without relying on parsing an XML file is near difficult...so we bundle with Galaxy.
-        set_meta_tool = toolbox.load_hidden_lib_tool(os.path.abspath(os.path.join(os.path.dirname(__file__), "set_metadata_tool.xml")))
+        set_meta_tool = toolbox.load_hidden_lib_tool(
+            os.path.abspath(os.path.join(os.path.dirname(__file__), "set_metadata_tool.xml"))
+        )
         self.set_external_metadata_tool = set_meta_tool
         self.log.debug("Loaded external metadata tool: %s", self.set_external_metadata_tool.id)
 
     def set_default_values(self):
         # Default values.
         if not self.datatypes_by_extension:
             self.datatypes_by_extension = {
-                'ab1': binary.Ab1(),
-                'axt': sequence.Axt(),
-                'bam': binary.Bam(),
-                'jp2': binary.JP2(),
-                'bed': interval.Bed(),
-                'coverage': coverage.LastzCoverage(),
-                'customtrack': interval.CustomTrack(),
-                'csfasta': sequence.csFasta(),
-                'fasta': sequence.Fasta(),
-                'eland': tabular.Eland(),
-                'fastq': sequence.Fastq(),
-                'fastqsanger': sequence.FastqSanger(),
-                'gtf': interval.Gtf(),
-                'gff': interval.Gff(),
-                'gff3': interval.Gff3(),
-                'genetrack': tracks.GeneTrack(),
-                'h5': binary.H5(),
-                'interval': interval.Interval(),
-                'laj': images.Laj(),
-                'lav': sequence.Lav(),
-                'maf': sequence.Maf(),
-                'pileup': tabular.Pileup(),
-                'qualsolid': qualityscore.QualityScoreSOLiD(),
-                'qualsolexa': qualityscore.QualityScoreSolexa(),
-                'qual454': qualityscore.QualityScore454(),
-                'sam': tabular.Sam(),
-                'scf': binary.Scf(),
-                'sff': binary.Sff(),
-                'tabular': tabular.Tabular(),
-                'csv': tabular.CSV(),
-                'taxonomy': tabular.Taxonomy(),
-                'txt': data.Text(),
-                'wig': interval.Wiggle(),
-                'xml': xml.GenericXml(),
+                "ab1": binary.Ab1(),
+                "axt": sequence.Axt(),
+                "bam": binary.Bam(),
+                "jp2": binary.JP2(),
+                "bed": interval.Bed(),
+                "coverage": coverage.LastzCoverage(),
+                "customtrack": interval.CustomTrack(),
+                "csfasta": sequence.csFasta(),
+                "fasta": sequence.Fasta(),
+                "eland": tabular.Eland(),
+                "fastq": sequence.Fastq(),
+                "fastqsanger": sequence.FastqSanger(),
+                "gtf": interval.Gtf(),
+                "gff": interval.Gff(),
+                "gff3": interval.Gff3(),
+                "genetrack": tracks.GeneTrack(),
+                "h5": binary.H5(),
+                "interval": interval.Interval(),
+                "laj": images.Laj(),
+                "lav": sequence.Lav(),
+                "maf": sequence.Maf(),
+                "pileup": tabular.Pileup(),
+                "qualsolid": qualityscore.QualityScoreSOLiD(),
+                "qualsolexa": qualityscore.QualityScoreSolexa(),
+                "qual454": qualityscore.QualityScore454(),
+                "sam": tabular.Sam(),
+                "scf": binary.Scf(),
+                "sff": binary.Sff(),
+                "tabular": tabular.Tabular(),
+                "csv": tabular.CSV(),
+                "taxonomy": tabular.Taxonomy(),
+                "txt": data.Text(),
+                "wig": interval.Wiggle(),
+                "xml": xml.GenericXml(),
             }
             self.mimetypes_by_extension = {
-                'ab1': 'application/octet-stream',
-                'axt': 'text/plain',
-                'bam': 'application/octet-stream',
-                'jp2': 'application/octet-stream',
-                'bed': 'text/plain',
-                'customtrack': 'text/plain',
-                'csfasta': 'text/plain',
-                'eland': 'application/octet-stream',
-                'fasta': 'text/plain',
-                'fastq': 'text/plain',
-                'fastqsanger': 'text/plain',
-                'gtf': 'text/plain',
-                'gff': 'text/plain',
-                'gff3': 'text/plain',
-                'h5': 'application/octet-stream',
-                'interval': 'text/plain',
-                'laj': 'text/plain',
-                'lav': 'text/plain',
-                'maf': 'text/plain',
-                'memexml': 'application/xml',
-                'pileup': 'text/plain',
-                'qualsolid': 'text/plain',
-                'qualsolexa': 'text/plain',
-                'qual454': 'text/plain',
-                'sam': 'text/plain',
-                'scf': 'application/octet-stream',
-                'sff': 'application/octet-stream',
-                'tabular': 'text/plain',
-                'csv': 'text/plain',
-                'taxonomy': 'text/plain',
-                'txt': 'text/plain',
-                'wig': 'text/plain',
-                'xml': 'application/xml',
+                "ab1": "application/octet-stream",
+                "axt": "text/plain",
+                "bam": "application/octet-stream",
+                "jp2": "application/octet-stream",
+                "bed": "text/plain",
+                "customtrack": "text/plain",
+                "csfasta": "text/plain",
+                "eland": "application/octet-stream",
+                "fasta": "text/plain",
+                "fastq": "text/plain",
+                "fastqsanger": "text/plain",
+                "gtf": "text/plain",
+                "gff": "text/plain",
+                "gff3": "text/plain",
+                "h5": "application/octet-stream",
+                "interval": "text/plain",
+                "laj": "text/plain",
+                "lav": "text/plain",
+                "maf": "text/plain",
+                "memexml": "application/xml",
+                "pileup": "text/plain",
+                "qualsolid": "text/plain",
+                "qualsolexa": "text/plain",
+                "qual454": "text/plain",
+                "sam": "text/plain",
+                "scf": "application/octet-stream",
+                "sff": "application/octet-stream",
+                "tabular": "text/plain",
+                "csv": "text/plain",
+                "taxonomy": "text/plain",
+                "txt": "text/plain",
+                "wig": "text/plain",
+                "xml": "application/xml",
             }
         # super supertype fix for input steps in workflows.
-        if 'data' not in self.datatypes_by_extension:
-            self.datatypes_by_extension['data'] = data.Data()
-            self.mimetypes_by_extension['data'] = 'application/octet-stream'
+        if "data" not in self.datatypes_by_extension:
+            self.datatypes_by_extension["data"] = data.Data()
+            self.mimetypes_by_extension["data"] = "application/octet-stream"
         # Default values - the order in which we attempt to determine data types is critical
         # because some formats are much more flexibly defined than others.
         if len(self.sniff_order) < 1:
             self.sniff_order = [
                 binary.Bam(),
                 binary.Sff(),
                 binary.JP2(),
@@ -861,15 +802,15 @@
                 interval.Gtf(),
                 interval.Gff(),
                 interval.Gff3(),
                 tabular.Pileup(),
                 interval.Interval(),
                 tabular.Sam(),
                 tabular.Eland(),
-                tabular.CSV()
+                tabular.CSV(),
             ]
 
     def get_converters_by_datatype(self, ext):
         """Returns available converters by source type"""
         if ext not in self._converters_by_datatype:
             converters = {}
             source_datatype = type(self.get_datatype_by_extension(ext))
@@ -901,21 +842,24 @@
         if hasattr(dataset_or_ext, "ext"):
             ext = dataset_or_ext.ext
             dataset = dataset_or_ext
         else:
             ext = dataset_or_ext
             dataset = None
 
-        if self.get_datatype_by_extension(ext) is not None and self.get_datatype_by_extension(ext).matches_any(accepted_formats):
+        datatype_by_extension = self.get_datatype_by_extension(ext)
+        if datatype_by_extension and datatype_by_extension.matches_any(accepted_formats):
             return True, None, None
 
         for convert_ext in self.get_converters_by_datatype(ext):
             convert_ext_datatype = self.get_datatype_by_extension(convert_ext)
             if convert_ext_datatype is None:
-                self.log.warning(f"Datatype class not found for extension '{convert_ext}', which is used as target for conversion from datatype '{dataset.ext}'")
+                self.log.warning(
+                    f"Datatype class not found for extension '{convert_ext}', which is used as target for conversion from datatype '{dataset.ext}'"
+                )
             elif convert_ext_datatype.matches_any(accepted_formats):
                 converted_dataset = dataset and dataset.get_converted_files_by_type(convert_ext)
                 if converted_dataset:
                     ret_data = converted_dataset
                 elif not converter_safe:
                     continue
                 else:
@@ -932,74 +876,98 @@
         for ext, d_type in self.datatypes_by_extension.items():
             inputs = []
             for meta_name, meta_spec in d_type.metadata_spec.items():
                 if meta_spec.set_in_upload:
                     help_txt = meta_spec.desc
                     if not help_txt or help_txt == meta_name:
                         help_txt = ""
-                    inputs.append(f'<param type="text" name="{meta_name}" label="Set metadata value for &quot;{meta_name}&quot;" value="{meta_spec.default}" help="{help_txt}"/>')
+                    inputs.append(
+                        f'<param type="text" name="{meta_name}" label="Set metadata value for &quot;{meta_name}&quot;" value="{meta_spec.default}" help="{help_txt}"/>'
+                    )
             rval[ext] = "\n".join(inputs)
-        if 'auto' not in rval and 'txt' in rval:  # need to manually add 'auto' datatype
-            rval['auto'] = rval['txt']
+        if "auto" not in rval and "txt" in rval:  # need to manually add 'auto' datatype
+            rval["auto"] = rval["txt"]
         return rval
 
     @property
     def edam_formats(self):
-        """
-        """
+        """ """
         if not self._edam_formats_mapping:
             self._edam_formats_mapping = {k: v.edam_format for k, v in self.datatypes_by_extension.items()}
         return self._edam_formats_mapping
 
     @property
     def edam_data(self):
-        """
-        """
+        """ """
         if not self._edam_data_mapping:
             self._edam_data_mapping = {k: v.edam_data for k, v in self.datatypes_by_extension.items()}
         return self._edam_data_mapping
 
     def to_xml_file(self, path):
         if not self._registry_xml_string:
-            registry_string_template = Template("""<?xml version="1.0"?>
+            registry_string_template = Template(
+                """<?xml version="1.0"?>
             <datatypes>
               <registration converters_path="$converters_path" display_path="$display_path">
                 $datatype_elems
               </registration>
               <sniffers>
                 $sniffer_elems
               </sniffers>
             </datatypes>
-            """)
-            converters_path = self.converters_path_attr or ''
-            display_path = self.display_path_attr or ''
+            """
+            )
+            converters_path = self.converters_path_attr or ""
+            display_path = self.display_path_attr or ""
             datatype_elems = "".join(galaxy.util.xml_to_string(elem) for elem in self.datatype_elems)
             sniffer_elems = "".join(galaxy.util.xml_to_string(elem) for elem in self.sniffer_elems)
-            self._registry_xml_string = registry_string_template.substitute(converters_path=converters_path,
-                                                                            display_path=display_path,
-                                                                            datatype_elems=datatype_elems,
-                                                                            sniffer_elems=sniffer_elems)
-        with open(os.path.abspath(path), 'w') as registry_xml:
+            self._registry_xml_string = registry_string_template.substitute(
+                converters_path=converters_path,
+                display_path=display_path,
+                datatype_elems=datatype_elems,
+                sniffer_elems=sniffer_elems,
+            )
+        with open(os.path.abspath(path), "w") as registry_xml:
             os.chmod(path, RW_R__R__)
             registry_xml.write(self._registry_xml_string)
 
     def get_extension(self, elem):
         """
         Function which returns the extension lowercased
         :param elem:
         :return extension:
         """
-        extension = elem.get('extension', None)
+        extension = elem.get("extension", None)
         # If extension is not None and is uppercase or mixed case, we need to lowercase it
         if extension is not None and not extension.islower():
-            self.log.debug("%s is not lower case, that could cause troubles in the future. \
-            Please change it to lower case" % extension)
+            self.log.debug(
+                "%s is not lower case, that could cause troubles in the future. \
+            Please change it to lower case"
+                % extension
+            )
             extension = extension.lower()
         return extension
 
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        # Don't pickle xml elements
+        unpickleable_attributes = [
+            "converter_tools",
+            "datatype_converters",
+            "datatype_elems",
+            "display_app_containers",
+            "display_applications",
+            "inherit_display_application_by_class",
+            "set_external_metadata_tool",
+            "sniffer_elems",
+        ]
+        for unpicklable in unpickleable_attributes:
+            state[unpicklable] = []
+        return state
+
 
 def example_datatype_registry_for_sample(sniff_compressed_dynamic_datatypes_default=True):
     galaxy_dir = galaxy.util.galaxy_directory()
     sample_conf = os.path.join(galaxy_dir, "lib", "galaxy", "config", "sample", "datatypes_conf.xml.sample")
     config = Bunch(sniff_compressed_dynamic_datatypes_default=sniff_compressed_dynamic_datatypes_default)
     datatypes_registry = Registry(config)
     datatypes_registry.load_datatypes(root_dir=galaxy_dir, config=sample_conf)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/sequence.py` & `galaxy-data-23.0.1/galaxy/datatypes/sequence.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,41 +6,55 @@
 import logging
 import math
 import os
 import re
 import string
 import subprocess
 from itertools import islice
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    TYPE_CHECKING,
+)
 
 import bx.align.maf
 from markupsafe import escape
 
 from galaxy import util
 from galaxy.datatypes import metadata
-from galaxy.datatypes.binary import (
-    Binary
-)
+from galaxy.datatypes.binary import Binary
 from galaxy.datatypes.data import DatatypeValidation
-from galaxy.datatypes.metadata import DictParameter, MetadataElement
+from galaxy.datatypes.metadata import (
+    DictParameter,
+    MetadataElement,
+)
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     get_headers,
     iter_headers,
 )
 from galaxy.util import (
     compression_utils,
-    nice_size
-)
-from galaxy.util.checkers import (
-    is_gzip
+    nice_size,
 )
+from galaxy.util.checkers import is_gzip
 from galaxy.util.image_util import check_image_type
 from . import data
 
+if TYPE_CHECKING:
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class SequenceSplitLocations(data.Text):
     """
     Class storing information about a sequence file composed of multiple gzip files concatenated as
@@ -50,94 +64,98 @@
 
       { "sections" : [
               { "start" : "x", "end" : "y", "sequences" : "z" },
               ...
       ]}
 
     """
+
     file_ext = "fqtoc"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             try:
                 parsed_data = json.load(open(dataset.file_name))
                 # dataset.peek = json.dumps(data, sort_keys=True, indent=4)
                 dataset.peek = data.get_file_peek(dataset.file_name)
-                dataset.blurb = '%d sections' % len(parsed_data['sections'])
+                dataset.blurb = "%d sections" % len(parsed_data["sections"])
             except Exception:
-                dataset.peek = 'Not FQTOC file'
-                dataset.blurb = 'Not FQTOC file'
+                dataset.peek = "Not FQTOC file"
+                dataset.blurb = "Not FQTOC file"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         if file_prefix.file_size < 50000 and not file_prefix.truncated:
             try:
                 data = json.loads(file_prefix.contents_header)
-                sections = data['sections']
+                sections = data["sections"]
                 for section in sections:
-                    if 'start' not in section or 'end' not in section or 'sequences' not in section:
+                    if "start" not in section or "end" not in section or "sequences" not in section:
                         return False
                 return True
             except Exception:
                 pass
         return False
 
 
 class Sequence(data.Text):
     """Class describing a sequence"""
+
     edam_data = "data_2044"
 
-    MetadataElement(name="sequences", default=0, desc="Number of sequences", readonly=True, visible=False, optional=True, no_value=0)
+    MetadataElement(
+        name="sequences", default=0, desc="Number of sequences", readonly=True, visible=False, optional=True, no_value=0
+    )
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of sequences and the number of data lines in dataset.
         """
         data_lines = 0
         sequences = 0
         with compression_utils.get_fileobj(dataset.file_name) as fh:
             for line in fh:
                 line = line.strip()
-                if line and line.startswith('#'):
+                if line and line.startswith("#"):
                     # We don't count comment lines for sequence data types
                     continue
-                if line and line.startswith('>'):
+                if line and line.startswith(">"):
                     sequences += 1
                     data_lines += 1
                 else:
                     data_lines += 1
             dataset.metadata.data_lines = data_lines
             dataset.metadata.sequences = sequences
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
             if dataset.metadata.sequences:
                 dataset.blurb = f"{util.commaify(str(dataset.metadata.sequences))} sequences"
             else:
                 dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
     @staticmethod
-    def get_sequences_per_file(total_sequences, split_params):
-        if split_params['split_mode'] == 'number_of_parts':
+    def get_sequences_per_file(total_sequences: int, split_params: Dict) -> List:
+        if split_params["split_mode"] == "number_of_parts":
             # legacy basic mode - split into a specified number of parts
-            parts = int(split_params['split_size'])
+            parts = int(split_params["split_size"])
             sequences_per_file = [total_sequences / parts for i in range(parts)]
             for i in range(total_sequences % parts):
                 sequences_per_file[i] += 1
-        elif split_params['split_mode'] == 'to_size':
+        elif split_params["split_mode"] == "to_size":
             # loop through the sections and calculate the number of sequences
-            chunk_size = int(split_params['split_size'])
+            chunk_size = int(split_params["split_size"])
             rem = total_sequences % chunk_size
-            sequences_per_file = [chunk_size for i in range(total_sequences / chunk_size)]
+            sequences_per_file = [chunk_size for i in range(total_sequences // chunk_size)]
             # TODO: Should we invest the time in a better way to handle small remainders?
             if rem > 0:
                 sequences_per_file.append(rem)
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
         return sequences_per_file
 
@@ -154,18 +172,18 @@
 
         sequences_per_file = cls.get_sequences_per_file(total_sequences, split_params)
         return cls.write_split_files(input_datasets, None, subdir_generator_function, sequences_per_file)
 
     @classmethod
     def do_fast_split(cls, input_datasets, toc_file_datasets, subdir_generator_function, split_params):
         data = json.load(open(toc_file_datasets[0].file_name))
-        sections = data['sections']
+        sections = data["sections"]
         total_sequences = int(0)
         for section in sections:
-            total_sequences += int(section['sequences'])
+            total_sequences += int(section["sequences"])
         sequences_per_file = cls.get_sequences_per_file(total_sequences, split_params)
         return cls.write_split_files(input_datasets, toc_file_datasets, subdir_generator_function, sequences_per_file)
 
     @classmethod
     def write_split_files(cls, input_datasets, toc_file_datasets, subdir_generator_function, sequences_per_file):
         directories = []
 
@@ -182,34 +200,39 @@
         start_sequence = 0
         for part_no in range(len(sequences_per_file)):
             dir = get_subdir(part_no)
             for ds_no in range(len(input_datasets)):
                 ds = input_datasets[ds_no]
                 base_name = os.path.basename(ds.file_name)
                 part_path = os.path.join(dir, base_name)
-                split_data = dict(class_name=f'{cls.__module__}.{cls.__name__}',
-                                  output_name=part_path,
-                                  input_name=ds.file_name,
-                                  args=dict(start_sequence=start_sequence, num_sequences=sequences_per_file[part_no]))
+                split_data = dict(
+                    class_name=f"{cls.__module__}.{cls.__name__}",
+                    output_name=part_path,
+                    input_name=ds.file_name,
+                    args=dict(start_sequence=start_sequence, num_sequences=sequences_per_file[part_no]),
+                )
                 if toc_file_datasets is not None:
                     toc = toc_file_datasets[ds_no]
-                    split_data['args']['toc_file'] = toc.file_name
-                with open(os.path.join(dir, f'split_info_{base_name}.json'), 'w') as f:
+                    split_data["args"]["toc_file"] = toc.file_name
+                with open(os.path.join(dir, f"split_info_{base_name}.json"), "w") as f:
                     json.dump(split_data, f)
             start_sequence += sequences_per_file[part_no]
         return directories
 
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    @classmethod
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """Split a generic sequence file (not sensible or possible, see subclasses)."""
         if split_params is None:
             return None
         raise NotImplementedError("Can't split generic sequence files")
 
     @staticmethod
-    def get_split_commands_with_toc(input_name, output_name, toc_file, start_sequence, sequence_count):
+    def get_split_commands_with_toc(
+        input_name: str, output_name: str, toc_file: Any, start_sequence: int, sequence_count: int
+    ) -> List:
         """
         Uses a Table of Contents dict, parsed from an FQTOC file, to come up with a set of
         shell commands that will extract the parts necessary
         >>> three_sections=[dict(start=0, end=74, sequences=10), dict(start=74, end=148, sequences=10), dict(start=148, end=148+76, sequences=10)]
         >>> Sequence.get_split_commands_with_toc('./input.gz', './output.gz', dict(sections=three_sections), start_sequence=0, sequence_count=10)
         ['dd bs=1 skip=0 count=74 if=./input.gz 2> /dev/null >> ./output.gz']
         >>> Sequence.get_split_commands_with_toc('./input.gz', './output.gz', dict(sections=three_sections), start_sequence=1, sequence_count=5)
@@ -219,66 +242,77 @@
         >>> Sequence.get_split_commands_with_toc('./input.gz', './output.gz', dict(sections=three_sections), start_sequence=5, sequence_count=10)
         ['(dd bs=1 skip=0 count=74 if=./input.gz 2> /dev/null )| zcat | ( tail -n +21 2> /dev/null) | head -20 | gzip -c >> ./output.gz', '(dd bs=1 skip=74 count=74 if=./input.gz 2> /dev/null )| zcat | ( tail -n +1 2> /dev/null) | head -20 | gzip -c >> ./output.gz']
         >>> Sequence.get_split_commands_with_toc('./input.gz', './output.gz', dict(sections=three_sections), start_sequence=10, sequence_count=10)
         ['dd bs=1 skip=74 count=74 if=./input.gz 2> /dev/null >> ./output.gz']
         >>> Sequence.get_split_commands_with_toc('./input.gz', './output.gz', dict(sections=three_sections), start_sequence=5, sequence_count=20)
         ['(dd bs=1 skip=0 count=74 if=./input.gz 2> /dev/null )| zcat | ( tail -n +21 2> /dev/null) | head -20 | gzip -c >> ./output.gz', 'dd bs=1 skip=74 count=74 if=./input.gz 2> /dev/null >> ./output.gz', '(dd bs=1 skip=148 count=76 if=./input.gz 2> /dev/null )| zcat | ( tail -n +1 2> /dev/null) | head -20 | gzip -c >> ./output.gz']
         """
-        sections = toc_file['sections']
+        sections = toc_file["sections"]
         result = []
 
         current_sequence = int(0)
         i = 0
         # skip to the section that contains my starting sequence
-        while i < len(sections) and start_sequence >= current_sequence + int(sections[i]['sequences']):
-            current_sequence += int(sections[i]['sequences'])
+        while i < len(sections) and start_sequence >= current_sequence + int(sections[i]["sequences"]):
+            current_sequence += int(sections[i]["sequences"])
             i += 1
         if i == len(sections):  # bad input data!
-            raise Exception(f'No FQTOC section contains starting sequence {start_sequence}')
+            raise Exception(f"No FQTOC section contains starting sequence {start_sequence}")
 
         # These two variables act as an accumulator for consecutive entire blocks that
         # can be copied verbatim (without decompressing)
         start_chunk = int(-1)
         end_chunk = int(-1)
-        copy_chunk_cmd = 'dd bs=1 skip=%s count=%s if=%s 2> /dev/null >> %s'
+        copy_chunk_cmd = "dd bs=1 skip=%s count=%s if=%s 2> /dev/null >> %s"
 
         while sequence_count > 0 and i < len(sections):
             # we need to extract partial data. So, find the byte offsets of the chunks that contain the data we need
             # use a combination of dd (to pull just the right sections out) tail (to skip lines) and head (to get the
             # right number of lines
-            sequences = int(sections[i]['sequences'])
+            sequences = int(sections[i]["sequences"])
             skip_sequences = start_sequence - current_sequence
             sequences_to_extract = min(sequence_count, sequences - skip_sequences)
-            start_copy = int(sections[i]['start'])
-            end_copy = int(sections[i]['end'])
+            start_copy = int(sections[i]["start"])
+            end_copy = int(sections[i]["end"])
             if sequences_to_extract < sequences:
                 if start_chunk > -1:
                     result.append(copy_chunk_cmd % (start_chunk, end_chunk - start_chunk, input_name, output_name))
                     start_chunk = -1
                 # extract, unzip, trim, recompress
-                result.append('(dd bs=1 skip=%s count=%s if=%s 2> /dev/null )| zcat | ( tail -n +%s 2> /dev/null) | head -%s | gzip -c >> %s' %
-                              (start_copy, end_copy - start_copy, input_name, skip_sequences * 4 + 1, sequences_to_extract * 4, output_name))
+                result.append(
+                    "(dd bs=1 skip=%s count=%s if=%s 2> /dev/null )| zcat | ( tail -n +%s 2> /dev/null) | head -%s | gzip -c >> %s"
+                    % (
+                        start_copy,
+                        end_copy - start_copy,
+                        input_name,
+                        skip_sequences * 4 + 1,
+                        sequences_to_extract * 4,
+                        output_name,
+                    )
+                )
             else:  # whole section - add it to the start_chunk/end_chunk accumulator
                 if start_chunk == -1:
                     start_chunk = start_copy
                 end_chunk = end_copy
             sequence_count -= sequences_to_extract
             start_sequence += sequences_to_extract
             current_sequence += sequences
             i += 1
         if start_chunk > -1:
             result.append(copy_chunk_cmd % (start_chunk, end_chunk - start_chunk, input_name, output_name))
 
         if sequence_count > 0:
-            raise Exception(f'{sequence_count} sequences not found in file')
+            raise Exception(f"{sequence_count} sequences not found in file")
 
         return result
 
     @staticmethod
-    def get_split_commands_sequential(is_compressed, input_name, output_name, start_sequence, sequence_count):
+    def get_split_commands_sequential(
+        is_compressed: bool, input_name: str, output_name: str, start_sequence: int, sequence_count: int
+    ) -> List:
         """
         Does a brain-dead sequential scan & extract of certain sequences
         >>> Sequence.get_split_commands_sequential(True, './input.gz', './output.gz', start_sequence=0, sequence_count=10)
         ['zcat "./input.gz" | ( tail -n +1 2> /dev/null) | head -40 | gzip -c > "./output.gz"']
         >>> Sequence.get_split_commands_sequential(False, './input.fastq', './output.fastq', start_sequence=10, sequence_count=10)
         ['tail -n +41 "./input.fastq" 2> /dev/null | head -40 > "./output.fastq"']
         """
@@ -292,32 +326,37 @@
         cmd += f' > "{output_name}"'
 
         return [cmd]
 
 
 class Alignment(data.Text):
     """Class describing an alignment"""
+
     edam_data = "data_0863"
 
-    MetadataElement(name="species", desc="Species", default=[], param=metadata.SelectParameter, multiple=True, readonly=True)
+    MetadataElement(
+        name="species", desc="Species", default=[], param=metadata.SelectParameter, multiple=True, readonly=True
+    )
 
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    @classmethod
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """Split a generic alignment file (not sensible or possible, see subclasses)."""
         if split_params is None:
             return None
         raise NotImplementedError("Can't split generic alignment files")
 
 
 @build_sniff_from_prefix
 class Fasta(Sequence):
     """Class representing a FASTA sequence"""
+
     edam_format = "format_1929"
     file_ext = "fasta"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in fasta format
 
         A sequence in FASTA format consists of a single-line description, followed by lines of sequence data.
         The first character of the description line is a greater-than (">") symbol in the first column.
         All lines should be shorter than 80 characters
 
@@ -347,33 +386,33 @@
         >>> Fasta().sniff( fname )
         True
         """
         fh = file_prefix.string_io()
         for line in fh:
             line = line.strip()
             if line:  # first non-empty line
-                if line.startswith('>'):
+                if line.startswith(">"):
                     # The next line.strip() must not be '', nor startwith '>'
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         return False
 
                     # If there is a third line, and it isn't a header line, it may not contain chars like '()[].' otherwise it's most likely a DotBracket file
                     line = fh.readline()
                     if not line:
                         return True
-                    if not line.startswith('>') and re.search(r"[\(\)\[\]\.]", line):
+                    if not line.startswith(">") and re.search(r"[\(\)\[\]\.]", line):
                         return False
                     return True
                 else:
                     return False
         return False
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """Split a FASTA file sequence by sequence.
 
         Note that even if split_mode="number_of_parts", the actual number of
         sub-files produced may not match that requested by split_size.
 
         If split_mode="to_size" then split_size is treated as the number of
         FASTA records to put in each sub-file (not size in bytes).
@@ -381,18 +420,18 @@
         if split_params is None:
             return
         if len(input_datasets) > 1:
             raise Exception("FASTA file splitting does not support multiple files")
         input_file = input_datasets[0].file_name
 
         # Counting chunk size as number of sequences.
-        if 'split_mode' not in split_params:
-            raise Exception('Tool does not define a split mode')
-        elif split_params['split_mode'] == 'number_of_parts':
-            split_size = int(split_params['split_size'])
+        if "split_mode" not in split_params:
+            raise Exception("Tool does not define a split mode")
+        elif split_params["split_mode"] == "number_of_parts":
+            split_size = int(split_params["split_size"])
             log.debug("Split %s into %i parts..." % (input_file, split_size))
             # if split_mode = number_of_parts, and split_size = 10, and
             # we know the number of sequences (say 1234), then divide by
             # by ten, giving ten files of approx 123 sequences each.
             if input_datasets[0].metadata is not None and input_datasets[0].metadata.sequences:
                 # Galaxy has already counted/estimated the number
                 batch_size = 1 + input_datasets[0].metadata.sequences // split_size
@@ -400,104 +439,105 @@
             else:
                 # OK, if Galaxy hasn't counted them, it may be a big file.
                 # We're not going to count the records which would be slow
                 # and a waste of disk IO time - instead we'll split using
                 # the file size.
                 chunk_size = os.path.getsize(input_file) // split_size
                 cls._size_split(input_file, chunk_size, subdir_generator_function)
-        elif split_params['split_mode'] == 'to_size':
+        elif split_params["split_mode"] == "to_size":
             # Split the input file into as many sub-files as required,
             # each containing to_size many sequences
-            batch_size = int(split_params['split_size'])
+            batch_size = int(split_params["split_size"])
             log.debug("Split %s into batches of %i records..." % (input_file, batch_size))
             cls._count_split(input_file, batch_size, subdir_generator_function)
         else:
             raise Exception(f"Unsupported split mode {split_params['split_mode']}")
 
     @classmethod
-    def _size_split(cls, input_file, chunk_size, subdir_generator_function):
+    def _size_split(cls, input_file: str, chunk_size: int, subdir_generator_function: Callable) -> None:
         """Split a FASTA file into chunks based on size on disk.
 
         This does of course preserve complete records - it only splits at the
         start of a new FASTQ sequence record.
         """
         log.debug("Attemping to split FASTA file %s into chunks of %i bytes" % (input_file, chunk_size))
         with open(input_file) as f:
             part_file = None
             try:
                 # Note if the input FASTA file has no sequences, we will
                 # produce just one sub-file which will be a copy of it.
                 part_dir = subdir_generator_function()
                 part_path = os.path.join(part_dir, os.path.basename(input_file))
-                part_file = open(part_path, 'w')
+                part_file = open(part_path, "w")
                 log.debug(f"Writing {input_file} part to {part_path}")
                 start_offset = 0
-                for line in iter(f.readline, ''):
+                for line in iter(f.readline, ""):
                     offset = f.tell()
                     if not line:
                         break
                     if line[0] == ">" and offset - start_offset >= chunk_size:
                         # Start a new sub-file
                         part_file.close()
                         part_dir = subdir_generator_function()
                         part_path = os.path.join(part_dir, os.path.basename(input_file))
-                        part_file = open(part_path, 'w')
+                        part_file = open(part_path, "w")
                         log.debug(f"Writing {input_file} part to {part_path}")
                         start_offset = f.tell()
                     part_file.write(line)
             except Exception as e:
-                log.error('Unable to size split FASTA file: %s', util.unicodify(e))
+                log.error("Unable to size split FASTA file: %s", util.unicodify(e))
                 raise
             finally:
                 if part_file:
                     part_file.close()
 
     @classmethod
-    def _count_split(cls, input_file, chunk_size, subdir_generator_function):
+    def _count_split(cls, input_file: str, chunk_size: int, subdir_generator_function: Callable) -> None:
         """Split a FASTA file into chunks based on counting records."""
         log.debug("Attemping to split FASTA file %s into chunks of %i sequences" % (input_file, chunk_size))
         with open(input_file) as f:
             part_file = None
             try:
                 # Note if the input FASTA file has no sequences, we will
                 # produce just one sub-file which will be a copy of it.
                 part_dir = subdir_generator_function()
                 part_path = os.path.join(part_dir, os.path.basename(input_file))
-                part_file = open(part_path, 'w')
+                part_file = open(part_path, "w")
                 log.debug(f"Writing {input_file} part to {part_path}")
                 rec_count = 0
                 for line in f:
                     if not line:
                         break
                     if line[0] == ">":
                         rec_count += 1
                         if rec_count > chunk_size:
                             # Start a new sub-file
                             part_file.close()
                             part_dir = subdir_generator_function()
                             part_path = os.path.join(part_dir, os.path.basename(input_file))
-                            part_file = open(part_path, 'w')
+                            part_file = open(part_path, "w")
                             log.debug(f"Writing {input_file} part to {part_path}")
                             rec_count = 1
                     part_file.write(line)
             except Exception as e:
-                log.error('Unable to count split FASTA file: %s', util.unicodify(e))
+                log.error("Unable to count split FASTA file: %s", util.unicodify(e))
                 raise
             finally:
                 if part_file:
                     part_file.close()
 
 
 @build_sniff_from_prefix
 class csFasta(Sequence):
-    """ Class representing the SOLID Color-Space sequence ( csfasta ) """
+    """Class representing the SOLID Color-Space sequence ( csfasta )"""
+
     edam_format = "format_3589"
     file_ext = "csfasta"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Color-space sequence:
             >2_15_85_F3
             T213021013012303002332212012112221222112212222
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'sequence.fasta' )
@@ -506,47 +546,58 @@
         >>> fname = get_test_fname( 'sequence.csfasta' )
         >>> csFasta().sniff( fname )
         True
         """
         fh = file_prefix.string_io()
         for line in fh:
             line = line.strip()
-            if line and not line.startswith('#'):  # first non-empty non-comment line
-                if line.startswith('>'):
+            if line and not line.startswith("#"):  # first non-empty non-comment line
+                if line.startswith(">"):
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         break
                     elif line[0] not in string.ascii_uppercase:
                         return False
-                    elif len(line) > 1 and not re.search(r'^[\d.]+$', line[1:]):
+                    elif len(line) > 1 and not re.search(r"^[\d.]+$", line[1:]):
                         return False
                     return True
                 else:
                     return False
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
             dataset.metadata.data_lines = None
             dataset.metadata.sequences = None
             return
-        return Sequence.set_meta(self, dataset, **kwd)
+        return Sequence.set_meta(self, dataset, overwrite=overwrite, **kwd)
 
 
 @build_sniff_from_prefix
 class Fastg(Sequence):
-    """ Class representing a FASTG sequence
-    http://fastg.sourceforge.net/FASTG_Spec_v1.00.pdf """
+    """Class representing a FASTG sequence
+    http://fastg.sourceforge.net/FASTG_Spec_v1.00.pdf"""
+
     edam_format = "format_3823"
     file_ext = "fastg"
 
-    MetadataElement(name="version", default='1.0', desc="FASTG format version", readonly=True, visible=True, no_value='1.0')
-    MetadataElement(name="properties", default={}, param=DictParameter, desc="FASTG properites", readonly=True, visible=True, no_value={})
+    MetadataElement(
+        name="version", default="1.0", desc="FASTG format version", readonly=True, visible=True, no_value="1.0"
+    )
+    MetadataElement(
+        name="properties",
+        default={},
+        param=DictParameter,
+        desc="FASTG properites",
+        readonly=True,
+        visible=True,
+        no_value={},
+    )
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """FASTG must begin with lines:
            #FASTG:begin;
            #FASTG:version=*.*;
            #FASTG:properties;
 
         Or these can be combined on a line:
            #FASTG:begin:version=*.*:properties;
@@ -573,106 +624,110 @@
         """
         fh = file_prefix.string_io()
         for i, line in enumerate(fh):
             if not line:
                 break  # EOF
             line = line.strip()
             if i == 0:
-                if not line.startswith('#FASTG:begin'):
+                if not line.startswith("#FASTG:begin"):
                     break
-            elif line and not line.startswith('#'):  # first non-empty non-comment line
-                if line.startswith('>'):
+            elif line and not line.startswith("#"):  # first non-empty non-comment line
+                if line.startswith(">"):
                     # The next line.strip() must not be '', nor startwith '>'
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         break
                     return True
                 else:
                     break  # we found a non-empty line, but it's not a header
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         with open(dataset.file_name) as fh:
             for i, line in enumerate(fh):
                 if not line:
                     break  # EOF
                 line = line.strip()
                 if i == 0:
-                    if not line.startswith('#FASTG:begin'):
+                    if not line.startswith("#FASTG:begin"):
                         break
-                if line.startswith('#FASTG'):
-                    props = {x.split('=')[0][1:]: x.split('=')[1] for x in re.findall(':[a-zA-Z0-9_]+=[a-zA-Z0-9_().,\" ]+', line)}
+                if line.startswith("#FASTG"):
+                    props = {
+                        x.split("=")[0][1:]: x.split("=")[1]
+                        for x in re.findall(':[a-zA-Z0-9_]+=[a-zA-Z0-9_().," ]+', line)
+                    }
                     dataset.metadata.properties.update(props)
-                    if 'version' in props:
-                        dataset.metadata.version = props['version']
-                if line and line.startswith('>'):
+                    if "version" in props:
+                        dataset.metadata.version = props["version"]
+                if line and line.startswith(">"):
                     break
         if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
             dataset.metadata.data_lines = None
             dataset.metadata.sequences = None
             return
-        return Sequence.set_meta(self, dataset, **kwd)
+        return Sequence.set_meta(self, dataset, overwrite=overwrite, **kwd)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
             if dataset.metadata.sequences:
                 dataset.blurb = f"{util.commaify(str(dataset.metadata.sequences))} sequences"
             else:
                 dataset.blurb = nice_size(dataset.get_size())
-            dataset.blurb += f'\nversion={dataset.metadata.version}'
+            dataset.blurb += f"\nversion={dataset.metadata.version}"
             for k, v in dataset.metadata.properties.items():
-                if k != 'version':
-                    dataset.blurb += f'\n{k}={v}'
+                if k != "version":
+                    dataset.blurb += f"\n{k}={v}"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class BaseFastq(Sequence):
     """Base class for FastQ sequences"""
+
     edam_format = "format_1930"
     file_ext = "fastq"
     bases_regexp = re.compile(r"^[NGTAC 0123\.]*$", re.IGNORECASE)
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of sequences and the number of data lines
         in dataset.
         FIXME: This does not properly handle line wrapping
         """
         if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
             dataset.metadata.data_lines = None
             dataset.metadata.sequences = None
             return
         data_lines = 0
         sequences = 0
-        seq_counter = 0     # blocks should be 4 lines long
+        seq_counter = 0  # blocks should be 4 lines long
         with compression_utils.get_fileobj(dataset.file_name) as in_file:
             for line in in_file:
                 line = line.strip()
-                if line and line.startswith('#') and not data_lines:
+                if line and line.startswith("#") and not data_lines:
                     # We don't count comment lines for sequence data types
                     continue
                 seq_counter += 1
                 data_lines += 1
-                if line and line.startswith('@'):
+                if line and line.startswith("@"):
                     if seq_counter >= 4:
                         # count previous block
                         # blocks should be 4 lines long
                         sequences += 1
                         seq_counter = 1
             if seq_counter >= 4:
                 # count final block
                 sequences += 1
             dataset.metadata.data_lines = data_lines
             dataset.metadata.sequences = sequences
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in generic fastq format
         For details, see http://maq.sourceforge.net/fastq.shtml
 
         Note: There are three kinds of FASTQ files, known as "Sanger" (sometimes called "Standard"), Solexa, and Illumina
               These differ in the representation of the quality scores
 
@@ -703,293 +758,343 @@
         True
         >>> FastqCSSanger().sniff(fname)
         True
         """
         compressed = file_prefix.compressed_format is not None
         if compressed and not isinstance(self, Binary):
             return False
-        headers = iter_headers(file_prefix, sep='\n', count=1000)
+        headers = iter_headers(file_prefix, sep="\n", count=1000)
         # check to see if the base qualities match
         if not self.quality_check(headers):
             return False
         return self.check_first_block(file_prefix)
 
-    def display_data(self, trans, dataset, preview=False, filename=None, to_ext=None, **kwd):
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        **kwd,
+    ):
         headers = kwd.get("headers", {})
         if preview:
             with compression_utils.get_fileobj(dataset.file_name) as fh:
                 max_peek_size = 1000000  # 1 MB
                 if os.stat(dataset.file_name).st_size < max_peek_size:
                     mime = "text/plain"
                     self._clean_and_set_mime_type(trans, mime, headers)
                     return fh.read(), headers
-                return trans.fill_template_mako("/dataset/large_file.mako",
-                                                truncated_data=fh.read(max_peek_size),
-                                                data=dataset), headers
+                return (
+                    trans.fill_template_mako(
+                        "/dataset/large_file.mako", truncated_data=fh.read(max_peek_size), data=dataset
+                    ),
+                    headers,
+                )
         else:
             return Sequence.display_data(self, trans, dataset, preview, filename, to_ext, **kwd)
 
     @classmethod
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """
         FASTQ files are split on cluster boundaries, in increments of 4 lines
         """
         if split_params is None:
             return None
 
         # first, see if there are any associated FQTOC files that will give us the split locations
         # if so, we don't need to read the files to do the splitting
         toc_file_datasets = []
         for ds in input_datasets:
             tmp_ds = ds
             fqtoc_file = None
             while fqtoc_file is None and tmp_ds is not None:
-                fqtoc_file = tmp_ds.get_converted_files_by_type('fqtoc')
+                fqtoc_file = tmp_ds.get_converted_files_by_type("fqtoc")
                 tmp_ds = tmp_ds.copied_from_library_dataset_dataset_association
 
             if fqtoc_file is not None:
                 toc_file_datasets.append(fqtoc_file)
 
         if len(toc_file_datasets) == len(input_datasets):
             return cls.do_fast_split(input_datasets, toc_file_datasets, subdir_generator_function, split_params)
         return cls.do_slow_split(input_datasets, subdir_generator_function, split_params)
 
     @staticmethod
-    def process_split_file(data):
+    def process_split_file(data: Dict) -> bool:
         """
         This is called in the context of an external process launched by a Task (possibly not on the Galaxy machine)
         to create the input files for the Task. The parameters:
         data - a dict containing the contents of the split file
         """
-        args = data['args']
-        input_name = data['input_name']
-        output_name = data['output_name']
-        start_sequence = int(args['start_sequence'])
-        sequence_count = int(args['num_sequences'])
+        args = data["args"]
+        input_name = data["input_name"]
+        output_name = data["output_name"]
+        start_sequence = int(args["start_sequence"])
+        sequence_count = int(args["num_sequences"])
 
-        if 'toc_file' in args:
-            with open(args['toc_file']) as f:
+        if "toc_file" in args:
+            with open(args["toc_file"]) as f:
                 toc_file = json.load(f)
-            commands = Sequence.get_split_commands_with_toc(input_name, output_name, toc_file, start_sequence, sequence_count)
+            commands = Sequence.get_split_commands_with_toc(
+                input_name, output_name, toc_file, start_sequence, sequence_count
+            )
         else:
-            commands = Sequence.get_split_commands_sequential(is_gzip(input_name), input_name, output_name, start_sequence, sequence_count)
+            commands = Sequence.get_split_commands_sequential(
+                is_gzip(input_name), input_name, output_name, start_sequence, sequence_count
+            )
         for cmd in commands:
             subprocess.check_call(cmd, shell=True)
         return True
 
     @staticmethod
-    def quality_check(lines):
+    def quality_check(lines: Iterable) -> bool:
         return True
 
     @classmethod
     def check_first_block(cls, file_prefix: FilePrefix):
         # check that first block looks like a fastq block
-        block = get_headers(file_prefix, sep='\n', count=4)
+        block = get_headers(file_prefix, sep="\n", count=4)
         return cls.check_block(block)
 
     @classmethod
-    def check_block(cls, block):
-        if len(block) == 4 and block[0][0] and block[0][0][0] == "@" and block[2][0] and block[2][0][0] == "+" and block[1][0]:
+    def check_block(cls, block: List) -> bool:
+        if (
+            len(block) == 4
+            and block[0][0]
+            and block[0][0][0] == "@"
+            and block[2][0]
+            and block[2][0][0] == "+"
+            and block[1][0]
+        ):
             # Check the sequence line, make sure it contains only G/C/A/T/N
             match = cls.bases_regexp.match(block[1][0])
             if match:
                 start, end = match.span()
                 if (end - start) == len(block[1][0]):
                     return True
         return False
 
-    def validate(self, dataset, **kwd):
-        headers = iter_headers(dataset.file_name, sep='\n', count=-1)
+    def validate(self, dataset: "DatasetInstance", **kwd) -> DatatypeValidation:
+        headers = iter_headers(dataset.file_name, sep="\n", count=-1)
         # check to see if the base qualities match
         if not self.quality_check(headers):
             return DatatypeValidation.invalid("Invalid quality score(s) found for this fastq datatype.")
 
-        headers = iter_headers(dataset.file_name, sep='\n', count=-1)
+        headers = iter_headers(dataset.file_name, sep="\n", count=-1)
         while True:
             block = list(islice(headers, 4))
             if len(block) == 0:
                 break
             if not self.check_block(block):
                 return DatatypeValidation.invalid("Invalid FASTQ structure found.")
 
         return DatatypeValidation.validated()
 
 
 class Fastq(BaseFastq):
     """Class representing a generic FASTQ sequence"""
+
     edam_format = "format_1930"
     file_ext = "fastq"
 
 
 class FastqSanger(Fastq):
     """Class representing a FASTQ sequence (the Sanger variant)
 
     phred scored quality values 0:50 represented by ASCII 33:83
     """
+
     edam_format = "format_1932"
     file_ext = "fastqsanger"
     bases_regexp = re.compile("^[NGTAC]*$", re.IGNORECASE)
 
     @staticmethod
-    def quality_check(lines):
+    def quality_check(lines: Iterable) -> bool:
         """
         Presuming lines are lines from a fastq file,
         return True if the qualities are compatible with sanger encoding
         """
         for line in islice(lines, 3, None, 4):
-            if not all(q >= '!' and q <= 'S' for q in line[0]):
+            if not all(q >= "!" and q <= "S" for q in line[0]):
                 return False
         return True
 
 
 class FastqSolexa(Fastq):
     """Class representing a FASTQ sequence ( the Solexa variant )
 
     solexa scored quality values -5:40 represented by ASCII 59:104
     """
+
     edam_format = "format_1933"
     file_ext = "fastqsolexa"
 
     @staticmethod
-    def quality_check(lines):
+    def quality_check(lines: Iterable) -> bool:
         """
         Presuming lines are lines from a fastq file,
         return True if the qualities are compatible with sanger encoding
         """
         for line in islice(lines, 3, None, 4):
-            if not all(q >= ';' and q <= 'h' for q in line[0]):
+            if not all(q >= ";" and q <= "h" for q in line[0]):
                 return False
         return True
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # we expicitely do not want to have this sniffed. so we keep this here
         # such that it can not be enabled in datatypes_conf
         raise NotImplementedError
 
 
 class FastqIllumina(Fastq):
     """Class representing a FASTQ sequence ( the Illumina 1.3+ variant )
 
     phred scored quality values 0:40 represented by ASCII 64:104
     """
+
     edam_format = "format_1931"
     file_ext = "fastqillumina"
 
     @staticmethod
-    def quality_check(lines):
+    def quality_check(lines: Iterable) -> bool:
         """
         Presuming lines are lines from a fastq file,
         return True if the qualities are compatible with sanger encoding
         """
         for line in islice(lines, 3, None, 4):
-            if not all(q >= '@' and q <= 'h' for q in line[0]):
+            if not all(q >= "@" and q <= "h" for q in line[0]):
                 return False
         return True
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # we expicitely do not want to have this sniffed. so we keep this here
         # such that it can not be enabled in datatypes_conf
         raise NotImplementedError
 
 
 class FastqCSSanger(Fastq):
     """Class representing a Color Space FASTQ sequence ( e.g a SOLiD variant )
 
     sequence in in color space
     phred scored quality values 0:93 represented by ASCII 33:126
     """
+
     file_ext = "fastqcssanger"
     bases_regexp = re.compile(r"^[NGTAC][0123\.]*$", re.IGNORECASE)
 
 
 @build_sniff_from_prefix
 class Maf(Alignment):
     """Class describing a Maf alignment"""
+
     edam_format = "format_3008"
     file_ext = "maf"
 
     # Readonly and optional, users can't unset it, but if it is not set, we are generally ok; if required use a metadata validator in the tool definition
-    MetadataElement(name="blocks", default=0, desc="Number of blocks", readonly=True, optional=True, visible=False, no_value=0)
-    MetadataElement(name="species_chromosomes", desc="Species Chromosomes", param=metadata.FileParameter, readonly=True, visible=False, optional=True)
-    MetadataElement(name="maf_index", desc="MAF Index File", param=metadata.FileParameter, readonly=True, visible=False, optional=True)
+    MetadataElement(
+        name="blocks", default=0, desc="Number of blocks", readonly=True, optional=True, visible=False, no_value=0
+    )
+    MetadataElement(
+        name="species_chromosomes",
+        desc="Species Chromosomes",
+        param=metadata.FileParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+    MetadataElement(
+        name="maf_index",
+        desc="MAF Index File",
+        param=metadata.FileParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         Alignment.init_meta(self, dataset, copy_from=copy_from)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
         """
         Parses and sets species, chromosomes, index from MAF file.
         """
         # these metadata values are not accessable by users, always overwrite
         # Imported here to avoid circular dependency
         from galaxy.tools.util.maf_utilities import build_maf_index_species_chromosomes
+
         indexes, species, species_chromosomes, blocks = build_maf_index_species_chromosomes(dataset.file_name)
         if indexes is None:
             return  # this is not a MAF file
         dataset.metadata.species = species
         dataset.metadata.blocks = blocks
 
         # write species chromosomes to a file
         chrom_file = dataset.metadata.species_chromosomes
         if not chrom_file:
-            chrom_file = dataset.metadata.spec['species_chromosomes'].param.new_file(dataset=dataset)
-        with open(chrom_file.file_name, 'w') as chrom_out:
+            chrom_file = dataset.metadata.spec["species_chromosomes"].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
+        with open(chrom_file.file_name, "w") as chrom_out:
             for spec, chroms in species_chromosomes.items():
                 chrom_out.write("{}\t{}\n".format(spec, "\t".join(chroms)))
         dataset.metadata.species_chromosomes = chrom_file
 
         index_file = dataset.metadata.maf_index
         if not index_file:
-            index_file = dataset.metadata.spec['maf_index'].param.new_file(dataset=dataset)
-        indexes.write(open(index_file.file_name, 'wb'))
+            index_file = dataset.metadata.spec["maf_index"].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
+        indexes.write(open(index_file.file_name, "wb"))
         dataset.metadata.maf_index = index_file
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             # The file must exist on disk for the get_file_peek() method
             dataset.peek = data.get_file_peek(dataset.file_name)
             if dataset.metadata.blocks:
                 dataset.blurb = f"{util.commaify(str(dataset.metadata.blocks))} blocks"
             else:
                 # Number of blocks is not known ( this should not happen ), and auto-detect is
                 # needed to set metadata
                 dataset.blurb = "? blocks"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset)
 
-    def make_html_table(self, dataset, skipchars=None):
+    def make_html_table(self, dataset: "DatasetInstance", skipchars: Optional[List] = None) -> str:
         """Create HTML table, used for displaying peek"""
         skipchars = skipchars or []
-        out = ['<table cellspacing="0" cellpadding="3">']
         try:
-            out.append('<tr><th>Species:&nbsp;')
+            out = ['<table cellspacing="0" cellpadding="3">']
+            out.append("<tr><th>Species:&nbsp;")
             for species in dataset.metadata.species:
-                out.append(f'{species}&nbsp;')
-            out.append('</th></tr>')
+                out.append(f"{species}&nbsp;")
+            out.append("</th></tr>")
             if not dataset.peek:
                 dataset.set_peek()
             data = dataset.peek
             lines = data.splitlines()
             for line in lines:
                 line = line.strip()
                 if not line:
                     continue
-                out.append(f'<tr><td>{escape(line)}</td></tr>')
-            out.append('</table>')
-            out = "".join(out)
+                out.append(f"<tr><td>{escape(line)}</td></tr>")
+            out.append("</table>")
+            return "".join(out)
         except Exception as exc:
-            out = f"Can't create peek {exc}"
-        return out
+            return f"Can't create peek {exc}"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines wether the file is in maf format
 
         The .maf format is line-oriented. Each multiple alignment ends with a blank line.
         Each sequence in an alignment is on a single line, which can get quite long, but
         there is no length limit. Words in a line are delimited by any white space.
         Lines starting with # are considered to be comments. Lines starting with ## can
@@ -1017,19 +1122,31 @@
         except Exception:
             return False
 
 
 class MafCustomTrack(data.Text):
     file_ext = "mafcustomtrack"
 
-    MetadataElement(name="vp_chromosome", default='chr1', desc="Viewport Chromosome", readonly=True, optional=True, visible=False, no_value='')
-    MetadataElement(name="vp_start", default='1', desc="Viewport Start", readonly=True, optional=True, visible=False, no_value='')
-    MetadataElement(name="vp_end", default='100', desc="Viewport End", readonly=True, optional=True, visible=False, no_value='')
+    MetadataElement(
+        name="vp_chromosome",
+        default="chr1",
+        desc="Viewport Chromosome",
+        readonly=True,
+        optional=True,
+        visible=False,
+        no_value="",
+    )
+    MetadataElement(
+        name="vp_start", default="1", desc="Viewport Start", readonly=True, optional=True, visible=False, no_value=""
+    )
+    MetadataElement(
+        name="vp_end", default="100", desc="Viewport End", readonly=True, optional=True, visible=False, no_value=""
+    )
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Parses and sets viewport metadata from MAF file.
         """
         max_block_check = 10
         chrom = None
         forward_strand_start = math.inf
         forward_strand_end = 0
@@ -1055,23 +1172,24 @@
         except Exception:
             pass
 
 
 @build_sniff_from_prefix
 class Axt(data.Text):
     """Class describing an axt alignment"""
+
     # gvk- 11/19/09 - This is really an alignment, but we no longer have tools that use this data type, and it is
     # here simply for backward compatibility ( although it is still in the datatypes registry ).  Subclassing
     # from data.Text eliminates managing metadata elements inherited from the Alignemnt class.
 
     edam_data = "data_0863"
     edam_format = "format_3013"
     file_ext = "axt"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in axt format
 
         axt alignment files are produced from Blastz, an alignment tool available from Webb Miller's lab
         at Penn State University.
 
         Each alignment block in an axt file contains three lines: a summary line and 2 sequence lines.
@@ -1107,28 +1225,30 @@
                         int(_)
                 except ValueError:
                     return False
                 if hdr[7] not in data.valid_strand:
                     return False
                 else:
                     return True
+        return False
 
 
 @build_sniff_from_prefix
 class Lav(data.Text):
     """Class describing a LAV alignment"""
+
     # gvk- 11/19/09 - This is really an alignment, but we no longer have tools that use this data type, and it is
     # here simply for backward compatibility ( although it is still in the datatypes registry ).  Subclassing
     # from data.Text eliminates managing metadata elements inherited from the Alignment class.
 
     edam_data = "data_0863"
     edam_format = "format_3014"
     file_ext = "lav"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in lav format
 
         LAV is an alignment format developed by Webb Miller's group. It is the primary output format for BLASTZ.
         The first line of a .lav file begins with #:lav.
 
         For complete details see http://www.bioperl.org/wiki/LAV_alignment_format
@@ -1139,49 +1259,49 @@
         True
         >>> fname = get_test_fname( 'alignment.axt' )
         >>> Lav().sniff( fname )
         False
         """
         headers = get_headers(file_prefix, None)
         try:
-            if len(headers) > 1 and headers[0][0] and headers[0][0].startswith('#:lav'):
+            if len(headers) > 1 and headers[0][0] and headers[0][0].startswith("#:lav"):
                 return True
             else:
                 return False
         except Exception:
             return False
 
 
 class RNADotPlotMatrix(data.Data):
     edam_format = "format_3466"
     file_ext = "rna_eps"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
-            dataset.peek = 'RNA Dot Plot format (Postscript derivative)'
+            dataset.peek = "RNA Dot Plot format (Postscript derivative)"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """Determine if the file is in RNA dot plot format."""
-        if check_image_type(filename, ['EPS']):
+        if check_image_type(filename, ["EPS"]):
             seq = False
             coor = False
             pairs = False
             with open(filename) as handle:
                 for line in handle:
                     line = line.strip()
                     if line:
-                        if line.startswith('/sequence'):
+                        if line.startswith("/sequence"):
                             seq = True
-                        elif line.startswith('/coor'):
+                        elif line.startswith("/coor"):
                             coor = True
-                        elif line.startswith('/pairs'):
+                        elif line.startswith("/pairs"):
                             pairs = True
                     if seq and coor and pairs:
                         return True
         return False
 
 
 @build_sniff_from_prefix
@@ -1189,15 +1309,15 @@
     edam_data = "data_0880"
     edam_format = "format_1457"
     file_ext = "dbn"
 
     sequence_regexp = re.compile(r"^[ACGTURYKMSWBDHVN]+$", re.I)
     structure_regexp = re.compile(r"^[\(\)\.\[\]{}]+$")
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of sequences and the number of data lines
         in dataset.
         """
         if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
             dataset.metadata.data_lines = None
             dataset.metadata.sequences = None
@@ -1207,21 +1327,21 @@
         data_lines = 0
         sequences = 0
 
         for line in open(dataset.file_name):
             line = line.strip()
             data_lines += 1
 
-            if line and line.startswith('>'):
+            if line and line.startswith(">"):
                 sequences += 1
 
         dataset.metadata.data_lines = data_lines
         dataset.metadata.sequences = sequences
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Galaxy Dbn (Dot-Bracket notation) rules:
 
         * The first non-empty line is a header line: no comment lines are allowed.
 
           * A header line starts with a '>' symbol and continues with 0 or multiple symbols until the line ends.
 
@@ -1241,80 +1361,85 @@
             * In a structure line, the number of '(' symbols equals the number of ')' symbols, the number of '[' symbols equals the number of ']' symbols and the number of '{' symbols equals the number of '}' symbols.
 
         * The format accepts multiple entries per file, given that each entry is provided as three lines: the header, sequence and structure line.
 
             * Sniffing is only applied on the first entry.
 
         * Empty lines are allowed.
-         """
+        """
 
         state = 0
 
         for line in file_prefix.line_iterator():
             line = line.strip()
 
             if line:
                 # header line
                 if state == 0:
-                    if(line[0] != '>'):
+                    if line[0] != ">":
                         return False
                     else:
                         state = 1
 
                 # sequence line
                 elif state == 1:
                     if not self.sequence_regexp.match(line):
                         return False
                     else:
                         sequence_size = len(line)
                         state = 2
 
                 # dot-bracket structure line
                 elif state == 2:
-                    if sequence_size != len(line) or not self.structure_regexp.match(line) or \
-                            line.count('(') != line.count(')') or \
-                            line.count('[') != line.count(']') or \
-                            line.count('{') != line.count('}'):
+                    if (
+                        sequence_size != len(line)
+                        or not self.structure_regexp.match(line)
+                        or line.count("(") != line.count(")")
+                        or line.count("[") != line.count("]")
+                        or line.count("{") != line.count("}")
+                    ):
                         return False
                     else:
                         return True
 
         # Number of lines is less than 3
         return False
 
 
 @build_sniff_from_prefix
 class Genbank(data.Text):
     """Class representing a Genbank sequence"""
+
     edam_format = "format_1936"
     edam_data = "data_0849"
     file_ext = "genbank"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determine whether the file is in genbank format.
         Works for compressed files.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( '1.genbank' )
         >>> Genbank().sniff( fname )
         True
         """
         compressed = file_prefix.compressed_format
         if compressed and not isinstance(self, Binary):
             return False
-        return 'LOCUS ' == file_prefix.contents_header[0:6]
+        return "LOCUS " == file_prefix.contents_header[0:6]
 
 
 @build_sniff_from_prefix
 class MemePsp(Sequence):
     """Class representing MEME Position Specific Priors"""
+
     file_ext = "memepsp"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         The format of an entry in a PSP file is:
 
         >ID WIDTH
         PRIORS
 
         For complete details see http://meme-suite.org/doc/psp-format.html
@@ -1323,42 +1448,44 @@
         >>> fname = get_test_fname('1.memepsp')
         >>> MemePsp().sniff(fname)
         True
         >>> fname = get_test_fname('sequence.fasta')
         >>> MemePsp().sniff(fname)
         False
         """
+
         def floats_verified(line):
             for item in line.split():
                 try:
                     float(item)
                 except ValueError:
                     return False
                 try:
                     int(item)
                 except ValueError:
                     return True
             return False
+
         num_lines = 0
         fh = file_prefix.string_io()
         got_header = False
         got_priors = False
         while num_lines < 100:
             line = fh.readline()
             if not line:
                 # EOF.
                 break
             num_lines += 1
             line = line.strip()
             if line:
-                if line.startswith('>'):
+                if line.startswith(">"):
                     got_header = True
                     # The line must not be blank, nor start with '>'
                     line = fh.readline().strip()
-                    if line == '' or line.startswith('>'):
+                    if line == "" or line.startswith(">"):
                         return False
                     # All items within the line must be floats.
                     if not floats_verified(line):
                         return False
                     else:
                         got_priors = True
                     # If there is a second line within the ID section,
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/set_metadata_tool.xml` & `galaxy-data-23.0.1/galaxy/datatypes/set_metadata_tool.xml`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/sniff.py` & `galaxy-data-23.0.1/galaxy/datatypes/sniff.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,85 +6,71 @@
 import gzip
 import io
 import logging
 import os
 import re
 import shutil
 import struct
-import sys
 import tempfile
-import urllib.request
 import zipfile
+from functools import partial
 from typing import (
     Dict,
     IO,
     NamedTuple,
     Optional,
     Union,
 )
 
 from typing_extensions import Protocol
 
 from galaxy import util
-from galaxy.files import ConfiguredFileSources
+from galaxy.files.uris import stream_url_to_file as files_stream_url_to_file
 from galaxy.util import (
     compression_utils,
     file_reader,
-    stream_to_open_named_file
+    is_binary,
 )
 from galaxy.util.checkers import (
-    check_binary,
     check_html,
     COMPRESSION_CHECK_FUNCTIONS,
     is_tar,
 )
 
+import pylibmagic  # noqa: F401  # isort:skip
+import magic  # isort:skip
+
+
 log = logging.getLogger(__name__)
 
-SNIFF_PREFIX_BYTES = int(os.environ.get("GALAXY_SNIFF_PREFIX_BYTES", None) or 2 ** 20)
+SNIFF_PREFIX_BYTES = int(os.environ.get("GALAXY_SNIFF_PREFIX_BYTES", None) or 2**20)
+BINARY_MIMETYPES = {"application/pdf", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"}
 
 
 def get_test_fname(fname):
     """Returns test data filename"""
     path, name = os.path.split(__file__)
-    full_path = os.path.join(path, 'test', fname)
+    full_path = os.path.join(path, "test", fname)
     return full_path
 
 
 def sniff_with_cls(cls, fname):
     path = get_test_fname(fname)
     try:
         return bool(cls().sniff(path))
     except Exception:
         return False
 
 
-def stream_url_to_file(path: str, file_sources: Optional[ConfiguredFileSources] = None):
-    prefix = "url_paste"
-    if file_sources and file_sources.looks_like_uri(path):
-        file_source_path = file_sources.get_file_source_path(path)
-        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False) as temp:
-            temp_name = temp.name
-        file_source_path.file_source.realize_to(file_source_path.path, temp_name)
-        return temp_name
-    else:
-        page = urllib.request.urlopen(path, timeout=util.DEFAULT_SOCKET_TIMEOUT)  # page will be .close()ed in stream_to_file
-        temp_name = stream_to_file(page, prefix=prefix, source_encoding=util.get_charset_from_http_headers(page.headers))
-        return temp_name
-
-
-def stream_to_file(stream, suffix='', prefix='', dir=None, text=False, **kwd):
-    """Writes a stream to a temporary file, returns the temporary file's name"""
-    fd, temp_name = tempfile.mkstemp(suffix=suffix, prefix=prefix, dir=dir, text=text)
-    return stream_to_open_named_file(stream, fd, temp_name, **kwd)
+stream_url_to_file = partial(files_stream_url_to_file, prefix="gx_url_paste")
 
 
 def handle_composite_file(datatype, src_path, extra_files, name, is_binary, tmp_dir, tmp_prefix, upload_opts):
     if not is_binary:
-        if upload_opts.get('space_to_tab'):
+        if upload_opts.get("space_to_tab"):
             convert_newlines_sep2tabs(src_path, tmp_dir=tmp_dir, tmp_prefix=tmp_prefix)
         else:
             convert_newlines(src_path, tmp_dir=tmp_dir, tmp_prefix=tmp_prefix)
 
     file_output_path = os.path.join(extra_files, name)
     shutil.move(src_path, file_output_path)
 
@@ -97,30 +83,40 @@
     line_count: int
     converted_path: Optional[str]
     converted_newlines: bool
     converted_regex: bool
 
 
 class ConvertFunction(Protocol):
-
-    def __call__(self, fname: str, in_place: bool = True, tmp_dir: Optional[str] = None, tmp_prefix: Optional[str] = "gxupload") -> ConvertResult:
+    def __call__(
+        self, fname: str, in_place: bool = True, tmp_dir: Optional[str] = None, tmp_prefix: Optional[str] = "gxupload"
+    ) -> ConvertResult:
         ...
 
 
-def convert_newlines(fname: str, in_place: bool = True, tmp_dir: Optional[str] = None, tmp_prefix: Optional[str] = "gxupload", block_size: int = 128 * 1024, regexp=None) -> ConvertResult:
+def convert_newlines(
+    fname: str,
+    in_place: bool = True,
+    tmp_dir: Optional[str] = None,
+    tmp_prefix: Optional[str] = "gxupload",
+    block_size: int = 128 * 1024,
+    regexp=None,
+) -> ConvertResult:
     """
     Converts in place a file from universal line endings
     to Posix line endings.
     """
     i = 0
     converted_newlines = False
     converted_regex = False
     NEWLINE_BYTE = 10
     CR_BYTE = 13
-    with tempfile.NamedTemporaryFile(mode='wb', prefix=tmp_prefix, dir=tmp_dir, delete=False) as fp, open(fname, mode='rb') as fi:
+    with tempfile.NamedTemporaryFile(mode="wb", prefix=tmp_prefix, dir=tmp_dir, delete=False) as fp, open(
+        fname, mode="rb"
+    ) as fi:
         last_char = None
         block = fi.read(block_size)
         last_block = b""
         while block:
             if last_char == CR_BYTE and block.startswith(b"\n"):
                 # Last block ended with CR, new block startswith newline.
                 # Since we replace CR with newline in the previous iteration we skip the first byte
@@ -147,24 +143,32 @@
         shutil.move(fp.name, fname)
         # Return number of lines in file.
         return ConvertResult(i, None, converted_newlines, converted_regex)
     else:
         return ConvertResult(i, fp.name, converted_newlines, converted_regex)
 
 
-def convert_sep2tabs(fname: str, in_place: bool = True, tmp_dir: Optional[str] = None, tmp_prefix: Optional[str] = "gxupload", block_size: int = 128 * 1024):
+def convert_sep2tabs(
+    fname: str,
+    in_place: bool = True,
+    tmp_dir: Optional[str] = None,
+    tmp_prefix: Optional[str] = "gxupload",
+    block_size: int = 128 * 1024,
+):
     """
     Transforms in place a 'sep' separated file to a tab separated one
     """
-    patt: bytes = br"[^\S\r\n]+"
+    patt: bytes = rb"[^\S\r\n]+"
     regexp = re.compile(patt)
     i = 0
     converted_newlines = False
     converted_regex = False
-    with tempfile.NamedTemporaryFile(mode='wb', prefix=tmp_prefix, dir=tmp_dir, delete=False) as fp, open(fname, mode='rb') as fi:
+    with tempfile.NamedTemporaryFile(mode="wb", prefix=tmp_prefix, dir=tmp_dir, delete=False) as fp, open(
+        fname, mode="rb"
+    ) as fi:
         block = fi.read(block_size)
         while block:
             if block:
                 split_block = regexp.split(block)
                 if len(split_block) > 1:
                     converted_regex = True
                 block = b"\t".join(split_block)
@@ -175,32 +179,34 @@
         shutil.move(fp.name, fname)
         # Return number of lines in file.
         return ConvertResult(i, None, converted_newlines, converted_regex)
     else:
         return ConvertResult(i, fp.name, converted_newlines, converted_regex)
 
 
-def convert_newlines_sep2tabs(fname: str, in_place: bool = True, tmp_dir: Optional[str] = None, tmp_prefix: Optional[str] = "gxupload") -> ConvertResult:
+def convert_newlines_sep2tabs(
+    fname: str, in_place: bool = True, tmp_dir: Optional[str] = None, tmp_prefix: Optional[str] = "gxupload"
+) -> ConvertResult:
     """
     Converts newlines in a file to posix newlines and replaces spaces with tabs.
     """
-    patt: bytes = br"[^\S\n]+"
+    patt: bytes = rb"[^\S\n]+"
     regexp = re.compile(patt)
     return convert_newlines(fname, in_place, tmp_dir, tmp_prefix, regexp=regexp)
 
 
 def iter_headers(fname_or_file_prefix, sep, count=60, comment_designator=None):
     idx = 0
     if isinstance(fname_or_file_prefix, FilePrefix):
         file_iterator = fname_or_file_prefix.line_iterator()
     else:
         file_iterator = compression_utils.get_fileobj(fname_or_file_prefix)
     for line in file_iterator:
-        line = line.rstrip('\n\r')
-        if comment_designator is not None and comment_designator != '' and line.startswith(comment_designator):
+        line = line.rstrip("\n\r")
+        if comment_designator is not None and comment_designator != "" and line.startswith(comment_designator):
             continue
         yield line.split(sep)
         idx += 1
         if idx == count:
             break
 
 
@@ -217,18 +223,22 @@
     >>> fname = get_test_fname('complete.bed')
     >>> get_headers(fname,'\\t') == [['chr7', '127475281', '127491632', 'NM_000230', '0', '+', '127486022', '127488767', '0', '3', '29,172,3225,', '0,10713,13126,'], ['chr7', '127486011', '127488900', 'D49487', '0', '+', '127486022', '127488767', '0', '2', '155,490,', '0,2399']]
     True
     >>> fname = get_test_fname('test.gff')
     >>> get_headers(fname, '\\t', count=5, comment_designator='#') == [[''], ['chr7', 'bed2gff', 'AR', '26731313', '26731437', '.', '+', '.', 'score'], ['chr7', 'bed2gff', 'AR', '26731491', '26731536', '.', '+', '.', 'score'], ['chr7', 'bed2gff', 'AR', '26731541', '26731649', '.', '+', '.', 'score'], ['chr7', 'bed2gff', 'AR', '26731659', '26731841', '.', '+', '.', 'score']]
     True
     """
-    return list(iter_headers(fname_or_file_prefix=fname_or_file_prefix, sep=sep, count=count, comment_designator=comment_designator))
+    return list(
+        iter_headers(
+            fname_or_file_prefix=fname_or_file_prefix, sep=sep, count=count, comment_designator=comment_designator
+        )
+    )
 
 
-def is_column_based(fname_or_file_prefix, sep='\t', skip=0):
+def is_column_based(fname_or_file_prefix, sep="\t", skip=0):
     """
     Checks whether the file is column based with respect to a separator
     (defaults to tab separator).
 
     >>> fname = get_test_fname('test.gff')
     >>> is_column_based(fname)
     True
@@ -252,46 +262,52 @@
     >>> is_column_based(fname)
     True
     """
     if getattr(fname_or_file_prefix, "binary", None) is True:
         return False
 
     try:
-        headers = get_headers(fname_or_file_prefix, sep, comment_designator='#')[skip:]
+        headers = get_headers(fname_or_file_prefix, sep, comment_designator="#")[skip:]
     except UnicodeDecodeError:
         return False
     count = 0
     if not headers:
         return False
     for hdr in headers:
-        if hdr and hdr != ['']:
+        if hdr and hdr != [""]:
             if count:
                 if len(hdr) != count:
                     return False
             else:
                 count = len(hdr)
                 if count < 2:
                     return False
     return count >= 2
 
 
-def guess_ext(fname, sniff_order, is_binary=False):
+def guess_ext(fname_or_file_prefix: Union[str, "FilePrefix"], sniff_order, is_binary=None, auto_decompress=True):
     """
     Returns an extension that can be used in the datatype factory to
     generate a data for the 'fname' file
 
     >>> from galaxy.datatypes.registry import example_datatype_registry_for_sample
     >>> datatypes_registry = example_datatype_registry_for_sample()
     >>> sniff_order = datatypes_registry.sniff_order
     >>> fname = get_test_fname('empty.txt')
     >>> guess_ext(fname, sniff_order)
     'txt'
     >>> fname = get_test_fname('megablast_xml_parser_test1.blastxml')
     >>> guess_ext(fname, sniff_order)
     'blastxml'
+    >>> fname = get_test_fname('1.psl')
+    >>> guess_ext(fname, sniff_order)
+    'psl'
+    >>> fname = get_test_fname('2.psl')
+    >>> guess_ext(fname, sniff_order)
+    'psl'
     >>> fname = get_test_fname('interval.interval')
     >>> guess_ext(fname, sniff_order)
     'interval'
     >>> fname = get_test_fname('interv1.bed')
     >>> guess_ext(fname, sniff_order)
     'bed'
     >>> fname = get_test_fname('test_tab.bed')
@@ -386,14 +402,23 @@
     'xyz'
     >>> fname = get_test_fname('Si_multi.xyz')
     >>> guess_ext(fname, sniff_order)
     'xyz'
     >>> fname = get_test_fname('Si.extxyz')
     >>> guess_ext(fname, sniff_order)
     'extxyz'
+    >>> fname = get_test_fname('Si.castep')
+    >>> guess_ext(fname, sniff_order)
+    'castep'
+    >>> fname = get_test_fname('Si.param')
+    >>> guess_ext(fname, sniff_order)
+    'param'
+    >>> fname = get_test_fname('Si.den_fmt')
+    >>> guess_ext(fname, sniff_order)
+    'den_fmt'
     >>> fname = get_test_fname('mothur_datatypetest_true.mothur.otu')
     >>> guess_ext(fname, sniff_order)
     'mothur.otu'
     >>> fname = get_test_fname('mothur_datatypetest_true.mothur.lower.dist')
     >>> guess_ext(fname, sniff_order)
     'mothur.lower.dist'
     >>> fname = get_test_fname('mothur_datatypetest_true.mothur.square.dist')
@@ -500,41 +525,46 @@
     >>> fname = get_test_fname('1imzml')
     >>> guess_ext(fname, sniff_order)  # This test case is ensuring doesn't throw exception, actual value could change if non-utf encoding handling improves.
     'data'
     >>> fname = get_test_fname('too_many_comments_gff3.tabular')
     >>> guess_ext(fname, sniff_order)  # It's a VCF but is sniffed as tabular because of the limit on the number of header lines we read
     'tabular'
     """
-    file_prefix = FilePrefix(fname)
-    file_ext = run_sniffers_raw(file_prefix, sniff_order, is_binary)
+    file_prefix = _get_file_prefix(fname_or_file_prefix, auto_decompress=auto_decompress)
+    file_ext = run_sniffers_raw(file_prefix, sniff_order)
 
     # Ugly hack for tsv vs tabular sniffing, we want to prefer tabular
     # to tsv but it doesn't have a sniffer - is TSV was sniffed just check
     # if it is an okay tabular and use that instead.
-    if file_ext == 'tsv':
-        if is_column_based(file_prefix, '\t', 1):
-            file_ext = 'tabular'
+    if file_ext == "tsv":
+        if is_column_based(file_prefix, "\t", 1):
+            file_ext = "tabular"
     if file_ext is not None:
         return file_ext
 
     # skip header check if data is already known to be binary
-    if is_binary:
-        return file_ext or 'binary'
+    if file_prefix.binary:
+        return file_ext or "binary"
     try:
         get_headers(file_prefix, None)
     except UnicodeDecodeError:
-        return 'data'  # default data type file extension
-    if is_column_based(file_prefix, '\t', 1):
-        return 'tabular'  # default tabular data type file extension
-    return 'txt'  # default text data type file extension
+        return "data"  # default data type file extension
+    if is_column_based(file_prefix, "\t", 1):
+        return "tabular"  # default tabular data type file extension
+    return "txt"  # default text data type file extension
 
 
-class FilePrefix:
+def guess_ext_from_file_name(fname, registry, requested_ext="auto"):
+    if requested_ext != "auto":
+        return requested_ext
+    return registry.get_datatype_from_filename(fname).file_ext
+
 
-    def __init__(self, filename):
+class FilePrefix:
+    def __init__(self, filename, auto_decompress=True):
         non_utf8_error = None
         compressed_format = None
         contents_header_bytes = None
         contents_header = None  # First MAX_BYTES of the file.
         truncated = False
         # A future direction to optimize sniffing even more for sniffers at the top of the list
         # is to lazy load contents_header based on what interface is requested. For instance instead
@@ -548,24 +578,52 @@
                 truncated = len(contents_header_bytes) == SNIFF_PREFIX_BYTES
                 contents_header = contents_header_bytes.decode("utf-8")
             finally:
                 f.close()
         except UnicodeDecodeError as e:
             non_utf8_error = e
 
+        self.auto_decompress = auto_decompress
         self.truncated = truncated
         self.filename = filename
         self.non_utf8_error = non_utf8_error
-        self.binary = non_utf8_error is not None  # obviously wrong
+        file_magic = magic.detect_from_content(contents_header_bytes)
+        self.encoding = file_magic.encoding
+        self.mime_type = file_magic.mime_type
+        self.compressed_mime_type = None
+        self.compressed_encoding = None
+        if compressed_format:
+            compressed_magic = magic.detect_from_filename(filename)
+            self.compressed_mime_type = compressed_magic.mime_type
+            self.compressed_encoding = compressed_magic.encoding
         self.compressed_format = compressed_format
         self.contents_header = contents_header
         self.contents_header_bytes = contents_header_bytes
+        self._is_binary = None
         self._file_size = None
 
     @property
+    def binary(self):
+        if self._is_binary is None:
+            self._is_binary = bool({self.mime_type, self.compressed_mime_type} & BINARY_MIMETYPES) or is_binary(
+                self.contents_header_bytes
+            )
+            if (
+                not self._is_binary
+                and self.encoding == "binary"
+                and self.non_utf8_error
+                or not self.auto_decompress
+                and self.compressed_encoding == "binary"
+            ):
+                # Try harder ... if we have a non-utf-8 error, the file could be latin-1 encoded,
+                # but magic would recognize this and set the encoding appropriately
+                self._is_binary = True
+        return self._is_binary
+
+    @property
     def file_size(self):
         if self._file_size is None:
             self._file_size = os.path.getsize(self.filename)
         return self._file_size
 
     def string_io(self) -> io.StringIO:
         if self.non_utf8_error is not None:
@@ -578,15 +636,15 @@
 
     def startswith(self, prefix):
         return self.string_io().read(len(prefix)) == prefix
 
     def line_iterator(self):
         s = self.string_io()
         s_len = len(s.getvalue())
-        for line in iter(s.readline, ''):
+        for line in iter(s.readline, ""):
             if line.endswith("\n") or line.endswith("\r"):
                 yield line
             elif s.tell() == s_len and not self.truncated:
                 # At the end, return the last line if it wasn't truncated when reading it in.
                 yield line
 
     # Convenience wrappers around contents_header, shielding contents_header means we can
@@ -607,65 +665,70 @@
             return None
         return struct.unpack(pattern, header_bytes)[0]
 
     def startswith_bytes(self, test_bytes):
         return self.contents_header_bytes.startswith(test_bytes)
 
 
-def _get_file_prefix(filename_or_file_prefix: Union[str, FilePrefix]) -> FilePrefix:
+def _get_file_prefix(filename_or_file_prefix: Union[str, FilePrefix], auto_decompress: bool = True) -> FilePrefix:
     if not isinstance(filename_or_file_prefix, FilePrefix):
-        return FilePrefix(filename_or_file_prefix)
+        return FilePrefix(filename_or_file_prefix, auto_decompress=auto_decompress)
     return filename_or_file_prefix
 
 
-def run_sniffers_raw(filename_or_file_prefix: Union[str, FilePrefix], sniff_order, is_binary=False):
-    """Run through sniffers specified by sniff_order, return None of None match.
-    """
-    file_prefix = _get_file_prefix(filename_or_file_prefix)
+def run_sniffers_raw(file_prefix: FilePrefix, sniff_order):
+    """Run through sniffers specified by sniff_order, return None of None match."""
     fname = file_prefix.filename
     file_ext = None
     for datatype in sniff_order:
         """
         Some classes may not have a sniff function, which is ok.  In fact,
         Binary, Data, Tabular and Text are examples of classes that should never
         have a sniff function. Since these classes are default classes, they contain
         few rules to filter out data of other formats, so they should be called
         from this function after all other datatypes in sniff_order have not been
         successfully discovered.
         """
+        datatype_compressed = getattr(datatype, "compressed", False)
+        if datatype_compressed and not file_prefix.compressed_format and not datatype.file_ext.endswith(".tar"):
+            # we don't auto-detect tar as compressed
+            continue
+        if not datatype_compressed and file_prefix.compressed_format:
+            continue
+        if file_prefix.binary != datatype.is_binary and not datatype.is_binary == "maybe":
+            # Binary detection doesn't match datatype ...
+            compressed_data_for_compressed_text_datatype = (
+                file_prefix.binary and file_prefix.compressed_format and datatype_compressed and not datatype.is_binary
+            )
+            if not compressed_data_for_compressed_text_datatype:
+                # ... and mismatch is not due to compressed text data for a compressed text datatype
+                continue
         try:
             if hasattr(datatype, "sniff_prefix"):
-                datatype_compressed = getattr(datatype, "compressed", False)
-                if datatype_compressed and not file_prefix.compressed_format:
-                    continue
-                if not datatype_compressed and file_prefix.compressed_format:
-                    continue
                 if file_prefix.compressed_format and getattr(datatype, "compressed_format", None):
-                    # In this case go a step further and compare the compressed format detected
+                    # Compare the compressed format detected
                     # to the expected.
                     if file_prefix.compressed_format != datatype.compressed_format:
                         continue
                 if datatype.sniff_prefix(file_prefix):
                     file_ext = datatype.file_ext
                     break
-            elif is_binary and not datatype.is_binary:
-                continue
             elif datatype.sniff(fname):
                 file_ext = datatype.file_ext
                 break
         except Exception:
             pass
 
     return file_ext
 
 
 def zip_single_fileobj(path):
     z = zipfile.ZipFile(path)
     for name in z.namelist():
-        if not name.endswith('/'):
+        if not name.endswith("/"):
             return z.open(name)
 
 
 def build_sniff_from_prefix(klass):
     # Build and attach a sniff function to this class (klass) from the sniff_prefix function
     # expected to be defined for the class.
     def auto_sniff(self, filename):
@@ -695,25 +758,25 @@
 
 
 class HandleCompressedFileResponse(NamedTuple):
     is_valid: bool
     ext: str
     uncompressed_path: str
     compressed_type: Optional[str]
+    is_compressed: Optional[bool]
 
 
 def handle_compressed_file(
-        filename: str,
-        datatypes_registry,
-        ext: str = 'auto',
-        tmp_prefix: Optional[str] = 'sniff_uncompress_',
-        tmp_dir: Optional[str] = None,
-        in_place: bool = False,
-        check_content: bool = True,
-        auto_decompress: bool = True,
+    file_prefix: FilePrefix,
+    datatypes_registry,
+    ext: str = "auto",
+    tmp_prefix: Optional[str] = "sniff_uncompress_",
+    tmp_dir: Optional[str] = None,
+    in_place: bool = False,
+    check_content: bool = True,
 ) -> HandleCompressedFileResponse:
     """
     Check uploaded files for compression, check compressed file contents, and uncompress if necessary.
 
     Supports GZip, BZip2, and the first file in a Zip file.
 
     For performance reasons, the temporary file used for uncompression is located in the same directory as the
@@ -722,64 +785,71 @@
     ``ext`` as returned will only be changed from the ``ext`` input param if the param was an autodetect type (``auto``)
     and the file was sniffed as a keep-compressed datatype.
 
     ``is_valid`` as returned will only be set if the file is compressed and contains invalid contents (or the first file
     in the case of a zip file), this is so lengthy decompression can be bypassed if there is invalid content in the
     first 32KB. Otherwise the caller should be checking content.
     """
-    CHUNK_SIZE = 2 ** 20  # 1Mb
+    CHUNK_SIZE = 2**20  # 1Mb
     is_compressed = False
     compressed_type = None
     keep_compressed = False
     is_valid = False
+    filename = file_prefix.filename
     uncompressed_path = filename
     tmp_dir = tmp_dir or os.path.dirname(filename)
-    for key, check_compressed_function in COMPRESSION_CHECK_FUNCTIONS:
+    check_compressed_function = COMPRESSION_CHECK_FUNCTIONS.get(file_prefix.compressed_format)
+    if check_compressed_function:
         is_compressed, is_valid = check_compressed_function(filename, check_content=check_content)
-        if is_compressed:
-            compressed_type = key
-            break  # found compression type
+        compressed_type = file_prefix.compressed_format
     if is_compressed and is_valid:
         if ext in AUTO_DETECT_EXTENSIONS:
             # attempt to sniff for a keep-compressed datatype (observing the sniff order)
-            sniff_datatypes = filter(lambda d: getattr(d, 'compressed', False), datatypes_registry.sniff_order)
-            sniffed_ext = run_sniffers_raw(filename, sniff_datatypes)
+            sniff_datatypes = filter(lambda d: getattr(d, "compressed", False), datatypes_registry.sniff_order)
+            sniffed_ext = run_sniffers_raw(file_prefix, sniff_datatypes)
             if sniffed_ext:
                 ext = sniffed_ext
                 keep_compressed = True
         else:
             datatype = datatypes_registry.get_datatype_by_extension(ext)
-            keep_compressed = getattr(datatype, 'compressed', False)
+            keep_compressed = getattr(datatype, "compressed", False)
     # don't waste time decompressing if we sniff invalid contents
-    if is_compressed and is_valid and auto_decompress and not keep_compressed:
+    if is_compressed and is_valid and file_prefix.auto_decompress and not keep_compressed:
         assert compressed_type  # Tell type checker is_compressed will only be true if compressed_type is also set.
         with tempfile.NamedTemporaryFile(prefix=tmp_prefix, dir=tmp_dir, delete=False) as uncompressed:
             with DECOMPRESSION_FUNCTIONS[compressed_type](filename) as compressed_file:
                 # TODO: it'd be ideal to convert to posix newlines and space-to-tab here as well
                 try:
                     for chunk in file_reader(compressed_file, CHUNK_SIZE):
                         if not chunk:
                             break
                         uncompressed.write(chunk)
                 except OSError as e:
                     os.remove(uncompressed.name)
-                    raise OSError('Problem uncompressing {} data, please try retrieving the data uncompressed: {}'.format(compressed_type, util.unicodify(e)))
+                    raise OSError(
+                        "Problem uncompressing {} data, please try retrieving the data uncompressed: {}".format(
+                            compressed_type, util.unicodify(e)
+                        )
+                    )
+                finally:
+                    is_compressed = False
         uncompressed_path = uncompressed.name
         if in_place:
             # Replace the compressed file with the uncompressed file
             shutil.move(uncompressed_path, filename)
             uncompressed_path = filename
     elif not is_compressed or not check_content:
         is_valid = True
-    return HandleCompressedFileResponse(is_valid, ext, uncompressed_path, compressed_type)
+    return HandleCompressedFileResponse(is_valid, ext, uncompressed_path, compressed_type, is_compressed)
 
 
-def handle_uploaded_dataset_file(*args, **kwds) -> str:
+def handle_uploaded_dataset_file(filename, *args, **kwds) -> str:
     """Legacy wrapper about handle_uploaded_dataset_file_internal for tools using it."""
-    return handle_uploaded_dataset_file_internal(*args, **kwds)[0]
+    file_prefix = FilePrefix(filename)
+    return handle_uploaded_dataset_file_internal(file_prefix, *args, **kwds)[0]
 
 
 class HandleUploadedDatasetFileInternalResponse(NamedTuple):
     ext: str
     converted_path: str
     compressed_type: Optional[str]
     converted_newlines: bool
@@ -794,91 +864,86 @@
         convert_fxn = convert_newlines
     else:
         convert_fxn = convert_sep2tabs
     return convert_fxn
 
 
 def handle_uploaded_dataset_file_internal(
-        filename: str,
-        datatypes_registry,
-        ext: str = 'auto',
-        tmp_prefix: Optional[str] = 'sniff_upload_',
-        tmp_dir: Optional[str] = None,
-        in_place: bool = False,
-        check_content: bool = True,
-        is_binary: Optional[bool] = None,
-        auto_decompress: bool = True,
-        uploaded_file_ext: Optional[str] = None,
-        convert_to_posix_lines: Optional[bool] = None,
-        convert_spaces_to_tabs: Optional[bool] = None,
+    file_prefix: FilePrefix,
+    datatypes_registry,
+    ext: str = "auto",
+    tmp_prefix: Optional[str] = "sniff_upload_",
+    tmp_dir: Optional[str] = None,
+    in_place: bool = False,
+    check_content: bool = True,
+    is_binary: Optional[bool] = None,
+    uploaded_file_ext: Optional[str] = None,
+    convert_to_posix_lines: Optional[bool] = None,
+    convert_spaces_to_tabs: Optional[bool] = None,
 ) -> HandleUploadedDatasetFileInternalResponse:
-    is_valid, ext, converted_path, compressed_type = handle_compressed_file(
-        filename,
+    is_valid, ext, converted_path, compressed_type, is_compressed = handle_compressed_file(
+        file_prefix,
         datatypes_registry,
         ext=ext,
         tmp_prefix=tmp_prefix,
         tmp_dir=tmp_dir,
         in_place=in_place,
         check_content=check_content,
-        auto_decompress=auto_decompress,
     )
     converted_newlines = False
     converted_spaces = False
     try:
         if not is_valid:
             if is_tar(converted_path):
-                raise InappropriateDatasetContentError('TAR file uploads are not supported')
-            raise InappropriateDatasetContentError('The uploaded compressed file contains invalid content')
+                raise InappropriateDatasetContentError("TAR file uploads are not supported")
+            raise InappropriateDatasetContentError("The uploaded compressed file contains invalid content")
 
-        # This needs to be checked again after decompression
-        is_binary = check_binary(converted_path)
+        is_binary = file_prefix.binary
         guessed_ext = ext
         if ext in AUTO_DETECT_EXTENSIONS:
-            guessed_ext = guess_ext(converted_path, sniff_order=datatypes_registry.sniff_order, is_binary=is_binary)
-            guessed_datatype = datatypes_registry.get_datatype_by_extension(guessed_ext)
-            if not is_binary and guessed_datatype.is_binary:
-                # It's possible to have a datatype that is binary but not within the first 1024 bytes,
-                # so check_binary might return a false negative. This is for instance true for PDF files
-                is_binary = True
+            # TODO: skip this if we haven't actually converted the dataset
+            guessed_ext = guess_ext(
+                converted_path,
+                sniff_order=datatypes_registry.sniff_order,
+                auto_decompress=file_prefix.auto_decompress,
+            )
 
-        if not is_binary and (convert_to_posix_lines or convert_spaces_to_tabs):
+        if not is_binary and not is_compressed and (convert_to_posix_lines or convert_spaces_to_tabs):
             # Convert universal line endings to Posix line endings, spaces to tabs (if desired)
             convert_fxn = convert_function(convert_to_posix_lines, convert_spaces_to_tabs)
-            line_count, _converted_path, converted_newlines, converted_spaces = convert_fxn(converted_path, in_place=in_place, tmp_dir=tmp_dir, tmp_prefix=tmp_prefix)
+            line_count, _converted_path, converted_newlines, converted_spaces = convert_fxn(
+                converted_path, in_place=in_place, tmp_dir=tmp_dir, tmp_prefix=tmp_prefix
+            )
             if not in_place:
-                if converted_path and filename != converted_path:
+                if converted_path and file_prefix.filename != converted_path:
                     os.unlink(converted_path)
                 assert _converted_path
                 converted_path = _converted_path
             if ext in AUTO_DETECT_EXTENSIONS:
-                ext = guess_ext(converted_path, sniff_order=datatypes_registry.sniff_order, is_binary=is_binary)
+                ext = guess_ext(converted_path, sniff_order=datatypes_registry.sniff_order)
         else:
             ext = guessed_ext
 
         if not is_binary and check_content and check_html(converted_path):
-            raise InappropriateDatasetContentError('The uploaded file contains invalid HTML content')
+            raise InappropriateDatasetContentError("The uploaded file contains invalid HTML content")
     except Exception:
-        if filename != converted_path:
+        if file_prefix.filename != converted_path:
             os.unlink(converted_path)
         raise
-    return HandleUploadedDatasetFileInternalResponse(ext, converted_path, compressed_type, converted_newlines, converted_spaces)
+    return HandleUploadedDatasetFileInternalResponse(
+        ext, converted_path, compressed_type, converted_newlines, converted_spaces
+    )
 
 
-AUTO_DETECT_EXTENSIONS = ['auto']  # should 'data' also cause auto detect?
+AUTO_DETECT_EXTENSIONS = ["auto"]  # should 'data' also cause auto detect?
 
 
 class Decompress(Protocol):
-
     def __call__(self, path: str) -> IO[bytes]:
         ...
 
 
-DECOMPRESSION_FUNCTIONS: Dict[str, Decompress] = dict(gz=gzip.GzipFile, bz2=bz2.BZ2File, zip=zip_single_fileobj)
+DECOMPRESSION_FUNCTIONS: Dict[str, Decompress] = dict(gzip=gzip.GzipFile, bz2=bz2.BZ2File, zip=zip_single_fileobj)
 
 
 class InappropriateDatasetContentError(Exception):
     pass
-
-
-if __name__ == '__main__':
-    import doctest
-    doctest.testmod(sys.modules[__name__])
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/spaln.py` & `galaxy-data-23.0.1/galaxy/datatypes/spaln.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,34 @@
 """
 spaln Composite Dataset
 """
 
 import logging
 import os.path
-
-from galaxy.datatypes.data import Data
+from typing import (
+    Callable,
+    Dict,
+    List,
+    Optional,
+    TYPE_CHECKING,
+)
+
+from galaxy.datatypes.data import (
+    Data,
+    GeneratePrimaryFileDataset,
+)
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.util import smart_str
 
+if TYPE_CHECKING:
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+
 log = logging.getLogger(__name__)
 verbose = True
 
 
 class _SpalnDb(Data):
     composite_type = "auto_primary_file"
 
@@ -48,38 +64,31 @@
         self.add_composite_file(
             "%s.seq",
             is_binary=True,
             description="spalndb.seq",
             substitute_name_with_metadata="spalndb_name",
         )
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         rval = ["<html><head><title>Spaln Database</title></head><p/>"]
-        rval.append(
-            "<div>This composite dataset is composed of the following files:<p/><ul>"
-        )
-        for composite_name, composite_file in self.get_composite_files(
-            dataset=dataset
-        ).items():
+        rval.append("<div>This composite dataset is composed of the following files:<p/><ul>")
+        for composite_name, composite_file in self.get_composite_files(dataset=dataset).items():
             fn = composite_name
             opt_text = ""
             if composite_file.get("description"):
                 rval.append(
                     '<li><a href="%s" type="application/binary">%s (%s)</a>%s</li>'
                     % (fn, fn, composite_file.get("description"), opt_text)
                 )
             else:
-                rval.append(
-                    '<li><a href="%s" type="application/binary">%s</a>%s</li>'
-                    % (fn, fn, opt_text)
-                )
+                rval.append(f'<li><a href="{fn}" type="application/binary">{fn}</a>{opt_text}</li>')
         rval.append("</ul></div></html>")
         return "\n".join(rval)
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         """
         cannot do this until we are setting metadata
         """
         efp = dataset.extra_files_path
         flist = os.listdir(efp)
         rval = [
             "<html><head><title>Files for Composite Dataset %s</title></head><body><p/>Composite %s contains:<p/><ul>"
@@ -90,94 +99,96 @@
             f, e = os.path.splitext(fname)
             rval.append(f'<li><a href="{sfname}">{sfname}</a></li>')
         rval.append("</ul></body></html>")
         with open(dataset.file_name, "w") as f:
             f.write("\n".join(rval))
             f.write("\n")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text."""
         if not dataset.dataset.purged:
             dataset.peek = "spaln database (multiple files)"
             dataset.blurb = "spaln database (multiple files)"
         else:
             dataset.peek = "file does not exist"
             dataset.blurb = "file purged from disk"
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Create HTML content, used for displaying peek."""
         try:
             return dataset.peek
         except Exception:
             return "spaln database (multiple files)"
 
     def display_data(
         self,
         trans,
-        data,
-        preview=False,
-        filename=None,
-        to_ext=None,
-        size=None,
-        offset=None,
-        **kwd
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        offset: Optional[int] = None,
+        ck_size: Optional[int] = None,
+        **kwd,
     ):
         """
         If preview is `True` allows us to format the data shown in the central pane via the "eye" icon.
         If preview is `False` triggers download.
         """
-        headers = kwd.get("headers", {})
+        headers = kwd.pop("headers", {})
         if not preview:
             return super().display_data(
                 trans,
-                data=data,
+                dataset=dataset,
                 preview=preview,
                 filename=filename,
                 to_ext=to_ext,
-                size=size,
                 offset=offset,
+                ck_size=ck_size,
                 headers=headers,
-                **kwd
+                **kwd,
             )
         if self.file_ext == "spalndbn":
             title = "This is a nucleotide-query spaln database"
         elif self.file_ext == "spalndbp":
             title = "This is a protein-query spaln database"
         elif self.file_ext == "spalndba":
             title = "This is a protein spaln database"
         else:
             # Error?
             title = "This is a spaln database (unknown format)."
         msg = ""
         try:
             # Try to use any text recorded in the dummy index file:
-            with open(data.file_name, encoding="utf-8") as handle:
+            with open(dataset.file_name, encoding="utf-8") as handle:
                 msg = handle.read().strip()
         except Exception:
             pass
         if not msg:
             msg = title
         # Galaxy assumes HTML for the display of composite datatypes,
-        return smart_str(
-            "<html><head><title>%s</title></head><body><pre>%s</pre></body></html>"
-            % (title, msg)
-        ), headers
+        return (
+            smart_str(f"<html><head><title>{title}</title></head><body><pre>{msg}</pre></body></html>"),
+            headers,
+        )
 
-    def merge(split_files, output_file):
+    @staticmethod
+    def merge(split_files: List[str], output_file: str) -> None:
         """Merge spaln databases (not implemented)."""
         raise NotImplementedError("Merging spaln databases is not possible")
 
-    def split(cls, input_datasets, subdir_generator_function, split_params):
+    @classmethod
+    def split(cls, input_datasets: List, subdir_generator_function: Callable, split_params: Dict) -> None:
         """Split a spaln database (not implemented)."""
         if split_params is None:
             return None
         raise NotImplementedError("Can't split spaln database")
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         efp = dataset.extra_files_path
         for filename in os.listdir(efp):
             if filename.endswith(".ent"):
                 dataset.metadata.spalndb_name = os.path.splitext(filename)[0]
         self.regenerate_primary_file(dataset)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/speech.py` & `galaxy-data-23.0.1/galaxy/datatypes/speech.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,18 @@
-from galaxy.datatypes.metadata import ListParameter, MetadataElement
+from typing import TYPE_CHECKING
+
+from galaxy.datatypes.data import Text
+from galaxy.datatypes.metadata import (
+    ListParameter,
+    MetadataElement,
+)
 from galaxy.datatypes.sniff import get_headers
-from galaxy.datatypes.text import Text
+
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
 
 
 class TextGrid(Text):
     """Praat Textgrid file for speech annotations
 
     >>> from galaxy.datatypes.sniff import get_test_fname
     >>> fname = get_test_fname('1_1119_2_22_001.textgrid')
@@ -17,18 +25,26 @@
     """
 
     file_ext = "textgrid"
     header = 'File type = "ooTextFile"\nObject class = "TextGrid"\n'
 
     blurb = "Praat TextGrid file"
 
-    MetadataElement(name="annotations", default=[], desc="Annotation types", param=ListParameter, readonly=True, visible=True, optional=True, no_value=[])
-
-    def sniff(self, filename):
+    MetadataElement(
+        name="annotations",
+        default=[],
+        desc="Annotation types",
+        param=ListParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
 
+    def sniff(self, filename: str) -> bool:
         with open(filename) as fd:
             text = fd.read(len(self.header))
             return text == self.header
 
 
 class BPF(Text):
     """Munich BPF annotation format
@@ -45,45 +61,77 @@
     >>> BPF().sniff(fname)
     False
 
     """
 
     file_ext = "par"
 
-    MetadataElement(name="annotations", default=[], desc="Annotation types", param=ListParameter, readonly=True, visible=True, optional=True, no_value=[])
-    mandatory_headers = ['LHD', 'REP', 'SNB', 'SAM', 'SBF', 'SSB', 'NCH', 'SPN', 'LBD']
-    optional_headers = ['FIL', 'TYP', 'DBN', 'VOL', 'DIR', 'SRC', 'BEG', 'END', 'RED', 'RET', 'RCC', 'CMT', 'SPI', 'PCF', 'PCN', 'EXP', 'SYS', 'DAT', 'SPA', 'MAO', 'GPO', 'SAO']
+    MetadataElement(
+        name="annotations",
+        default=[],
+        desc="Annotation types",
+        param=ListParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    mandatory_headers = ["LHD", "REP", "SNB", "SAM", "SBF", "SSB", "NCH", "SPN", "LBD"]
+    optional_headers = [
+        "FIL",
+        "TYP",
+        "DBN",
+        "VOL",
+        "DIR",
+        "SRC",
+        "BEG",
+        "END",
+        "RED",
+        "RET",
+        "RCC",
+        "CMT",
+        "SPI",
+        "PCF",
+        "PCN",
+        "EXP",
+        "SYS",
+        "DAT",
+        "SPA",
+        "MAO",
+        "GPO",
+        "SAO",
+    ]
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """Set the metadata for this dataset from the file contents"""
         types = set()
         with open(dataset.dataset.file_name) as fd:
             for line in fd:
                 # Split the line on a colon rather than regexing it
-                parts = line.split(':')
+                parts = line.split(":")
 
                 # And if the first part is a 3 character string, then it's
                 # interesting.
                 if len(parts) and len(parts[0]) == 3:
                     types.add(parts[0])
                 else:
-                    return False
+                    return
 
         dataset.metadata.annotations = list(types)
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         # We loop over 30 as there are 9 mandatory headers (the last should be
         # `LBD:`), while there are 21 optional headers that can be
         # interspersed.
-        seen_headers = [line[0] for line in get_headers(filename, sep=':', count=40)]
+        seen_headers = [line[0] for line in get_headers(filename, sep=":", count=40)]
 
         # We cut everything after LBD, where the headers end and contents
         # start. We choose not to validate contents.
-        if 'LBD' in seen_headers:
-            seen_headers = seen_headers[0:seen_headers.index('LBD') + 1]
+        if "LBD" in seen_headers:
+            seen_headers = seen_headers[0 : seen_headers.index("LBD") + 1]
 
         # Check that every mandatory header is present in the seen headers
         for header in self.mandatory_headers:
             if header not in seen_headers:
                 return False
 
         # Check that every seen header is either in mandatory or optional
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/tabular.py` & `galaxy-data-23.0.1/galaxy/datatypes/tabular.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,150 +8,255 @@
 import logging
 import os
 import re
 import shutil
 import subprocess
 import tempfile
 from json import dumps
+from typing import (
+    cast,
+    Dict,
+    List,
+    Optional,
+    TYPE_CHECKING,
+    Union,
+)
 
 import pysam
 from markupsafe import escape
 
 from galaxy import util
-from galaxy.datatypes import binary, data, metadata
+from galaxy.datatypes import (
+    binary,
+    data,
+    metadata,
+)
 from galaxy.datatypes.binary import _BamOrSam
+from galaxy.datatypes.data import (
+    DatatypeValidation,
+    Text,
+)
+from galaxy.datatypes.dataproviders.column import (
+    ColumnarDataProvider,
+    DictDataProvider,
+)
+from galaxy.datatypes.dataproviders.dataset import (
+    DatasetColumnarDataProvider,
+    DatasetDataProvider,
+    DatasetDictDataProvider,
+    GenomicRegionDataProvider,
+)
+from galaxy.datatypes.dataproviders.line import (
+    FilteredLineDataProvider,
+    RegexLineDataProvider,
+)
 from galaxy.datatypes.metadata import (
     MetadataElement,
     MetadataParameter,
 )
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     get_headers,
     iter_headers,
     validate_tabular,
 )
 from galaxy.util import compression_utils
+from galaxy.util.compression_utils import (
+    FileObjType,
+    FileObjTypeStr,
+)
+from galaxy.util.markdown import (
+    indicate_data_truncated,
+    pre_formatted_contents,
+)
 from . import dataproviders
 
+if TYPE_CHECKING:
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+
 log = logging.getLogger(__name__)
 
 MAX_DATA_LINES = 100000
 
 
 @dataproviders.decorators.has_dataproviders
-class TabularData(data.Text):
+class TabularData(Text):
     """Generic tabular data"""
+
     edam_format = "format_3475"
     # All tabular data is chunkable.
     CHUNKABLE = True
     data_line_offset = 0
     max_peek_columns = 50
 
-    MetadataElement(name="comment_lines", default=0, desc="Number of comment lines", readonly=False, optional=True, no_value=0)
-    MetadataElement(name="data_lines", default=0, desc="Number of data lines", readonly=True, visible=False, optional=True, no_value=0)
+    MetadataElement(
+        name="comment_lines", default=0, desc="Number of comment lines", readonly=False, optional=True, no_value=0
+    )
+    MetadataElement(
+        name="data_lines",
+        default=0,
+        desc="Number of data lines",
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=0,
+    )
     MetadataElement(name="columns", default=0, desc="Number of columns", readonly=True, visible=False, no_value=0)
-    MetadataElement(name="column_types", default=[], desc="Column types", param=metadata.ColumnTypesParameter, readonly=True, visible=False, no_value=[])
-    MetadataElement(name="column_names", default=[], desc="Column names", readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="delimiter", default='\t', desc="Data delimiter", readonly=True, visible=False, optional=True, no_value=[])
+    MetadataElement(
+        name="column_types",
+        default=[],
+        desc="Column types",
+        param=metadata.ColumnTypesParameter,
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
+    MetadataElement(
+        name="column_names", default=[], desc="Column names", readonly=True, visible=False, optional=True, no_value=[]
+    )
+    MetadataElement(
+        name="delimiter", default="\t", desc="Data delimiter", readonly=True, visible=False, optional=True, no_value=[]
+    )
 
     @abc.abstractmethod
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", *, overwrite: bool = True, **kwd) -> None:
         raise NotImplementedError
 
-    def set_peek(self, dataset, line_count=None, WIDTH=256, skipchars=None, line_wrap=False, **kwd):
-        super().set_peek(dataset, line_count=line_count, WIDTH=WIDTH, skipchars=skipchars, line_wrap=line_wrap)
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
+        kwd.setdefault("line_wrap", False)
+        super().set_peek(dataset, **kwd)
         if dataset.metadata.comment_lines:
             dataset.blurb = f"{dataset.blurb}, {util.commaify(str(dataset.metadata.comment_lines))} comments"
 
-    def displayable(self, dataset):
+    def displayable(self, dataset: "DatasetInstance"):
         try:
-            return dataset.has_data() \
-                and dataset.state == dataset.states.OK \
-                and dataset.metadata.columns > 0 \
+            return (
+                not dataset.dataset.purged
+                and dataset.has_data()
+                and dataset.state == dataset.states.OK
+                and dataset.metadata.columns > 0
                 and dataset.metadata.data_lines != 0
+            )
         except Exception:
             return False
 
-    def get_chunk(self, trans, dataset, offset=0, ck_size=None):
+    def get_chunk(self, trans, dataset: "DatasetInstance", offset: int = 0, ck_size: Optional[int] = None) -> str:
+        ck_data, last_read = self._read_chunk(trans, dataset, offset, ck_size)
+        return dumps(
+            {
+                "ck_data": util.unicodify(ck_data),
+                "offset": last_read,
+                "data_line_offset": self.data_line_offset,
+            }
+        )
+
+    def _read_chunk(self, trans, dataset: "DatasetInstance", offset: int, ck_size: Optional[int] = None):
         with compression_utils.get_fileobj(dataset.file_name) as f:
             f.seek(offset)
             ck_data = f.read(ck_size or trans.app.config.display_chunk_size)
-            if ck_data and ck_data[-1] != '\n':
+            if ck_data and ck_data[-1] != "\n":
                 cursor = f.read(1)
-                while cursor and cursor != '\n':
+                while cursor and cursor != "\n":
                     ck_data += cursor
                     cursor = f.read(1)
             last_read = f.tell()
-        return dumps({'ck_data': util.unicodify(ck_data),
-                      'offset': last_read,
-                      'data_line_offset': self.data_line_offset,
-                      })
+        return ck_data, last_read
 
-    def display_data(self, trans, dataset, preview=False, filename=None, to_ext=None, offset=None, ck_size=None, **kwd):
-        headers = kwd.get("headers", {})
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        offset: Optional[int] = None,
+        ck_size: Optional[int] = None,
+        **kwd,
+    ):
+        headers = kwd.pop("headers", {})
         preview = util.string_as_bool(preview)
         if offset is not None:
             return self.get_chunk(trans, dataset, offset, ck_size), headers
         elif to_ext or not preview:
             to_ext = to_ext or dataset.extension
             return self._serve_raw(dataset, to_ext, headers, **kwd)
         elif dataset.metadata.columns > 100:
             # Fancy tabular display is only suitable for datasets without an incredibly large number of columns.
             # We should add a new datatype 'matrix', with its own draw method, suitable for this kind of data.
             # For now, default to the old behavior, ugly as it is.  Remove this after adding 'matrix'.
             max_peek_size = 1000000  # 1 MB
             if os.stat(dataset.file_name).st_size < max_peek_size:
                 self._clean_and_set_mime_type(trans, dataset.get_mime(), headers)
-                return open(dataset.file_name, mode='rb'), headers
+                return open(dataset.file_name, mode="rb"), headers
             else:
                 headers["content-type"] = "text/html"
-                return trans.fill_template_mako("/dataset/large_file.mako",
-                                                truncated_data=open(dataset.file_name).read(max_peek_size),
-                                                data=dataset), headers
+                return (
+                    trans.fill_template_mako(
+                        "/dataset/large_file.mako",
+                        truncated_data=open(dataset.file_name).read(max_peek_size),
+                        data=dataset,
+                    ),
+                    headers,
+                )
         else:
-            column_names = 'null'
+            column_names = "null"
             if dataset.metadata.column_names:
                 column_names = dataset.metadata.column_names
-            elif hasattr(dataset.datatype, 'column_names'):
+            elif hasattr(dataset.datatype, "column_names"):
                 column_names = dataset.datatype.column_names
             column_types = dataset.metadata.column_types
             if not column_types:
                 column_types = []
             column_number = dataset.metadata.columns
             if column_number is None:
-                column_number = 'null'
-            return trans.fill_template("/dataset/tabular_chunked.mako",
-                                       dataset=dataset,
-                                       chunk=self.get_chunk(trans, dataset, 0),
-                                       column_number=column_number,
-                                       column_names=column_names,
-                                       column_types=column_types), headers
+                column_number = "null"
+            return (
+                trans.fill_template(
+                    "/dataset/tabular_chunked.mako",
+                    dataset=dataset,
+                    chunk=self.get_chunk(trans, dataset, 0),
+                    column_number=column_number,
+                    column_names=column_names,
+                    column_types=column_types,
+                ),
+                headers,
+            )
 
-    def display_as_markdown(self, dataset_instance, markdown_format_helpers):
+    def display_as_markdown(self, dataset_instance: "DatasetInstance") -> str:
         with open(dataset_instance.file_name) as f:
             contents = f.read(data.DEFAULT_MAX_PEEK_SIZE)
         markdown = self.make_html_table(dataset_instance, peek=contents)
         if len(contents) == data.DEFAULT_MAX_PEEK_SIZE:
-            markdown += markdown_format_helpers.indicate_data_truncated()
-        return markdown_format_helpers.pre_formatted_contents(markdown)
+            markdown += indicate_data_truncated()
+        return pre_formatted_contents(markdown)
 
-    def make_html_table(self, dataset, **kwargs):
+    def make_html_table(self, dataset: "DatasetInstance", **kwargs) -> str:
         """Create HTML table, used for displaying peek"""
-        out = ['<table cellspacing="0" cellpadding="3">']
         try:
+            out = ['<table cellspacing="0" cellpadding="3">']
             out.append(self.make_html_peek_header(dataset, **kwargs))
             out.append(self.make_html_peek_rows(dataset, **kwargs))
-            out.append('</table>')
-            out = "".join(out)
+            out.append("</table>")
+            return "".join(out)
         except Exception as exc:
-            out = f"Can't create peek: {util.unicodify(exc)}"
-        return out
+            return f"Can't create peek: {util.unicodify(exc)}"
 
-    def make_html_peek_header(self, dataset, skipchars=None, column_names=None, column_number_format='%s', column_parameter_alias=None, **kwargs):
+    def make_html_peek_header(
+        self,
+        dataset: "DatasetInstance",
+        skipchars: Optional[List] = None,
+        column_names: Optional[List] = None,
+        column_number_format: str = "%s",
+        column_parameter_alias: Optional[Dict] = None,
+        **kwargs,
+    ) -> str:
         if skipchars is None:
             skipchars = []
         if column_names is None:
             column_names = []
         if column_parameter_alias is None:
             column_parameter_alias = {}
         out = []
@@ -162,43 +267,44 @@
             columns = dataset.metadata.columns
             if columns is None:
                 columns = dataset.metadata.spec.columns.no_value
             columns = min(columns, self.max_peek_columns)
             column_headers = [None] * columns
 
             # fill in empty headers with data from column_names
+            assert column_names is not None
             for i in range(min(columns, len(column_names))):
                 if column_headers[i] is None and column_names[i] is not None:
                     column_headers[i] = column_names[i]
 
             # fill in empty headers from ColumnParameters set in the metadata
             for name, spec in dataset.metadata.spec.items():
                 if isinstance(spec.param, metadata.ColumnParameter):
                     try:
                         i = int(getattr(dataset.metadata, name)) - 1
                     except Exception:
                         i = -1
                     if 0 <= i < columns and column_headers[i] is None:
                         column_headers[i] = column_parameter_alias.get(name, name)
 
-            out.append('<tr>')
+            out.append("<tr>")
             for i, header in enumerate(column_headers):
-                out.append('<th>')
+                out.append("<th>")
                 if header is None:
                     out.append(column_number_format % str(i + 1))
                 else:
-                    out.append(f'{str(i + 1)}.{escape(header)}')
-                out.append('</th>')
-            out.append('</tr>')
+                    out.append(f"{str(i + 1)}.{escape(header)}")
+                out.append("</th>")
+            out.append("</tr>")
         except Exception as exc:
-            log.exception('make_html_peek_header failed on HDA %s', dataset.id)
+            log.exception("make_html_peek_header failed on HDA %s", dataset.id)
             raise Exception(f"Can't create peek header: {util.unicodify(exc)}")
         return "".join(out)
 
-    def make_html_peek_rows(self, dataset, skipchars=None, **kwargs):
+    def make_html_peek_rows(self, dataset: "DatasetInstance", skipchars: Optional[List] = None, **kwargs) -> str:
         if skipchars is None:
             skipchars = []
         out = []
         try:
             peek = kwargs.get("peek")
             if peek is None:
                 if not dataset.peek:
@@ -210,73 +316,112 @@
             columns = min(columns, self.max_peek_columns)
             for i, line in enumerate(peek.splitlines()):
                 if i >= self.data_line_offset:
                     if line.startswith(tuple(skipchars)):
                         out.append(f'<tr><td colspan="100%">{escape(line)}</td></tr>')
                     elif line:
                         elems = line.split(dataset.metadata.delimiter)
-                        elems = elems[:min(len(elems), self.max_peek_columns)]
+                        elems = elems[: min(len(elems), self.max_peek_columns)]
                         # pad shortened elems, since lines could have been truncated by width
                         if len(elems) < columns:
-                            elems.extend([''] * (columns - len(elems)))
+                            elems.extend([""] * (columns - len(elems)))
                         # we may have an invalid comment line or invalid data
                         if len(elems) != columns:
                             out.append(f'<tr><td colspan="100%">{escape(line)}</td></tr>')
                         else:
-                            out.append('<tr>')
+                            out.append("<tr>")
                             for elem in elems:
-                                out.append(f'<td>{escape(elem)}</td>')
-                            out.append('</tr>')
+                                out.append(f"<td>{escape(elem)}</td>")
+                            out.append("</tr>")
         except Exception as exc:
-            log.exception('make_html_peek_rows failed on HDA %s', dataset.id)
+            log.exception("make_html_peek_rows failed on HDA %s", dataset.id)
             raise Exception(f"Can't create peek rows: {util.unicodify(exc)}")
         return "".join(out)
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formatted html of peek"""
         return self.make_html_table(dataset)
 
+    def is_int(self, column_text: str) -> bool:
+        # Don't allow underscores in numeric literals (PEP 515)
+        if "_" in column_text:
+            return False
+        try:
+            int(column_text)
+            return True
+        except ValueError:
+            return False
+
+    def is_float(self, column_text: str) -> bool:
+        # Don't allow underscores in numeric literals (PEP 515)
+        if "_" in column_text:
+            return False
+        try:
+            float(column_text)
+            return True
+        except ValueError:
+            if column_text.strip().lower() == "na":
+                return True  # na is special cased to be a float
+            return False
+
+    def guess_type(self, text: str) -> str:
+        if self.is_int(text):
+            return "int"
+        if self.is_float(text):
+            return "float"
+        else:
+            return "str"
+
     # ------------- Dataproviders
-    @dataproviders.decorators.dataprovider_factory('column', dataproviders.column.ColumnarDataProvider.settings)
-    def column_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("column", ColumnarDataProvider.settings)
+    def column_dataprovider(self, dataset: "DatasetInstance", **settings) -> ColumnarDataProvider:
         """Uses column settings that are passed in"""
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
+        dataset_source = DatasetDataProvider(dataset)
         delimiter = dataset.metadata.delimiter
-        return dataproviders.column.ColumnarDataProvider(dataset_source, deliminator=delimiter, **settings)
+        return ColumnarDataProvider(dataset_source, deliminator=delimiter, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('dataset-column',
-                                                   dataproviders.column.ColumnarDataProvider.settings)
-    def dataset_column_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("dataset-column", ColumnarDataProvider.settings)
+    def dataset_column_dataprovider(self, dataset: "DatasetInstance", **settings) -> DatasetColumnarDataProvider:
         """Attempts to get column settings from dataset.metadata"""
         delimiter = dataset.metadata.delimiter
-        return dataproviders.dataset.DatasetColumnarDataProvider(dataset, deliminator=delimiter, **settings)
+        return DatasetColumnarDataProvider(dataset, deliminator=delimiter, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('dict', dataproviders.column.DictDataProvider.settings)
-    def dict_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("dict", DictDataProvider.settings)
+    def dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> DictDataProvider:
         """Uses column settings that are passed in"""
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
+        dataset_source = DatasetDataProvider(dataset)
         delimiter = dataset.metadata.delimiter
-        return dataproviders.column.DictDataProvider(dataset_source, deliminator=delimiter, **settings)
+        return DictDataProvider(dataset_source, deliminator=delimiter, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('dataset-dict', dataproviders.column.DictDataProvider.settings)
-    def dataset_dict_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("dataset-dict", DictDataProvider.settings)
+    def dataset_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> DatasetDictDataProvider:
         """Attempts to get column settings from dataset.metadata"""
         delimiter = dataset.metadata.delimiter
-        return dataproviders.dataset.DatasetDictDataProvider(dataset, deliminator=delimiter, **settings)
+        return DatasetDictDataProvider(dataset, deliminator=delimiter, **settings)
 
 
 @dataproviders.decorators.has_dataproviders
 class Tabular(TabularData):
     """Tab delimited data"""
+
     file_ext = "tabular"
 
-    def get_column_names(self, first_line=None):
+    def get_column_names(self, first_line: str) -> Optional[List[str]]:
         return None
 
-    def set_meta(self, dataset, overwrite=True, skip=None, max_data_lines=MAX_DATA_LINES, max_guess_type_data_lines=None, **kwd):
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        *,
+        overwrite: bool = True,
+        skip: Optional[int] = None,
+        max_data_lines: Optional[int] = MAX_DATA_LINES,
+        max_guess_type_data_lines: Optional[int] = None,
+        **kwd,
+    ) -> None:
         """
         Tries to determine the number of columns as well as those columns that
         contain numerical values in the dataset.  A skip parameter is used
         because various tabular data types reuse this function, and their data
         type classes are responsible to determine how many invalid comment
         lines should be skipped. Using None for skip will cause skip to be
         zero, but the first line will be processed as a header. A
@@ -297,15 +442,15 @@
            Since metadata can now be processed on cluster nodes, we've merged the line count portion
            of the set_peek() processing here, and we now check the entire contents of the file.
         """
         # Store original skip value to check with later
         requested_skip = skip
         if skip is None:
             skip = 0
-        column_type_set_order = ['int', 'float', 'list', 'str']  # Order to set column types in
+        column_type_set_order = ["int", "float", "list", "str"]  # Order to set column types in
         default_column_type = column_type_set_order[-1]  # Default column type is lowest in list
         column_type_compare_order = list(column_type_set_order)  # Order to compare column types
         column_type_compare_order.reverse()
 
         def type_overrules_type(column_type1, column_type2):
             if column_type1 is None or column_type1 == column_type2:
                 return False
@@ -317,31 +462,31 @@
                 if column_type2 == column_type:
                     return False
             # neither column type was found in our ordered list, this cannot happen
             raise ValueError(f"Tried to compare unknown column types: {column_type1} and {column_type2}")
 
         def is_int(column_text):
             # Don't allow underscores in numeric literals (PEP 515)
-            if '_' in column_text:
+            if "_" in column_text:
                 return False
             try:
                 int(column_text)
                 return True
             except ValueError:
                 return False
 
         def is_float(column_text):
             # Don't allow underscores in numeric literals (PEP 515)
-            if '_' in column_text:
+            if "_" in column_text:
                 return False
             try:
                 float(column_text)
                 return True
             except ValueError:
-                if column_text.strip().lower() == 'na':
+                if column_text.strip().lower() == "na":
                     return True  # na is special cased to be a float
                 return False
 
         def is_list(column_text):
             return "," in column_text
 
         def is_str(column_text):
@@ -359,33 +504,35 @@
                 if is_column_type[column_type](column_text):
                     return column_type
             return None
 
         data_lines = 0
         comment_lines = 0
         column_names = None
-        column_types = []
+        column_types: List = []
         first_line_column_types = [default_column_type]  # default value is one column of type str
         if dataset.has_data():
             # NOTE: if skip > num_check_lines, we won't detect any metadata, and will use default
             with compression_utils.get_fileobj(dataset.file_name) as dataset_fh:
                 i = 0
-                for line in iter(dataset_fh.readline, ''):
-                    line = line.rstrip('\r\n')
+                for line in iter(dataset_fh.readline, ""):
+                    line = line.rstrip("\r\n")
                     if i == 0:
                         column_names = self.get_column_names(first_line=line)
-                    if i < skip or not line or line.startswith('#'):
+                    if i < skip or not line or line.startswith("#"):
                         # We'll call blank lines comments
                         comment_lines += 1
                     else:
                         data_lines += 1
                         if max_guess_type_data_lines is None or data_lines <= max_guess_type_data_lines:
-                            fields = line.split('\t')
+                            fields = line.split("\t")
                             for field_count, field in enumerate(fields):
-                                if field_count >= len(column_types):  # found a previously unknown column, we append None
+                                if field_count >= len(
+                                    column_types
+                                ):  # found a previously unknown column, we append None
                                     column_types.append(None)
                                 column_type = guess_column_type(field)
                                 if type_overrules_type(column_type, column_types[field_count]):
                                     column_types[field_count] = column_type
                         if i == 0 and requested_skip is None:
                             # This is our first line, people seem to like to upload files that have a header line, but do not
                             # start with '#' (i.e. all column types would then most likely be detected as str).  We will assume
@@ -400,105 +547,192 @@
                             # "column_types": ["list", "float", "float", "str"]  *** would seem to be the 'Truth' by manual
                             # observation that the first line should be included as data.  The old method would have detected as
                             # "column_types": ["int", "int", "str", "list"]
                             first_line_column_types = column_types
                             column_types = [None for col in first_line_column_types]
                     if max_data_lines is not None and data_lines >= max_data_lines:
                         if dataset_fh.tell() != dataset.get_size():
-                            data_lines = None  # Clear optional data_lines metadata value
-                            comment_lines = None  # Clear optional comment_lines metadata value; additional comment lines could appear below this point
+                            # Clear optional data_lines metadata value
+                            data_lines = None  # type: ignore [assignment]
+                            # Clear optional comment_lines metadata value; additional comment lines could appear below this point
+                            comment_lines = None  # type: ignore [assignment]
                         break
                     i += 1
 
         # we error on the larger number of columns
         # first we pad our column_types by using data from first line
         if len(first_line_column_types) > len(column_types):
-            for column_type in first_line_column_types[len(column_types):]:
+            for column_type in first_line_column_types[len(column_types) :]:
                 column_types.append(column_type)
         # Now we fill any unknown (None) column_types with data from first line
         for i in range(len(column_types)):
             if column_types[i] is None:
                 if len(first_line_column_types) <= i or first_line_column_types[i] is None:
                     column_types[i] = default_column_type
                 else:
                     column_types[i] = first_line_column_types[i]
         # Set the discovered metadata values for the dataset
         dataset.metadata.data_lines = data_lines
         dataset.metadata.comment_lines = comment_lines
         dataset.metadata.column_types = column_types
         dataset.metadata.columns = len(column_types)
-        dataset.metadata.delimiter = '\t'
+        dataset.metadata.delimiter = "\t"
         if column_names is not None:
             dataset.metadata.column_names = column_names
 
-    def as_gbrowse_display_file(self, dataset, **kwd):
-        return open(dataset.file_name, 'rb')
+    def as_gbrowse_display_file(self, dataset: "DatasetInstance", **kwd) -> Union[FileObjType, str]:
+        return open(dataset.file_name, "rb")
 
-    def as_ucsc_display_file(self, dataset, **kwd):
-        return open(dataset.file_name, 'rb')
+    def as_ucsc_display_file(self, dataset: "DatasetInstance", **kwd) -> Union[FileObjType, str]:
+        return open(dataset.file_name, "rb")
 
 
 class SraManifest(Tabular):
     """A manifest received from the sra_source tool."""
-    file_ext = 'sra_manifest.tabular'
+
+    file_ext = "sra_manifest.tabular"
     data_line_offset = 1
 
-    def set_meta(self, dataset, **kwds):
-        super().set_meta(dataset, **kwds)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         dataset.metadata.comment_lines = 1
 
-    def get_column_names(self, first_line):
-        return first_line.strip().split('\t')
+    def get_column_names(self, first_line: str) -> Optional[List[str]]:
+        return first_line.strip().split("\t")
 
 
 class Taxonomy(Tabular):
     file_ext = "taxonomy"
 
     def __init__(self, **kwd):
         """Initialize taxonomy datatype"""
         super().__init__(**kwd)
-        self.column_names = ['Name', 'TaxId', 'Root', 'Superkingdom', 'Kingdom', 'Subkingdom',
-                             'Superphylum', 'Phylum', 'Subphylum', 'Superclass', 'Class', 'Subclass',
-                             'Superorder', 'Order', 'Suborder', 'Superfamily', 'Family', 'Subfamily',
-                             'Tribe', 'Subtribe', 'Genus', 'Subgenus', 'Species', 'Subspecies'
-                             ]
+        self.column_names = [
+            "Name",
+            "TaxId",
+            "Root",
+            "Superkingdom",
+            "Kingdom",
+            "Subkingdom",
+            "Superphylum",
+            "Phylum",
+            "Subphylum",
+            "Superclass",
+            "Class",
+            "Subclass",
+            "Superorder",
+            "Order",
+            "Suborder",
+            "Superfamily",
+            "Family",
+            "Subfamily",
+            "Tribe",
+            "Subtribe",
+            "Genus",
+            "Subgenus",
+            "Species",
+            "Subspecies",
+        ]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class Sam(Tabular, _BamOrSam):
     edam_format = "format_2573"
     edam_data = "data_0863"
-    file_ext = 'sam'
+    file_ext = "sam"
     track_type = "ReadTrack"
     data_sources = {"data": "bam", "index": "bigwig"}
 
-    MetadataElement(name="bam_version", default=None, desc="BAM Version", param=MetadataParameter, readonly=True, visible=False, optional=True)
-    MetadataElement(name="sort_order", default=None, desc="Sort Order", param=MetadataParameter, readonly=True, visible=False, optional=True)
-    MetadataElement(name="read_groups", default=[], desc="Read Groups", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="reference_names", default=[], desc="Chromosome Names", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="reference_lengths", default=[], desc="Chromosome Lengths", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="bam_header", default={}, desc="Dictionary of BAM Headers", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value={})
+    MetadataElement(
+        name="bam_version",
+        default=None,
+        desc="BAM Version",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+    MetadataElement(
+        name="sort_order",
+        default=None,
+        desc="Sort Order",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
+    MetadataElement(
+        name="read_groups",
+        default=[],
+        desc="Read Groups",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="reference_names",
+        default=[],
+        desc="Chromosome Names",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="reference_lengths",
+        default=[],
+        desc="Chromosome Lengths",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="bam_header",
+        default={},
+        desc="Dictionary of BAM Headers",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value={},
+    )
 
     def __init__(self, **kwd):
         """Initialize sam datatype"""
         super().__init__(**kwd)
-        self.column_names = ['QNAME', 'FLAG', 'RNAME', 'POS', 'MAPQ', 'CIGAR',
-                             'MRNM', 'MPOS', 'ISIZE', 'SEQ', 'QUAL', 'OPT'
-                             ]
+        self.column_names = [
+            "QNAME",
+            "FLAG",
+            "RNAME",
+            "POS",
+            "MAPQ",
+            "CIGAR",
+            "MRNM",
+            "MPOS",
+            "ISIZE",
+            "SEQ",
+            "QUAL",
+            "OPT",
+        ]
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in SAM format
 
         A file in SAM format consists of lines of tab-separated data.
         The following header line may be the first line::
 
           @QNAME  FLAG    RNAME   POS     MAPQ    CIGAR   MRNM    MPOS    ISIZE   SEQ     QUAL
@@ -523,16 +757,16 @@
         >>> Sam().sniff( fname )
         True
         """
         count = 0
         for line in file_prefix.line_iterator():
             line = line.strip()
             if line:
-                if line[0] != '@':
-                    line_pieces = line.split('\t')
+                if line[0] != "@":
+                    line_pieces = line.split("\t")
                     if len(line_pieces) < 11:
                         return False
                     try:
                         int(line_pieces[1])
                         int(line_pieces[3])
                         int(line_pieces[4])
                         int(line_pieces[7])
@@ -542,15 +776,22 @@
                     count += 1
                     if count == 5:
                         return True
         if count < 5 and count > 0:
             return True
         return False
 
-    def set_meta(self, dataset, overwrite=True, skip=None, max_data_lines=5, **kwd):
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = None,
+        max_data_lines: Optional[int] = 5,
+        **kwd,
+    ) -> None:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> from galaxy.datatypes.registry import example_datatype_registry_for_sample
         >>> from galaxy.model import Dataset, set_datatypes_registry
         >>> from galaxy.model import History, HistoryDatasetAssociation
         >>> from galaxy.model.mapping import init
         >>> sa_session = init("/tmp", "sqlite:///:memory:", create_tables=True).session
@@ -566,138 +807,150 @@
         2
         >>> hda.metadata.reference_names
         ['ref', 'ref2']
         """
         if dataset.has_data():
             with open(dataset.file_name) as dataset_fh:
                 comment_lines = 0
-                if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
+                if (
+                    self.max_optional_metadata_filesize >= 0
+                    and dataset.get_size() > self.max_optional_metadata_filesize
+                ):
                     # If the dataset is larger than optional_metadata, just count comment lines.
                     for line in dataset_fh:
-                        if line.startswith('@'):
+                        if line.startswith("@"):
                             comment_lines += 1
                         else:
                             # No more comments, and the file is too big to look at the whole thing. Give up.
                             dataset.metadata.data_lines = None
                             break
                 else:
                     # Otherwise, read the whole thing and set num data lines.
-                    for i, l in enumerate(dataset_fh):  # noqa: B007
-                        if l.startswith('@'):
+                    for i, line in enumerate(dataset_fh):  # noqa: B007
+                        if line.startswith("@"):
                             comment_lines += 1
                     dataset.metadata.data_lines = i + 1 - comment_lines
             dataset.metadata.comment_lines = comment_lines
             dataset.metadata.columns = 12
-            dataset.metadata.column_types = ['str', 'int', 'str', 'int', 'int', 'str', 'str', 'int', 'int', 'str', 'str', 'str']
+            dataset.metadata.column_types = [
+                "str",
+                "int",
+                "str",
+                "int",
+                "int",
+                "str",
+                "str",
+                "int",
+                "int",
+                "str",
+                "str",
+                "str",
+            ]
 
-            _BamOrSam().set_meta(dataset)
+            _BamOrSam().set_meta(dataset, overwrite=overwrite, **kwd)
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """
         Multiple SAM files may each have headers. Since the headers should all be the same, remove
         the headers from files 1-n, keeping them in the first file only
         """
         shutil.move(split_files[0], output_file)
 
         if len(split_files) > 1:
-            cmd = ['egrep', '-v', '-h', '^@'] + split_files[1:] + ['>>', output_file]
+            cmd = ["egrep", "-v", "-h", "^@"] + split_files[1:] + [">>", output_file]
             subprocess.check_call(cmd, shell=True)
 
     # Dataproviders
     # sam does not use '#' to indicate comments/headers - we need to strip out those headers from the std. providers
     # TODO:?? seems like there should be an easier way to do this - metadata.comment_char?
-    @dataproviders.decorators.dataprovider_factory('line', dataproviders.line.FilteredLineDataProvider.settings)
-    def line_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
+    @dataproviders.decorators.dataprovider_factory("line", FilteredLineDataProvider.settings)
+    def line_dataprovider(self, dataset: "DatasetInstance", **settings) -> FilteredLineDataProvider:
+        settings["comment_char"] = "@"
         return super().line_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('regex-line', dataproviders.line.RegexLineDataProvider.settings)
-    def regex_line_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
+    @dataproviders.decorators.dataprovider_factory("regex-line", RegexLineDataProvider.settings)
+    def regex_line_dataprovider(self, dataset: "DatasetInstance", **settings) -> RegexLineDataProvider:
+        settings["comment_char"] = "@"
         return super().regex_line_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('column', dataproviders.column.ColumnarDataProvider.settings)
-    def column_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
+    @dataproviders.decorators.dataprovider_factory("column", ColumnarDataProvider.settings)
+    def column_dataprovider(self, dataset: "DatasetInstance", **settings) -> ColumnarDataProvider:
+        settings["comment_char"] = "@"
         return super().column_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('dataset-column',
-                                                   dataproviders.column.ColumnarDataProvider.settings)
-    def dataset_column_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
+    @dataproviders.decorators.dataprovider_factory("dataset-column", ColumnarDataProvider.settings)
+    def dataset_column_dataprovider(self, dataset: "DatasetInstance", **settings) -> DatasetColumnarDataProvider:
+        settings["comment_char"] = "@"
         return super().dataset_column_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('dict', dataproviders.column.DictDataProvider.settings)
-    def dict_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
+    @dataproviders.decorators.dataprovider_factory("dict", DictDataProvider.settings)
+    def dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> DictDataProvider:
+        settings["comment_char"] = "@"
         return super().dict_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('dataset-dict', dataproviders.column.DictDataProvider.settings)
-    def dataset_dict_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
+    @dataproviders.decorators.dataprovider_factory("dataset-dict", DictDataProvider.settings)
+    def dataset_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> DatasetDictDataProvider:
+        settings["comment_char"] = "@"
         return super().dataset_dict_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('header', dataproviders.line.RegexLineDataProvider.settings)
-    def header_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        headers_source = dataproviders.line.RegexLineDataProvider(dataset_source, regex_list=['^@'])
-        return dataproviders.line.RegexLineDataProvider(headers_source, **settings)
+    @dataproviders.decorators.dataprovider_factory("header", RegexLineDataProvider.settings)
+    def header_dataprovider(self, dataset: "DatasetInstance", **settings) -> RegexLineDataProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        headers_source = RegexLineDataProvider(dataset_source, regex_list=["^@"])
+        return RegexLineDataProvider(headers_source, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('id-seq-qual', dict_dataprovider.settings)
-    def id_seq_qual_dataprovider(self, dataset, **settings):
+    @dataproviders.decorators.dataprovider_factory("id-seq-qual", dict_dataprovider.settings)
+    def id_seq_qual_dataprovider(self, dataset: "DatasetInstance", **settings) -> DictDataProvider:
         # provided as an example of a specified column dict (w/o metadata)
-        settings['indeces'] = [0, 9, 10]
-        settings['column_names'] = ['id', 'seq', 'qual']
+        settings["indeces"] = [0, 9, 10]
+        settings["column_names"] = ["id", "seq", "qual"]
         return self.dict_dataprovider(dataset, **settings)
 
-    @dataproviders.decorators.dataprovider_factory('genomic-region',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
-        return dataproviders.dataset.GenomicRegionDataProvider(dataset, 2, 3, 3, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('genomic-region-dict',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dict_dataprovider(self, dataset, **settings):
-        settings['comment_char'] = '@'
-        return dataproviders.dataset.GenomicRegionDataProvider(dataset, 2, 3, 3, True, **settings)
+    @dataproviders.decorators.dataprovider_factory("genomic-region", GenomicRegionDataProvider.settings)
+    def genomic_region_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        settings["comment_char"] = "@"
+        return GenomicRegionDataProvider(dataset, 2, 3, 3, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("genomic-region-dict", GenomicRegionDataProvider.settings)
+    def genomic_region_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        settings["comment_char"] = "@"
+        return GenomicRegionDataProvider(dataset, 2, 3, 3, True, **settings)
 
     # @dataproviders.decorators.dataprovider_factory( 'samtools' )
     # def samtools_dataprovider( self, dataset, **settings ):
     #     dataset_source = dataproviders.dataset.DatasetDataProvider( dataset )
     #     return dataproviders.dataset.SamtoolsDataProvider( dataset_source, **settings )
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class Pileup(Tabular):
     """Tab delimited data in pileup (6- or 10-column) format"""
+
     edam_format = "format_3015"
     file_ext = "pileup"
     line_class = "genomic coordinate"
     data_sources = {"data": "tabix"}
 
     MetadataElement(name="chromCol", default=1, desc="Chrom column", param=metadata.ColumnParameter)
     MetadataElement(name="startCol", default=2, desc="Start column", param=metadata.ColumnParameter)
     MetadataElement(name="endCol", default=2, desc="End column", param=metadata.ColumnParameter)
     MetadataElement(name="baseCol", default=3, desc="Reference base column", param=metadata.ColumnParameter)
 
-    def init_meta(self, dataset, copy_from=None):
+    def init_meta(self, dataset: "DatasetInstance", copy_from: Optional["DatasetInstance"] = None) -> None:
         super().init_meta(dataset, copy_from=copy_from)
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
-        return self.make_html_table(dataset, column_parameter_alias={'chromCol': 'Chrom', 'startCol': 'Start', 'baseCol': 'Base'})
+        return self.make_html_table(
+            dataset, column_parameter_alias={"chromCol": "Chrom", "startCol": "Start", "baseCol": "Base"}
+        )
 
-    def repair_methods(self, dataset):
-        """Return options for removing errors along with a description"""
-        return [("lines", "Remove erroneous lines")]
-
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checks for 'pileup-ness'
 
         There are two main types of pileup: 6-column and 10-column. For both,
         the first three and last two columns are the same. We only check the
         first three to allow for some personalization of the format.
 
@@ -719,202 +972,290 @@
         False
         >>> fname = get_test_fname( 'test_tab2.tabular' )
         >>> Pileup().sniff( fname )
         False
         """
         found_non_comment_lines = False
         try:
-            headers = iter_headers(file_prefix, '\t')
+            headers = iter_headers(file_prefix, "\t")
             for hdr in headers:
-                if hdr and not hdr[0].startswith('#'):
+                if hdr and not hdr[0].startswith("#"):
                     if len(hdr) < 5:
                         return False
                     # chrom start in column 1 (with 0-based columns)
                     # and reference base is in column 2
                     chrom = int(hdr[1])
                     assert chrom >= 0
-                    assert hdr[2] in ['A', 'C', 'G', 'T', 'N', 'a', 'c', 'g', 't', 'n']
+                    assert hdr[2] in ["A", "C", "G", "T", "N", "a", "c", "g", "t", "n"]
                     found_non_comment_lines = True
         except Exception:
             return False
         return found_non_comment_lines
 
     # Dataproviders
-    @dataproviders.decorators.dataprovider_factory('genomic-region',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dataprovider(self, dataset, **settings):
-        return dataproviders.dataset.GenomicRegionDataProvider(dataset, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('genomic-region-dict',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dict_dataprovider(self, dataset, **settings):
-        settings['named_columns'] = True
+    @dataproviders.decorators.dataprovider_factory("genomic-region", GenomicRegionDataProvider.settings)
+    def genomic_region_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        return GenomicRegionDataProvider(dataset, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("genomic-region-dict", GenomicRegionDataProvider.settings)
+    def genomic_region_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        settings["named_columns"] = True
         return self.genomic_region_dataprovider(dataset, **settings)
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class BaseVcf(Tabular):
-    """ Variant Call Format for describing SNPs and other simple genome variations. """
+    """Variant Call Format for describing SNPs and other simple genome variations."""
+
     edam_format = "format_3016"
     track_type = "VariantTrack"
     data_sources = {"data": "tabix", "index": "bigwig"}
 
-    column_names = ['Chrom', 'Pos', 'ID', 'Ref', 'Alt', 'Qual', 'Filter', 'Info', 'Format', 'data']
+    column_names = ["Chrom", "Pos", "ID", "Ref", "Alt", "Qual", "Filter", "Info", "Format", "data"]
 
     MetadataElement(name="columns", default=10, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=['str', 'int', 'str', 'str', 'str', 'int', 'str', 'list', 'str', 'str'], param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False)
-    MetadataElement(name="viz_filter_cols", desc="Score column for visualization", default=[5], param=metadata.ColumnParameter, optional=True, multiple=True, visible=False)
-    MetadataElement(name="sample_names", default=[], desc="Sample names", readonly=True, visible=False, optional=True, no_value=[])
+    MetadataElement(
+        name="column_types",
+        default=["str", "int", "str", "str", "str", "int", "str", "list", "str", "str"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+    )
+    MetadataElement(
+        name="viz_filter_cols",
+        desc="Score column for visualization",
+        default=[5],
+        param=metadata.ColumnParameter,
+        optional=True,
+        multiple=True,
+        visible=False,
+    )
+    MetadataElement(
+        name="sample_names", default=[], desc="Sample names", readonly=True, visible=False, optional=True, no_value=[]
+    )
 
-    def _sniff(self, fname_or_file_prefix):
+    def _sniff(self, fname_or_file_prefix: Union[str, FilePrefix]) -> bool:
         # Because this sniffer is run on compressed files that might be BGZF (due to the VcfGz subclass), we should
         # handle unicode decode errors. This should ultimately be done in get_headers(), but guess_ext() currently
         # relies on get_headers() raising this exception.
-        headers = get_headers(fname_or_file_prefix, '\n', count=1)
+        headers = get_headers(fname_or_file_prefix, "\n", count=1)
         return headers[0][0].startswith("##fileformat=VCF")
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         """Returns formated html of peek"""
         return self.make_html_table(dataset, column_names=self.column_names)
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         line = None
         with compression_utils.get_fileobj(dataset.file_name) as fh:
             # Skip comments.
             for line in fh:
-                if not line.startswith('##'):
+                if not line.startswith("##"):
                     break
 
-        if line and line.startswith('#'):
+        if line and line.startswith("#"):
             # Found header line, get sample names.
             dataset.metadata.sample_names = line.split()[9:]
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         stderr_f = tempfile.NamedTemporaryFile(prefix="bam_merge_stderr")
         stderr_name = stderr_f.name
         command = ["bcftools", "concat"] + split_files + ["-o", output_file]
         log.info(f"Merging vcf files with command [{' '.join(command)}]")
-        exit_code = subprocess.call(args=command, stderr=open(stderr_name, 'wb'))
+        exit_code = subprocess.call(args=command, stderr=open(stderr_name, "wb"))
         with open(stderr_name, "rb") as f:
             stderr = f.read().strip()
         # Did merge succeed?
         if exit_code != 0:
-            raise Exception(f"Error merging VCF files: {stderr}")
+            raise Exception(f"Error merging VCF files: {stderr!r}")
 
-    def validate(self, dataset, **kwd):
+    def validate(self, dataset: "DatasetInstance", **kwd) -> DatatypeValidation:
         def validate_row(row):
             if len(row) < 8:
                 raise Exception("Not enough columns in row %s" % row.join("\t"))
-        validate_tabular(dataset.file_name, sep='\t', validate_row=validate_row, comment_designator="#")
-        return data.DatatypeValidation.validated()
+
+        validate_tabular(dataset.file_name, sep="\t", validate_row=validate_row, comment_designator="#")
+        return DatatypeValidation.validated()
 
     # Dataproviders
-    @dataproviders.decorators.dataprovider_factory('genomic-region',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dataprovider(self, dataset, **settings):
-        return dataproviders.dataset.GenomicRegionDataProvider(dataset, 0, 1, 1, **settings)
-
-    @dataproviders.decorators.dataprovider_factory('genomic-region-dict',
-                                                   dataproviders.dataset.GenomicRegionDataProvider.settings)
-    def genomic_region_dict_dataprovider(self, dataset, **settings):
-        settings['named_columns'] = True
+    @dataproviders.decorators.dataprovider_factory("genomic-region", GenomicRegionDataProvider.settings)
+    def genomic_region_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        return GenomicRegionDataProvider(dataset, 0, 1, 1, **settings)
+
+    @dataproviders.decorators.dataprovider_factory("genomic-region-dict", GenomicRegionDataProvider.settings)
+    def genomic_region_dict_dataprovider(self, dataset: "DatasetInstance", **settings) -> GenomicRegionDataProvider:
+        settings["named_columns"] = True
         return self.genomic_region_dataprovider(dataset, **settings)
 
 
 class Vcf(BaseVcf):
-    file_ext = 'vcf'
+    file_ext = "vcf"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         return self._sniff(file_prefix)
 
 
 class VcfGz(BaseVcf, binary.Binary):
     # This class name is a misnomer, should be VcfBgzip
-    file_ext = 'vcf_bgzip'
-    file_ext_export_alias = 'vcf.gz'
+    file_ext = "vcf_bgzip"
+    file_ext_export_alias = "vcf.gz"
     compressed = True
     compressed_format = "gzip"
 
-    MetadataElement(name="tabix_index", desc="Vcf Index File", param=metadata.FileParameter, file_ext="tbi", readonly=True, visible=False, optional=True)
+    MetadataElement(
+        name="tabix_index",
+        desc="Vcf Index File",
+        param=metadata.FileParameter,
+        file_ext="tbi",
+        readonly=True,
+        visible=False,
+        optional=True,
+    )
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         if not self._sniff(filename):
             return False
         # Check that the file is compressed with bgzip (not gzip), i.e. the
         # compressed format is BGZF, as explained in
         # http://samtools.github.io/hts-specs/SAMv1.pdf
-        with open(filename, 'rb') as fh:
+        with open(filename, "rb") as fh:
             fh.seek(-28, 2)
             last28 = fh.read()
-            return binascii.hexlify(last28) == b'1f8b08040000000000ff0600424302001b0003000000000000000000'
+            return binascii.hexlify(last28) == b"1f8b08040000000000ff0600424302001b0003000000000000000000"
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(
+        self, dataset: "DatasetInstance", overwrite: bool = True, metadata_tmp_files_dir: Optional[str] = None, **kwd
+    ) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         # Creates the index for the VCF file.
         # These metadata values are not accessible by users, always overwrite
         index_file = dataset.metadata.tabix_index
         if not index_file:
-            index_file = dataset.metadata.spec['tabix_index'].param.new_file(dataset=dataset)
+            index_file = dataset.metadata.spec["tabix_index"].param.new_file(
+                dataset=dataset, metadata_tmp_files_dir=metadata_tmp_files_dir
+            )
 
         try:
-            pysam.tabix_index(dataset.file_name, index=index_file.file_name, preset='vcf', keep_original=True, force=True)
+            pysam.tabix_index(
+                dataset.file_name, index=index_file.file_name, preset="vcf", keep_original=True, force=True
+            )
         except Exception as e:
-            raise Exception(f'Error setting VCF.gz metadata: {util.unicodify(e)}')
+            raise Exception(f"Error setting VCF.gz metadata: {util.unicodify(e)}")
         dataset.metadata.tabix_index = index_file
 
 
 @build_sniff_from_prefix
 class Eland(Tabular):
     """Support for the export.txt.gz file used by Illumina's ELANDv2e aligner"""
+
     compressed = True
     compressed_format = "gzip"
-    file_ext = '_export.txt.gz'
+    file_ext = "_export.txt.gz"
     MetadataElement(name="columns", default=0, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=[], param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False, no_value=[])
+    MetadataElement(
+        name="column_types",
+        default=[],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
     MetadataElement(name="comment_lines", default=0, desc="Number of comments", readonly=True, visible=False)
-    MetadataElement(name="tiles", default=[], param=metadata.ListParameter, desc="Set of tiles", readonly=True, visible=False, no_value=[])
-    MetadataElement(name="reads", default=[], param=metadata.ListParameter, desc="Set of reads", readonly=True, visible=False, no_value=[])
-    MetadataElement(name="lanes", default=[], param=metadata.ListParameter, desc="Set of lanes", readonly=True, visible=False, no_value=[])
-    MetadataElement(name="barcodes", default=[], param=metadata.ListParameter, desc="Set of barcodes", readonly=True, visible=False, no_value=[])
+    MetadataElement(
+        name="tiles",
+        default=[],
+        param=metadata.ListParameter,
+        desc="Set of tiles",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
+    MetadataElement(
+        name="reads",
+        default=[],
+        param=metadata.ListParameter,
+        desc="Set of reads",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
+    MetadataElement(
+        name="lanes",
+        default=[],
+        param=metadata.ListParameter,
+        desc="Set of lanes",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
+    MetadataElement(
+        name="barcodes",
+        default=[],
+        param=metadata.ListParameter,
+        desc="Set of barcodes",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
 
     def __init__(self, **kwd):
         """Initialize eland datatype"""
         super().__init__(**kwd)
-        self.column_names = ['MACHINE', 'RUN_NO', 'LANE', 'TILE', 'X', 'Y',
-                             'INDEX', 'READ_NO', 'SEQ', 'QUAL', 'CHROM', 'CONTIG',
-                             'POSITION', 'STRAND', 'DESC', 'SRAS', 'PRAS', 'PART_CHROM'
-                             'PART_CONTIG', 'PART_OFFSET', 'PART_STRAND', 'FILT'
-                             ]
-
-    def make_html_table(self, dataset, skipchars=None, peek=None):
+        self.column_names = [
+            "MACHINE",
+            "RUN_NO",
+            "LANE",
+            "TILE",
+            "X",
+            "Y",
+            "INDEX",
+            "READ_NO",
+            "SEQ",
+            "QUAL",
+            "CHROM",
+            "CONTIG",
+            "POSITION",
+            "STRAND",
+            "DESC",
+            "SRAS",
+            "PRAS",
+            "PART_CHROM" "PART_CONTIG",
+            "PART_OFFSET",
+            "PART_STRAND",
+            "FILT",
+        ]
+
+    def make_html_table(
+        self, dataset: "DatasetInstance", skipchars: Optional[List] = None, peek: Optional[List] = None, **kwargs
+    ) -> str:
         """Create HTML table, used for displaying peek"""
-        if skipchars is None:
-            skipchars = []
-        out = ['<table cellspacing="0" cellpadding="3">']
+        skipchars = skipchars or []
         try:
+            out = ['<table cellspacing="0" cellpadding="3">']
             # Generate column header
-            out.append('<tr>')
+            out.append("<tr>")
             for i, name in enumerate(self.column_names):
-                out.append(f'<th>{str(i + 1)}.{name}</th>')
+                out.append(f"<th>{str(i + 1)}.{name}</th>")
             # This data type requires at least 11 columns in the data
             if dataset.metadata.columns - len(self.column_names) > 0:
                 for i in range(len(self.column_names), max(dataset.metadata.columns, self.max_peek_columns)):
-                    out.append(f'<th>{str(i + 1)}</th>')
-                out.append('</tr>')
+                    out.append(f"<th>{str(i + 1)}</th>")
+                out.append("</tr>")
             out.append(self.make_html_peek_rows(dataset, skipchars=skipchars, peek=peek))
-            out.append('</table>')
-            out = "".join(out)
+            out.append("</table>")
+            return "".join(out)
         except Exception as exc:
-            out = f"Can't create peek {exc}"
-        return out
+            return f"Can't create peek {exc}"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in ELAND export format
 
         A file in ELAND export format consists of lines of tab-separated data.
         There is no header.
 
         Rules for sniffing as True::
@@ -925,128 +1266,149 @@
         """
         count = 0
         for line in file_prefix.line_iterator():
             line = line.strip()
             if not line:
                 break  # Had a EOF comment previously, but this does not indicate EOF. I assume empty lines are not valid and this was intentional.
             if line:
-                line_pieces = line.split('\t')
+                line_pieces = line.split("\t")
                 if len(line_pieces) != 22:
                     return False
                 if int(line_pieces[1]) < 0:
-                    raise Exception('Out of range')
+                    raise Exception("Out of range")
                 if int(line_pieces[2]) < 0:
-                    raise Exception('Out of range')
+                    raise Exception("Out of range")
                 if int(line_pieces[3]) < 0:
-                    raise Exception('Out of range')
+                    raise Exception("Out of range")
                 int(line_pieces[4])
                 int(line_pieces[5])
                 # can get a lot more specific
                 count += 1
                 if count == 5:
                     break
         if count > 0:
             return True
+        return False
 
-    def set_meta(self, dataset, overwrite=True, skip=None, max_data_lines=5, **kwd):
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = None,
+        max_data_lines: Optional[int] = 5,
+        **kwd,
+    ) -> None:
         if dataset.has_data():
-            with compression_utils.get_fileobj(dataset.file_name, compressed_formats=['gzip']) as dataset_fh:
+            with compression_utils.get_fileobj(dataset.file_name, compressed_formats=["gzip"]) as dataset_fh:
+                dataset_fh = cast(FileObjTypeStr, dataset_fh)
                 lanes = {}
                 tiles = {}
                 barcodes = {}
                 reads = {}
                 # Should always read the entire file (until we devise a more clever way to pass metadata on)
                 # if self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
                 # If the dataset is larger than optional_metadata, just count comment lines.
                 #     dataset.metadata.data_lines = None
                 # else:
                 # Otherwise, read the whole thing and set num data lines.
                 for i, line in enumerate(dataset_fh):
                     if line:
-                        line_pieces = line.split('\t')
+                        line_pieces = line.split("\t")
                         if len(line_pieces) != 22:
-                            raise Exception('%s:%d:Corrupt line!' % (dataset.file_name, i))
+                            raise Exception("%s:%d:Corrupt line!" % (dataset.file_name, i))
                         lanes[line_pieces[2]] = 1
                         tiles[line_pieces[3]] = 1
                         barcodes[line_pieces[6]] = 1
                         reads[line_pieces[7]] = 1
                 dataset.metadata.data_lines = i + 1
             dataset.metadata.comment_lines = 0
             dataset.metadata.columns = 21
-            dataset.metadata.column_types = ['str', 'int', 'int', 'int', 'int', 'int', 'str', 'int', 'str', 'str', 'str', 'str', 'str', 'str', 'str', 'str', 'str', 'str', 'str', 'str', 'str']
+            dataset.metadata.column_types = [
+                "str",
+                "int",
+                "int",
+                "int",
+                "int",
+                "int",
+                "str",
+                "int",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+                "str",
+            ]
             dataset.metadata.lanes = list(lanes.keys())
             dataset.metadata.tiles = ["%04d" % int(t) for t in tiles.keys()]
-            dataset.metadata.barcodes = [_ for _ in barcodes.keys() if _ != '0'] + ['NoIndex' for _ in barcodes.keys() if _ == '0']
+            dataset.metadata.barcodes = [_ for _ in barcodes.keys() if _ != "0"] + [
+                "NoIndex" for _ in barcodes.keys() if _ == "0"
+            ]
             dataset.metadata.reads = list(reads.keys())
 
 
 @build_sniff_from_prefix
 class ElandMulti(Tabular):
-    file_ext = 'elandmulti'
+    file_ext = "elandmulti"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         return False
 
 
 class FeatureLocationIndex(Tabular):
     """
     An index that stores feature locations in tabular format.
     """
-    file_ext = 'fli'
+
+    file_ext = "fli"
     MetadataElement(name="columns", default=2, desc="Number of columns", readonly=True, visible=False)
-    MetadataElement(name="column_types", default=['str', 'str'], param=metadata.ColumnTypesParameter, desc="Column types", readonly=True, visible=False, no_value=[])
+    MetadataElement(
+        name="column_types",
+        default=["str", "str"],
+        param=metadata.ColumnTypesParameter,
+        desc="Column types",
+        readonly=True,
+        visible=False,
+        no_value=[],
+    )
 
 
 @dataproviders.decorators.has_dataproviders
 class BaseCSV(TabularData):
     """
     Delimiter-separated table data.
     This includes CSV, TSV and other dialects understood by the
     Python 'csv' module https://docs.python.org/2/library/csv.html
     Must be extended to define the dialect to use, strict_width and file_ext.
     See the Python module csv for documentation of dialect settings
     """
-    delimiter = ','
-    peek_size = 1024  # File chunk used for sniffing CSV dialect
-    big_peek_size = 10240  # Large File chunk used for sniffing CSV dialect
 
-    def is_int(self, column_text):
-        # Don't allow underscores in numeric literals (PEP 515)
-        if '_' in column_text:
-            return False
-        try:
-            int(column_text)
-            return True
-        except ValueError:
-            return False
+    @property
+    def dialect(self):
+        raise NotImplementedError
 
-    def is_float(self, column_text):
-        # Don't allow underscores in numeric literals (PEP 515)
-        if '_' in column_text:
-            return False
-        try:
-            float(column_text)
-            return True
-        except ValueError:
-            if column_text.strip().lower() == 'na':
-                return True  # na is special cased to be a float
-            return False
+    @property
+    def strict_width(self):
+        raise NotImplementedError
 
-    def guess_type(self, text):
-        if self.is_int(text):
-            return 'int'
-        if self.is_float(text):
-            return 'float'
-        else:
-            return 'str'
+    delimiter = ","
+    peek_size = 1024  # File chunk used for sniffing CSV dialect
+    big_peek_size = 10240  # Large File chunk used for sniffing CSV dialect
 
-    def sniff(self, filename):
-        """ Return True if if recognizes dialect and header. """
+    def sniff(self, filename: str) -> bool:
+        """Return True if if recognizes dialect and header."""
         # check the dialect works
-        with open(filename, newline='') as f:
+        with open(filename, newline="") as f:
             reader = csv.reader(f, self.dialect)
             # Check we can read header and get columns
             header_row = next(reader)
             if len(header_row) < 2:
                 # No columns so not separated by this dialect.
                 return False
 
@@ -1071,48 +1433,48 @@
                 for _ in reader:
                     pass
 
         # Optional: Check Python's csv comes up with a similar dialect
         with open(filename) as f:
             big_peek = f.read(self.big_peek_size)
         auto_dialect = csv.Sniffer().sniff(big_peek)
-        if (auto_dialect.delimiter != self.dialect.delimiter):
+        if auto_dialect.delimiter != self.dialect.delimiter:
             return False
-        if (auto_dialect.quotechar != self.dialect.quotechar):
+        if auto_dialect.quotechar != self.dialect.quotechar:
             return False
         # Not checking for other dialect options
         # They may be mis detected from just the sample.
         # Or not effect the read such as doublequote
 
         # Optional: Check for headers as in the past.
         # Note: No way around Python's csv calling Sniffer.sniff again.
         # Note: Without checking the dialect returned by sniff
         #       this test may be checking the wrong dialect.
         if not csv.Sniffer().has_header(big_peek):
             return False
         return True
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         column_types = []
         header_row = []
         data_row = []
         data_lines = 0
         if dataset.has_data():
-            with open(dataset.file_name, newline='') as csvfile:
+            with open(dataset.file_name, newline="") as csvfile:
                 # Parse file with the correct dialect
                 reader = csv.reader(csvfile, self.dialect)
                 try:
                     header_row = next(reader)
                     data_row = next(reader)
                     for _ in reader:
                         pass
                 except StopIteration:
                     pass
                 except csv.Error as e:
-                    raise Exception('CSV reader error - line %d: %s' % (reader.line_num, e))
+                    raise Exception("CSV reader error - line %d: %s" % (reader.line_num, e))
                 else:
                     data_lines = reader.line_num - 1
 
         # Guess column types
         for cell in data_row:
             column_types.append(self.guess_type(cell))
 
@@ -1127,15 +1489,16 @@
 
 @dataproviders.decorators.has_dataproviders
 class CSV(BaseCSV):
     """
     Comma-separated table data.
     Only sniffs comma-separated files with at least 2 rows and 2 columns.
     """
-    file_ext = 'csv'
+
+    file_ext = "csv"
     dialect = csv.excel  # This is the default
     strict_width = False  # Previous csv type did not check column width
 
 
 @dataproviders.decorators.has_dataproviders
 class TSV(BaseCSV):
     """
@@ -1146,43 +1509,46 @@
     handle most tab-separated files. This datatype is only required for datasets
     with tabs INSIDE double quotes.
 
     This datatype currently does not support TSV files where the header has one
     column less to indicate first column is row names. This kind of file is
     handled fine by the tabular datatype.
     """
-    file_ext = 'tsv'
+
+    file_ext = "tsv"
     dialect = csv.excel_tab
     strict_width = True  # Leave files with different width to tabular
 
 
 @build_sniff_from_prefix
 class ConnectivityTable(Tabular):
     edam_format = "format_3309"
     file_ext = "ct"
 
     header_regexp = re.compile("^[0-9]+(?:	|[ ]+).*?(?:ENERGY|energy|dG)[ 	].*?=")
-    structure_regexp = re.compile("^[0-9]+(?:	|[ ]+)[ACGTURYKMSWBDHVN]+(?:	|[ ]+)[^	]+(?:	|[ ]+)[^	]+(?:	|[ ]+)[^	]+(?:	|[ ]+)[^	]+")
+    structure_regexp = re.compile(
+        "^[0-9]+(?:	|[ ]+)[ACGTURYKMSWBDHVN]+(?:	|[ ]+)[^	]+(?:	|[ ]+)[^	]+(?:	|[ ]+)[^	]+(?:	|[ ]+)[^	]+"
+    )
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         self.columns = 6
-        self.column_names = ['base_index', 'base', 'neighbor_left', 'neighbor_right', 'partner', 'natural_numbering']
-        self.column_types = ['int', 'str', 'int', 'int', 'int', 'int']
+        self.column_names = ["base_index", "base", "neighbor_left", "neighbor_right", "partner", "natural_numbering"]
+        self.column_types = ["int", "str", "int", "int", "int", "int"]
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         data_lines = 0
 
         with open(dataset.file_name) as fh:
             for _ in fh:
                 data_lines += 1
 
         dataset.metadata.data_lines = data_lines
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         The ConnectivityTable (CT) is a file format used for describing
         RNA 2D structures by tools including MFOLD, UNAFOLD and
         the RNAStructure package. The tabular file format is defined as
         follows::
 
             5	energy = -12.3	sequence name
@@ -1218,51 +1584,48 @@
             line = line.strip()
 
             if len(line) > 0:
                 if i == 0:
                     if not self.header_regexp.match(line):
                         return False
                     else:
-                        length = int(re.split(r'\W+', line, 1)[0])
+                        length = int(re.split(r"\W+", line, 1)[0])
                 else:
                     if not self.structure_regexp.match(line.upper()):
                         return False
                     else:
-                        if j != int(re.split(r'\W+', line, 1)[0]):
+                        if j != int(re.split(r"\W+", line, 1)[0]):
                             return False
                         elif j == length:  # Last line of first sequence has been reached
                             return True
                         else:
                             j += 1
                 i += 1
         return False
 
-    def get_chunk(self, trans, dataset, chunk):
-        ck_index = int(chunk)
-        f = open(dataset.file_name)
-        f.seek(ck_index * trans.app.config.display_chunk_size)
-        # If we aren't at the start of the file, seek to next newline.  Do this better eventually.
-        if f.tell() != 0:
-            cursor = f.read(1)
-            while cursor and cursor != '\n':
-                cursor = f.read(1)
-        ck_data = f.read(trans.app.config.display_chunk_size)
-        cursor = f.read(1)
-        while cursor and ck_data[-1] != '\n':
-            ck_data += cursor
-            cursor = f.read(1)
-
-        # The ConnectivityTable format has several derivatives of which one is delimited by (multiple) spaces.
-        # By converting these spaces back to tabs, chucks can still be interpreted by tab delimited file parsers
-        ck_data_header, ck_data_body = ck_data.split('\n', 1)
-        ck_data_header = re.sub('^([0-9]+)[ ]+', r'\1\t', ck_data_header)
-        ck_data_body = re.sub('\n[ \t]+', '\n', ck_data_body)
-        ck_data_body = re.sub('[ ]+', '\t', ck_data_body)
+    def get_chunk(self, trans, dataset: "DatasetInstance", offset: int = 0, ck_size: Optional[int] = None) -> str:
+        ck_data, last_read = self._read_chunk(trans, dataset, offset, ck_size)
+        try:
+            # The ConnectivityTable format has several derivatives of which one is delimited by (multiple) spaces.
+            # By converting these spaces back to tabs, chunks can still be interpreted by tab delimited file parsers
+            ck_data_header, ck_data_body = ck_data.split("\n", 1)
+            ck_data_header = re.sub("^([0-9]+)[ ]+", r"\1\t", ck_data_header)
+            ck_data_body = re.sub("\n[ \t]+", "\n", ck_data_body)
+            ck_data_body = re.sub("[ ]+", "\t", ck_data_body)
+            ck_data = f"{ck_data_header}\n{ck_data_body}"
+        except ValueError:
+            pass  # 1 or 0 lines left
 
-        return dumps({'ck_data': util.unicodify(f"{ck_data_header}\n{ck_data_body}"), 'ck_index': ck_index + 1})
+        return dumps(
+            {
+                "ck_data": util.unicodify(ck_data),
+                "offset": last_read,
+                "data_line_offset": self.data_line_offset,
+            }
+        )
 
 
 @build_sniff_from_prefix
 class MatrixMarket(TabularData):
     """
     The Matrix Market (MM) exchange formats provide a simple mechanism
     to facilitate the exchange of matrix data. MM coordinate format is
@@ -1292,47 +1655,58 @@
     >>> MatrixMarket().sniff( get_test_fname( '1.mtx' ) )
     True
     >>> MatrixMarket().sniff( get_test_fname( '2.mtx' ) )
     True
     >>> MatrixMarket().sniff( get_test_fname( '3.mtx' ) )
     True
     """
+
     file_ext = "mtx"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        return file_prefix.startswith('%%MatrixMarket matrix coordinate')
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        return file_prefix.startswith("%%MatrixMarket matrix coordinate")
 
-    def set_meta(self, dataset, overwrite=True, skip=None, max_data_lines=5, **kwd):
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = None,
+        max_data_lines: Optional[int] = 5,
+        **kwd,
+    ) -> None:
         if dataset.has_data():
             # If the dataset is larger than optional_metadata, just count comment lines.
             with open(dataset.file_name) as dataset_fh:
-                line = ''
+                line = ""
                 data_lines = 0
                 comment_lines = 0
                 # If the dataset is larger than optional_metadata, just count comment lines.
-                count_comments_only = self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize
+                count_comments_only = (
+                    self.max_optional_metadata_filesize >= 0
+                    and dataset.get_size() > self.max_optional_metadata_filesize
+                )
                 for line in dataset_fh:
-                    if line.startswith('%'):
+                    if line.startswith("%"):
                         comment_lines += 1
                     elif count_comments_only:
-                        data_lines = None
+                        data_lines = None  # type: ignore [assignment]
                         break
                     else:
                         data_lines += 1
-                if ' ' in line:
-                    dataset.metadata.delimiter = ' '
+                if " " in line:
+                    dataset.metadata.delimiter = " "
                 else:
-                    dataset.metadata.delimiter = '\t'
+                    dataset.metadata.delimiter = "\t"
             dataset.metadata.comment_lines = comment_lines
             dataset.metadata.data_lines = data_lines
             dataset.metadata.columns = 3
-            dataset.metadata.column_types = ['int', 'int', 'float']
+            dataset.metadata.column_types = ["int", "int", "float"]
 
 
 @build_sniff_from_prefix
 class CMAP(TabularData):
     """
     # CMAP File Version:    2.0
     # Label Channels:   1
@@ -1342,80 +1716,269 @@
     # Values corresponding to intervals (StdDev, HapDelta) refer to the interval between current site and next site
     #h  CMapId  ContigLength    NumSites    SiteID  LabelChannel    Position    StdDev  Coverage    Occurrence  ChimQuality SegDupL SegDupR FragileL    FragileR    OutlierFrac ChimNorm    Mask
     #f  int float   int int int float   float   float   float   float   float   float   float   float   float   float   Hex
     182 58474736.7  10235   1   1   58820.9 35.4    13.5    13.5    -1.00   -1.00   -1.00   3.63    0.00    0.00    -1.00   0
     182 58474736.7  10235   1   1   58820.9 35.4    13.5    13.5    -1.00   -1.00   -1.00   3.63    0.00    0.00    -1.00   0
     182 58474736.7  10235   1   1   58820.9 35.4    13.5    13.5    -1.00   -1.00   -1.00   3.63    0.00    0.00    -1.00   0
     """
+
     file_ext = "cmap"
 
-    MetadataElement(name="cmap_version", default='0.2', desc="version of cmap", readonly=True, visible=True, optional=False, no_value='0.2')
-    MetadataElement(name="label_channels", default=1, desc="the number of label channels", readonly=True, visible=True, optional=False, no_value=1)
-    MetadataElement(name="nickase_recognition_site_1", default=[], desc="comma separated list of label motif recognition sequences for channel 1", readonly=True, visible=True, optional=False, no_value=[])
-    MetadataElement(name="number_of_consensus_nanomaps", default=0, desc="the total number of consensus genome maps in the CMAP file", readonly=True, visible=True, optional=False, no_value=0)
-    MetadataElement(name="nickase_recognition_site_2", default=[], desc="comma separated list of label motif recognition sequences for channel 2", readonly=True, visible=True, optional=True, no_value=[])
-    MetadataElement(name="channel_1_color", default=[], desc="channel 1 color", readonly=True, visible=True, optional=True, no_value=[])
-    MetadataElement(name="channel_2_color", default=[], desc="channel 2 color", readonly=True, visible=True, optional=True, no_value=[])
+    MetadataElement(
+        name="cmap_version",
+        default="0.2",
+        desc="version of cmap",
+        readonly=True,
+        visible=True,
+        optional=False,
+        no_value="0.2",
+    )
+    MetadataElement(
+        name="label_channels",
+        default=1,
+        desc="the number of label channels",
+        readonly=True,
+        visible=True,
+        optional=False,
+        no_value=1,
+    )
+    MetadataElement(
+        name="nickase_recognition_site_1",
+        default=[],
+        desc="comma separated list of label motif recognition sequences for channel 1",
+        readonly=True,
+        visible=True,
+        optional=False,
+        no_value=[],
+    )
+    MetadataElement(
+        name="number_of_consensus_nanomaps",
+        default=0,
+        desc="the total number of consensus genome maps in the CMAP file",
+        readonly=True,
+        visible=True,
+        optional=False,
+        no_value=0,
+    )
+    MetadataElement(
+        name="nickase_recognition_site_2",
+        default=[],
+        desc="comma separated list of label motif recognition sequences for channel 2",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="channel_1_color",
+        default=[],
+        desc="channel 1 color",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="channel_2_color",
+        default=[],
+        desc="channel 2 color",
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         handle = file_prefix.string_io()
         for line in handle:
-            if not line.startswith('#'):
+            if not line.startswith("#"):
                 return False
-            if line.startswith('# CMAP File Version:'):
+            if line.startswith("# CMAP File Version:"):
                 return True
         return False
 
-    def set_meta(self, dataset, overwrite=True, skip=None, max_data_lines=7, **kwd):
+    def set_meta(
+        self,
+        dataset: "DatasetInstance",
+        overwrite: bool = True,
+        skip: Optional[int] = None,
+        max_data_lines: Optional[int] = 7,
+        **kwd,
+    ) -> None:
         if dataset.has_data():
             with open(dataset.file_name) as dataset_fh:
                 comment_lines = 0
                 column_headers = None
                 cleaned_column_types = None
                 number_of_columns = 0
                 for i, line in enumerate(dataset_fh):
-                    line = line.strip('\n')
-                    if line.startswith('#'):
-
-                        if line.startswith('#h'):
-
+                    line = line.strip("\n")
+                    if line.startswith("#"):
+                        if line.startswith("#h"):
                             column_headers = line.split("\t")[1:]
-                        elif line.startswith('#f'):
+                        elif line.startswith("#f"):
                             cleaned_column_types = []
-                            for column_type in line.split('\t')[1:]:
-                                if column_type == 'Hex':
-                                    cleaned_column_types.append('str')
+                            for column_type in line.split("\t")[1:]:
+                                if column_type == "Hex":
+                                    cleaned_column_types.append("str")
                                 else:
                                     cleaned_column_types.append(column_type)
                         comment_lines += 1
-                        fields = line.split('\t')
+                        fields = line.split("\t")
                         if len(fields) == 2:
-                            if fields[0] == '# CMAP File Version:':
+                            if fields[0] == "# CMAP File Version:":
                                 dataset.metadata.cmap_version = fields[1]
-                            elif fields[0] == '# Label Channels:':
+                            elif fields[0] == "# Label Channels:":
                                 dataset.metadata.label_channels = int(fields[1])
-                            elif fields[0] == '# Nickase Recognition Site 1:':
-                                fields2 = fields[1].split(';')
+                            elif fields[0] == "# Nickase Recognition Site 1:":
+                                fields2 = fields[1].split(";")
                                 if len(fields2) == 2:
                                     dataset.metadata.channel_1_color = fields2[1]
-                                dataset.metadata.nickase_recognition_site_1 = fields2[0].split(',')
-                            elif fields[0] == '# Number of Consensus Maps:':
+                                dataset.metadata.nickase_recognition_site_1 = fields2[0].split(",")
+                            elif fields[0] == "# Number of Consensus Maps:":
                                 dataset.metadata.number_of_consensus_nanomaps = int(fields[1])
-                            elif fields[0] == '# Nickase Recognition Site 2:':
-                                fields2 = fields[1].split(';')
+                            elif fields[0] == "# Nickase Recognition Site 2:":
+                                fields2 = fields[1].split(";")
                                 if len(fields2) == 2:
                                     dataset.metadata.channel_2_color = fields2[1]
-                                dataset.metadata.nickase_recognition_site_2 = fields2[0].split(',')
-                    elif self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize:
+                                dataset.metadata.nickase_recognition_site_2 = fields2[0].split(",")
+                    elif (
+                        self.max_optional_metadata_filesize >= 0
+                        and dataset.get_size() > self.max_optional_metadata_filesize
+                    ):
                         # If the dataset is larger than optional_metadata, just count comment lines.
                         # No more comments, and the file is too big to look at the whole thing. Give up.
                         dataset.metadata.data_lines = None
                         break
                     elif i == comment_lines + 1:
-                        number_of_columns = len(line.split('\t'))
-                if not (self.max_optional_metadata_filesize >= 0 and dataset.get_size() > self.max_optional_metadata_filesize):
+                        number_of_columns = len(line.split("\t"))
+                if not (
+                    self.max_optional_metadata_filesize >= 0
+                    and dataset.get_size() > self.max_optional_metadata_filesize
+                ):
                     dataset.metadata.data_lines = i + 1 - comment_lines
             dataset.metadata.comment_lines = comment_lines
             dataset.metadata.column_names = column_headers
             dataset.metadata.column_types = cleaned_column_types
             dataset.metadata.columns = number_of_columns
-            dataset.metadata.delimiter = '\t'
+            dataset.metadata.delimiter = "\t"
+
+
+@build_sniff_from_prefix
+class Psl(Tabular):
+    """Tab delimited data in psl format."""
+
+    edam_format = "format_3007"
+    file_ext = "psl"
+    line_class = "assemblies"
+    data_sources = {"data": "tabix"}
+
+    def __init__(self, **kwd):
+        """Initialize psl datatype"""
+        super().__init__(**kwd)
+        self.column_names = [
+            "matches",
+            "misMatches",
+            "repMatches",
+            "nCount",
+            "qNumInsert",
+            "qBaseInsert",
+            "tNumInsert",
+            "tBaseInsert",
+            "strand",
+            "qName",
+            "qSize",
+            "qStart",
+            "qEnd",
+            "tName",
+            "tSize",
+            "tStart",
+            "tEnd",
+            "blockCount",
+            "blockSizes",
+            "qStarts",
+            "tStarts",
+        ]
+
+    def sniff_prefix(self, file_prefix: FilePrefix):
+        """
+        PSL lines represent alignments, and are typically generated
+        by BLAT. Each line consists of 21 required fields, and track
+        lines may optionally be used to provide more information.
+
+        Fields are tab-separated, and all 21 are required.
+        Although not part of the formal PSL specification, track lines
+        may be used to further configure sets of features.  Track lines
+        are placed at the beginning of the list of features they are
+        to affect.
+
+        Rules for sniffing as True::
+
+            - There must be 21 columns on each fields line
+            - matches, misMatches repMatches, nCount, qNumInsert,
+              qBaseInsert, tNumInsert, tBaseInsert, strand, qSize, qStart,
+              qEnd, tName, tSize, tStart, tEnd, blockCount, blockSizes,
+              qStarts, tStarts  must be correct
+            - We will only check that up to the first 10 alignments are
+              correctly formatted.
+        >>> from galaxy.datatypes.sniff import get_test_fname
+        >>> fname = get_test_fname( '1.psl' )
+        >>> Psl().sniff( fname )
+        True
+        >>> fname = get_test_fname( '2.psl' )
+        >>> Psl().sniff( fname )
+        True
+        >>> fname = get_test_fname( 'interval.interval' )
+        >>> Psl().sniff( fname )
+        False
+        >>> fname = get_test_fname( '2.txt' )
+        >>> Psl().sniff( fname )
+        False
+        >>> fname = get_test_fname( 'test_tab2.tabular' )
+        >>> Psl().sniff( fname )
+        False
+        >>> fname = get_test_fname( 'mothur_datatypetest_true.mothur.ref.taxonomy' )
+        >>> Psl().sniff( fname )
+        False
+        """
+
+        def check_items(s):
+            s_items = s.split(",")
+            for item in s_items:
+                if int(item) < 0:
+                    raise Exception("Out of range")
+
+        count = 0
+        for line in file_prefix.line_iterator():
+            line = line.strip()
+            if not line:
+                break
+            if line:
+                if line.startswith("browser") or line.startswith("track"):
+                    # Skip track lines.
+                    continue
+                items = line.split("\t")
+                if len(items) != 21:
+                    return False
+                # tName is a string
+                items.pop(13)
+                # qName is a string
+                items.pop(9)
+                # strand
+                if items.pop(8) not in ["-", "+", "+-", "-+"]:
+                    raise Exception("Invalid strand")
+                # blockSizes
+                s = items.pop(15).rstrip(",")
+                check_items(s)
+                # qStarts
+                s = items.pop(15).rstrip(",")
+                check_items(s)
+                # tStarts
+                s = items.pop(15).rstrip(",")
+                check_items(s)
+                if any(int(item) < 0 for item in items):
+                    raise Exception("Out of range")
+                count += 1
+                if count == 10:
+                    break
+        if count > 0:
+            return True
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/text.py` & `galaxy-data-23.0.1/galaxy/datatypes/text.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,97 +2,118 @@
 """
 
 import gzip
 import json
 import logging
 import os
 import re
-import shlex
 import subprocess
 import tempfile
+from typing import (
+    IO,
+    Optional,
+    Tuple,
+    TYPE_CHECKING,
+)
 
 import yaml
 
-from galaxy.datatypes.data import get_file_peek, Headers, Text
-from galaxy.datatypes.metadata import MetadataElement, MetadataParameter
+from galaxy.datatypes.data import (
+    GeneratePrimaryFileDataset,
+    get_file_peek,
+    Headers,
+    Text,
+)
+from galaxy.datatypes.metadata import (
+    MetadataElement,
+    MetadataParameter,
+)
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
     iter_headers,
 )
 from galaxy.util import (
     nice_size,
+    shlex_join,
     string_as_bool,
     unicodify,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import (
+        DatasetInstance,
+        HistoryDatasetAssociation,
+    )
+
 log = logging.getLogger(__name__)
 
 
 @build_sniff_from_prefix
 class Html(Text):
     """Class describing an html file"""
+
     edam_format = "format_2331"
     file_ext = "html"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = "HTML file"
             dataset.blurb = nice_size(dataset.get_size())
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'text/html'
+        return "text/html"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in html format
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'complete.bed' )
         >>> Html().sniff( fname )
         False
         >>> fname = get_test_fname( 'file.html' )
         >>> Html().sniff( fname )
         True
         """
         headers = iter_headers(file_prefix, None)
         for hdr in headers:
-            if hdr and hdr[0].lower().find('<html>') >= 0:
+            if hdr and hdr[0].lower().find("<html>") >= 0:
                 return True
         return False
 
 
 @build_sniff_from_prefix
 class Json(Text):
     edam_format = "format_3464"
     file_ext = "json"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "JavaScript Object Notation (JSON)"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/json'
+        return "application/json"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Try to load the string with the json module. If successful it's a json file.
+        Try to load the string with the json module. If successful it's a json file.
         """
         return self._looks_like_json(file_prefix)
 
-    def _looks_like_json(self, file_prefix):
+    def _looks_like_json(self, file_prefix: FilePrefix) -> bool:
         # Pattern used by SequenceSplitLocations
         if file_prefix.file_size < 50000 and not file_prefix.truncated:
             # If the file is small enough - don't guess just check.
             try:
                 item = json.loads(file_prefix.contents_header)
                 # exclude simple types, must set format in these cases
                 assert isinstance(item, (list, dict))
@@ -103,30 +124,31 @@
             start = file_prefix.string_io().read(100).strip()
             if start:
                 # simple types are valid JSON as well,
                 # but if necessary format has to be set explicitly
                 return start.startswith("[") or start.startswith("{")
             return False
 
-    def display_peek(self, dataset):
+    def display_peek(self, dataset: "DatasetInstance") -> str:
         try:
             return dataset.peek
         except Exception:
             return f"JSON file ({nice_size(dataset.get_size())})"
 
 
 class ExpressionJson(Json):
-    """ Represents the non-data input or output to a tool or workflow.
-    """
+    """Represents the non-data input or output to a tool or workflow."""
+
     file_ext = "json"
-    MetadataElement(name="json_type", default=None, desc="JavaScript or JSON type of expression", readonly=True, visible=True)
+    MetadataElement(
+        name="json_type", default=None, desc="JavaScript or JSON type of expression", readonly=True, visible=True
+    )
 
-    def set_meta(self, dataset, **kwd):
-        """
-        """
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        """ """
         if dataset.has_data():
             json_type = "null"
             file_path = dataset.file_name
             try:
                 with open(file_path) as f:
                     obj = json.load(f)
                     if isinstance(obj, int):
@@ -144,163 +166,302 @@
             dataset.metadata.json_type = json_type
 
 
 @build_sniff_from_prefix
 class Ipynb(Json):
     file_ext = "ipynb"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "Jupyter Notebook"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Try to load the string with the json module. If successful it's a json file.
+        Try to load the string with the json module. If successful it's a json file.
         """
         if self._looks_like_json(file_prefix):
             try:
                 with open(file_prefix.filename) as f:
                     ipynb = json.load(f)
-                if ipynb.get('nbformat', False) is not False and ipynb.get('metadata', False):
+                if ipynb.get("nbformat", False) is not False and ipynb.get("metadata", False):
                     return True
                 else:
                     return False
             except Exception:
                 return False
+        return False
 
-    def display_data(self, trans, dataset, preview=False, filename=None, to_ext=None, **kwd):
-        headers = kwd.get("headers", {})
+    def display_data(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        **kwd,
+    ):
         config = trans.app.config
-        trust = getattr(config, 'trust_jupyter_notebook_conversion', False)
+        trust = getattr(config, "trust_jupyter_notebook_conversion", False)
         if trust:
-            return self._display_data_trusted(trans, dataset, preview=preview, filename=filename, to_ext=to_ext, headers=headers, **kwd)
+            return self._display_data_trusted(trans, dataset, preview=preview, filename=filename, to_ext=to_ext, **kwd)
         else:
-            return super().display_data(trans, dataset, preview=preview, filename=filename, to_ext=to_ext, headers=headers, **kwd)
+            return super().display_data(trans, dataset, preview=preview, filename=filename, to_ext=to_ext, **kwd)
 
-    def _display_data_trusted(self, trans, dataset, preview=False, filename=None, to_ext=None, **kwd):
-        headers = kwd.get("headers", {})
+    def _display_data_trusted(
+        self,
+        trans,
+        dataset: "HistoryDatasetAssociation",
+        preview: bool = False,
+        filename: Optional[str] = None,
+        to_ext: Optional[str] = None,
+        **kwd,
+    ) -> Tuple[IO, Headers]:
+        headers = kwd.pop("headers", {})
         preview = string_as_bool(preview)
         if to_ext or not preview:
             return self._serve_raw(dataset, to_ext, headers, **kwd)
         else:
             with tempfile.NamedTemporaryFile(delete=False) as ofile_handle:
                 ofilename = ofile_handle.name
             try:
-                cmd = ['jupyter', 'nbconvert', '--to', 'html', '--template', 'full', dataset.file_name, '--output', ofilename]
+                cmd = [
+                    "jupyter",
+                    "nbconvert",
+                    "--to",
+                    "html",
+                    "--template",
+                    "full",
+                    dataset.file_name,
+                    "--output",
+                    ofilename,
+                ]
                 subprocess.check_call(cmd)
-                ofilename = f'{ofilename}.html'
+                ofilename = f"{ofilename}.html"
             except subprocess.CalledProcessError:
                 ofilename = dataset.file_name
-                log.exception('Command "%s" failed. Could not convert the Jupyter Notebook to HTML, defaulting to plain text.', ' '.join(map(shlex.quote, cmd)))
-            return open(ofilename, mode='rb'), headers
+                log.exception(
+                    'Command "%s" failed. Could not convert the Jupyter Notebook to HTML, defaulting to plain text.',
+                    shlex_join(cmd),
+                )
+            return open(ofilename, mode="rb"), headers
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
         Set the number of models in dataset.
         """
 
 
 @build_sniff_from_prefix
 class Biom1(Json):
     """
-        BIOM version 1.0 file format description
-        http://biom-format.org/documentation/format_versions/biom-1.0.html
+    BIOM version 1.0 file format description
+    http://biom-format.org/documentation/format_versions/biom-1.0.html
     """
+
     file_ext = "biom1"
     edam_format = "format_3746"
 
-    MetadataElement(name="table_rows", default=[], desc="table_rows", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="table_matrix_element_type", default="", desc="table_matrix_element_type", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value="")
-    MetadataElement(name="table_format", default="", desc="table_format", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value="")
-    MetadataElement(name="table_generated_by", default="", desc="table_generated_by", param=MetadataParameter, readonly=True, visible=True, optional=True, no_value="")
-    MetadataElement(name="table_matrix_type", default="", desc="table_matrix_type", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value="")
-    MetadataElement(name="table_shape", default=[], desc="table_shape", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="table_format_url", default="", desc="table_format_url", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value="")
-    MetadataElement(name="table_date", default="", desc="table_date", param=MetadataParameter, readonly=True, visible=True, optional=True, no_value="")
-    MetadataElement(name="table_type", default="", desc="table_type", param=MetadataParameter, readonly=True, visible=True, optional=True, no_value="")
-    MetadataElement(name="table_id", default=None, desc="table_id", param=MetadataParameter, readonly=True, visible=True, optional=True)
-    MetadataElement(name="table_columns", default=[], desc="table_columns", param=MetadataParameter, readonly=True, visible=False, optional=True, no_value=[])
-    MetadataElement(name="table_column_metadata_headers", default=[], desc="table_column_metadata_headers", param=MetadataParameter, readonly=True, visible=True, optional=True, no_value=[])
+    MetadataElement(
+        name="table_rows",
+        default=[],
+        desc="table_rows",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="table_matrix_element_type",
+        default="",
+        desc="table_matrix_element_type",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_format",
+        default="",
+        desc="table_format",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_generated_by",
+        default="",
+        desc="table_generated_by",
+        param=MetadataParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_matrix_type",
+        default="",
+        desc="table_matrix_type",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_shape",
+        default=[],
+        desc="table_shape",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="table_format_url",
+        default="",
+        desc="table_format_url",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_date",
+        default="",
+        desc="table_date",
+        param=MetadataParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_type",
+        default="",
+        desc="table_type",
+        param=MetadataParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value="",
+    )
+    MetadataElement(
+        name="table_id",
+        default=None,
+        desc="table_id",
+        param=MetadataParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+    )
+    MetadataElement(
+        name="table_columns",
+        default=[],
+        desc="table_columns",
+        param=MetadataParameter,
+        readonly=True,
+        visible=False,
+        optional=True,
+        no_value=[],
+    )
+    MetadataElement(
+        name="table_column_metadata_headers",
+        default=[],
+        desc="table_column_metadata_headers",
+        param=MetadataParameter,
+        readonly=True,
+        visible=True,
+        optional=True,
+        no_value=[],
+    )
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         super().set_peek(dataset)
         if not dataset.dataset.purged:
             dataset.blurb = "Biological Observation Matrix v1"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         is_biom = False
         if self._looks_like_json(file_prefix):
             is_biom = self._looks_like_biom(file_prefix)
         return is_biom
 
-    def _looks_like_biom(self, file_prefix: FilePrefix, load_size=50000):
+    def _looks_like_biom(self, file_prefix: FilePrefix, load_size: int = 50000) -> bool:
         """
         @param filepath: [str] The path to the evaluated file.
         @param load_size: [int] The size of the file block load in RAM (in
                           bytes).
         """
         is_biom = False
         segment_size = int(load_size / 2)
         try:
             with open(file_prefix.filename) as fh:
                 prev_str = ""
                 segment_str = fh.read(segment_size)
-                if segment_str.strip().startswith('{'):
+                if segment_str.strip().startswith("{"):
                     while segment_str:
                         current_str = prev_str + segment_str
                         if '"format"' in current_str:
-                            current_str = re.sub(r'\s', '', current_str)
+                            current_str = re.sub(r"\s", "", current_str)
                             if '"format":"BiologicalObservationMatrix' in current_str:
                                 is_biom = True
                                 break
                         prev_str = segment_str
                         segment_str = fh.read(segment_size)
         except Exception:
             pass
         return is_biom
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
-            Store metadata information from the BIOM file.
+        Store metadata information from the BIOM file.
         """
         if dataset.has_data():
             with open(dataset.file_name) as fh:
                 try:
                     json_dict = json.load(fh)
                 except Exception:
                     return
 
                 def _transform_dict_list_ids(dict_list):
                     if dict_list:
-                        return [x.get('id', None) for x in dict_list]
+                        return [x.get("id", None) for x in dict_list]
                     return []
 
-                b_transform = {'rows': _transform_dict_list_ids, 'columns': _transform_dict_list_ids}
-                for (m_name, b_name) in [('table_rows', 'rows'),
-                                         ('table_matrix_element_type', 'matrix_element_type'),
-                                         ('table_format', 'format'),
-                                         ('table_generated_by', 'generated_by'),
-                                         ('table_matrix_type', 'matrix_type'),
-                                         ('table_shape', 'shape'),
-                                         ('table_format_url', 'format_url'),
-                                         ('table_date', 'date'),
-                                         ('table_type', 'type'),
-                                         ('table_id', 'id'),
-                                         ('table_columns', 'columns')]:
+                b_transform = {"rows": _transform_dict_list_ids, "columns": _transform_dict_list_ids}
+                for m_name, b_name in [
+                    ("table_rows", "rows"),
+                    ("table_matrix_element_type", "matrix_element_type"),
+                    ("table_format", "format"),
+                    ("table_generated_by", "generated_by"),
+                    ("table_matrix_type", "matrix_type"),
+                    ("table_shape", "shape"),
+                    ("table_format_url", "format_url"),
+                    ("table_date", "date"),
+                    ("table_type", "type"),
+                    ("table_id", "id"),
+                    ("table_columns", "columns"),
+                ]:
                     try:
                         metadata_value = json_dict.get(b_name, None)
                         if b_name == "columns" and metadata_value:
                             keep_columns = set()
                             for column in metadata_value:
-                                if column['metadata'] is not None:
-                                    for k, v in column['metadata'].items():
+                                if column["metadata"] is not None:
+                                    for k, v in column["metadata"].items():
                                         if v is not None:
                                             keep_columns.add(k)
                             final_list = sorted(list(keep_columns))
                             dataset.metadata.table_column_metadata_headers = final_list
                         if b_name in b_transform:
                             metadata_value = b_transform[b_name](metadata_value)
                         setattr(dataset.metadata, m_name, metadata_value)
@@ -313,24 +474,25 @@
     """
     https://github.com/repseqio/library-imgt/releases
     Data coming from IMGT server may be used for academic research only,
     provided that it is referred to IMGT®, and cited as:
     "IMGT®, the international ImMunoGeneTics information system®
     http://www.imgt.org (founder and director: Marie-Paule Lefranc, Montpellier, France)."
     """
+
     file_ext = "imgt.json"
 
     MetadataElement(name="taxon_names", default=[], desc="taxonID: names", readonly=True, visible=True, no_value=[])
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         super().set_peek(dataset)
         if not dataset.dataset.purged:
             dataset.blurb = "IMGT Library"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in json format with imgt elements
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( '1.json' )
         >>> ImgtJson().sniff( fname )
         False
@@ -339,63 +501,64 @@
         True
         """
         is_imgt = False
         if self._looks_like_json(file_prefix):
             is_imgt = self._looks_like_imgt(file_prefix)
         return is_imgt
 
-    def _looks_like_imgt(self, file_prefix: FilePrefix, load_size=5000):
+    def _looks_like_imgt(self, file_prefix: FilePrefix, load_size: int = 5000) -> bool:
         """
         @param filepath: [str] The path to the evaluated file.
         @param load_size: [int] The size of the file block load in RAM (in
                           bytes).
         """
         is_imgt = False
         try:
             with open(file_prefix.filename) as fh:
                 segment_str = fh.read(load_size)
-                if segment_str.strip().startswith('['):
+                if segment_str.strip().startswith("["):
                     if '"taxonId"' in segment_str and '"anchorPoints"' in segment_str:
                         is_imgt = True
         except Exception:
             pass
         return is_imgt
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
-            Store metadata information from the imgt file.
+        Store metadata information from the imgt file.
         """
         if dataset.has_data():
             with open(dataset.file_name) as fh:
                 try:
                     json_dict = json.load(fh)
                     tax_names = []
                     for entry in json_dict:
-                        if 'taxonId' in entry:
-                            names = "%d: %s" % (entry['taxonId'], ','.join(entry['speciesNames']))
+                        if "taxonId" in entry:
+                            names = "%d: %s" % (entry["taxonId"], ",".join(entry["speciesNames"]))
                             tax_names.append(names)
                     dataset.metadata.taxon_names = tax_names
                 except Exception:
                     return
 
 
 @build_sniff_from_prefix
 class GeoJson(Json):
     """
-        GeoJSON is a geospatial data interchange format based on JavaScript Object Notation (JSON).
-        https://tools.ietf.org/html/rfc7946
+    GeoJSON is a geospatial data interchange format based on JavaScript Object Notation (JSON).
+    https://tools.ietf.org/html/rfc7946
     """
+
     file_ext = "geojson"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         super().set_peek(dataset)
         if not dataset.dataset.purged:
             dataset.blurb = "GeoJSON"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is in json format with imgt elements
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( '1.json' )
         >>> GeoJson().sniff( fname )
         False
@@ -404,93 +567,108 @@
         True
         """
         is_geojson = False
         if self._looks_like_json(file_prefix):
             is_geojson = self._looks_like_geojson(file_prefix)
         return is_geojson
 
-    def _looks_like_geojson(self, file_prefix: FilePrefix, load_size=5000):
+    def _looks_like_geojson(self, file_prefix: FilePrefix, load_size: int = 5000) -> bool:
         """
         One of "Point", "MultiPoint", "LineString", "MultiLineString", "Polygon", "MultiPolygon", and "GeometryCollection" needs to be present.
         All of "type", "geometry", and "coordinates" needs to be present.
         """
         is_geojson = False
         try:
             with open(file_prefix.filename) as fh:
                 segment_str = fh.read(load_size)
-                if any(x in segment_str for x in ["Point", "MultiPoint", "LineString", "MultiLineString", "Polygon", "MultiPolygon", "GeometryCollection"]):
+                if any(
+                    x in segment_str
+                    for x in [
+                        "Point",
+                        "MultiPoint",
+                        "LineString",
+                        "MultiLineString",
+                        "Polygon",
+                        "MultiPolygon",
+                        "GeometryCollection",
+                    ]
+                ):
                     if all(x in segment_str for x in ["type", "geometry", "coordinates"]):
                         return True
         except Exception:
             pass
         return is_geojson
 
 
 @build_sniff_from_prefix
 class Obo(Text):
     """
-        OBO file format description
-        https://owlcollab.github.io/oboformat/doc/GO.format.obo-1_2.html
+    OBO file format description
+    https://owlcollab.github.io/oboformat/doc/GO.format.obo-1_2.html
     """
+
     edam_data = "data_0582"
     edam_format = "format_2549"
     file_ext = "obo"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "Open Biomedical Ontology (OBO)"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Try to guess the Obo filetype.
-            It usually starts with a "format-version:" string and has several stanzas which starts with "id:".
+        Try to guess the Obo filetype.
+        It usually starts with a "format-version:" string and has several stanzas which starts with "id:".
         """
-        stanza = re.compile(r'^\[.*\]$')
+        stanza = re.compile(r"^\[.*\]$")
         handle = file_prefix.string_io()
         first_line = handle.readline()
-        if not first_line.startswith('format-version:'):
+        if not first_line.startswith("format-version:"):
             return False
 
         for line in handle:
             if stanza.match(line.strip()):
                 # a stanza needs to begin with an ID tag
-                if next(handle).startswith('id:'):
+                if next(handle).startswith("id:"):
                     return True
         return False
 
 
 @build_sniff_from_prefix
 class Arff(Text):
     """
     An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes.
     http://weka.wikispaces.com/ARFF
     """
+
     edam_format = "format_3581"
     file_ext = "arff"
 
-    MetadataElement(name="comment_lines", default=0, desc="Number of comment lines", readonly=True, optional=True, no_value=0)
+    MetadataElement(
+        name="comment_lines", default=0, desc="Number of comment lines", readonly=True, optional=True, no_value=0
+    )
     MetadataElement(name="columns", default=0, desc="Number of columns", readonly=True, visible=True, no_value=0)
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = get_file_peek(dataset.file_name)
             dataset.blurb = "Attribute-Relation File Format (ARFF)"
             dataset.blurb += f", {dataset.metadata.comment_lines} comments, {dataset.metadata.columns} attributes"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Try to guess the Arff filetype.
-            It usually starts with a "format-version:" string and has several stanzas which starts with "id:".
+        Try to guess the Arff filetype.
+        It usually starts with a "format-version:" string and has several stanzas which starts with "id:".
         """
         handle = file_prefix.string_io()
         relation_found = False
         attribute_found = False
         for line_count, line in enumerate(handle):
             if line_count > 1000:
                 # only investigate the first 1000 lines
@@ -506,131 +684,136 @@
                 attribute_found = True
             elif start_string.startswith("@DATA"):
                 # @DATA should be the last data block
                 if relation_found and attribute_found:
                     return True
         return False
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         """
-            Trying to count the comment lines and the number of columns included.
-            A typical ARFF data block looks like this:
-            @DATA
-            5.1,3.5,1.4,0.2,Iris-setosa
-            4.9,3.0,1.4,0.2,Iris-setosa
+        Trying to count the comment lines and the number of columns included.
+        A typical ARFF data block looks like this:
+        @DATA
+        5.1,3.5,1.4,0.2,Iris-setosa
+        4.9,3.0,1.4,0.2,Iris-setosa
         """
         comment_lines = column_count = 0
         if dataset.has_data():
             first_real_line = False
             data_block = False
             with open(dataset.file_name) as handle:
                 for line in handle:
                     line = line.strip()
                     if not line:
                         continue
-                    if line.startswith('%') and not first_real_line:
+                    if line.startswith("%") and not first_real_line:
                         comment_lines += 1
                     else:
                         first_real_line = True
                     if data_block:
-                        if line.startswith('{'):
+                        if line.startswith("{"):
                             # Sparse representation
                             """
                                 @data
                                 0, X, 0, Y, "class A", {5}
                             or
                                 @data
                                 {1 X, 3 Y, 4 "class A"}, {5}
                             """
-                            token = line.split('}', 1)
+                            token = line.split("}", 1)
                             first_part = token[0]
-                            last_column = first_part.split(',')[-1].strip()
+                            last_column = first_part.split(",")[-1].strip()
                             numeric_value = last_column.split()[0]
                             column_count = int(numeric_value)
                             if len(token) > 1:
                                 # we have an additional weight
                                 column_count -= 1
                         else:
-                            columns = line.strip().split(',')
+                            columns = line.strip().split(",")
                             column_count = len(columns)
-                            if columns[-1].strip().startswith('{'):
+                            if columns[-1].strip().startswith("{"):
                                 # we have an additional weight at the end
                                 column_count -= 1
 
                         # We have now the column_count and we know the initial comment lines. So we can terminate here.
                         break
                     if line[:5].upper() == "@DATA":
                         data_block = True
         dataset.metadata.comment_lines = comment_lines
         dataset.metadata.columns = column_count
 
 
 class SnpEffDb(Text):
     """Class describing a SnpEff genome build"""
+
     edam_format = "format_3624"
     file_ext = "snpeffdb"
     MetadataElement(name="genome_version", default=None, desc="Genome Version", readonly=True, visible=True)
     MetadataElement(name="snpeff_version", default="SnpEff4.0", desc="SnpEff Version", readonly=True, visible=True)
-    MetadataElement(name="regulation", default=[], desc="Regulation Names", readonly=True, visible=True, no_value=[], optional=True)
-    MetadataElement(name="annotation", default=[], desc="Annotation Names", readonly=True, visible=True, no_value=[], optional=True)
+    MetadataElement(
+        name="regulation", default=[], desc="Regulation Names", readonly=True, visible=True, no_value=[], optional=True
+    )
+    MetadataElement(
+        name="annotation", default=[], desc="Annotation Names", readonly=True, visible=True, no_value=[], optional=True
+    )
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
     # The SnpEff version line was added in SnpEff version 4.1
-    def getSnpeffVersionFromFile(self, path):
+    def getSnpeffVersionFromFile(self, path: str) -> Optional[str]:
         snpeff_version = None
         try:
-            with gzip.open(path, 'rt') as fh:
+            with gzip.open(path, "rt") as fh:
                 buf = fh.read(100)
                 lines = buf.splitlines()
-                m = re.match(r'^(SnpEff)\s+(\d+\.\d+).*$', lines[0].strip())
+                m = re.match(r"^(SnpEff)\s+(\d+\.\d+).*$", lines[0].strip())
                 if m:
                     snpeff_version = m.groups()[0] + m.groups()[1]
         except Exception:
             pass
         return snpeff_version
 
-    def set_meta(self, dataset, **kwd):
-        super().set_meta(dataset, **kwd)
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
+        super().set_meta(dataset, overwrite=overwrite, **kwd)
         data_dir = dataset.extra_files_path
         # search data_dir/genome_version for files
-        regulation_pattern = 'regulation_(.+).bin'
+        regulation_pattern = "regulation_(.+).bin"
         #  annotation files that are included in snpEff by a flag
-        annotations_dict = {'nextProt.bin': '-nextprot', 'motif.bin': '-motif', 'interactions.bin': '-interaction'}
+        annotations_dict = {"nextProt.bin": "-nextprot", "motif.bin": "-motif", "interactions.bin": "-interaction"}
         regulations = []
         annotations = []
         genome_version = None
         snpeff_version = None
         if data_dir and os.path.isdir(data_dir):
             for root, _, files in os.walk(data_dir):
                 for fname in files:
-                    if fname.startswith('snpEffectPredictor'):
+                    if fname.startswith("snpEffectPredictor"):
                         # if snpEffectPredictor.bin download succeeded
                         genome_version = os.path.basename(root)
                         dataset.metadata.genome_version = genome_version
                         # read the first line of the gzipped snpEffectPredictor.bin file to get the SnpEff version
                         snpeff_version = self.getSnpeffVersionFromFile(os.path.join(root, fname))
                         if snpeff_version:
                             dataset.metadata.snpeff_version = snpeff_version
                     else:
                         m = re.match(regulation_pattern, fname)
                         if m:
                             name = m.groups()[0]
                             regulations.append(name)
                         elif fname in annotations_dict:
                             value = annotations_dict[fname]
-                            name = value.lstrip('-')
+                            name = value.lstrip("-")
                             annotations.append(name)
             dataset.metadata.regulation = regulations
             dataset.metadata.annotation = annotations
             try:
-                with open(dataset.file_name, 'w') as fh:
-                    fh.write(f"{genome_version}\n" if genome_version else 'Genome unknown')
-                    fh.write(f"{snpeff_version}\n" if snpeff_version else 'SnpEff version unknown')
+                with open(dataset.file_name, "w") as fh:
+                    fh.write(f"{genome_version}\n" if genome_version else "Genome unknown")
+                    fh.write(f"{snpeff_version}\n" if snpeff_version else "SnpEff version unknown")
                     if annotations:
                         fh.write(f"annotations: {','.join(annotations)}\n")
                     if regulations:
                         fh.write(f"regulations: {','.join(regulations)}\n")
             except Exception:
                 pass
 
@@ -646,83 +829,101 @@
 
     Example:
     - Compress using block-gzip algorithm:
     $ bgzip dbNSFP2.3.txt
     - Create tabix index
     $ tabix -s 1 -b 2 -e 2 dbNSFP2.3.txt.gz
     """
+
     file_ext = "snpsiftdbnsfp"
-    composite_type = 'auto_primary_file'
+    composite_type = "auto_primary_file"
 
-    MetadataElement(name='reference_name', default='dbSNFP', desc='Reference Name', readonly=True, visible=True, set_in_upload=True, no_value='dbSNFP')
+    MetadataElement(
+        name="reference_name",
+        default="dbSNFP",
+        desc="Reference Name",
+        readonly=True,
+        visible=True,
+        set_in_upload=True,
+        no_value="dbSNFP",
+    )
     MetadataElement(name="bgzip", default=None, desc="dbNSFP bgzip", readonly=True, visible=True)
     MetadataElement(name="index", default=None, desc="Tabix Index File", readonly=True, visible=True)
     MetadataElement(name="annotation", default=[], desc="Annotation Names", readonly=True, visible=True, no_value=[])
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
-        self.add_composite_file('%s.gz', description='dbNSFP bgzip', substitute_name_with_metadata='reference_name', is_binary=True)
-        self.add_composite_file('%s.gz.tbi', description='Tabix Index File', substitute_name_with_metadata='reference_name', is_binary=True)
+        self.add_composite_file(
+            "%s.gz", description="dbNSFP bgzip", substitute_name_with_metadata="reference_name", is_binary=True
+        )
+        self.add_composite_file(
+            "%s.gz.tbi", description="Tabix Index File", substitute_name_with_metadata="reference_name", is_binary=True
+        )
 
-    def generate_primary_file(self, dataset=None):
+    def generate_primary_file(self, dataset: GeneratePrimaryFileDataset) -> str:
         """
         This is called only at upload to write the html file
         cannot rename the datasets here - they come with the default unfortunately
         """
-        return '<html><head><title>SnpSiftDbNSFP Composite Dataset</title></head></html>'
+        return "<html><head><title>SnpSiftDbNSFP Composite Dataset</title></head></html>"
 
-    def regenerate_primary_file(self, dataset):
+    def regenerate_primary_file(self, dataset: "DatasetInstance") -> None:
         """
         cannot do this until we are setting metadata
         """
         annotations = f"dbNSFP Annotations: {','.join(dataset.metadata.annotation)}\n"
-        with open(dataset.file_name, 'a') as f:
+        with open(dataset.file_name, "a") as f:
             if dataset.metadata.bgzip:
                 bn = dataset.metadata.bgzip
                 f.write(bn)
-                f.write('\n')
+                f.write("\n")
             f.write(annotations)
 
-    def set_meta(self, dataset, overwrite=True, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         try:
             efp = dataset.extra_files_path
             if os.path.exists(efp):
                 flist = os.listdir(efp)
                 for fname in flist:
-                    if fname.endswith('.gz'):
+                    if fname.endswith(".gz"):
                         dataset.metadata.bgzip = fname
                         try:
-                            with gzip.open(os.path.join(efp, fname), 'rt') as fh:
+                            with gzip.open(os.path.join(efp, fname), "rt") as fh:
                                 buf = fh.read(5000)
                                 lines = buf.splitlines()
-                                headers = lines[0].split('\t')
+                                headers = lines[0].split("\t")
                                 dataset.metadata.annotation = headers[4:]
                         except Exception as e:
                             log.warning("set_meta fname: %s  %s", fname, unicodify(e))
-                    if fname.endswith('.tbi'):
+                    if fname.endswith(".tbi"):
                         dataset.metadata.index = fname
             self.regenerate_primary_file(dataset)
         except Exception as e:
-            log.warning("set_meta fname: %s  %s", dataset.file_name if dataset and dataset.file_name else 'Unkwown', unicodify(e))
+            log.warning(
+                "set_meta fname: %s  %s",
+                dataset.file_name if dataset and dataset.file_name else "Unkwown",
+                unicodify(e),
+            )
 
-        def set_peek(self, dataset):
-            if not dataset.dataset.purged:
-                dataset.peek = f"{dataset.metadata.reference_name} :  {','.join(dataset.metadata.annotation)}"
-                dataset.blurb = f'{dataset.metadata.reference_name}'
-            else:
-                dataset.peek = 'file does not exist'
-                dataset.blurb = 'file purged from disc'
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
+        if not dataset.dataset.purged:
+            dataset.peek = f"{dataset.metadata.reference_name} :  {','.join(dataset.metadata.annotation)}"
+            dataset.blurb = f"{dataset.metadata.reference_name}"
+        else:
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
 
 @build_sniff_from_prefix
 class IQTree(Text):
     """IQ-TREE format"""
-    file_ext = 'iqtree'
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    file_ext = "iqtree"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Detect the IQTree file
 
         Scattered text file containing various headers and data
         types.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
@@ -744,82 +945,84 @@
 @build_sniff_from_prefix
 class Paf(Text):
     """
     PAF: a Pairwise mApping Format
 
     https://github.com/lh3/miniasm/blob/master/PAF.md
     """
+
     file_ext = "paf"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('A-3105.paf')
         >>> Paf().sniff(fname)
         True
         """
         found_valid_lines = False
         for line in iter_headers(file_prefix, "\t"):
             if len(line) < 12:
                 return False
             for i in (1, 2, 3, 6, 7, 8, 9, 10, 11):
                 int(line[i])
-            if line[4] not in ('+', '-'):
+            if line[4] not in ("+", "-"):
                 return False
             if not (0 <= int(line[11]) <= 255):
                 return False
             # Check that the optional columns after the 12th contain SAM-like typed key-value pairs
             for i in range(12, len(line)):
-                if len(line[i].split(':')) != 3:
+                if len(line[i].split(":")) != 3:
                     return False
             found_valid_lines = True
         return found_valid_lines
 
 
 @build_sniff_from_prefix
 class Gfa1(Text):
     """
     Graphical Fragment Assembly (GFA) 1.0
 
     http://gfa-spec.github.io/GFA-spec/GFA1.html
     """
+
     file_ext = "gfa1"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('big.gfa1')
         >>> Gfa1().sniff(fname)
         True
         >>> Gfa2().sniff(fname)
         False
         """
         found_valid_lines = False
         for line in iter_headers(file_prefix, "\t"):
-            if line[0].startswith('#'):
+            if line[0].startswith("#"):
                 continue
-            if line[0] == 'H':
-                return len(line) == 2 and line[1] == 'VN:Z:1.0'
-            elif line[0] == 'S':
+            if line[0] == "H":
+                return len(line) == 2 and line[1] == "VN:Z:1.0"
+            elif line[0] == "S":
                 if len(line) < 3:
                     return False
-            elif line[0] == 'L':
+            elif line[0] == "L":
                 if len(line) < 6:
                     return False
                 for i in (2, 4):
-                    if line[i] not in ('+', '-'):
+                    if line[i] not in ("+", "-"):
                         return False
-            elif line[0] == 'C':
+            elif line[0] == "C":
                 if len(line) < 7:
                     return False
                 for i in (2, 4):
-                    if line[i] not in ('+', '-'):
+                    if line[i] not in ("+", "-"):
                         return False
                 int(line[5])
-            elif line[0] == 'P':
+            elif line[0] == "P":
                 if len(line) < 4:
                     return False
             else:
                 return False
             found_valid_lines = True
         return found_valid_lines
 
@@ -827,75 +1030,77 @@
 @build_sniff_from_prefix
 class Gfa2(Text):
     """
     Graphical Fragment Assembly (GFA) 2.0
 
     https://github.com/GFA-spec/GFA-spec/blob/master/GFA2.md
     """
+
     file_ext = "gfa2"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('sample.gfa2')
         >>> Gfa2().sniff(fname)
         True
         >>> Gfa1().sniff(fname)
         False
         """
         found_valid_lines = False
         for line in iter_headers(file_prefix, "\t"):
-            if line[0].startswith('#'):
+            if line[0].startswith("#"):
                 continue
-            if line[0] == 'H':
-                return len(line) >= 2 and line[1] == 'VN:Z:2.0'
-            elif line[0] == 'S':
+            if line[0] == "H":
+                return len(line) >= 2 and line[1] == "VN:Z:2.0"
+            elif line[0] == "S":
                 if len(line) < 3:
                     return False
-            elif line[0] == 'F':
+            elif line[0] == "F":
                 if len(line) < 8:
                     return False
-            elif line[0] == 'E':
+            elif line[0] == "E":
                 if len(line) < 9:
                     return False
-            elif line[0] == 'G':
+            elif line[0] == "G":
                 if len(line) < 6:
                     return False
-            elif line[0] == 'O' or line[0] == 'U':
+            elif line[0] == "O" or line[0] == "U":
                 if len(line) < 3:
                     return False
             else:
                 return False
             found_valid_lines = True
         return found_valid_lines
 
 
 @build_sniff_from_prefix
 class Yaml(Text):
     """Yaml files"""
+
     file_ext = "yaml"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Try to load the string with the yaml module. If successful it's a yaml file.
+        Try to load the string with the yaml module. If successful it's a yaml file.
         """
         return self._looks_like_yaml(file_prefix)
 
-    def get_mime(self):
+    def get_mime(self) -> str:
         """Returns the mime type of the datatype"""
-        return 'application/yaml'
+        return "application/yaml"
 
-    def _yield_user_file_content(self, trans, from_dataset, filename, headers: Headers):
+    def _yield_user_file_content(self, trans, from_dataset: "DatasetInstance", filename: str, headers: Headers) -> IO:
         # Override non-standard application/yaml mediatype with
         # text/plain, so preview is shown in preview iframe,
         # instead of downloading the file.
         headers["content-type"] = "text/plain"
         return super()._yield_user_file_content(trans, from_dataset, filename, headers)
 
-    def _looks_like_yaml(self, file_prefix: FilePrefix):
+    def _looks_like_yaml(self, file_prefix: FilePrefix) -> bool:
         # Pattern used by SequenceSplitLocations
         if file_prefix.file_size < 50000 and not file_prefix.truncated:
             # If the file is small enough - don't guess just check.
             try:
                 item = yaml.safe_load(file_prefix.contents_header)
                 assert isinstance(item, (list, dict))
                 return True
@@ -907,7 +1112,302 @@
             try:
                 item = yaml.safe_load(file_start)
                 assert isinstance(item, (list, dict))
                 return True
             except yaml.YAMLError:
                 return False
             return False
+
+
+@build_sniff_from_prefix
+class BCSLmodel(Text):
+    """BioChemical Space Language model file"""
+
+    file_ext = "bcsl.model"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .bcsl.model format
+        """
+        reg = r"^#! rules|^#! inits|^#! definitions"
+        return re.search(reg, file_prefix.contents_header, re.MULTILINE) is not None
+
+
+@build_sniff_from_prefix
+class BCSLts(Json):
+    """BioChemical Space Language transition system file"""
+
+    file_ext = "bcsl.ts"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .bcsl.ts format
+        """
+        is_bcsl_ts = False
+        if self._looks_like_json(file_prefix):
+            is_bcsl_ts = self._looks_like_bcsl_ts(file_prefix)
+        return is_bcsl_ts
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
+        if not dataset.dataset.purged:
+            lines = "States: {}\nTransitions: {}\nUnique agents: {}\nInitial state: {}"
+            ts = json.load(open(dataset.file_name))
+            dataset.peek = lines.format(len(ts["nodes"]), len(ts["edges"]), len(ts["ordering"]), ts["initial"])
+            dataset.blurb = nice_size(dataset.get_size())
+        else:
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
+
+    def _looks_like_bcsl_ts(self, file_prefix: FilePrefix) -> bool:
+        content = open(file_prefix.filename).read()
+        keywords = ['"edges":', '"nodes":', '"ordering":', '"initial":']
+        if all(keyword in content for keyword in keywords):
+            return self._looks_like_json(file_prefix)
+        return False
+
+
+@build_sniff_from_prefix
+class StormSample(Text):
+    """
+    Storm PCTL parameter synthesis result file
+    containing probability function of parameters.
+    """
+
+    file_ext = "storm.sample"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .storm.sample format
+        """
+        keywords = ["Storm-pars", "Result (initial states)"]
+        return all(keyword in file_prefix.contents_header for keyword in keywords)
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
+        if not dataset.dataset.purged:
+            dataset.peek = "Storm-pars sample results."
+            dataset.blurb = nice_size(dataset.get_size())
+        else:
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
+
+
+@build_sniff_from_prefix
+class StormCheck(Text):
+    """
+    Storm PCTL model checking result file
+    containing boolean or numerical result.
+    """
+
+    file_ext = "storm.check"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .storm.check format
+        """
+        keywords = ["Storm ", "Result (for initial states)"]
+        return all(keyword in file_prefix.contents_header for keyword in keywords)
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
+        if not dataset.dataset.purged:
+            with open(dataset.file_name) as result:
+                answer = ""
+                for line in result:
+                    if "Result (for initial states):" in line:
+                        answer = line.split()[-1]
+                        break
+            dataset.peek = f"Model checking result: {answer}"
+            dataset.blurb = nice_size(dataset.get_size())
+        else:
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
+
+
+@build_sniff_from_prefix
+class CTLresult(Text):
+    """CTL model checking result"""
+
+    file_ext = "ctl.result"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .ctl.result format
+        """
+        keywords = ["Result:", "Number of satisfying states:"]
+        return all(keyword in file_prefix.contents_header for keyword in keywords)
+
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
+        if not dataset.dataset.purged:
+            with open(dataset.file_name) as result:
+                answer = ""
+                for line in result:
+                    if "Result:" in line:
+                        answer = line.split()[-1]
+            dataset.peek = f"Model checking result: {answer}"
+            dataset.blurb = nice_size(dataset.get_size())
+        else:
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
+
+
+@build_sniff_from_prefix
+class PithyaProperty(Text):
+    """Pithya CTL property format"""
+
+    file_ext = "pithya.property"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .pithya.property format
+        """
+        return re.search(r":\?[a-zA-Z0-9_]+[ ]*=", file_prefix.contents_header) is not None
+
+
+@build_sniff_from_prefix
+class PithyaModel(Text):
+    """Pithya model format"""
+
+    file_ext = "pithya.model"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .pithya.model format
+        """
+        keywords = ["VARS", "EQ", "THRES"]
+        return all(keyword in file_prefix.contents_header for keyword in keywords)
+
+
+@build_sniff_from_prefix
+class PithyaResult(Json):
+    """Pithya result format"""
+
+    file_ext = "pithya.result"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Determines whether the file is in .pithya.result format
+        """
+        is_pithya_result = False
+        if self._looks_like_json(file_prefix):
+            is_pithya_result = self._looks_like_pithya_result(file_prefix)
+        return is_pithya_result
+
+    def _looks_like_pithya_result(self, file_prefix: FilePrefix) -> bool:
+        content = open(file_prefix.filename).read()
+        keywords = ['"variables":', '"states":', '"parameter_values":', '"results":']
+        if all(keyword in content for keyword in keywords):
+            return self._looks_like_json(file_prefix)
+        return False
+
+
+@build_sniff_from_prefix
+class Castep(Text):
+    """Report on a CASTEP calculation"""
+
+    file_ext = "castep"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file is a CASTEP log
+
+        >>> from galaxy.datatypes.sniff import get_test_fname
+        >>> fname = get_test_fname('Si.castep')
+        >>> Castep().sniff(fname)
+        True
+        >>> fname = get_test_fname('Si.param')
+        >>> Castep().sniff(fname)
+        False
+        """
+        castep_header = [
+            "+-------------------------------------------------+",
+            "|                                                 |",
+            "|      CCC   AA    SSS  TTTTT  EEEEE  PPPP        |",
+            "|     C     A  A  S       T    E      P   P       |",
+            "|     C     AAAA   SS     T    EEE    PPPP        |",
+            "|     C     A  A     S    T    E      P           |",
+            "|      CCC  A  A  SSS     T    EEEEE  P           |",
+            "|                                                 |",
+            "+-------------------------------------------------+",
+        ]
+        handle = file_prefix.string_io()
+        for header_line in castep_header:
+            if handle.readline().strip() != header_line:
+                return False
+        return True
+
+
+@build_sniff_from_prefix
+class Param(Yaml):
+    """CASTEP parameter input file"""
+
+    file_ext = "param"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """
+        Modified version of the normal Yaml sniff that also checks
+        for a valid CASTEP task key-value pair, which is not case
+        sensitive
+
+        >>> from galaxy.datatypes.sniff import get_test_fname
+        >>> fname = get_test_fname('Si.param')
+        >>> Param().sniff(fname)
+        True
+        >>> fname = get_test_fname('Si.castep')
+        >>> Param().sniff(fname)
+        False
+        """
+        valid_tasks = [
+            "SINGLEPOINT",
+            "BANDSTRUCTURE",
+            "GEOMETRYOPTIMIZATION",
+            "GEOMETRYOPTIMISATION",
+            "MOLECULARDYNAMICS",
+            "OPTICS",
+            "PHONON",
+            "EFIELD",
+            "PHONON+EFIELD",
+            "TRANSITIONSTATESEARCH",
+            "MAGRES",
+            "ELNES",
+            "ELECTRONICSPECTROSCOPY",
+        ]
+
+        # check it looks like YAML
+        if not super().sniff_prefix(file_prefix):
+            return False
+
+        # check the TASK keyword is present
+        # and that it is set to a valid CASTEP task
+        pattern = re.compile(r"^TASK ?: ?([A-Z\+]*)$", flags=re.IGNORECASE | re.MULTILINE)
+        task = file_prefix.search(pattern)
+        return task and task.group(1).upper() in valid_tasks
+
+
+@build_sniff_from_prefix
+class FormattedDensity(Text):
+    """Final electron density from a CASTEP calculation written to an ASCII file"""
+
+    file_ext = "den_fmt"
+
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """Determines whether the file contains electron densities in the CASTEP den_fmt format
+
+        >>> from galaxy.datatypes.sniff import get_test_fname
+        >>> fname = get_test_fname('Si.den_fmt')
+        >>> FormattedDensity().sniff(fname)
+        True
+        >>> fname = get_test_fname('YbCuAs2.den_fmt')
+        >>> FormattedDensity().sniff(fname)
+        True
+        >>> fname = get_test_fname('Si.param')
+        >>> FormattedDensity().sniff(fname)
+        False
+        """
+        begin_header = "BEGIN header"
+        end_header = 'END header: data is "<a b c> charge" in units of electrons/grid_point * number'
+        grid_points = "of grid_points"
+        end_header_spin = 'END header: data is "<a b c> charge spin" in units of electrons/grid_point * nu'
+        grid_points_spin = "mber of grid_points"
+        handle = file_prefix.string_io()
+        lines = handle.readlines()
+        return lines[0].strip() == begin_header and (
+            (lines[9].strip() == end_header and lines[10].strip() == grid_points)
+            or (lines[9].strip() == end_header_spin and lines[10].strip() == grid_points_spin)
+        )
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/triples.py` & `galaxy-data-23.0.1/galaxy/datatypes/triples.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,192 +1,206 @@
 """
 Triple format classes
 """
 import logging
 import re
+from typing import TYPE_CHECKING
 
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     FilePrefix,
 )
 from . import (
     binary,
     data,
     text,
-    xml
+    xml,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
-TURTLE_PREFIX_PATTERN = re.compile(r'@prefix\s+[^:]*:\s+<[^>]*>\s\.')
-TURTLE_BASE_PATTERN = re.compile(r'@base\s+<[^>]*>\s\.')
+TURTLE_PREFIX_PATTERN = re.compile(r"@prefix\s+[^:]*:\s+<[^>]*>\s\.")
+TURTLE_BASE_PATTERN = re.compile(r"@base\s+<[^>]*>\s\.")
 
 
 class Triples(data.Data):
     """
     The abstract base class for the file format that can contain triples
     """
+
     edam_data = "data_0582"
     edam_format = "format_2376"
     file_ext = "triples"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Returns false and the user must manually set.
         """
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'Triple data'
+            dataset.blurb = "Triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class NTriples(data.Text, Triples):
     """
     The N-Triples triple data format
     """
+
     edam_format = "format_3256"
     file_ext = "nt"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # <http://example.org/dir/relfile> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://example.org/type> .
-        if re.compile(r'<[^>]*>\s<[^>]*>\s<[^>]*>\s\.').search(file_prefix.contents_header):
+        if re.compile(r"<[^>]*>\s<[^>]*>\s<[^>]*>\s\.").search(file_prefix.contents_header):
             return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'N-Triples triple data'
+            dataset.blurb = "N-Triples triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class N3(data.Text, Triples):
     """
     The N3 triple data format
     """
+
     edam_format = "format_3257"
     file_ext = "n3"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         """
         Returns false and the user must manually set.
         """
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'Notation-3 Triple data'
+            dataset.blurb = "Notation-3 Triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class Turtle(data.Text, Triples):
     """
     The Turtle triple data format
     """
+
     edam_format = "format_3255"
     file_ext = "ttl"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
         if file_prefix.search(TURTLE_PREFIX_PATTERN):
             return True
 
         if file_prefix.search(TURTLE_BASE_PATTERN):
             return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'Turtle triple data'
+            dataset.blurb = "Turtle triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 # TODO: we might want to look at rdflib or a similar, larger lib/egg
 @build_sniff_from_prefix
 class Rdf(xml.GenericXml, Triples):
     """
     Resource Description Framework format (http://www.w3.org/RDF/).
     """
+
     edam_format = "format_3261"
     file_ext = "rdf"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         # <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" ...
-        match = re.compile(r'xmlns:([^=]*)="http://www.w3.org/1999/02/22-rdf-syntax-ns#"').search(file_prefix.contents_header)
+        match = re.compile(r'xmlns:([^=]*)="http://www.w3.org/1999/02/22-rdf-syntax-ns#"').search(
+            file_prefix.contents_header
+        )
         if match and (f"{match.group(1)}:RDF") in file_prefix.contents_header:
             return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'RDF/XML triple data'
+            dataset.blurb = "RDF/XML triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @build_sniff_from_prefix
 class Jsonld(text.Json, Triples):
     """
     The JSON-LD data format
     """
+
     # format not defined in edam so we use the json format number
     edam_format = "format_3464"
     file_ext = "jsonld"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         if self._looks_like_json(file_prefix):
-            if "\"@id\"" in file_prefix.contents_header or "\"@context\"" in file_prefix.contents_header:
+            if '"@id"' in file_prefix.contents_header or '"@context"' in file_prefix.contents_header:
                 return True
         return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'JSON-LD triple data'
+            dataset.blurb = "JSON-LD triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class HDT(binary.Binary, Triples):
     """
     The HDT triple data format
     """
+
     edam_format = "format_2376"
     file_ext = "hdt"
 
-    def sniff(self, filename):
+    def sniff(self, filename: str) -> bool:
         with open(filename, "rb") as f:
             if f.read(4) == b"$HDT":
                 return True
+        return False
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'HDT triple data'
+            dataset.blurb = "HDT triple data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/upload_util.py` & `galaxy-data-23.0.1/galaxy/datatypes/upload_util.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 import os
-from typing import NamedTuple, Optional
+from typing import (
+    NamedTuple,
+    Optional,
+)
 
-from galaxy.datatypes import data, sniff
-from galaxy.util.checkers import (
-    check_binary,
-    is_single_file_zip,
-    is_zip,
+from galaxy.datatypes import (
+    data,
+    sniff,
 )
+from galaxy.util.checkers import is_single_file_zip
 
 
 class UploadProblemException(Exception):
     pass
 
 
 class HandleUploadResponse(NamedTuple):
@@ -37,71 +39,80 @@
     convert_to_posix_lines: bool,
     convert_spaces_to_tabs: bool,
 ) -> HandleUploadResponse:
     stdout = None
     converted_path = None
     multi_file_zip = False
 
-    # Does the first 1K contain a null?
-    is_binary = check_binary(path)
+    # Does the first 1MB look like binary content?
+    file_prefix = sniff.FilePrefix(path, auto_decompress=auto_decompress)
+    is_binary = file_prefix.binary
 
     converted_newlines, converted_spaces = False, False
 
     # Decompress if needed/desired and determine/validate filetype. If a keep-compressed datatype is explicitly selected
     # or if autodetection is selected and the file sniffs as a keep-compressed datatype, it will not be decompressed.
     if not link_data_only:
-        if auto_decompress and is_zip(path) and not is_single_file_zip(path):
+        if auto_decompress and file_prefix.compressed_format == "zip" and not is_single_file_zip(path):
             multi_file_zip = True
         try:
-            ext, converted_path, compression_type, converted_newlines, converted_spaces = sniff.handle_uploaded_dataset_file_internal(
-                path,
+            (
+                ext,
+                converted_path,
+                compression_type,
+                converted_newlines,
+                converted_spaces,
+            ) = sniff.handle_uploaded_dataset_file_internal(
+                file_prefix,
                 registry,
                 ext=requested_ext,
                 tmp_prefix=tmp_prefix,
                 tmp_dir=tmp_dir,
                 in_place=in_place,
                 check_content=check_content,
-                is_binary=is_binary,
-                auto_decompress=auto_decompress,
-                uploaded_file_ext=os.path.splitext(name)[1].lower().lstrip('.'),
+                uploaded_file_ext=os.path.splitext(name)[1].lower().lstrip("."),
                 convert_to_posix_lines=convert_to_posix_lines,
                 convert_spaces_to_tabs=convert_spaces_to_tabs,
             )
         except sniff.InappropriateDatasetContentError as exc:
             raise UploadProblemException(exc)
-    elif requested_ext == 'auto':
-        ext = sniff.guess_ext(path, registry.sniff_order, is_binary=is_binary)
+    elif requested_ext == "auto":
+        ext = sniff.guess_ext(file_prefix, registry.sniff_order)
     else:
         ext = requested_ext
 
     # The converted path will be the same as the input path if no conversion was done (or in-place conversion is used)
     converted_path = None if converted_path == path else converted_path
 
     # Validate datasets where the filetype was explicitly set using the filetype's sniffer (if any)
-    if requested_ext != 'auto':
+    if requested_ext != "auto":
         datatype = registry.get_datatype_by_extension(requested_ext)
         # Enable sniffer "validate mode" (prevents certain sniffers from disabling themselves)
-        if check_content and hasattr(datatype, 'sniff'):
+        if check_content and hasattr(datatype, "sniff"):
             try:
                 is_of_datatype = datatype.sniff(path)
             except Exception:
                 is_of_datatype = False
             if not is_of_datatype:
                 stdout = f"Warning: The file 'Type' was set to '{requested_ext}' but the file does not appear to be of that type"
 
     # Handle unsniffable binaries
-    if is_binary and ext == 'binary':
-        upload_ext = os.path.splitext(name)[1].lower().lstrip('.')
+    if is_binary and ext == "binary":
+        upload_ext = os.path.splitext(name)[1].lower().lstrip(".")
         if registry.is_extension_unsniffable_binary(upload_ext):
-            stdout = ("Warning: The file's datatype cannot be determined from its contents and was guessed based on"
-                     " its extension, to avoid this warning, manually set the file 'Type' to '{ext}' when uploading"
-                     " this type of file".format(ext=upload_ext))
+            stdout = (
+                "Warning: The file's datatype cannot be determined from its contents and was guessed based on"
+                " its extension, to avoid this warning, manually set the file 'Type' to '{ext}' when uploading"
+                " this type of file".format(ext=upload_ext)
+            )
             ext = upload_ext
         else:
-            stdout = ("The uploaded binary file format cannot be determined automatically, please set the file 'Type'"
-                      " manually")
+            stdout = (
+                "The uploaded binary file format cannot be determined automatically, please set the file 'Type'"
+                " manually"
+            )
 
     datatype = registry.get_datatype_by_extension(ext)
-    if multi_file_zip and not getattr(datatype, 'compressed', False):
-        stdout = 'ZIP file contained more than one file, only the first file was added to Galaxy.'
+    if multi_file_zip and not getattr(datatype, "compressed", False):
+        stdout = "ZIP file contained more than one file, only the first file was added to Galaxy."
 
     return HandleUploadResponse(stdout, ext, datatype, is_binary, converted_path, converted_newlines, converted_spaces)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/util/gff_util.py` & `galaxy-data-23.0.1/galaxy/datatypes/util/gff_util.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,60 @@
 """
 Provides utilities for working with GFF files.
 """
 import copy
 
-from bx.intervals.io import GenomicInterval, GenomicIntervalReader, MissingFieldError, NiceReaderWrapper, ParseError
-from bx.tabular.io import Comment, Header
+from bx.intervals.io import (
+    GenomicInterval,
+    GenomicIntervalReader,
+    MissingFieldError,
+    NiceReaderWrapper,
+    ParseError,
+)
+from bx.tabular.io import (
+    Comment,
+    Header,
+)
 
 from galaxy.util import unicodify
 
-FASTA_DIRECTIVE = '##FASTA'
+FASTA_DIRECTIVE = "##FASTA"
 
 
 class GFFInterval(GenomicInterval):
     """
     A GFF interval, including attributes. If file is strictly a GFF file,
     only attribute is 'group.'
     """
 
-    def __init__(self, reader, fields, chrom_col=0, feature_col=2, start_col=3, end_col=4,
-                 strand_col=6, score_col=5, default_strand='.', fix_strand=False):
+    def __init__(
+        self,
+        reader,
+        fields,
+        chrom_col=0,
+        feature_col=2,
+        start_col=3,
+        end_col=4,
+        strand_col=6,
+        score_col=5,
+        default_strand=".",
+        fix_strand=False,
+    ):
         # HACK: GFF format allows '.' for strand but GenomicInterval does not. To get around this,
         # temporarily set strand and then unset after initing GenomicInterval.
         unknown_strand = False
-        if not fix_strand and fields[strand_col] == '.':
+        if not fix_strand and fields[strand_col] == ".":
             unknown_strand = True
-            fields[strand_col] = '+'
-        GenomicInterval.__init__(self, reader, fields, chrom_col, start_col, end_col, strand_col,
-                                 default_strand, fix_strand=fix_strand)
+            fields[strand_col] = "+"
+        GenomicInterval.__init__(
+            self, reader, fields, chrom_col, start_col, end_col, strand_col, default_strand, fix_strand=fix_strand
+        )
         if unknown_strand:
-            self.strand = '.'
-            self.fields[strand_col] = '.'
+            self.strand = "."
+            self.fields[strand_col] = "."
 
         # Handle feature, score column.
         self.feature_col = feature_col
         if self.nfields <= self.feature_col:
             raise MissingFieldError("No field for feature_col (%d)" % feature_col)
         self.feature = self.fields[self.feature_col]
         self.score_col = score_col
@@ -43,87 +64,135 @@
 
         # GFF attributes.
         if self.nfields < 9:
             raise MissingFieldError("No field for attribute column (8)")
         self.attributes = parse_gff_attributes(fields[8])
 
     def copy(self):
-        return GFFInterval(self.reader, list(self.fields), self.chrom_col, self.feature_col, self.start_col,
-                           self.end_col, self.strand_col, self.score_col, self.strand)
+        return GFFInterval(
+            self.reader,
+            list(self.fields),
+            self.chrom_col,
+            self.feature_col,
+            self.start_col,
+            self.end_col,
+            self.strand_col,
+            self.score_col,
+            self.strand,
+        )
 
 
 class GFFFeature(GFFInterval):
     """
     A GFF feature, which can include multiple intervals.
     """
 
-    def __init__(self, reader, chrom_col=0, feature_col=2, start_col=3, end_col=4,
-                 strand_col=6, score_col=5, default_strand='.', fix_strand=False, intervals=None,
-                 raw_size=0):
+    def __init__(
+        self,
+        reader,
+        chrom_col=0,
+        feature_col=2,
+        start_col=3,
+        end_col=4,
+        strand_col=6,
+        score_col=5,
+        default_strand=".",
+        fix_strand=False,
+        intervals=None,
+        raw_size=0,
+    ):
         # Use copy so that first interval and feature do not share fields.
         intervals = intervals or []
-        GFFInterval.__init__(self, reader, copy.deepcopy(intervals[0].fields), chrom_col, feature_col,
-                             start_col, end_col, strand_col, score_col, default_strand,
-                             fix_strand=fix_strand)
+        GFFInterval.__init__(
+            self,
+            reader,
+            copy.deepcopy(intervals[0].fields),
+            chrom_col,
+            feature_col,
+            start_col,
+            end_col,
+            strand_col,
+            score_col,
+            default_strand,
+            fix_strand=fix_strand,
+        )
         self.intervals = intervals
         self.raw_size = raw_size
         # Use intervals to set feature attributes.
         for interval in self.intervals:
             # Error checking. NOTE: intervals need not share the same strand.
             if interval.chrom != self.chrom:
-                raise ValueError("interval chrom does not match self chrom: %s != %s" %
-                                 (interval.chrom, self.chrom))
+                raise ValueError(f"interval chrom does not match self chrom: {interval.chrom} != {self.chrom}")
             # Set start, end of interval.
             if interval.start < self.start:
                 self.start = interval.start
             if interval.end > self.end:
                 self.end = interval.end
 
     def name(self):
-        """ Returns feature's name. """
+        """Returns feature's name."""
         name = None
         # Preference for name: GTF, GFF3, GFF.
         for attr_name in [
-                # GTF:
-                'gene_id', 'transcript_id',
-                # GFF3:
-                'ID', 'id',
-                # GFF (TODO):
-                'group']:
+            # GTF:
+            "gene_id",
+            "transcript_id",
+            # GFF3:
+            "ID",
+            "id",
+            # GFF (TODO):
+            "group",
+        ]:
             name = self.attributes.get(attr_name, None)
             if name is not None:
                 break
         return name
 
     def copy(self):
         intervals_copy = []
         for interval in self.intervals:
             intervals_copy.append(interval.copy())
-        return GFFFeature(self.reader, self.chrom_col, self.feature_col, self.start_col, self.end_col, self.strand_col,
-                          self.score_col, self.strand, intervals=intervals_copy)
+        return GFFFeature(
+            self.reader,
+            self.chrom_col,
+            self.feature_col,
+            self.start_col,
+            self.end_col,
+            self.strand_col,
+            self.score_col,
+            self.strand,
+            intervals=intervals_copy,
+        )
 
     def lines(self):
         lines = []
         for interval in self.intervals:
-            lines.append('\t'.join(interval.fields))
+            lines.append("\t".join(interval.fields))
         return lines
 
 
 class GFFIntervalToBEDReaderWrapper(NiceReaderWrapper):
     """
     Reader wrapper that reads GFF intervals/lines and automatically converts
     them to BED format.
     """
 
     def parse_row(self, line):
         # HACK: this should return a GFF interval, but bx-python operations
         # require GenomicInterval objects and subclasses will not work.
-        interval = GenomicInterval(self, line.split("\t"), self.chrom_col, self.start_col,
-                                   self.end_col, self.strand_col, self.default_strand,
-                                   fix_strand=self.fix_strand)
+        interval = GenomicInterval(
+            self,
+            line.split("\t"),
+            self.chrom_col,
+            self.start_col,
+            self.end_col,
+            self.strand_col,
+            self.default_strand,
+            fix_strand=self.fix_strand,
+        )
         interval = convert_gff_coords_to_bed(interval)
         return interval
 
 
 class GFFReaderWrapper(NiceReaderWrapper):
     """
     Reader wrapper for GFF files.
@@ -134,42 +203,70 @@
        or GTF (via gene_id/transcript id);
     2. convert coordinates from GFF format--starting and ending coordinates
        are 1-based, closed--to the 'traditional'/BED interval format--0 based,
        half-open. This is useful when using GFF files as inputs to tools that
        expect traditional interval format.
     """
 
-    def __init__(self, reader, chrom_col=0, feature_col=2, start_col=3,
-                 end_col=4, strand_col=6, score_col=5, fix_strand=False, convert_to_bed_coord=False, **kwargs):
-        NiceReaderWrapper.__init__(self, reader, chrom_col=chrom_col, start_col=start_col, end_col=end_col,
-                                   strand_col=strand_col, fix_strand=fix_strand, **kwargs)
+    def __init__(
+        self,
+        reader,
+        chrom_col=0,
+        feature_col=2,
+        start_col=3,
+        end_col=4,
+        strand_col=6,
+        score_col=5,
+        fix_strand=False,
+        convert_to_bed_coord=False,
+        **kwargs,
+    ):
+        NiceReaderWrapper.__init__(
+            self,
+            reader,
+            chrom_col=chrom_col,
+            start_col=start_col,
+            end_col=end_col,
+            strand_col=strand_col,
+            fix_strand=fix_strand,
+            **kwargs,
+        )
         self.feature_col = feature_col
         self.score_col = score_col
         self.convert_to_bed_coord = convert_to_bed_coord
         self.last_line = None
         self.cur_offset = 0
         self.seed_interval = None
         self.seed_interval_line_len = 0
         self.__end_of_intervals = False
 
     def parse_row(self, line):
-        interval = GFFInterval(self, line.split("\t"), self.chrom_col, self.feature_col,
-                               self.start_col, self.end_col, self.strand_col, self.score_col,
-                               self.default_strand, fix_strand=self.fix_strand)
+        interval = GFFInterval(
+            self,
+            line.split("\t"),
+            self.chrom_col,
+            self.feature_col,
+            self.start_col,
+            self.end_col,
+            self.strand_col,
+            self.score_col,
+            self.default_strand,
+            fix_strand=self.fix_strand,
+        )
         return interval
 
     def __next__(self):
-        """ Returns next GFFFeature. """
+        """Returns next GFFFeature."""
 
         #
         # Helper function.
         #
 
         def handle_parse_error(e):
-            """ Actions to take when ParseError found. """
+            """Actions to take when ParseError found."""
             if self.outstream:
                 if self.print_delegate and callable(self.print_delegate):
                     self.print_delegate(self.outstream, e, self)
             self.skipped += 1
             # no reason to stuff an entire bad file into memmory
             if self.skipped < 10:
                 self.skipped_lines.append((self.linenum, self.current_line, unicodify(e)))
@@ -203,19 +300,19 @@
             return_val = self.seed_interval
             return_val.raw_size = len(self.current_line)
             self.seed_interval = None
             self.seed_interval_line_len = 0
             return return_val
 
         # Initialize feature identifier from seed.
-        feature_group = self.seed_interval.attributes.get('group', None)  # For GFF
+        feature_group = self.seed_interval.attributes.get("group", None)  # For GFF
         # For GFF3
-        feature_id = self.seed_interval.attributes.get('ID', None)
+        feature_id = self.seed_interval.attributes.get("ID", None)
         # For GTF.
-        feature_transcript_id = self.seed_interval.attributes.get('transcript_id', None)
+        feature_transcript_id = self.seed_interval.attributes.get("transcript_id", None)
 
         # Read all intervals associated with seed.
         feature_intervals = []
         feature_intervals.append(self.seed_interval)
         while True:
             try:
                 interval = super(GenomicIntervalReader, self).__next__()
@@ -235,25 +332,25 @@
                 if self.current_line.rstrip() == FASTA_DIRECTIVE:
                     self.__end_of_intervals = True
                     break
                 continue
 
             # Determine if interval is part of feature.
             part_of = False
-            group = interval.attributes.get('group', None)
+            group = interval.attributes.get("group", None)
             # GFF test:
             if group and feature_group == group:
                 part_of = True
             # GFF3 test:
-            parent_id = interval.attributes.get('Parent', None)
-            cur_id = interval.attributes.get('ID', None)
+            parent_id = interval.attributes.get("Parent", None)
+            cur_id = interval.attributes.get("ID", None)
             if (cur_id and cur_id == feature_id) or (parent_id and parent_id == feature_id):
                 part_of = True
             # GTF test:
-            transcript_id = interval.attributes.get('transcript_id', None)
+            transcript_id = interval.attributes.get("transcript_id", None)
             if transcript_id and transcript_id == feature_transcript_id:
                 part_of = True
 
             # If interval is not part of feature, clean up and break.
             if not part_of:
                 # Adjust raw size because current line is not part of feature.
                 raw_size -= len(self.current_line)
@@ -263,18 +360,27 @@
             feature_intervals.append(interval)
 
         # Last interval read is the seed for the next interval.
         self.seed_interval = interval
         self.seed_interval_line_len = len(self.current_line)
 
         # Return feature.
-        feature = GFFFeature(self, self.chrom_col, self.feature_col, self.start_col,
-                             self.end_col, self.strand_col, self.score_col,
-                             self.default_strand, fix_strand=self.fix_strand,
-                             intervals=feature_intervals, raw_size=raw_size)
+        feature = GFFFeature(
+            self,
+            self.chrom_col,
+            self.feature_col,
+            self.start_col,
+            self.end_col,
+            self.strand_col,
+            self.score_col,
+            self.default_strand,
+            fix_strand=self.fix_strand,
+            intervals=feature_intervals,
+            raw_size=raw_size,
+        )
 
         # Convert to BED coords?
         if self.convert_to_bed_coord:
             convert_gff_coords_to_bed(feature)
 
         return feature
 
@@ -331,78 +437,78 @@
     attributes_list = attr_str.split(";")
     attributes = {}
     for name_value_pair in attributes_list:
         # Try splitting by '=' (GFF3) first because spaces are allowed in GFF3
         # attribute; next, try double quotes for GTF.
         pair = name_value_pair.strip().split("=")
         if len(pair) == 1:
-            pair = name_value_pair.strip().split("\"")
+            pair = name_value_pair.strip().split('"')
         if len(pair) == 1:
             # Could not split for some reason -- raise exception?
             continue
-        if pair == '':
+        if pair == "":
             continue
         name = pair[0].strip()
-        if name == '':
+        if name == "":
             continue
         # Need to strip double quote from values
-        value = pair[1].strip(" \"")
+        value = pair[1].strip(' "')
         attributes[name] = value
 
     if len(attributes) == 0:
         # Could not split attributes string, so entire string must be
         # 'group' attribute. This is the case for strictly GFF files.
-        attributes['group'] = attr_str
+        attributes["group"] = attr_str
     return attributes
 
 
 def parse_gff3_attributes(attr_str):
     """
     Parses a GFF3 attribute string and returns a dictionary of name-value
     pairs. The general format for a GFF3 attributes string is
 
         name1=value1;name2=value2
     """
     attributes_list = attr_str.split(";")
     attributes = {}
     for tag_value_pair in attributes_list:
         tag_value_pair = tag_value_pair.strip()
-        if tag_value_pair == '':
+        if tag_value_pair == "":
             continue
         pair = tag_value_pair.split("=")
         if len(pair) == 1:
             raise Exception(f"Attribute '{tag_value_pair}' does not contain a '='")
-        if pair == '':
+        if pair == "":
             continue
         tag = pair[0].strip()
-        if tag == '':
+        if tag == "":
             raise Exception(f"Empty tag in attribute '{tag_value_pair}'")
         value = pair[1].strip()
         attributes[tag] = value
     return attributes
 
 
 def gff_attributes_to_str(attrs, gff_format):
     """
     Convert GFF attributes to string. Supported formats are GFF3, GTF.
     """
-    if gff_format == 'GTF':
+    if gff_format == "GTF":
         format_string = '%s "%s"'
         # Convert group (GFF) and ID, parent (GFF3) attributes to transcript_id, gene_id
         id_attr = None
-        if 'group' in attrs:
-            id_attr = 'group'
-        elif 'ID' in attrs:
-            id_attr = 'ID'
-        elif 'Parent' in attrs:
-            id_attr = 'Parent'
+        if "group" in attrs:
+            id_attr = "group"
+        elif "ID" in attrs:
+            id_attr = "ID"
+        elif "Parent" in attrs:
+            id_attr = "Parent"
         if id_attr:
-            attrs['transcript_id'] = attrs['gene_id'] = attrs[id_attr]
-    elif gff_format == 'GFF3':
-        format_string = '%s=%s'
+            attrs["transcript_id"] = attrs["gene_id"] = attrs[id_attr]
+    elif gff_format == "GFF3":
+        format_string = "%s=%s"
     attrs_strs = []
     for name, value in attrs.items():
         attrs_strs.append(format_string % (name, value))
     return " ; ".join(attrs_strs)
 
 
 def read_unordered_gtf(iterator, strict=False):
@@ -410,40 +516,41 @@
     Returns GTF features found in an iterator. GTF lines need not be ordered
     or clustered for reader to work. Reader returns GFFFeature objects sorted
     by transcript_id, chrom, and start position.
     """
 
     # -- Get function that generates line/feature key. --
     def get_transcript_id(fields):
-        return parse_gff_attributes(fields[8])['transcript_id']
+        return parse_gff_attributes(fields[8])["transcript_id"]
+
     if strict:
         # Strict GTF parsing uses transcript_id only to group lines into feature.
         key_fn = get_transcript_id
     else:
         # Use lenient parsing where chromosome + transcript_id is the key. This allows
         # transcripts with same ID on different chromosomes; this occurs in some popular
         # datasources, such as RefGenes in UCSC.
         def key_fn(fields):
             return f"{fields[0]}_{get_transcript_id(fields)}"
 
     # Aggregate intervals by transcript_id and collect comments.
     feature_intervals = {}
     comments = []
     for line in iterator:
-        if line.startswith('#'):
+        if line.startswith("#"):
             comments.append(Comment(line))
             continue
 
-        line_key = key_fn(line.split('\t'))
+        line_key = key_fn(line.split("\t"))
         if line_key in feature_intervals:
             feature = feature_intervals[line_key]
         else:
             feature = []
             feature_intervals[line_key] = feature
-        feature.append(GFFInterval(None, line.split('\t')))
+        feature.append(GFFInterval(None, line.split("\t")))
 
     # Create features.
     chroms_features = {}
     for intervals in feature_intervals.values():
         # Sort intervals by start position.
         intervals.sort(key=lambda _: _.start)
         feature = GFFFeature(None, intervals=intervals)
@@ -461,7 +568,21 @@
     # Ideally, then comments would be associated with features and output
     # just before feature/line.
     yield from comments
 
     for chrom_features in chroms_features_sorted:
         for feature in chrom_features:
             yield feature
+
+
+__all__ = (
+    "GFFInterval",
+    "GFFFeature",
+    "GFFIntervalToBEDReaderWrapper",
+    "GFFReaderWrapper",
+    "convert_bed_coords_to_gff",
+    "convert_gff_coords_to_bed",
+    "parse_gff_attributes",
+    "parse_gff3_attributes",
+    "gff_attributes_to_str",
+    "read_unordered_gtf",
+)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/util/maf_utilities.py` & `galaxy-data-23.0.1/galaxy/datatypes/util/maf_utilities.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,16 +17,16 @@
 import bx.interval_index_file
 import bx.intervals
 
 maketrans = str.maketrans
 
 log = logging.getLogger(__name__)
 
-GAP_CHARS = ['-']
-SRC_SPLIT_CHAR = '.'
+GAP_CHARS = ["-"]
+SRC_SPLIT_CHAR = "."
 
 
 def src_split(src):
     fields = src.split(SRC_SPLIT_CHAR, 1)
     spec = fields.pop(0)
     if fields:
         chrom = fields.pop(0)
@@ -51,18 +51,18 @@
 
 
 def tool_fail(msg="Unknown Error"):
     sys.exit(f"Fatal Error: {msg}")
 
 
 class TempFileHandler:
-    '''
+    """
     Handles creating, opening, closing, and deleting of Temp files, with a
     maximum number of files open at one time.
-    '''
+    """
 
     DEFAULT_MAX_OPEN_FILES = max(resource.getrlimit(resource.RLIMIT_NOFILE)[0] / 2, 1)
 
     def __init__(self, max_open_files=None, **kwds):
         if max_open_files is None:
             max_open_files = self.DEFAULT_MAX_OPEN_FILES
         self.max_open_files = max_open_files
@@ -76,15 +76,15 @@
         else:
             if self.max_open_files:
                 while len(self.open_file_indexes) >= self.max_open_files:
                     self.close(self.open_file_indexes[0])
             if index is None:
                 index = len(self.files)
                 temp_kwds = dict(self.kwds)
-                temp_kwds['delete'] = False
+                temp_kwds["delete"] = False
                 temp_kwds.update(kwds)
                 # Being able to use delete=True here, would simplify a bit,
                 # but we support python2.4 in these tools
                 while True:
                     try:
                         tmp_file = tempfile.NamedTemporaryFile(**temp_kwds)
                         filename = tmp_file.name
@@ -92,19 +92,19 @@
                     except OSError as e:
                         if self.open_file_indexes and e.errno == EMFILE:
                             self.max_open_files = len(self.open_file_indexes)
                             self.close(self.open_file_indexes[0])
                         else:
                             raise e
                 tmp_file.close()
-                self.files.append(open(filename, 'r+'))
+                self.files.append(open(filename, "r+"))
             else:
                 while True:
                     try:
-                        self.files[index] = open(self.files[index].name, 'r+')
+                        self.files[index] = open(self.files[index].name, "r+")
                         break
                     except OSError as e:
                         if self.open_file_indexes and e.errno == EMFILE:
                             self.max_open_files = len(self.open_file_indexes)
                             self.close(self.open_file_indexes[0])
                         else:
                             raise e
@@ -130,20 +130,24 @@
     def __del__(self):
         for i in range(len(self.files)):
             self.close(i, delete=True)
 
 
 # an object corresponding to a reference layered alignment
 class RegionAlignment:
-
     DNA_COMPLEMENT = maketrans("ACGTacgt", "TGCAtgca")
     MAX_SEQUENCE_SIZE = sys.maxsize  # Maximum length of sequence allowed
 
     def __init__(self, size, species=None, temp_file_handler=None):
-        assert size <= self.MAX_SEQUENCE_SIZE, "Maximum length allowed for an individual sequence has been exceeded (%i > %i)." % (size, self.MAX_SEQUENCE_SIZE)
+        assert (
+            size <= self.MAX_SEQUENCE_SIZE
+        ), "Maximum length allowed for an individual sequence has been exceeded (%i > %i)." % (
+            size,
+            self.MAX_SEQUENCE_SIZE,
+        )
         species = species or []
         self.size = size
         if not temp_file_handler:
             temp_file_handler = TempFileHandler()
         self.temp_file_handler = temp_file_handler
         self.sequences = {}
         if not isinstance(species, list):
@@ -184,14 +188,15 @@
         return "".join(complement)
 
     # sets a position for a species
     def set_position(self, index, species, base):
         if len(base) != 1:
             raise Exception("A genomic position can only have a length of 1.")
         return self.set_range(index, species, base)
+
     # sets a range for a species
 
     def set_range(self, index, species, bases):
         if index >= self.size or index < 0:
             raise Exception("Your index (%i) is out of range (0 - %i)." % (index, self.size - 1))
         if len(bases) == 0:
             raise Exception("A set of genomic positions can only have a positive length.")
@@ -208,39 +213,39 @@
         elif not isinstance(species, list):
             species = [species]
         for spec in species:
             self.temp_file_handler.flush(self.sequences[spec])
 
 
 class GenomicRegionAlignment(RegionAlignment):
-
     def __init__(self, start, end, species=None, temp_file_handler=None):
         species = species or []
         RegionAlignment.__init__(self, end - start, species, temp_file_handler=temp_file_handler)
         self.start = start
         self.end = end
 
 
 class SplicedAlignment:
-
     DNA_COMPLEMENT = maketrans("ACGTacgt", "TGCAtgca")
 
     def __init__(self, exon_starts, exon_ends, species=None, temp_file_handler=None):
         species = species or []
         if not isinstance(exon_starts, list):
             exon_starts = [exon_starts]
         if not isinstance(exon_ends, list):
             exon_ends = [exon_ends]
         assert len(exon_starts) == len(exon_ends), "The number of starts does not match the number of sizes."
         self.exons = []
         if not temp_file_handler:
             temp_file_handler = TempFileHandler()
         self.temp_file_handler = temp_file_handler
         for i in range(len(exon_starts)):
-            self.exons.append(GenomicRegionAlignment(exon_starts[i], exon_ends[i], species, temp_file_handler=temp_file_handler))
+            self.exons.append(
+                GenomicRegionAlignment(exon_starts[i], exon_ends[i], species, temp_file_handler=temp_file_handler)
+            )
 
     # returns the names for species found in alignment, skipping names as requested
     def get_species_names(self, skip=None):
         skip = skip or []
         if not isinstance(skip, list):
             skip = [skip]
         names = []
@@ -286,21 +291,21 @@
 # Open a MAF index using a UID
 def maf_index_by_uid(maf_uid, index_location_file):
     for line in open(index_location_file):
         try:
             # read each line, if not enough fields, go to next line
             if line[0:1] == "#":
                 continue
-            fields = line.split('\t')
+            fields = line.split("\t")
             if maf_uid == fields[1]:
                 try:
                     maf_files = fields[4].replace("\n", "").replace("\r", "").split(",")
                     return bx.align.maf.MultiIndexed(maf_files, keep_open=True, parse_e_rows=False)
                 except Exception as e:
-                    raise Exception(f'MAF UID ({maf_uid}) found, but configuration appears to be malformed: {e}')
+                    raise Exception(f"MAF UID ({maf_uid}) found, but configuration appears to be malformed: {e}")
         except Exception:
             pass
     return None
 
 
 # return ( index, temp_index_filename ) for user maf, if available, or build one and return it, return None when no tempfile is created
 def open_or_build_maf_index(maf_file, index_filename, species=None):
@@ -344,26 +349,29 @@
                         # this likely only occurs when parse_e_rows is True?
                         # could a species exist as only e rows? should the
                     if forward_strand_end > forward_strand_start:
                         # require positive length; i.e. certain lines have start = end = 0 and cannot be indexed
                         indexes.add(c.src, forward_strand_start, forward_strand_end, pos, max=c.src_size)
     except Exception as e:
         # most likely a bad MAF
-        log.debug(f'Building MAF index on {filename} failed: {e}')
+        log.debug(f"Building MAF index on {filename} failed: {e}")
         return (None, [], {}, 0)
     return (indexes, species, species_chromosomes, blocks)
 
 
 # builds and returns ( index, index_filename ) for specified maf_file
 def build_maf_index(maf_file, species=None):
     indexes, *_ = build_maf_index_species_chromosomes(maf_file, species)
     if indexes is not None:
-        with tempfile.NamedTemporaryFile(mode='w', delete=False) as index:
+        with tempfile.NamedTemporaryFile(mode="w", delete=False) as index:
             indexes.write(index)
-        return (bx.align.maf.Indexed(maf_file, index_filename=index.name, keep_open=True, parse_e_rows=False), index.name)
+        return (
+            bx.align.maf.Indexed(maf_file, index_filename=index.name, keep_open=True, parse_e_rows=False),
+            index.name,
+        )
     return (None, None)
 
 
 def component_overlaps_region(c, region):
     if c is None:
         return False
     start, end = c.get_forward_strand_start(), c.get_forward_strand_end()
@@ -416,37 +424,47 @@
 def orient_block_by_region(block, src, region, force_strand=None):
     # loop through components matching src,
     # make sure each of these components overlap region
     # cache strand for each of overlaping regions
     # if force_strand / region.strand not in strand cache, reverse complement
     # we could have 2 sequences with same src, overlapping region, on different strands, this would cause no reverse_complementing
     strands = [c.strand for c in iter_components_by_src(block, src) if component_overlaps_region(c, region)]
-    if strands and (force_strand is None and region.strand not in strands) or (force_strand is not None and force_strand not in strands):
+    if (
+        strands
+        and (force_strand is None and region.strand not in strands)
+        or (force_strand is not None and force_strand not in strands)
+    ):
         block = block.reverse_complement()
     return block
 
 
 def get_oriented_chopped_blocks_for_region(index, src, region, species=None, mincols=0, force_strand=None):
-    for block, _, _ in get_oriented_chopped_blocks_with_index_offset_for_region(index, src, region, species, mincols, force_strand):
+    for block, _, _ in get_oriented_chopped_blocks_with_index_offset_for_region(
+        index, src, region, species, mincols, force_strand
+    ):
         yield block
 
 
-def get_oriented_chopped_blocks_with_index_offset_for_region(index, src, region, species=None, mincols=0, force_strand=None):
+def get_oriented_chopped_blocks_with_index_offset_for_region(
+    index, src, region, species=None, mincols=0, force_strand=None
+):
     for block, idx, offset in get_chopped_blocks_with_index_offset_for_region(index, src, region, species, mincols):
         yield orient_block_by_region(block, src, region, force_strand), idx, offset
 
 
 # split a block with multiple occurances of src into one block per src
 def iter_blocks_split_by_src(block, src):
     for src_c in iter_components_by_src(block, src):
         new_block = bx.align.Alignment(score=block.score, attributes=deepcopy(block.attributes))
         new_block.text_size = block.text_size
         for c in block.components:
             if c == src_c or c.src != src:
-                new_block.add_component(deepcopy(c))  # components have reference to alignment, don't want to lose reference to original alignment block in original components
+                new_block.add_component(
+                    deepcopy(c)
+                )  # components have reference to alignment, don't want to lose reference to original alignment block in original components
         yield new_block
 
 
 # split a block into multiple blocks with all combinations of a species appearing only once per block
 def iter_blocks_split_by_species(block, species=None):
     def __split_components_by_species(components_by_species, new_block):
         if components_by_species:
@@ -473,15 +491,17 @@
             spec_dict[spec].append(c)
     else:
         for spec in species:
             spec_dict[spec] = []
             for c in iter_components_by_src_start(block, spec):
                 spec_dict[spec].append(c)
 
-    empty_block = bx.align.Alignment(score=block.score, attributes=deepcopy(block.attributes))  # should we copy attributes?
+    empty_block = bx.align.Alignment(
+        score=block.score, attributes=deepcopy(block.attributes)
+    )  # should we copy attributes?
     empty_block.text_size = block.text_size
     # call recursive function to split into each combo of spec/blocks
     for value in __split_components_by_species(list(spec_dict.values()), empty_block):
         sort_block_components_by_block(value, block)  # restore original component order
         yield value
 
 
@@ -495,44 +515,59 @@
     for block, idx, offset in index.get_as_iterator_with_index_and_offset(src, region.start, region.end):
         block = chop_block_by_region(block, src, region, species, mincols)
         if block is not None:
             yield block, idx, offset
 
 
 # returns a filled region alignment for specified regions
-def get_region_alignment(index, primary_species, chrom, start, end, strand='+', species=None, mincols=0, overwrite_with_gaps=True, temp_file_handler=None):
+def get_region_alignment(
+    index,
+    primary_species,
+    chrom,
+    start,
+    end,
+    strand="+",
+    species=None,
+    mincols=0,
+    overwrite_with_gaps=True,
+    temp_file_handler=None,
+):
     if species is not None:
         alignment = RegionAlignment(end - start, species, temp_file_handler=temp_file_handler)
     else:
         alignment = RegionAlignment(end - start, primary_species, temp_file_handler=temp_file_handler)
-    return fill_region_alignment(alignment, index, primary_species, chrom, start, end, strand, species, mincols, overwrite_with_gaps)
+    return fill_region_alignment(
+        alignment, index, primary_species, chrom, start, end, strand, species, mincols, overwrite_with_gaps
+    )
 
 
 # reduces a block to only positions exisiting in the src provided
 def reduce_block_by_primary_genome(block, species, chromosome, region_start):
     # returns ( startIndex, {species:texts}
     # where texts' contents are reduced to only positions existing in the primary genome
     src = f"{species}.{chromosome}"
     ref = block.get_component_by_src(src)
     start_offset = ref.start - region_start
     species_texts = {}
     for c in block.components:
-        species_texts[c.src.split('.')[0]] = list(c.text)
+        species_texts[c.src.split(".")[0]] = list(c.text)
     # remove locations which are gaps in the primary species, starting from the downstream end
     for i in range(len(species_texts[species]) - 1, -1, -1):
-        if species_texts[species][i] == '-':
+        if species_texts[species][i] == "-":
             for text in species_texts.values():
                 text.pop(i)
     for spec, text in species_texts.items():
-        species_texts[spec] = ''.join(text)
+        species_texts[spec] = "".join(text)
     return (start_offset, species_texts)
 
 
 # fills a region alignment
-def fill_region_alignment(alignment, index, primary_species, chrom, start, end, strand='+', species=None, mincols=0, overwrite_with_gaps=True):
+def fill_region_alignment(
+    alignment, index, primary_species, chrom, start, end, strand="+", species=None, mincols=0, overwrite_with_gaps=True
+):
     region = bx.intervals.Interval(start, end)
     region.chrom = chrom
     region.strand = strand
     primary_src = f"{primary_species}.{chrom}"
 
     # Order blocks overlaping this position by score, lowest first
     blocks = []
@@ -542,28 +577,32 @@
             if score < blocks[i][0]:
                 blocks.insert(i, (score, idx, offset))
                 break
         else:
             blocks.append((score, idx, offset))
 
     # gap_chars_tuple = tuple( GAP_CHARS )
-    gap_chars_str = ''.join(GAP_CHARS)
+    gap_chars_str = "".join(GAP_CHARS)
     # Loop through ordered blocks and layer by increasing score
     for block_dict in blocks:
-        for block in iter_blocks_split_by_species(block_dict[1].get_at_offset(block_dict[2])):  # need to handle each occurance of sequence in block seperately
+        for block in iter_blocks_split_by_species(
+            block_dict[1].get_at_offset(block_dict[2])
+        ):  # need to handle each occurance of sequence in block seperately
             if component_overlaps_region(block.get_component_by_src(primary_src), region):
                 block = chop_block_by_region(block, primary_src, region, species, mincols)  # chop block
                 block = orient_block_by_region(block, primary_src, region)  # orient block
                 start_offset, species_texts = reduce_block_by_primary_genome(block, primary_species, chrom, start)
                 for spec, text in species_texts.items():
                     # we should trim gaps from both sides, since these are not positions in this species genome (sequence)
                     text = text.rstrip(gap_chars_str)
                     gap_offset = 0
                     # while text.startswith( gap_chars_tuple ):
-                    while True in [text.startswith(gap_char) for gap_char in GAP_CHARS]:  # python2.4 doesn't accept a tuple for .startswith()
+                    while True in [
+                        text.startswith(gap_char) for gap_char in GAP_CHARS
+                    ]:  # python2.4 doesn't accept a tuple for .startswith()
                         gap_offset += 1
                         text = text[1:]
                         if not text:
                             break
                     if text:
                         if overwrite_with_gaps:
                             alignment.set_range(start_offset + gap_offset, spec, text)
@@ -571,27 +610,40 @@
                             for i, char in enumerate(text):
                                 if char not in GAP_CHARS:
                                     alignment.set_position(start_offset + gap_offset + i, spec, char)
     return alignment
 
 
 # returns a filled spliced region alignment for specified region with start and end lists
-def get_spliced_region_alignment(index, primary_species, chrom, starts, ends, strand='+', species=None, mincols=0, overwrite_with_gaps=True, temp_file_handler=None):
+def get_spliced_region_alignment(
+    index,
+    primary_species,
+    chrom,
+    starts,
+    ends,
+    strand="+",
+    species=None,
+    mincols=0,
+    overwrite_with_gaps=True,
+    temp_file_handler=None,
+):
     # create spliced alignment object
     if species is not None:
         alignment = SplicedAlignment(starts, ends, species, temp_file_handler=temp_file_handler)
     else:
         alignment = SplicedAlignment(starts, ends, [primary_species], temp_file_handler=temp_file_handler)
     for exon in alignment.exons:
-        fill_region_alignment(exon, index, primary_species, chrom, exon.start, exon.end, strand, species, mincols, overwrite_with_gaps)
+        fill_region_alignment(
+            exon, index, primary_species, chrom, exon.start, exon.end, strand, species, mincols, overwrite_with_gaps
+        )
     return alignment
 
 
 # loop through string array, only return non-commented lines
-def line_enumerator(lines, comment_start='#'):
+def line_enumerator(lines, comment_start="#"):
     i = 0
     for line in lines:
         if not line.startswith(comment_start):
             i += 1
             yield (i, line)
 
 
@@ -603,24 +655,24 @@
 
     fields = line.split()
     # Requires atleast 12 BED columns
     if len(fields) < 12:
         raise Exception(f"Not a proper 12 column BED line ({line}).")
     tx_start = int(fields[1])
     strand = fields[5]
-    if strand != '-':
-        strand = '+'  # Default strand is +
+    if strand != "-":
+        strand = "+"  # Default strand is +
     cds_start = int(fields[6])
     cds_end = int(fields[7])
 
     # Calculate and store starts and ends of coding exons
     region_start, region_end = cds_start, cds_end
-    exon_starts = list(map(int, fields[11].rstrip(',\n').split(',')))
+    exon_starts = list(map(int, fields[11].rstrip(",\n").split(",")))
     exon_starts = [x + tx_start for x in exon_starts]
-    exon_ends = list(map(int, fields[10].rstrip(',').split(',')))
+    exon_ends = list(map(int, fields[10].rstrip(",").split(",")))
     exon_ends = [x + y for x, y in zip(exon_starts, exon_ends)]
     for start, end in zip(exon_starts, exon_ends):
         start = max(start, region_start)
         end = min(end, region_end)
         if start < end:
             starts.append(start)
             ends.append(end)
@@ -647,110 +699,119 @@
     return [value for value in iter_components_by_src_start(block, src)]
 
 
 def sort_block_components_by_block(block1, block2):
     # orders the components in block1 by the index of the component in block2
     # block1 must be a subset of block2
     # occurs in-place
-    return block1.components.sort(key=functools.cmp_to_key(lambda x, y: block2.components.index(x) - block2.components.index(y)))
+    return block1.components.sort(
+        key=functools.cmp_to_key(lambda x, y: block2.components.index(x) - block2.components.index(y))
+    )
 
 
 def get_species_in_maf(maf_filename):
     species = []
     for block in bx.align.maf.Reader(open(maf_filename)):
         for spec in get_species_in_block(block):
             if spec not in species:
                 species.append(spec)
     return species
 
 
 def parse_species_option(species):
     if species:
-        species = species.split(',')
-        if 'None' not in species:
+        species = species.split(",")
+        if "None" not in species:
             return species
     return None  # provided species was '', None, or had 'None' in it
 
 
 def remove_temp_index_file(index_filename):
     try:
         os.unlink(index_filename)
     except Exception:
         pass
 
+
 # Below are methods to deal with FASTA files
 
 
 def get_fasta_header(component, attributes=None, suffix=None):
     attributes = attributes or {}
-    header = ">%s(%s):%i-%i|" % (component.src, component.strand, component.get_forward_strand_start(), component.get_forward_strand_end())
+    header = ">%s(%s):%i-%i|" % (
+        component.src,
+        component.strand,
+        component.get_forward_strand_start(),
+        component.get_forward_strand_end(),
+    )
     for key, value in attributes.items():
         header = f"{header}{key}={value}|"
     if suffix:
         header = f"{header}{suffix}"
     else:
         header = f"{header}{src_split(component.src)[0]}"
     return header
 
 
 def get_attributes_from_fasta_header(header):
     if not header:
         return {}
     attributes = {}
-    header = header.lstrip('>')
+    header = header.lstrip(">")
     header = header.strip()
-    fields = header.split('|')
+    fields = header.split("|")
     try:
         region = fields[0]
-        region = region.split('(', 1)
-        temp = region[0].split('.', 1)
-        attributes['species'] = temp[0]
+        region = region.split("(", 1)
+        temp = region[0].split(".", 1)
+        attributes["species"] = temp[0]
         if len(temp) == 2:
-            attributes['chrom'] = temp[1]
+            attributes["chrom"] = temp[1]
         else:
-            attributes['chrom'] = temp[0]
-        region = region[1].split(')', 1)
-        attributes['strand'] = region[0]
-        region = region[1].lstrip(':').split('-')
-        attributes['start'] = int(region[0])
-        attributes['end'] = int(region[1])
+            attributes["chrom"] = temp[0]
+        region = region[1].split(")", 1)
+        attributes["strand"] = region[0]
+        region = region[1].lstrip(":").split("-")
+        attributes["start"] = int(region[0])
+        attributes["end"] = int(region[1])
     except Exception:
         # fields 0 is not a region coordinate
         pass
     if len(fields) > 2:
         for i in range(1, len(fields) - 1):
-            prop = fields[i].split('=', 1)
+            prop = fields[i].split("=", 1)
             if len(prop) == 2:
                 attributes[prop[0]] = prop[1]
     if len(fields) > 1:
-        attributes['__suffix__'] = fields[-1]
+        attributes["__suffix__"] = fields[-1]
     return attributes
 
 
 def iter_fasta_alignment(filename):
     class fastaComponent:
         def __init__(self, species, text=""):
             self.species = species
             self.text = text
 
         def extend(self, text):
-            self.text = self.text + text.replace('\n', '').replace('\r', '').strip()
+            self.text = self.text + text.replace("\n", "").replace("\r", "").strip()
+
     # yields a list of fastaComponents for a FASTA file
-    f = open(filename, 'rb')
+    f = open(filename, "rb")
     components = []
     # cur_component = None
     while True:
         line = f.readline()
         if not line:
             if components:
                 yield components
             return
         line = line.strip()
         if not line:
             if components:
                 yield components
             components = []
-        elif line.startswith('>'):
+        elif line.startswith(">"):
             attributes = get_attributes_from_fasta_header(line)
-            components.append(fastaComponent(attributes['species']))
+            components.append(fastaComponent(attributes["species"]))
         elif components:
             components[-1].extend(line)
```

### Comparing `galaxy-data-22.1.1/galaxy/datatypes/xml.py` & `galaxy-data-23.0.1/galaxy/datatypes/xml.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,260 +1,277 @@
 """
 XML format classes
 """
 import logging
 import re
+from typing import (
+    List,
+    TYPE_CHECKING,
+)
 
 from galaxy import util
+from galaxy.datatypes.dataproviders.dataset import DatasetDataProvider
+from galaxy.datatypes.dataproviders.hierarchy import XMLDataProvider
 from galaxy.datatypes.metadata import MetadataElement
 from galaxy.datatypes.sniff import (
     build_sniff_from_prefix,
     disable_parent_class_sniffing,
     FilePrefix,
 )
 from . import (
     data,
     dataproviders,
 )
 
+if TYPE_CHECKING:
+    from galaxy.model import DatasetInstance
+
 log = logging.getLogger(__name__)
 
-OWL_MARKER = re.compile(r'\<owl:')
-SBML_MARKER = re.compile(r'\<sbml')
+OWL_MARKER = re.compile(r"\<owl:")
+SBML_MARKER = re.compile(r"\<sbml")
 
 
 @dataproviders.decorators.has_dataproviders
 @build_sniff_from_prefix
 class GenericXml(data.Text):
     """Base format class for any XML file."""
+
     edam_format = "format_2332"
     file_ext = "xml"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'XML data'
+            dataset.blurb = "XML data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def _has_root_element_in_prefix(self, file_prefix: FilePrefix, root):
+    def _has_root_element_in_prefix(self, file_prefix: FilePrefix, root: str) -> bool:
         for line in file_prefix.line_iterator():
-            if not line.startswith('<?'):
+            if not line.startswith("<?"):
                 break
         # pattern match <root or <ns:root for any ns string
-        pattern = r'^<(\w*:)?%s' % root
+        pattern = r"^<(\w*:)?%s" % root
         return re.match(pattern, line) is not None
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Determines whether the file is XML or not
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( 'megablast_xml_parser_test1.blastxml' )
         >>> GenericXml().sniff( fname )
         True
         >>> fname = get_test_fname( 'interval.interval' )
         >>> GenericXml().sniff( fname )
         False
         """
-        return file_prefix.startswith('<?xml ')
+        return file_prefix.startswith("<?xml ")
 
     @staticmethod
-    def merge(split_files, output_file):
+    def merge(split_files: List[str], output_file: str) -> None:
         """Merging multiple XML files is non-trivial and must be done in subclasses."""
         if len(split_files) > 1:
-            raise NotImplementedError("Merging multiple XML files is non-trivial and must be implemented for each XML type")
+            raise NotImplementedError(
+                "Merging multiple XML files is non-trivial and must be implemented for each XML type"
+            )
         # For one file only, use base class method (move/copy)
         data.Text.merge(split_files, output_file)
 
-    @dataproviders.decorators.dataprovider_factory('xml', dataproviders.hierarchy.XMLDataProvider.settings)
-    def xml_dataprovider(self, dataset, **settings):
-        dataset_source = dataproviders.dataset.DatasetDataProvider(dataset)
-        return dataproviders.hierarchy.XMLDataProvider(dataset_source, **settings)
+    @dataproviders.decorators.dataprovider_factory("xml", XMLDataProvider.settings)
+    def xml_dataprovider(self, dataset: "DatasetInstance", **settings) -> XMLDataProvider:
+        dataset_source = DatasetDataProvider(dataset)
+        return XMLDataProvider(dataset_source, **settings)
 
 
 @disable_parent_class_sniffing
 class MEMEXml(GenericXml):
     """MEME XML Output data"""
+
     file_ext = "memexml"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'MEME XML data'
+            dataset.blurb = "MEME XML data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 @disable_parent_class_sniffing
 class CisML(GenericXml):
     """CisML XML data"""  # see: http://www.ncbi.nlm.nih.gov/pubmed/15001475
+
     file_ext = "cisml"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'CisML data'
+            dataset.blurb = "CisML data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
 
 class Dzi(GenericXml):
     """
     Deep zoom image format, see
     https://github.com/openseadragon/openseadragon/wiki/The-DZI-File-Format
     """
 
     # General elements.
-    MetadataElement(name="base_name", desc="Base name for this dataset", default='DeepZoomImage', readonly=True, set_in_upload=True)
+    MetadataElement(
+        name="base_name", desc="Base name for this dataset", default="DeepZoomImage", readonly=True, set_in_upload=True
+    )
     MetadataElement(name="format", desc="File format of the tiles", default=None, readonly=True, visible=True)
     MetadataElement(name="tile_size", desc="Size of tiles", default=None, readonly=True, visible=True)
     # Collection elements.
-    MetadataElement(name="max_level", desc="Max pyramid level", default=None, readonly=True, optional=True, visible=True)
+    MetadataElement(
+        name="max_level", desc="Max pyramid level", default=None, readonly=True, optional=True, visible=True
+    )
     MetadataElement(name="quality", desc="Quality", default=None, readonly=True, optional=True, visible=True)
     # Image elements.
     MetadataElement(name="height", desc="Size height", default=None, readonly=True, optional=True, visible=True)
-    MetadataElement(name="overlap", desc="Overlap of all four sides of tiles", default=None, readonly=True, optional=True, visible=True)
+    MetadataElement(
+        name="overlap",
+        desc="Overlap of all four sides of tiles",
+        default=None,
+        readonly=True,
+        optional=True,
+        visible=True,
+    )
     MetadataElement(name="width", desc="Size width", default=None, readonly=True, optional=True, visible=True)
 
-    file_ext = 'dzi'
+    file_ext = "dzi"
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
 
-    def set_meta(self, dataset, **kwd):
+    def set_meta(self, dataset: "DatasetInstance", overwrite: bool = True, **kwd) -> None:
         tree = util.parse_xml(dataset.file_name)
         root = tree.getroot()
-        dataset.metadata.format = root.get('Format')
-        dataset.metadata.tile_size = root.get('TileSize')
+        dataset.metadata.format = root.get("Format")
+        dataset.metadata.tile_size = root.get("TileSize")
         # DeepZoom image files can include
         # xml namespace attributes.
-        if root.tag.find('Collection') >= 0:
-            dataset.metadata.max_level = root.get('MaxLevel')
-            dataset.metadata.quality = root.get('Quality')
-        elif root.tag.find('Image') >= 0:
-            dataset.metadata.overlap = root.get('Overlap')
+        if root.tag.find("Collection") >= 0:
+            dataset.metadata.max_level = root.get("MaxLevel")
+            dataset.metadata.quality = root.get("Quality")
+        elif root.tag.find("Image") >= 0:
+            dataset.metadata.overlap = root.get("Overlap")
         for elem in root:
-            if elem.tag.find('Size') >= 0:
-                dataset.metadata.width = elem.get('Width')
-                dataset.metadata.height = elem.get('Height')
-
-    def get_visualizations(self, dataset):
-        """ Returns a list of visualizations for datatype"""
-        return ['openseadragon']
+            if elem.tag.find("Size") >= 0:
+                dataset.metadata.width = elem.get("Width")
+                dataset.metadata.height = elem.get("Height")
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
             dataset.blurb = "Deep Zoom Image"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
         Checking for keyword - 'Collection' or 'Image' in the first 200 lines.
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname('1.dzi')
         >>> Dzi().sniff(fname)
         True
         >>> fname = get_test_fname('megablast_xml_parser_test1.blastxml')
         >>> Dzi().sniff(fname)
         False
         """
         for line in file_prefix.line_iterator():
             line = line.lower()
-            if line.find('<collection') >= 0 or line.find('<image') >= 0:
+            if line.find("<collection") >= 0 or line.find("<image") >= 0:
                 return True
         return False
 
 
 class Phyloxml(GenericXml):
     """Format for defining phyloxml data http://www.phyloxml.org/"""
+
     edam_data = "data_0872"
     edam_format = "format_3159"
     file_ext = "phyloxml"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         """Set the peek and blurb text"""
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
-            dataset.blurb = 'Phyloxml data'
+            dataset.blurb = "Phyloxml data"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disk'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disk"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
-        """"Checking for keyword - 'phyloxml' always in lowercase in the first few lines.
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
+        """ "Checking for keyword - 'phyloxml' always in lowercase in the first few lines.
 
         >>> from galaxy.datatypes.sniff import get_test_fname
         >>> fname = get_test_fname( '1.phyloxml' )
         >>> Phyloxml().sniff( fname )
         True
         >>> fname = get_test_fname( 'interval.interval' )
         >>> Phyloxml().sniff( fname )
         False
         >>> fname = get_test_fname( 'megablast_xml_parser_test1.blastxml' )
         >>> Phyloxml().sniff( fname )
         False
         """
         return self._has_root_element_in_prefix(file_prefix, "phyloxml")
 
-    def get_visualizations(self, dataset):
-        """
-        Returns a list of visualizations for datatype.
-        """
-
-        return ['phyloviz']
-
 
 class Owl(GenericXml):
     """
-        Web Ontology Language OWL format description
-        http://www.w3.org/TR/owl-ref/
+    Web Ontology Language OWL format description
+    http://www.w3.org/TR/owl-ref/
     """
+
     edam_format = "format_3262"
     file_ext = "owl"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
             dataset.blurb = "Web Ontology Language OWL"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Checking for keyword - '<owl' in the first 200 lines.
+        Checking for keyword - '<owl' in the first 200 lines.
         """
         return file_prefix.search(OWL_MARKER)
 
 
 class Sbml(GenericXml):
     """
-        System Biology Markup Language
-        http://sbml.org
+    System Biology Markup Language
+    http://sbml.org
     """
+
     file_ext = "sbml"
     edam_data = "data_2024"
     edam_format = "format_2585"
 
-    def set_peek(self, dataset):
+    def set_peek(self, dataset: "DatasetInstance", **kwd) -> None:
         if not dataset.dataset.purged:
             dataset.peek = data.get_file_peek(dataset.file_name)
             dataset.blurb = "System Biology Markup Language SBML"
         else:
-            dataset.peek = 'file does not exist'
-            dataset.blurb = 'file purged from disc'
+            dataset.peek = "file does not exist"
+            dataset.blurb = "file purged from disc"
 
-    def sniff_prefix(self, file_prefix: FilePrefix):
+    def sniff_prefix(self, file_prefix: FilePrefix) -> bool:
         """
-            Checking for keyword - '<sbml' in the first 200 lines.
+        Checking for keyword - '<sbml' in the first 200 lines.
         """
         return file_prefix.search(SBML_MARKER)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/__init__.py` & `galaxy-data-23.0.1/galaxy/model/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,43 +13,53 @@
 import operator
 import os
 import pwd
 import random
 import string
 from collections import defaultdict
 from collections.abc import Callable
-from datetime import datetime, timedelta
+from datetime import timedelta
 from enum import Enum
 from string import Template
 from typing import (
     Any,
     Dict,
     Iterable,
     List,
     NamedTuple,
     Optional,
     Tuple,
     Type,
     TYPE_CHECKING,
     Union,
 )
-from uuid import UUID, uuid4
+from uuid import (
+    UUID,
+    uuid4,
+)
 
 import sqlalchemy
 from boltons.iterutils import remap
-from social_core.storage import AssociationMixin, CodeMixin, NonceMixin, PartialMixin, UserMixin
+from social_core.storage import (
+    AssociationMixin,
+    CodeMixin,
+    NonceMixin,
+    PartialMixin,
+    UserMixin,
+)
 from sqlalchemy import (
     alias,
     and_,
     asc,
     BigInteger,
     Boolean,
     Column,
     DateTime,
     desc,
+    event,
     false,
     ForeignKey,
     func,
     Index,
     inspect,
     Integer,
     join,
@@ -82,52 +92,74 @@
     joinedload,
     object_session,
     Query,
     reconstructor,
     registry,
     relationship,
 )
+from sqlalchemy.orm.attributes import flag_modified
 from sqlalchemy.orm.collections import attribute_mapped_collection
-from sqlalchemy.orm.decl_api import DeclarativeMeta
 from sqlalchemy.sql import exists
+from typing_extensions import Protocol
 
 import galaxy.exceptions
 import galaxy.model.metadata
-import galaxy.model.orm.now
 import galaxy.model.tags
 import galaxy.security.passwords
 import galaxy.util
 from galaxy.model.custom_types import (
     JSONType,
     MetadataType,
     MutableJSONType,
     TrimmedString,
     UUIDType,
 )
-from galaxy.model.item_attrs import get_item_annotation_str, UsesAnnotations
+from galaxy.model.item_attrs import (
+    get_item_annotation_str,
+    UsesAnnotations,
+)
 from galaxy.model.orm.now import now
+from galaxy.model.orm.util import add_object_to_object_session
 from galaxy.model.view import HistoryDatasetCollectionJobStateSummary
+from galaxy.objectstore import ObjectStore
 from galaxy.security import get_permitted_actions
 from galaxy.security.idencoding import IdEncodingHelper
 from galaxy.security.validate_user_input import validate_password_str
 from galaxy.util import (
     directory_hash_id,
     listify,
     ready_name_for_url,
     unicodify,
     unique_id,
 )
-from galaxy.util.dictifiable import dict_for, Dictifiable
-from galaxy.util.form_builder import (AddressField, CheckboxField, HistoryField,
-                                      PasswordField, SelectField, TextArea, TextField, WorkflowField,
-                                      WorkflowMappingField)
-from galaxy.util.hash_util import new_secure_hash
+from galaxy.util.dictifiable import (
+    dict_for,
+    Dictifiable,
+)
+from galaxy.util.form_builder import (
+    AddressField,
+    CheckboxField,
+    HistoryField,
+    PasswordField,
+    SelectField,
+    TextArea,
+    TextField,
+    WorkflowField,
+    WorkflowMappingField,
+)
+from galaxy.util.hash_util import (
+    md5_hash_str,
+    new_insecure_hash,
+)
 from galaxy.util.json import safe_loads
 from galaxy.util.sanitize_html import sanitize_html
 
+if TYPE_CHECKING:
+    from galaxy.schema.invocation import InvocationMessageUnion
+
 log = logging.getLogger(__name__)
 
 _datatypes_registry = None
 
 mapper_registry = registry()
 
 # When constructing filters with in for a fixed set of ids, maximum
@@ -143,48 +175,59 @@
 JOB_METRIC_SCALE = 7
 # Tags that get automatically propagated from inputs to outputs when running jobs.
 AUTO_PROPAGATED_TAGS = ["name"]
 YIELD_PER_ROWS = 100
 
 
 if TYPE_CHECKING:
+    # Workaround for https://github.com/python/mypy/issues/14182
+    from sqlalchemy.orm.decl_api import DeclarativeMeta as _DeclarativeMeta
+
+    class DeclarativeMeta(_DeclarativeMeta, type):
+        pass
+
     from galaxy.datatypes.data import Data
+    from galaxy.tools import DefaultToolState
+    from galaxy.workflow.modules import WorkflowModule
 
     class _HasTable:
         table: Table
         __table__: Table
+
 else:
+    from sqlalchemy.orm.decl_api import DeclarativeMeta
+
     _HasTable = object
 
 
-def get_uuid(uuid: Optional[Union[UUID, str]] = None):
+def get_uuid(uuid: Optional[Union[UUID, str]] = None) -> UUID:
     if isinstance(uuid, UUID):
         return uuid
     if not uuid:
         return uuid4()
     return UUID(str(uuid))
 
 
-class Base(metaclass=DeclarativeMeta):
+class Base(_HasTable, metaclass=DeclarativeMeta):
     __abstract__ = True
     registry = mapper_registry
     metadata = mapper_registry.metadata
     __init__ = mapper_registry.constructor
 
     @classmethod
     def __declare_last__(cls):
         cls.table = cls.__table__
 
 
-class RepresentById(_HasTable):
+class RepresentById:
     id: int
 
     def __repr__(self):
         try:
-            r = f'<galaxy.model.{self.__class__.__name__}({cached_id(self)}) at {hex(id(self))}>'
+            r = f"<galaxy.model.{self.__class__.__name__}({cached_id(self)}) at {hex(id(self))}>"
         except Exception:
             r = object.__repr__(self)
             log.exception("Caught exception attempting to generate repr for: %s", r)
         return r
 
 
 class NoConverterException(Exception):
@@ -201,15 +244,17 @@
 
     def __str__(self):
         return repr(self.value)
 
 
 def _get_datatypes_registry():
     if _datatypes_registry is None:
-        raise Exception("galaxy.model.set_datatypes_registry must be called before performing certain DatasetInstance operations.")
+        raise Exception(
+            "galaxy.model.set_datatypes_registry must be called before performing certain DatasetInstance operations."
+        )
     return _datatypes_registry
 
 
 def set_datatypes_registry(d_registry):
     """
     Set up datatypes_registry
     """
@@ -220,15 +265,15 @@
 class HasTags:
     dict_collection_visible_keys = ["tags"]
     dict_element_visible_keys = ["tags"]
     tags: List["ItemTagAssociation"]
 
     def to_dict(self, *args, **kwargs):
         rval = super().to_dict(*args, **kwargs)
-        rval['tags'] = self.make_tag_string_list()
+        rval["tags"] = self.make_tag_string_list()
         return rval
 
     def make_tag_string_list(self):
         # add tags string list
         tags_str_list = []
         for tag in self.tags:
             tag_str = tag.user_tname
@@ -244,17 +289,27 @@
             self.tags.append(new_tag_assoc)
 
     @property
     def auto_propagated_tags(self):
         return [t for t in self.tags if t.user_tname in AUTO_PROPAGATED_TAGS]
 
 
-class SerializationOptions:
+class SerializeFilesHandler(Protocol):
+    def serialize_files(self, dataset: "DatasetInstance", as_dict: Dict[str, Any]) -> None:
+        pass
 
-    def __init__(self, for_edit, serialize_dataset_objects=None, serialize_files_handler=None, strip_metadata_files=None):
+
+class SerializationOptions:
+    def __init__(
+        self,
+        for_edit: bool,
+        serialize_dataset_objects: Optional[bool] = None,
+        serialize_files_handler: Optional[SerializeFilesHandler] = None,
+        strip_metadata_files: Optional[bool] = None,
+    ) -> None:
         self.for_edit = for_edit
         if serialize_dataset_objects is None:
             serialize_dataset_objects = for_edit
         self.serialize_dataset_objects = serialize_dataset_objects
         self.serialize_files_handler = serialize_files_handler
         if strip_metadata_files is None:
             # If we're editing datasets - keep MetadataFile(s) in tact. For pure export
@@ -262,74 +317,71 @@
             strip_metadata_files = not for_edit
         self.strip_metadata_files = strip_metadata_files
 
     def attach_identifier(self, id_encoder, obj, ret_val):
         if self.for_edit and obj.id:
             ret_val["id"] = obj.id
         elif obj.id:
-            ret_val["encoded_id"] = id_encoder.encode_id(obj.id, kind='model_export')
+            ret_val["encoded_id"] = id_encoder.encode_id(obj.id, kind="model_export")
         else:
             if not hasattr(obj, "temp_id"):
                 obj.temp_id = uuid4().hex
             ret_val["encoded_id"] = obj.temp_id
 
     def get_identifier(self, id_encoder, obj):
         if self.for_edit and obj.id:
             return obj.id
         elif obj.id:
-            return id_encoder.encode_id(obj.id, kind='model_export')
+            return id_encoder.encode_id(obj.id, kind="model_export")
         else:
             if not hasattr(obj, "temp_id"):
                 obj.temp_id = uuid4().hex
             return obj.temp_id
 
     def get_identifier_for_id(self, id_encoder, obj_id):
         if self.for_edit and obj_id:
             return obj_id
         elif obj_id:
-            return id_encoder.encode_id(obj_id, kind='model_export')
+            return id_encoder.encode_id(obj_id, kind="model_export")
         else:
             raise NotImplementedError()
 
     def serialize_files(self, dataset, as_dict):
         if self.serialize_files_handler is not None:
             self.serialize_files_handler.serialize_files(dataset, as_dict)
 
 
 class Serializable(RepresentById):
-
-    def serialize(self, id_encoder: IdEncodingHelper, serialization_options: SerializationOptions, for_link: bool = False) -> Dict[str, Any]:
+    def serialize(
+        self, id_encoder: IdEncodingHelper, serialization_options: SerializationOptions, for_link: bool = False
+    ) -> Dict[str, Any]:
         """Serialize model for a re-population in (potentially) another Galaxy instance."""
         if for_link:
-            rval = dict_for(
-                self
-            )
+            rval = dict_for(self)
             serialization_options.attach_identifier(id_encoder, self, rval)
             return rval
         return self._serialize(id_encoder, serialization_options)
 
     @abc.abstractmethod
     def _serialize(self, id_encoder: IdEncodingHelper, serialization_options: SerializationOptions) -> Dict[str, Any]:
         """Serialize model for a re-population in (potentially) another Galaxy instance."""
 
 
 class HasName:
-
     def get_display_name(self):
         """
         These objects have a name attribute can be either a string or a unicode
         object. If string, convert to unicode object assuming 'utf-8' format.
         """
         name = self.name
-        name = unicodify(name, 'utf-8')
+        name = unicodify(name, "utf-8")
         return name
 
 
 class UsesCreateAndUpdateTime:
-
     update_time: DateTime
 
     @property
     def seconds_since_updated(self):
         update_time = self.update_time or now()  # In case not yet flushed
         return (now() - update_time).total_seconds()
 
@@ -338,19 +390,17 @@
         create_time = self.create_time or now()  # In case not yet flushed
         return (now() - create_time).total_seconds()
 
     def update(self):
         self.update_time = now()
 
 
-class WorkerProcess(Base, UsesCreateAndUpdateTime, _HasTable):
-    __tablename__ = 'worker_process'
-    __table_args__ = (
-        UniqueConstraint('server_name', 'hostname'),
-    )
+class WorkerProcess(Base, UsesCreateAndUpdateTime):
+    __tablename__ = "worker_process"
+    __table_args__ = (UniqueConstraint("server_name", "hostname"),)
 
     id = Column(Integer, primary_key=True)
     server_name = Column(String(255), index=True)
     hostname = Column(String(255))
     pid = Column(Integer)
     update_time = Column(DateTime, default=now, onupdate=now)
 
@@ -378,63 +428,69 @@
             assert len(identity) == 1
             return identity[0]
 
     return galaxy_model_object.id
 
 
 class JobLike:
-    MAX_NUMERIC = 10**(JOB_METRIC_PRECISION - JOB_METRIC_SCALE) - 1
+    MAX_NUMERIC = 10 ** (JOB_METRIC_PRECISION - JOB_METRIC_SCALE) - 1
 
     def _init_metrics(self):
         self.text_metrics = []
         self.numeric_metrics = []
 
     def add_metric(self, plugin, metric_name, metric_value):
-        plugin = unicodify(plugin, 'utf-8')
-        metric_name = unicodify(metric_name, 'utf-8')
+        plugin = unicodify(plugin, "utf-8")
+        metric_name = unicodify(metric_name, "utf-8")
         number = isinstance(metric_value, numbers.Number)
         if number and int(metric_value) <= JobLike.MAX_NUMERIC:
             metric = self._numeric_metric(plugin, metric_name, metric_value)
             self.numeric_metrics.append(metric)
         elif number:
-            log.warning("Cannot store metric due to database column overflow (max: %s): %s: %s",
-                        JobLike.MAX_NUMERIC, metric_name, metric_value)
+            log.warning(
+                "Cannot store metric due to database column overflow (max: %s): %s: %s",
+                JobLike.MAX_NUMERIC,
+                metric_name,
+                metric_value,
+            )
         else:
-            metric_value = unicodify(metric_value, 'utf-8')
+            metric_value = unicodify(metric_value, "utf-8")
             if len(metric_value) > (JOB_METRIC_MAX_LENGTH - 1):
                 # Truncate these values - not needed with sqlite
                 # but other backends must need it.
-                metric_value = metric_value[:(JOB_METRIC_MAX_LENGTH - 1)]
+                metric_value = metric_value[: (JOB_METRIC_MAX_LENGTH - 1)]
             metric = self._text_metric(plugin, metric_name, metric_value)
             self.text_metrics.append(metric)
 
     @property
     def metrics(self):
         # TODO: Make iterable, concatenate with chain
         return self.text_metrics + self.numeric_metrics
 
     def set_streams(self, tool_stdout, tool_stderr, job_stdout=None, job_stderr=None, job_messages=None):
         def shrink_and_unicodify(what, stream):
-            if len(stream) > galaxy.util.DATABASE_MAX_STRING_SIZE:
-                log.info("%s for %s %d is greater than %s, only a portion will be logged to database",
-                         what,
-                         type(self),
-                         self.id,
-                         galaxy.util.DATABASE_MAX_STRING_SIZE_PRETTY)
+            if stream and len(stream) > galaxy.util.DATABASE_MAX_STRING_SIZE:
+                log.info(
+                    "%s for %s %d is greater than %s, only a portion will be logged to database",
+                    what,
+                    type(self),
+                    self.id,
+                    galaxy.util.DATABASE_MAX_STRING_SIZE_PRETTY,
+                )
             return galaxy.util.shrink_and_unicodify(stream)
 
-        self.tool_stdout = shrink_and_unicodify('tool_stdout', tool_stdout)
-        self.tool_stderr = shrink_and_unicodify('tool_stderr', tool_stderr)
+        self.tool_stdout = shrink_and_unicodify("tool_stdout", tool_stdout)
+        self.tool_stderr = shrink_and_unicodify("tool_stderr", tool_stderr)
         if job_stdout is not None:
-            self.job_stdout = shrink_and_unicodify('job_stdout', job_stdout)
+            self.job_stdout = shrink_and_unicodify("job_stdout", job_stdout)
         else:
             self.job_stdout = None
 
         if job_stderr is not None:
-            self.job_stderr = shrink_and_unicodify('job_stderr', job_stderr)
+            self.job_stderr = shrink_and_unicodify("job_stderr", job_stderr)
         else:
             self.job_stderr = None
 
         if job_messages is not None:
             self.job_messages = job_messages
 
     def log_str(self):
@@ -445,130 +501,145 @@
         else:
             extra += "unflushed"
 
         return f"{self.__class__.__name__}[{extra},tool_id={self.tool_id}]"
 
     @property
     def stdout(self):
-        stdout = self.tool_stdout or ''
+        stdout = self.tool_stdout or ""
         if self.job_stdout:
             stdout += f"\n{self.job_stdout}"
         return stdout
 
     @stdout.setter
     def stdout(self, stdout):
         raise NotImplementedError("Attempt to set stdout, must set tool_stdout or job_stdout")
 
     @property
     def stderr(self):
-        stderr = self.tool_stderr or ''
+        stderr = self.tool_stderr or ""
         if self.job_stderr:
             stderr += f"\n{self.job_stderr}"
         return stderr
 
     @stderr.setter
     def stderr(self, stderr):
         raise NotImplementedError("Attempt to set stdout, must set tool_stderr or job_stderr")
 
 
 class User(Base, Dictifiable, RepresentById):
     """
     Data for a Galaxy user or admin and relations to their
     histories, credentials, and roles.
     """
+
     use_pbkdf2 = True
     bootstrap_admin_user = False
     # api_keys: 'List[APIKeys]'  already declared as relationship()
 
-    __tablename__ = 'galaxy_user'
+    __tablename__ = "galaxy_user"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     email = Column(TrimmedString(255), index=True, nullable=False)
     username = Column(TrimmedString(255), index=True, unique=True)
     password = Column(TrimmedString(255), nullable=False)
     last_password_change = Column(DateTime, default=now)
     external = Column(Boolean, default=False)
-    form_values_id = Column(Integer, ForeignKey('form_values.id'), index=True)
+    form_values_id = Column(Integer, ForeignKey("form_values.id"), index=True)
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
     disk_usage = Column(Numeric(15, 0), index=True)
     # Column("person_metadata", JSONType),  # TODO: add persistent, configurable metadata rep for workflow creator
     active = Column(Boolean, index=True, default=True, nullable=False)
     activation_token = Column(TrimmedString(64), nullable=True, index=True)
 
-    addresses = relationship('UserAddress',
-        back_populates='user',
-        order_by=lambda: desc(UserAddress.update_time))
-    cloudauthz = relationship('CloudAuthz', back_populates='user')
-    custos_auth = relationship('CustosAuthnzToken', back_populates='user')
-    default_permissions = relationship('DefaultUserPermissions', back_populates='user')
-    groups = relationship('UserGroupAssociation', back_populates='user')
-    histories = relationship('History',
-        back_populates='user',
-        order_by=lambda: desc(History.update_time))  # type: ignore[has-type]
-    active_histories = relationship('History',
+    addresses = relationship("UserAddress", back_populates="user", order_by=lambda: desc(UserAddress.update_time))
+    cloudauthz = relationship("CloudAuthz", back_populates="user")
+    custos_auth = relationship("CustosAuthnzToken", back_populates="user")
+    default_permissions = relationship("DefaultUserPermissions", back_populates="user")
+    groups = relationship("UserGroupAssociation", back_populates="user")
+    histories = relationship(
+        "History", back_populates="user", order_by=lambda: desc(History.update_time)  # type: ignore[has-type]
+    )
+    active_histories = relationship(
+        "History",
         primaryjoin=(lambda: (History.user_id == User.id) & (not_(History.deleted))),  # type: ignore[has-type]
         viewonly=True,
-        order_by=lambda: desc(History.update_time))  # type: ignore[has-type]
-    galaxy_sessions = relationship('GalaxySession',
-        back_populates='user',
-        order_by=lambda: desc(GalaxySession.update_time))  # type: ignore[has-type]
-    quotas = relationship('UserQuotaAssociation', back_populates='user')
-    social_auth = relationship('UserAuthnzToken', back_populates='user')
-    stored_workflow_menu_entries = relationship('StoredWorkflowMenuEntry',
-        primaryjoin=(lambda:
-            (StoredWorkflowMenuEntry.user_id == User.id)
+        order_by=lambda: desc(History.update_time),  # type: ignore[has-type]
+    )
+    galaxy_sessions = relationship(
+        "GalaxySession", back_populates="user", order_by=lambda: desc(GalaxySession.update_time)  # type: ignore[has-type]
+    )
+    quotas = relationship("UserQuotaAssociation", back_populates="user")
+    social_auth = relationship("UserAuthnzToken", back_populates="user")
+    stored_workflow_menu_entries = relationship(
+        "StoredWorkflowMenuEntry",
+        primaryjoin=(
+            lambda: (StoredWorkflowMenuEntry.user_id == User.id)
             & (StoredWorkflowMenuEntry.stored_workflow_id == StoredWorkflow.id)  # type: ignore[has-type]
             & not_(StoredWorkflow.deleted)  # type: ignore[has-type]
         ),
-        back_populates='user',
-        cascade='all, delete-orphan',
-        collection_class=ordering_list('order_index'))
-    _preferences = relationship('UserPreference', collection_class=attribute_mapped_collection('name'))
-    values = relationship('FormValues',
-        primaryjoin=(lambda: User.form_values_id == FormValues.id))  # type: ignore[has-type]
+        back_populates="user",
+        cascade="all, delete-orphan",
+        collection_class=ordering_list("order_index"),
+    )
+    _preferences = relationship("UserPreference", collection_class=attribute_mapped_collection("name"))
+    values = relationship(
+        "FormValues", primaryjoin=(lambda: User.form_values_id == FormValues.id)  # type: ignore[has-type]
+    )
     # Add type hint (will this work w/SA?)
-    api_keys: 'List[APIKeys]' = relationship('APIKeys',
-        back_populates='user',
-        order_by=lambda: desc(APIKeys.create_time))
-    data_manager_histories = relationship('DataManagerHistoryAssociation', back_populates='user')
-    roles = relationship('UserRoleAssociation', back_populates='user')
-    stored_workflows = relationship('StoredWorkflow', back_populates='user',
-        primaryjoin=(lambda: User.id == StoredWorkflow.user_id))  # type: ignore[has-type]
+    api_keys: "List[APIKeys]" = relationship(
+        "APIKeys", back_populates="user", order_by=lambda: desc(APIKeys.create_time)
+    )
+    data_manager_histories = relationship("DataManagerHistoryAssociation", back_populates="user")
+    roles = relationship("UserRoleAssociation", back_populates="user")
+    stored_workflows = relationship(
+        "StoredWorkflow", back_populates="user", primaryjoin=(lambda: User.id == StoredWorkflow.user_id)  # type: ignore[has-type]
+    )
     non_private_roles = relationship(
-        'UserRoleAssociation',
+        "UserRoleAssociation",
         viewonly=True,
-        primaryjoin=(lambda:
-            (User.id == UserRoleAssociation.user_id)  # type: ignore[has-type]
+        primaryjoin=(
+            lambda: (User.id == UserRoleAssociation.user_id)  # type: ignore[has-type]
             & (UserRoleAssociation.role_id == Role.id)  # type: ignore[has-type]
-            & not_(Role.name == User.email))  # type: ignore[has-type]
+            & not_(Role.name == User.email)  # type: ignore[has-type]
+        ),
     )
 
     preferences: association_proxy  # defined at the end of this module
 
     # attributes that will be accessed and returned when calling to_dict( view='collection' )
-    dict_collection_visible_keys = ['id', 'email', 'username', 'deleted', 'active', 'last_password_change']
+    dict_collection_visible_keys = ["id", "email", "username", "deleted", "active", "last_password_change"]
     # attributes that will be accessed and returned when calling to_dict( view='element' )
-    dict_element_visible_keys = ['id', 'email', 'username', 'total_disk_usage', 'nice_total_disk_usage', 'deleted', 'active', 'last_password_change']
+    dict_element_visible_keys = [
+        "id",
+        "email",
+        "username",
+        "total_disk_usage",
+        "nice_total_disk_usage",
+        "deleted",
+        "active",
+        "last_password_change",
+    ]
 
     def __init__(self, email=None, password=None, username=None):
         self.email = email
         self.password = password
         self.external = False
         self.deleted = False
         self.purged = False
         self.active = False
         self.username = username
 
     @property
     def extra_preferences(self):
         data = defaultdict(lambda: None)
-        extra_user_preferences = self.preferences.get('extra_user_preferences')
+        extra_user_preferences = self.preferences.get("extra_user_preferences")
         if extra_user_preferences:
             try:
                 data.update(json.loads(extra_user_preferences))
             except Exception:
                 pass
         return data
 
@@ -578,39 +649,40 @@
         """
         message = validate_password_str(cleartext)
         if message:
             raise Exception(f"Invalid password: {message}")
         if User.use_pbkdf2:
             self.password = galaxy.security.passwords.hash_password(cleartext)
         else:
-            self.password = new_secure_hash(text_type=cleartext)
-        self.last_password_change = datetime.now()
+            self.password = new_insecure_hash(text_type=cleartext)
+        self.last_password_change = now()
 
     def set_random_password(self, length=16):
         """
         Sets user password to a random string of the given length.
         :return: void
         """
         self.set_password_cleartext(
-            ''.join(random.SystemRandom().choice(string.ascii_letters + string.digits) for _ in range(length)))
+            "".join(random.SystemRandom().choice(string.ascii_letters + string.digits) for _ in range(length))
+        )
 
     def check_password(self, cleartext):
         """
         Check if `cleartext` matches user password when hashed.
         """
         return galaxy.security.passwords.check_password(cleartext, self.password)
 
     def system_user_pwent(self, real_system_username):
         """
         Gives the system user pwent entry based on e-mail or username depending
         on the value in real_system_username
         """
-        if real_system_username == 'user_email':
-            username = self.email.split('@')[0]
-        elif real_system_username == 'username':
+        if real_system_username == "user_email":
+            username = self.email.split("@")[0]
+        elif real_system_username == "username":
             username = self.username
         else:
             username = real_system_username
         try:
             return pwd.getpwnam(username)
         except Exception:
             log.exception(f"Error getting the password database entry for user {username}")
@@ -618,41 +690,41 @@
 
     def all_roles(self):
         """
         Return a unique list of Roles associated with this user or any of their groups.
         """
         try:
             db_session = object_session(self)
-            user = db_session.query(
-                User
-            ).filter_by(  # don't use get, it will use session variant.
-                id=self.id
-            ).options(
-                joinedload("roles"),
-                joinedload("roles.role"),
-                joinedload("groups"),
-                joinedload("groups.group"),
-                joinedload("groups.group.roles"),
-                joinedload("groups.group.roles.role")
-            ).one()
+            user = (
+                db_session.query(User)
+                .filter_by(id=self.id)  # don't use get, it will use session variant.
+                .options(
+                    joinedload(User.roles),
+                    joinedload(User.roles.role),
+                    joinedload(User.groups),
+                    joinedload(User.groups.group),
+                    joinedload(User.groups.group.roles),
+                    joinedload(User.groups.group.roles.role),
+                )
+                .one()
+            )
         except Exception:
             # If not persistent user, just use models normaly and
             # skip optimizations...
             user = self
 
         roles = [ura.role for ura in user.roles]
         for group in [uga.group for uga in user.groups]:
             for role in [gra.role for gra in group.roles]:
                 if role not in roles:
                     roles.append(role)
         return roles
 
     def all_roles_exploiting_cache(self):
-        """
-        """
+        """ """
         roles = [ura.role for ura in self.roles]
         for group in [uga.group for uga in self.groups]:
             for role in [gra.role for gra in group.roles]:
                 if role not in roles:
                     roles.append(role)
         return roles
 
@@ -702,15 +774,16 @@
         self._calculate_or_set_disk_usage(dryrun=False)
 
     def _calculate_or_set_disk_usage(self, dryrun=True):
         """
         Utility to calculate and return the disk usage.  If dryrun is False,
         the new value is set immediately.
         """
-        sql_calc = """
+        sql_calc = text(
+            """
             WITH per_user_histories AS
             (
                 SELECT id
                 FROM history
                 WHERE user_id = :id
                     AND NOT purged
             ),
@@ -722,16 +795,17 @@
             )
             SELECT SUM(COALESCE(dataset.total_size, dataset.file_size, 0))
             FROM dataset
             LEFT OUTER JOIN library_dataset_dataset_association ON dataset.id = library_dataset_dataset_association.dataset_id
             WHERE dataset.id IN (SELECT dataset_id FROM per_hist_hdas)
                 AND library_dataset_dataset_association.id IS NULL
         """
+        )
         sa_session = object_session(self)
-        usage = sa_session.scalar(sql_calc, {'id': self.id})
+        usage = sa_session.scalar(sql_calc, {"id": self.id})
         if not dryrun:
             self.set_disk_usage(usage)
             sa_session.flush()
         return usage
 
     @staticmethod
     def user_template_environment(user):
@@ -748,33 +822,32 @@
         >>> env = User.user_template_environment(user)
         >>> env['__user_id__']
         '6'
         >>> env['__user_name__']
         'foo2'
         """
         if user:
-            user_id = '%d' % user.id
+            user_id = "%d" % user.id
             user_email = str(user.email)
             user_name = str(user.username)
         else:
             user = None
-            user_id = 'Anonymous'
-            user_email = 'Anonymous'
-            user_name = 'Anonymous'
+            user_id = "Anonymous"
+            user_email = "Anonymous"
+            user_name = "Anonymous"
         environment = {}
-        environment['__user__'] = user
-        environment['__user_id__'] = environment['userId'] = user_id
-        environment['__user_email__'] = environment['userEmail'] = user_email
-        environment['__user_name__'] = user_name
+        environment["__user__"] = user
+        environment["__user_id__"] = environment["userId"] = user_id
+        environment["__user_email__"] = environment["userEmail"] = user_email
+        environment["__user_name__"] = user_name
         return environment
 
     @staticmethod
     def expand_user_properties(user, in_string):
-        """
-        """
+        """ """
         environment = User.user_template_environment(user)
         return Template(in_string).safe_substitute(environment)
 
     def is_active(self):
         return self.active
 
     def is_authenticated(self):
@@ -785,223 +858,230 @@
         # seems reasonable. Besides, this is also how a PSA example is implemented:
         # https://github.com/python-social-auth/social-examples/blob/master/example-cherrypy/example/db/user.py
         return True
 
     def attempt_create_private_role(self):
         session = object_session(self)
         role_name = self.email
-        role_desc = f'Private Role for {self.email}'
+        role_desc = f"Private Role for {self.email}"
         role_type = Role.types.PRIVATE
         role = Role(name=role_name, description=role_desc, type=role_type)
         assoc = UserRoleAssociation(self, role)
         session.add(assoc)
         session.flush()
 
 
-class PasswordResetToken(Base, _HasTable):
-    __tablename__ = 'password_reset_token'
+class PasswordResetToken(Base):
+    __tablename__ = "password_reset_token"
 
     token = Column(String(32), primary_key=True, unique=True, index=True)
     expiration_time = Column(DateTime)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    user = relationship('User')
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    user = relationship("User")
 
     def __init__(self, user, token=None):
         if token:
             self.token = token
         else:
             self.token = unique_id()
         self.user = user
         self.expiration_time = now() + timedelta(hours=24)
 
 
 class DynamicTool(Base, Dictifiable, RepresentById):
-    __tablename__ = 'dynamic_tool'
+    __tablename__ = "dynamic_tool"
 
     id = Column(Integer, primary_key=True)
     uuid = Column(UUIDType())
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, index=True, default=now, onupdate=now)
     tool_id = Column(Unicode(255))
     tool_version = Column(Unicode(255))
     tool_format = Column(Unicode(255))
     tool_path = Column(Unicode(255))
     tool_directory = Column(Unicode(255))
     hidden = Column(Boolean, default=True)
     active = Column(Boolean, default=True)
     value = Column(MutableJSONType)
 
-    dict_collection_visible_keys = ('id', 'tool_id', 'tool_format', 'tool_version', 'uuid', 'active', 'hidden')
-    dict_element_visible_keys = ('id', 'tool_id', 'tool_format', 'tool_version', 'uuid', 'active', 'hidden')
+    dict_collection_visible_keys = ("id", "tool_id", "tool_format", "tool_version", "uuid", "active", "hidden")
+    dict_element_visible_keys = ("id", "tool_id", "tool_format", "tool_version", "uuid", "active", "hidden")
 
     def __init__(self, active=True, hidden=True, **kwd):
         super().__init__(**kwd)
         self.active = active
         self.hidden = hidden
-        _uuid = kwd.get('uuid')
+        _uuid = kwd.get("uuid")
         self.uuid = get_uuid(_uuid)
 
 
 class BaseJobMetric(Base):
     __abstract__ = True
 
     def __init__(self, plugin, metric_name, metric_value):
         super().__init__()
         self.plugin = plugin
         self.metric_name = metric_name
         self.metric_value = metric_value
 
 
 class JobMetricText(BaseJobMetric, RepresentById):
-    __tablename__ = 'job_metric_text'
+    __tablename__ = "job_metric_text"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
     plugin = Column(Unicode(255))
     metric_name = Column(Unicode(255))
     metric_value = Column(Unicode(JOB_METRIC_MAX_LENGTH))
 
 
 class JobMetricNumeric(BaseJobMetric, RepresentById):
-    __tablename__ = 'job_metric_numeric'
+    __tablename__ = "job_metric_numeric"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
     plugin = Column(Unicode(255))
     metric_name = Column(Unicode(255))
     metric_value = Column(Numeric(JOB_METRIC_PRECISION, JOB_METRIC_SCALE))
 
 
 class TaskMetricText(BaseJobMetric, RepresentById):
-    __tablename__ = 'task_metric_text'
+    __tablename__ = "task_metric_text"
 
     id = Column(Integer, primary_key=True)
-    task_id = Column(Integer, ForeignKey('task.id'), index=True)
+    task_id = Column(Integer, ForeignKey("task.id"), index=True)
     plugin = Column(Unicode(255))
     metric_name = Column(Unicode(255))
     metric_value = Column(Unicode(JOB_METRIC_MAX_LENGTH))
 
 
 class TaskMetricNumeric(BaseJobMetric, RepresentById):
-    __tablename__ = 'task_metric_numeric'
+    __tablename__ = "task_metric_numeric"
 
     id = Column(Integer, primary_key=True)
-    task_id = Column(Integer, ForeignKey('task.id'), index=True)
+    task_id = Column(Integer, ForeignKey("task.id"), index=True)
     plugin = Column(Unicode(255))
     metric_name = Column(Unicode(255))
     metric_value = Column(Numeric(JOB_METRIC_PRECISION, JOB_METRIC_SCALE))
 
 
+class IoDicts(NamedTuple):
+    inp_data: Dict[str, Optional["DatasetInstance"]]
+    out_data: Dict[str, "DatasetInstance"]
+    out_collections: Dict[str, Union["DatasetCollectionInstance", "DatasetCollection"]]
+
+
 class Job(Base, JobLike, UsesCreateAndUpdateTime, Dictifiable, Serializable):
     """
     A job represents a request to run a tool given input datasets, tool
     parameters, and output datasets.
     """
-    __tablename__ = 'job'
+
+    __tablename__ = "job"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now, index=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    library_folder_id = Column(Integer, ForeignKey('library_folder.id'), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    library_folder_id = Column(Integer, ForeignKey("library_folder.id"), index=True)
     tool_id = Column(String(255))
-    tool_version = Column(TEXT, default='1.0.0')
+    tool_version = Column(TEXT, default="1.0.0")
     galaxy_version = Column(String(64), default=None)
-    dynamic_tool_id = Column(Integer, ForeignKey('dynamic_tool.id'), index=True, nullable=True)
+    dynamic_tool_id = Column(Integer, ForeignKey("dynamic_tool.id"), index=True, nullable=True)
     state = Column(String(64), index=True)
     info = Column(TrimmedString(255))
     copied_from_job_id = Column(Integer, nullable=True)
     command_line = Column(TEXT)
     dependencies = Column(MutableJSONType, nullable=True)
     job_messages = Column(MutableJSONType, nullable=True)
     param_filename = Column(String(1024))
     runner_name = Column(String(255))
     job_stdout = Column(TEXT)
     job_stderr = Column(TEXT)
     tool_stdout = Column(TEXT)
     tool_stderr = Column(TEXT)
     exit_code = Column(Integer, nullable=True)
     traceback = Column(TEXT)
-    session_id = Column(Integer, ForeignKey('galaxy_session.id'), index=True, nullable=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True, nullable=True)
+    session_id = Column(Integer, ForeignKey("galaxy_session.id"), index=True, nullable=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True, nullable=True)
     job_runner_name = Column(String(255))
     job_runner_external_id = Column(String(255), index=True)
     destination_id = Column(String(255), nullable=True)
     destination_params = Column(MutableJSONType, nullable=True)
     object_store_id = Column(TrimmedString(255), index=True)
     imported = Column(Boolean, default=False, index=True)
     params = Column(TrimmedString(255), index=True)
     handler = Column(TrimmedString(255), index=True)
 
-    user = relationship('User')
-    galaxy_session = relationship('GalaxySession')
-    history = relationship('History', back_populates='jobs')
-    library_folder = relationship('LibraryFolder')
-    parameters = relationship('JobParameter')
-    input_datasets = relationship('JobToInputDatasetAssociation', back_populates='job')
-    input_dataset_collections = relationship('JobToInputDatasetCollectionAssociation',
-        back_populates='job')
-    input_dataset_collection_elements = relationship('JobToInputDatasetCollectionElementAssociation',
-        back_populates='job')
-    output_dataset_collection_instances = relationship('JobToOutputDatasetCollectionAssociation',
-        back_populates='job')
-    output_dataset_collections = relationship('JobToImplicitOutputDatasetCollectionAssociation',
-        back_populates='job')
-    post_job_actions = relationship('PostJobActionAssociation', back_populates='job')
-    input_library_datasets = relationship('JobToInputLibraryDatasetAssociation',
-        back_populates='job')
-    output_library_datasets = relationship('JobToOutputLibraryDatasetAssociation',
-        back_populates='job')
-    external_output_metadata = relationship('JobExternalOutputMetadata', back_populates='job')
-    tasks = relationship('Task', back_populates='job')
-    output_datasets = relationship('JobToOutputDatasetAssociation', back_populates='job')
-    state_history = relationship('JobStateHistory')
-    text_metrics = relationship('JobMetricText')
-    numeric_metrics = relationship('JobMetricNumeric')
-    interactivetool_entry_points = relationship('InteractiveToolEntryPoint',
-        back_populates='job', uselist=True)
-    implicit_collection_jobs_association = relationship('ImplicitCollectionJobsJobAssociation',
-        back_populates='job', uselist=False)
-    container = relationship('JobContainerAssociation', back_populates='job', uselist=False)
-    data_manager_association = relationship('DataManagerJobAssociation',
-        back_populates='job', uselist=False)
-    history_dataset_collection_associations = relationship('HistoryDatasetCollectionAssociation',
-        back_populates='job')
-    workflow_invocation_step = relationship('WorkflowInvocationStep',
-        back_populates='job', uselist=False)
+    user = relationship("User")
+    galaxy_session = relationship("GalaxySession")
+    history = relationship("History", back_populates="jobs")
+    library_folder = relationship("LibraryFolder")
+    parameters = relationship("JobParameter")
+    input_datasets = relationship("JobToInputDatasetAssociation", back_populates="job")
+    input_dataset_collections = relationship("JobToInputDatasetCollectionAssociation", back_populates="job")
+    input_dataset_collection_elements = relationship(
+        "JobToInputDatasetCollectionElementAssociation", back_populates="job"
+    )
+    output_dataset_collection_instances = relationship("JobToOutputDatasetCollectionAssociation", back_populates="job")
+    output_dataset_collections = relationship("JobToImplicitOutputDatasetCollectionAssociation", back_populates="job")
+    post_job_actions = relationship("PostJobActionAssociation", back_populates="job")
+    input_library_datasets = relationship("JobToInputLibraryDatasetAssociation", back_populates="job")
+    output_library_datasets = relationship("JobToOutputLibraryDatasetAssociation", back_populates="job")
+    external_output_metadata = relationship("JobExternalOutputMetadata", back_populates="job")
+    tasks = relationship("Task", back_populates="job")
+    output_datasets = relationship("JobToOutputDatasetAssociation", back_populates="job")
+    state_history = relationship("JobStateHistory")
+    text_metrics = relationship("JobMetricText")
+    numeric_metrics = relationship("JobMetricNumeric")
+    interactivetool_entry_points = relationship("InteractiveToolEntryPoint", back_populates="job", uselist=True)
+    implicit_collection_jobs_association = relationship(
+        "ImplicitCollectionJobsJobAssociation", back_populates="job", uselist=False
+    )
+    container = relationship("JobContainerAssociation", back_populates="job", uselist=False)
+    data_manager_association = relationship("DataManagerJobAssociation", back_populates="job", uselist=False)
+    history_dataset_collection_associations = relationship("HistoryDatasetCollectionAssociation", back_populates="job")
+    workflow_invocation_step = relationship("WorkflowInvocationStep", back_populates="job", uselist=False)
 
     any_output_dataset_collection_instances_deleted: column_property  # defined at the end of this module
     any_output_dataset_deleted: column_property  # defined at the end of this module
 
-    dict_collection_visible_keys = ['id', 'state', 'exit_code', 'update_time', 'create_time', 'galaxy_version']
-    dict_element_visible_keys = ['id', 'state', 'exit_code', 'update_time', 'create_time', 'galaxy_version', 'command_version']
+    dict_collection_visible_keys = ["id", "state", "exit_code", "update_time", "create_time", "galaxy_version"]
+    dict_element_visible_keys = [
+        "id",
+        "state",
+        "exit_code",
+        "update_time",
+        "create_time",
+        "galaxy_version",
+        "command_version",
+    ]
 
     _numeric_metric = JobMetricNumeric
     _text_metric = JobMetricText
 
     class states(str, Enum):
-        NEW = 'new'
-        RESUBMITTED = 'resubmitted'
-        UPLOAD = 'upload'
-        WAITING = 'waiting'
-        QUEUED = 'queued'
-        RUNNING = 'running'
-        OK = 'ok'
-        ERROR = 'error'
-        FAILED = 'failed'
-        PAUSED = 'paused'
-        DELETING = 'deleting'
-        DELETED = 'deleted'
-        DELETED_NEW = 'deleted_new'  # now DELETING, remove after 21.0
-        STOPPING = 'stop'
-        STOPPED = 'stopped'
-
-    terminal_states = [states.OK,
-                       states.ERROR,
-                       states.DELETED]
+        NEW = "new"
+        RESUBMITTED = "resubmitted"
+        UPLOAD = "upload"
+        WAITING = "waiting"
+        QUEUED = "queued"
+        RUNNING = "running"
+        OK = "ok"
+        ERROR = "error"
+        FAILED = "failed"
+        PAUSED = "paused"
+        DELETING = "deleting"
+        DELETED = "deleted"
+        DELETED_NEW = "deleted_new"  # now DELETING, remove after 21.0
+        STOPPING = "stop"
+        STOPPED = "stopped"
+        SKIPPED = "skipped"
+
+    terminal_states = [states.OK, states.ERROR, states.DELETED]
     #: job states where the job hasn't finished and the model may still change
     non_ready_states = [
         states.NEW,
         states.RESUBMITTED,
         states.UPLOAD,
         states.WAITING,
         states.QUEUED,
@@ -1027,30 +1107,33 @@
             states.OK,
             states.ERROR,
             states.DELETING,
             states.DELETED,
             states.DELETED_NEW,
         ]
 
-    def io_dicts(self, exclude_implicit_outputs=False):
-        inp_data = {da.name: da.dataset for da in self.input_datasets}
-        out_data = {da.name: da.dataset for da in self.output_datasets}
+    def io_dicts(self, exclude_implicit_outputs=False) -> IoDicts:
+        inp_data: Dict[str, Optional["DatasetInstance"]] = {da.name: da.dataset for da in self.input_datasets}
+        out_data: Dict[str, "DatasetInstance"] = {da.name: da.dataset for da in self.output_datasets}
         inp_data.update([(da.name, da.dataset) for da in self.input_library_datasets])
         out_data.update([(da.name, da.dataset) for da in self.output_library_datasets])
 
+        out_collections: Dict[str, Union["DatasetCollectionInstance", "DatasetCollection"]]
         if not exclude_implicit_outputs:
-            out_collections = {obj.name: obj.dataset_collection_instance for obj in self.output_dataset_collection_instances}
+            out_collections = {
+                obj.name: obj.dataset_collection_instance for obj in self.output_dataset_collection_instances
+            }
         else:
             out_collections = {}
             for obj in self.output_dataset_collection_instances:
                 if obj.name not in out_data:
                     out_collections[obj.name] = obj.dataset_collection_instance
                 # else this is a mapped over output
         out_collections.update([(obj.name, obj.dataset_collection) for obj in self.output_dataset_collections])
-        return inp_data, out_data, out_collections
+        return IoDicts(inp_data, out_data, out_collections)
 
     # TODO: Add accessors for members defined in SQL Alchemy for the Job table and
     # for the mapper defined to the Job table.
     def get_external_output_metadata(self):
         """
         The external_output_metadata is currently a reference from Job to
         JobExternalOutputMetadata. It exists for a job but not a task.
@@ -1216,30 +1299,39 @@
     def add_parameter(self, name, value):
         self.parameters.append(JobParameter(name, value))
 
     def add_input_dataset(self, name, dataset=None, dataset_id=None):
         assoc = JobToInputDatasetAssociation(name, dataset)
         if dataset is None and dataset_id is not None:
             assoc.dataset_id = dataset_id
+        add_object_to_object_session(self, assoc)
         self.input_datasets.append(assoc)
 
     def add_output_dataset(self, name, dataset):
-        self.output_datasets.append(JobToOutputDatasetAssociation(name, dataset))
+        joda = JobToOutputDatasetAssociation(name, dataset)
+        add_object_to_object_session(self, joda)
+        self.output_datasets.append(joda)
 
     def add_input_dataset_collection(self, name, dataset_collection):
         self.input_dataset_collections.append(JobToInputDatasetCollectionAssociation(name, dataset_collection))
 
     def add_input_dataset_collection_element(self, name, dataset_collection_element):
-        self.input_dataset_collection_elements.append(JobToInputDatasetCollectionElementAssociation(name, dataset_collection_element))
+        self.input_dataset_collection_elements.append(
+            JobToInputDatasetCollectionElementAssociation(name, dataset_collection_element)
+        )
 
     def add_output_dataset_collection(self, name, dataset_collection_instance):
-        self.output_dataset_collection_instances.append(JobToOutputDatasetCollectionAssociation(name, dataset_collection_instance))
+        self.output_dataset_collection_instances.append(
+            JobToOutputDatasetCollectionAssociation(name, dataset_collection_instance)
+        )
 
     def add_implicit_output_dataset_collection(self, name, dataset_collection):
-        self.output_dataset_collections.append(JobToImplicitOutputDatasetCollectionAssociation(name, dataset_collection))
+        self.output_dataset_collections.append(
+            JobToImplicitOutputDatasetCollectionAssociation(name, dataset_collection)
+        )
 
     def add_input_library_dataset(self, name, dataset):
         self.input_library_datasets.append(JobToInputLibraryDatasetAssociation(name, dataset))
 
     def add_output_library_dataset(self, name, dataset):
         self.output_library_datasets.append(JobToOutputLibraryDatasetAssociation(name, dataset))
 
@@ -1308,24 +1400,24 @@
             # Do not modify the state/outputs of jobs that are already terminal
             return
         if track_jobs_in_database:
             self.state = Job.states.DELETING
         else:
             self.state = Job.states.DELETED
         self.info = "Job output deleted by user before job completed."
-        for dataset_assoc in self.output_datasets:
-            dataset = dataset_assoc.dataset
-            dataset.deleted = True
-            dataset.state = dataset.states.DISCARDED
-            for dataset in dataset.dataset.history_associations:
+        for jtoda in self.output_datasets:
+            output_hda = jtoda.dataset
+            output_hda.deleted = True
+            output_hda.state = output_hda.states.DISCARDED
+            for shared_hda in output_hda.dataset.history_associations:
                 # propagate info across shared datasets
-                dataset.deleted = True
-                dataset.blurb = 'deleted'
-                dataset.peek = 'Job deleted'
-                dataset.info = 'Job output deleted by user before job completed'
+                shared_hda.deleted = True
+                shared_hda.blurb = "deleted"
+                shared_hda.peek = "Job deleted"
+                shared_hda.info = "Job output deleted by user before job completed"
 
     def mark_failed(self, info="Job execution failed", blurb=None, peek=None):
         """
         Mark this job as failed, and mark any output datasets as errored.
         """
         self.state = self.states.FAILED
         self.info = info
@@ -1350,29 +1442,29 @@
                 job.resume(flush=False)
             if flush:
                 object_session(self).flush()
 
     def _serialize(self, id_encoder, serialization_options):
         job_attrs = dict_for(self)
         serialization_options.attach_identifier(id_encoder, self, job_attrs)
-        job_attrs['tool_id'] = self.tool_id
-        job_attrs['tool_version'] = self.tool_version
-        job_attrs['galaxy_version'] = self.galaxy_version
-        job_attrs['state'] = self.state
-        job_attrs['info'] = self.info
-        job_attrs['traceback'] = self.traceback
-        job_attrs['command_line'] = self.command_line
-        job_attrs['tool_stderr'] = self.tool_stderr
-        job_attrs['job_stderr'] = self.job_stderr
-        job_attrs['tool_stdout'] = self.tool_stdout
-        job_attrs['job_stdout'] = self.job_stdout
-        job_attrs['exit_code'] = self.exit_code
-        job_attrs['create_time'] = self.create_time.isoformat()
-        job_attrs['update_time'] = self.update_time.isoformat()
-        job_attrs['job_messages'] = self.job_messages
+        job_attrs["tool_id"] = self.tool_id
+        job_attrs["tool_version"] = self.tool_version
+        job_attrs["galaxy_version"] = self.galaxy_version
+        job_attrs["state"] = self.state
+        job_attrs["info"] = self.info
+        job_attrs["traceback"] = self.traceback
+        job_attrs["command_line"] = self.command_line
+        job_attrs["tool_stderr"] = self.tool_stderr
+        job_attrs["job_stderr"] = self.job_stderr
+        job_attrs["tool_stdout"] = self.tool_stdout
+        job_attrs["job_stdout"] = self.job_stdout
+        job_attrs["exit_code"] = self.exit_code
+        job_attrs["create_time"] = self.create_time.isoformat()
+        job_attrs["update_time"] = self.update_time.isoformat()
+        job_attrs["job_messages"] = self.job_messages
 
         # Get the job's parameters
         param_dict = self.raw_param_dict()
         params_objects = {}
         for key in param_dict:
             params_objects[key] = safe_loads(param_dict[key])
 
@@ -1385,125 +1477,141 @@
             return (k, obj)
 
         params_objects = remap(params_objects, remap_objects)
 
         params_dict = {}
         for name, value in params_objects.items():
             params_dict[name] = value
-        job_attrs['params'] = params_dict
+        job_attrs["params"] = params_dict
         return job_attrs
 
-    def to_dict(self, view='collection', system_details=False):
-        if view == 'admin_job_list':
-            rval = super().to_dict(view='collection')
+    def to_dict(self, view="collection", system_details=False):
+        if view == "admin_job_list":
+            rval = super().to_dict(view="collection")
         else:
             rval = super().to_dict(view=view)
-        rval['tool_id'] = self.tool_id
-        rval['history_id'] = self.history_id
-        if system_details or view == 'admin_job_list':
+        rval["tool_id"] = self.tool_id
+        rval["history_id"] = self.history_id
+        if system_details or view == "admin_job_list":
             # System level details that only admins should have.
-            rval['external_id'] = self.job_runner_external_id
-            rval['command_line'] = self.command_line
-            rval['traceback'] = self.traceback
-        if view == 'admin_job_list':
-            rval['user_email'] = self.user.email if self.user else None
-            rval['handler'] = self.handler
-            rval['job_runner_name'] = self.job_runner_name
-            rval['info'] = self.info
-            rval['session_id'] = self.session_id
+            rval["external_id"] = self.job_runner_external_id
+            rval["command_line"] = self.command_line
+            rval["traceback"] = self.traceback
+        if view == "admin_job_list":
+            rval["user_email"] = self.user.email if self.user else None
+            rval["handler"] = self.handler
+            rval["job_runner_name"] = self.job_runner_name
+            rval["info"] = self.info
+            rval["session_id"] = self.session_id
             if self.galaxy_session and self.galaxy_session.remote_host:
-                rval['remote_host'] = self.galaxy_session.remote_host
-        if view == 'element':
+                rval["remote_host"] = self.galaxy_session.remote_host
+        if view == "element":
             param_dict = {p.name: p.value for p in self.parameters}
-            rval['params'] = param_dict
+            rval["params"] = param_dict
 
             input_dict = {}
             for i in self.input_datasets:
                 if i.dataset is not None:
                     input_dict[i.name] = {
-                        "id": i.dataset.id, "src": "hda",
-                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None
+                        "id": i.dataset.id,
+                        "src": "hda",
+                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None,
                     }
             for i in self.input_library_datasets:
                 if i.dataset is not None:
                     input_dict[i.name] = {
-                        "id": i.dataset.id, "src": "ldda",
-                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None
+                        "id": i.dataset.id,
+                        "src": "ldda",
+                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None,
                     }
             for k in input_dict:
                 if k in param_dict:
                     del param_dict[k]
-            rval['inputs'] = input_dict
+            rval["inputs"] = input_dict
 
             output_dict = {}
             for i in self.output_datasets:
                 if i.dataset is not None:
                     output_dict[i.name] = {
-                        "id": i.dataset.id, "src": "hda",
-                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None
+                        "id": i.dataset.id,
+                        "src": "hda",
+                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None,
                     }
             for i in self.output_library_datasets:
                 if i.dataset is not None:
                     output_dict[i.name] = {
-                        "id": i.dataset.id, "src": "ldda",
-                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None
+                        "id": i.dataset.id,
+                        "src": "ldda",
+                        "uuid": str(i.dataset.dataset.uuid) if i.dataset.dataset.uuid is not None else None,
                     }
-            rval['outputs'] = output_dict
-            rval['output_collections'] = {jtodca.name: {'id': jtodca.dataset_collection_instance.id, 'src': 'hdca'} for jtodca in self.output_dataset_collection_instances}
+            rval["outputs"] = output_dict
+            rval["output_collections"] = {
+                jtodca.name: {"id": jtodca.dataset_collection_instance.id, "src": "hdca"}
+                for jtodca in self.output_dataset_collection_instances
+            }
 
         return rval
 
     def update_hdca_update_time_for_job(self, update_time, sa_session, supports_skip_locked):
-        subq = sa_session.query(HistoryDatasetCollectionAssociation.id) \
-            .join(ImplicitCollectionJobs) \
-            .join(ImplicitCollectionJobsJobAssociation) \
+        subq = (
+            sa_session.query(HistoryDatasetCollectionAssociation.id)
+            .join(ImplicitCollectionJobs)
+            .join(ImplicitCollectionJobsJobAssociation)
             .filter(ImplicitCollectionJobsJobAssociation.job_id == self.id)
+        )
         if supports_skip_locked:
             subq = subq.with_for_update(skip_locked=True).subquery()
-        implicit_statement = HistoryDatasetCollectionAssociation.table.update() \
-            .where(HistoryDatasetCollectionAssociation.table.c.id.in_(select(subq))) \
+        implicit_statement = (
+            HistoryDatasetCollectionAssociation.table.update()
+            .where(HistoryDatasetCollectionAssociation.table.c.id.in_(select(subq)))
             .values(update_time=update_time)
-        explicit_statement = HistoryDatasetCollectionAssociation.table.update() \
-            .where(HistoryDatasetCollectionAssociation.table.c.job_id == self.id) \
+        )
+        explicit_statement = (
+            HistoryDatasetCollectionAssociation.table.update()
+            .where(HistoryDatasetCollectionAssociation.table.c.job_id == self.id)
             .values(update_time=update_time)
+        )
         sa_session.execute(explicit_statement)
         if supports_skip_locked:
             sa_session.execute(implicit_statement)
         else:
-            conn = sa_session.connection(execution_options={'isolation_level': 'SERIALIZABLE'})
+            conn = sa_session.connection(execution_options={"isolation_level": "SERIALIZABLE"})
             with conn.begin() as trans:
                 try:
                     conn.execute(implicit_statement)
                     trans.commit()
                 except OperationalError as e:
                     # If this is a serialization failure on PostgreSQL, then e.orig is a psycopg2 TransactionRollbackError
                     # and should have attribute `code`. Other engines should just report the message and move on.
-                    if int(getattr(e.orig, 'pgcode', -1)) != 40001:
-                        log.debug(f"Updating implicit collection uptime_time for job {self.id} failed (this is expected for large collections and not a problem): {unicodify(e)}")
+                    if int(getattr(e.orig, "pgcode", -1)) != 40001:
+                        log.debug(
+                            f"Updating implicit collection uptime_time for job {self.id} failed (this is expected for large collections and not a problem): {unicodify(e)}"
+                        )
                     trans.rollback()
 
     def set_final_state(self, final_state, supports_skip_locked):
         self.set_state(final_state)
         # TODO: migrate to where-in subqueries?
-        statement = '''
+        statement = text(
+            """
             UPDATE workflow_invocation_step
             SET update_time = :update_time
             WHERE job_id = :job_id;
-        '''
+        """
+        )
         sa_session = object_session(self)
         update_time = now()
-        self.update_hdca_update_time_for_job(update_time=update_time, sa_session=sa_session, supports_skip_locked=supports_skip_locked)
-        params = {
-            'job_id': self.id,
-            'update_time': update_time
-        }
+        self.update_hdca_update_time_for_job(
+            update_time=update_time, sa_session=sa_session, supports_skip_locked=supports_skip_locked
+        )
+        params = {"job_id": self.id, "update_time": update_time}
         sa_session.execute(statement, params)
 
     def get_destination_configuration(self, dest_params, config, key, default=None):
-        """ Get a destination parameter that can be defaulted back
+        """Get a destination parameter that can be defaulted back
         in specified config if it needs to be applied globally.
         """
         param_unspecified = object()
         config_value = (self.destination_params or {}).get(key, param_unspecified)
         if config_value is param_unspecified:
             config_value = dest_params.get(key, param_unspecified)
         if config_value is param_unspecified:
@@ -1516,79 +1624,89 @@
     def command_version(self):
         # TODO: make actual database property and track properly - we should be recording this on the job and not on the datasets
         for dataset_assoc in self.output_datasets:
             return dataset_assoc.dataset.tool_version
 
     def update_output_states(self, supports_skip_locked):
         # TODO: migrate to where-in subqueries?
-        statements = ['''
+        statements = [
+            text(
+                """
             UPDATE dataset
             SET
                 state = :state,
                 update_time = :update_time
             WHERE id IN (
                 SELECT hda.dataset_id FROM history_dataset_association hda
                 INNER JOIN job_to_output_dataset jtod
                 ON jtod.dataset_id = hda.id AND jtod.job_id = :job_id
             );
-        ''', '''
+        """
+            ),
+            text(
+                """
             UPDATE dataset
             SET
                 state = :state,
                 update_time = :update_time
             WHERE id IN (
                 SELECT ldda.dataset_id FROM library_dataset_dataset_association ldda
                 INNER JOIN job_to_output_library_dataset jtold
                 ON jtold.ldda_id = ldda.id AND jtold.job_id = :job_id
             );
-        ''', '''
+        """
+            ),
+            text(
+                """
             UPDATE history_dataset_association
             SET
                 info = :info,
                 update_time = :update_time
             WHERE id IN (
                 SELECT jtod.dataset_id
                 FROM job_to_output_dataset jtod
                 WHERE jtod.job_id = :job_id
             );
-        ''', '''
+        """
+            ),
+            text(
+                """
             UPDATE library_dataset_dataset_association
             SET
                 info = :info,
                 update_time = :update_time
             WHERE id IN (
                 SELECT jtold.ldda_id
                 FROM job_to_output_library_dataset jtold
                 WHERE jtold.job_id = :job_id
             );
-        ''']
+        """
+            ),
+        ]
         sa_session = object_session(self)
         update_time = now()
-        self.update_hdca_update_time_for_job(update_time=update_time, sa_session=sa_session, supports_skip_locked=supports_skip_locked)
-        params = {
-            'job_id': self.id,
-            'state': self.state,
-            'info': self.info,
-            'update_time': update_time
-        }
+        self.update_hdca_update_time_for_job(
+            update_time=update_time, sa_session=sa_session, supports_skip_locked=supports_skip_locked
+        )
+        params = {"job_id": self.id, "state": self.state, "info": self.info, "update_time": update_time}
         for statement in statements:
             sa_session.execute(statement, params)
 
     def remappable(self):
         """
         Check whether job is remappable when rerun
         """
         if self.state == self.states.ERROR:
             try:
                 for jtod in self.output_datasets:
                     if jtod.dataset.dependent_jobs:
                         return True
                 if self.output_dataset_collection_instances:
                     # We'll want to replace this item
-                    return 'job_produced_collection_elements'
+                    return "job_produced_collection_elements"
             except Exception:
                 log.exception(f"Error trying to determine if job {self.id} is remappable")
         return False
 
     def hide_outputs(self, flush=True):
         for output_association in self.output_datasets + self.output_dataset_collection_instances:
             output_association.item.visible = False
@@ -1596,15 +1714,16 @@
             object_session(self).flush()
 
 
 class Task(Base, JobLike, RepresentById):
     """
     A task represents a single component of a job.
     """
-    __tablename__ = 'task'
+
+    __tablename__ = "task"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     execution_time = Column(DateTime)
     update_time = Column(DateTime, default=now, onupdate=now)
     state = Column(String(64), index=True)
     command_line = Column(TEXT)
@@ -1614,40 +1733,41 @@
     job_stderr = Column(TEXT)
     tool_stdout = Column(TEXT)
     tool_stderr = Column(TEXT)
     exit_code = Column(Integer, nullable=True)
     job_messages = Column(MutableJSONType, nullable=True)
     info = Column(TrimmedString(255))
     traceback = Column(TEXT)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True, nullable=False)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True, nullable=False)
     working_directory = Column(String(1024))
     task_runner_name = Column(String(255))
     task_runner_external_id = Column(String(255))
     prepare_input_files_cmd = Column(TEXT)
-    job = relationship('Job', back_populates='tasks')
-    text_metrics = relationship('TaskMetricText')
-    numeric_metrics = relationship('TaskMetricNumeric')
+    job = relationship("Job", back_populates="tasks")
+    text_metrics = relationship("TaskMetricText")
+    numeric_metrics = relationship("TaskMetricNumeric")
 
     _numeric_metric = TaskMetricNumeric
     _text_metric = TaskMetricText
 
     class states(str, Enum):
-        NEW = 'new'
-        WAITING = 'waiting'
-        QUEUED = 'queued'
-        RUNNING = 'running'
-        OK = 'ok'
-        ERROR = 'error'
-        DELETED = 'deleted'
+        NEW = "new"
+        WAITING = "waiting"
+        QUEUED = "queued"
+        RUNNING = "running"
+        OK = "ok"
+        ERROR = "error"
+        DELETED = "deleted"
 
     # Please include an accessor (get/set pair) for any new columns/members.
     def __init__(self, job, working_directory, prepare_files_cmd):
         self.parameters = []
         self.state = Task.states.NEW
         self.working_directory = working_directory
+        add_object_to_object_session(self, job)
         self.job = job
         self.prepare_input_files_cmd = prepare_files_cmd
         self._init_metrics()
 
     def get_param_values(self, app):
         """
         Read encoded parameter values from the database and turn back into a
@@ -1751,323 +1871,324 @@
     def set_task_runner_name(self, task_runner_name):
         self.task_runner_name = task_runner_name
 
     def set_job_runner_external_id(self, task_runner_external_id):
         # This method is available for runners that do not want/need to
         # differentiate between the kinds of Runnable things (Jobs and Tasks)
         # that they're using.
-        log.debug("Task %d: Set external id to %s"
-                  % (self.id, task_runner_external_id))
+        log.debug("Task %d: Set external id to %s" % (self.id, task_runner_external_id))
         self.task_runner_external_id = task_runner_external_id
 
     def set_task_runner_external_id(self, task_runner_external_id):
         self.task_runner_external_id = task_runner_external_id
 
     def set_job(self, job):
         self.job = job
 
     def set_prepare_input_files_cmd(self, prepare_input_files_cmd):
         self.prepare_input_files_cmd = prepare_input_files_cmd
 
 
 class JobParameter(Base, RepresentById):
-    __tablename__ = 'job_parameter'
+    __tablename__ = "job_parameter"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
     name = Column(String(255))
     value = Column(TEXT)
 
     def __init__(self, name, value):
         self.name = name
         self.value = value
 
     def copy(self):
         return JobParameter(name=self.name, value=self.value)
 
 
 class JobToInputDatasetAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_input_dataset'
+    __tablename__ = "job_to_input_dataset"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    dataset_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    dataset_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
     dataset_version = Column(Integer)
     name = Column(String(255))
-    dataset = relationship('HistoryDatasetAssociation', lazy="joined", back_populates='dependent_jobs')
-    job = relationship('Job', back_populates='input_datasets')
+    dataset = relationship("HistoryDatasetAssociation", lazy="joined", back_populates="dependent_jobs")
+    job = relationship("Job", back_populates="input_datasets")
 
     def __init__(self, name, dataset):
         self.name = name
+        add_object_to_object_session(self, dataset)
         self.dataset = dataset
         self.dataset_version = 0  # We start with version 0 and update once the job is ready
 
 
 class JobToOutputDatasetAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_output_dataset'
+    __tablename__ = "job_to_output_dataset"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    dataset_id = Column(Integer, ForeignKey('history_dataset_association.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    dataset_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
     name = Column(String(255))
-    dataset = relationship('HistoryDatasetAssociation',
-        lazy="joined", back_populates='creating_job_associations')
-    job = relationship('Job', back_populates='output_datasets')
+    dataset = relationship("HistoryDatasetAssociation", lazy="joined", back_populates="creating_job_associations")
+    job = relationship("Job", back_populates="output_datasets")
 
     def __init__(self, name, dataset):
         self.name = name
+        add_object_to_object_session(self, dataset)
         self.dataset = dataset
 
     @property
     def item(self):
         return self.dataset
 
 
 class JobToInputDatasetCollectionAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_input_dataset_collection'
+    __tablename__ = "job_to_input_dataset_collection"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    dataset_collection_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
     name = Column(String(255))
-    dataset_collection = relationship('HistoryDatasetCollectionAssociation', lazy="joined")
-    job = relationship('Job', back_populates='input_dataset_collections')
+    dataset_collection = relationship("HistoryDatasetCollectionAssociation", lazy="joined")
+    job = relationship("Job", back_populates="input_dataset_collections")
 
     def __init__(self, name, dataset_collection):
         self.name = name
         self.dataset_collection = dataset_collection
 
 
 class JobToInputDatasetCollectionElementAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_input_dataset_collection_element'
+    __tablename__ = "job_to_input_dataset_collection_element"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    dataset_collection_element_id = Column(Integer,
-        ForeignKey('dataset_collection_element.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    dataset_collection_element_id = Column(Integer, ForeignKey("dataset_collection_element.id"), index=True)
     name = Column(Unicode(255))
-    dataset_collection_element = relationship('DatasetCollectionElement', lazy="joined")
-    job = relationship('Job', back_populates='input_dataset_collection_elements')
+    dataset_collection_element = relationship("DatasetCollectionElement", lazy="joined")
+    job = relationship("Job", back_populates="input_dataset_collection_elements")
 
     def __init__(self, name, dataset_collection_element):
         self.name = name
         self.dataset_collection_element = dataset_collection_element
 
 
 # Many jobs may map to one HistoryDatasetCollection using these for a given
 # tool output (if mapping over an input collection).
 class JobToOutputDatasetCollectionAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_output_dataset_collection'
+    __tablename__ = "job_to_output_dataset_collection"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    dataset_collection_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
     name = Column(Unicode(255))
-    dataset_collection_instance = relationship('HistoryDatasetCollectionAssociation', lazy="joined")
-    job = relationship('Job', back_populates='output_dataset_collection_instances')
+    dataset_collection_instance = relationship("HistoryDatasetCollectionAssociation", lazy="joined")
+    job = relationship("Job", back_populates="output_dataset_collection_instances")
 
     def __init__(self, name, dataset_collection_instance):
         self.name = name
         self.dataset_collection_instance = dataset_collection_instance
 
     @property
     def item(self):
         return self.dataset_collection_instance
 
 
 # A DatasetCollection will be mapped to at most one job per tool output
 # using these. (You can think of many of these models as going into the
 # creation of a JobToOutputDatasetCollectionAssociation.)
 class JobToImplicitOutputDatasetCollectionAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_implicit_output_dataset_collection'
+    __tablename__ = "job_to_implicit_output_dataset_collection"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    dataset_collection_id = Column(Integer, ForeignKey('dataset_collection.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    dataset_collection_id = Column(Integer, ForeignKey("dataset_collection.id"), index=True)
     name = Column(Unicode(255))
-    dataset_collection = relationship('DatasetCollection')
-    job = relationship('Job', back_populates='output_dataset_collections')
+    dataset_collection = relationship("DatasetCollection")
+    job = relationship("Job", back_populates="output_dataset_collections")
 
     def __init__(self, name, dataset_collection):
         self.name = name
         self.dataset_collection = dataset_collection
 
 
 class JobToInputLibraryDatasetAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_input_library_dataset'
+    __tablename__ = "job_to_input_library_dataset"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    ldda_id = Column(Integer, ForeignKey('library_dataset_dataset_association.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    ldda_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True)
     name = Column(Unicode(255))
-    job = relationship('Job', back_populates='input_library_datasets')
-    dataset = relationship(
-        'LibraryDatasetDatasetAssociation', lazy="joined", back_populates='dependent_jobs')
+    job = relationship("Job", back_populates="input_library_datasets")
+    dataset = relationship("LibraryDatasetDatasetAssociation", lazy="joined", back_populates="dependent_jobs")
 
     def __init__(self, name, dataset):
         self.name = name
+        add_object_to_object_session(self, dataset)
         self.dataset = dataset
 
 
 class JobToOutputLibraryDatasetAssociation(Base, RepresentById):
-    __tablename__ = 'job_to_output_library_dataset'
+    __tablename__ = "job_to_output_library_dataset"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    ldda_id = Column(Integer, ForeignKey('library_dataset_dataset_association.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    ldda_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True)
     name = Column(Unicode(255))
-    job = relationship('Job', back_populates='output_library_datasets')
+    job = relationship("Job", back_populates="output_library_datasets")
     dataset = relationship(
-        'LibraryDatasetDatasetAssociation', lazy="joined", back_populates='creating_job_associations')
+        "LibraryDatasetDatasetAssociation", lazy="joined", back_populates="creating_job_associations"
+    )
 
     def __init__(self, name, dataset):
         self.name = name
+        add_object_to_object_session(self, dataset)
         self.dataset = dataset
 
 
 class JobStateHistory(Base, RepresentById):
-    __tablename__ = 'job_state_history'
+    __tablename__ = "job_state_history"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    update_time = Column(DateTime, default=now, onupdate=now)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
     state = Column(String(64), index=True)
     info = Column(TrimmedString(255))
 
     def __init__(self, job):
         self.job_id = job.id
         self.state = job.state
         self.info = job.info
 
 
 class ImplicitlyCreatedDatasetCollectionInput(Base, RepresentById):
-    __tablename__ = 'implicitly_created_dataset_collection_inputs'
+    __tablename__ = "implicitly_created_dataset_collection_inputs"
 
     id = Column(Integer, primary_key=True)
-    dataset_collection_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id'), index=True)
-    input_dataset_collection_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id'), index=True)
+    dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
+    input_dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
     name = Column(Unicode(255))
 
-    input_dataset_collection = relationship('HistoryDatasetCollectionAssociation',
-        primaryjoin=(lambda: HistoryDatasetCollectionAssociation.id  # type: ignore[has-type]
-            == ImplicitlyCreatedDatasetCollectionInput.input_dataset_collection_id)  # type: ignore[has-type]
+    input_dataset_collection = relationship(
+        "HistoryDatasetCollectionAssociation",
+        primaryjoin=(
+            lambda: HistoryDatasetCollectionAssociation.id  # type: ignore[has-type]
+            == ImplicitlyCreatedDatasetCollectionInput.input_dataset_collection_id
+        ),  # type: ignore[has-type]
     )
 
     def __init__(self, name, input_dataset_collection):
         self.name = name
         self.input_dataset_collection = input_dataset_collection
 
 
 class ImplicitCollectionJobs(Base, Serializable):
-    __tablename__ = 'implicit_collection_jobs'
+    __tablename__ = "implicit_collection_jobs"
 
     id = Column(Integer, primary_key=True)
-    populated_state = Column(TrimmedString(64), default='new', nullable=False)
-    jobs = relationship('ImplicitCollectionJobsJobAssociation',
-        back_populates='implicit_collection_jobs')
+    populated_state = Column(TrimmedString(64), default="new", nullable=False)
+    jobs = relationship("ImplicitCollectionJobsJobAssociation", back_populates="implicit_collection_jobs")
 
     class populated_states(str, Enum):
-        NEW = 'new'  # New implicit jobs object, unpopulated job associations
-        OK = 'ok'  # Job associations are set and fixed.
-        FAILED = 'failed'  # There were issues populating job associations, object is in error.
+        NEW = "new"  # New implicit jobs object, unpopulated job associations
+        OK = "ok"  # Job associations are set and fixed.
+        FAILED = "failed"  # There were issues populating job associations, object is in error.
 
     def __init__(self, populated_state=None):
         self.populated_state = populated_state or ImplicitCollectionJobs.populated_states.NEW
 
     @property
     def job_list(self):
         return [icjja.job for icjja in self.jobs]
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             populated_state=self.populated_state,
-            jobs=[serialization_options.get_identifier(id_encoder, j_a.job) for j_a in self.jobs]
+            jobs=[serialization_options.get_identifier(id_encoder, j_a.job) for j_a in self.jobs],
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
 
 class ImplicitCollectionJobsJobAssociation(Base, RepresentById):
-    __tablename__ = 'implicit_collection_jobs_job_association'
+    __tablename__ = "implicit_collection_jobs_job_association"
 
     id = Column(Integer, primary_key=True)
-    implicit_collection_jobs_id = Column(Integer, ForeignKey('implicit_collection_jobs.id'), index=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)  # Consider making this nullable...
+    implicit_collection_jobs_id = Column(Integer, ForeignKey("implicit_collection_jobs.id"), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)  # Consider making this nullable...
     order_index = Column(Integer, nullable=False)
-    implicit_collection_jobs = relationship('ImplicitCollectionJobs', back_populates='jobs')
-    job = relationship('Job', back_populates='implicit_collection_jobs_association')
+    implicit_collection_jobs = relationship("ImplicitCollectionJobs", back_populates="jobs")
+    job = relationship("Job", back_populates="implicit_collection_jobs_association")
 
 
 class PostJobAction(Base, RepresentById):
-    __tablename__ = 'post_job_action'
+    __tablename__ = "post_job_action"
 
     id = Column(Integer, primary_key=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True, nullable=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True, nullable=True)
     action_type = Column(String(255), nullable=False)
     output_name = Column(String(255), nullable=True)
     action_arguments = Column(MutableJSONType, nullable=True)
-    workflow_step = relationship('WorkflowStep',
-        back_populates='post_job_actions',
-        primaryjoin=(lambda: WorkflowStep.id == PostJobAction.workflow_step_id)  # type: ignore[has-type]
+    workflow_step = relationship(
+        "WorkflowStep",
+        back_populates="post_job_actions",
+        primaryjoin=(lambda: WorkflowStep.id == PostJobAction.workflow_step_id),  # type: ignore[has-type]
     )
 
     def __init__(self, action_type, workflow_step=None, output_name=None, action_arguments=None):
         self.action_type = action_type
         self.output_name = output_name
         self.action_arguments = action_arguments
         self.workflow_step = workflow_step
 
 
 class PostJobActionAssociation(Base, RepresentById):
-    __tablename__ = 'post_job_action_association'
+    __tablename__ = "post_job_action_association"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True, nullable=False)
-    post_job_action_id = Column(Integer, ForeignKey('post_job_action.id'), index=True, nullable=False)
-    post_job_action = relationship('PostJobAction')
-    job = relationship('Job', back_populates='post_job_actions')
+    job_id = Column(Integer, ForeignKey("job.id"), index=True, nullable=False)
+    post_job_action_id = Column(Integer, ForeignKey("post_job_action.id"), index=True, nullable=False)
+    post_job_action = relationship("PostJobAction")
+    job = relationship("Job", back_populates="post_job_actions")
 
     def __init__(self, pja, job=None, job_id=None):
         if job is not None:
             self.job = job
         elif job_id is not None:
             self.job_id = job_id
         else:
             raise Exception("PostJobActionAssociation must be created with a job or a job_id.")
         self.post_job_action = pja
 
 
 class JobExternalOutputMetadata(Base, RepresentById):
-    __tablename__ = 'job_external_output_metadata'
+    __tablename__ = "job_external_output_metadata"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True, nullable=True)
-    library_dataset_dataset_association_id = Column(Integer,
-        ForeignKey('library_dataset_dataset_association.id'), index=True, nullable=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    history_dataset_association_id = Column(
+        Integer, ForeignKey("history_dataset_association.id"), index=True, nullable=True
+    )
+    library_dataset_dataset_association_id = Column(
+        Integer, ForeignKey("library_dataset_dataset_association.id"), index=True, nullable=True
+    )
     is_valid = Column(Boolean, default=True)
     filename_in = Column(String(255))
     filename_out = Column(String(255))
     filename_results_code = Column(String(255))
     filename_kwds = Column(String(255))
     filename_override_metadata = Column(String(255))
     job_runner_external_pid = Column(String(255))
-    history_dataset_association = relationship('HistoryDatasetAssociation', lazy="joined")
-    library_dataset_dataset_association = relationship('LibraryDatasetDatasetAssociation', lazy="joined")
-    job = relationship('Job', back_populates='external_output_metadata')
+    history_dataset_association = relationship("HistoryDatasetAssociation", lazy="joined")
+    library_dataset_dataset_association = relationship("LibraryDatasetDatasetAssociation", lazy="joined")
+    job = relationship("Job", back_populates="external_output_metadata")
 
     def __init__(self, job=None, dataset=None):
+        add_object_to_object_session(self, job)
         self.job = job
         if isinstance(dataset, galaxy.model.HistoryDatasetAssociation):
             self.history_dataset_association = dataset
         elif isinstance(dataset, galaxy.model.LibraryDatasetDatasetAssociation):
             self.library_dataset_dataset_association = dataset
 
     @property
@@ -2092,48 +2213,49 @@
         self.metadata = dict()
 
     def __eq__(self, other):
         return isinstance(other, FakeDatasetAssociation) and self.dataset == other.dataset
 
 
 class JobExportHistoryArchive(Base, RepresentById):
-    __tablename__ = 'job_export_history_archive'
+    __tablename__ = "job_export_history_archive"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    dataset_id = Column(Integer, ForeignKey('dataset.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    dataset_id = Column(Integer, ForeignKey("dataset.id"), index=True)
     compressed = Column(Boolean, index=True, default=False)
     history_attrs_filename = Column(TEXT)
-    job = relationship('Job')
-    dataset = relationship('Dataset')
-    history = relationship('History', back_populates='exports')
+    job = relationship("Job")
+    dataset = relationship("Dataset")
+    history = relationship("History", back_populates="exports")
 
-    ATTRS_FILENAME_HISTORY = 'history_attrs.txt'
+    ATTRS_FILENAME_HISTORY = "history_attrs.txt"
 
     def __init__(self, compressed=False, **kwd):
+        if "history" in kwd:
+            add_object_to_object_session(self, kwd["history"])
         super().__init__(**kwd)
         self.compressed = compressed
 
     @property
     def fda(self):
         return FakeDatasetAssociation(self.dataset)
 
     @property
     def temp_directory(self):
         return os.path.split(self.history_attrs_filename)[0]
 
     @property
     def up_to_date(self):
-        """ Return False, if a new export should be generated for corresponding
+        """Return False, if a new export should be generated for corresponding
         history.
         """
         job = self.job
-        return job.state not in [Job.states.ERROR, Job.states.DELETED] \
-            and job.update_time > self.history.update_time
+        return job.state not in [Job.states.ERROR, Job.states.DELETED] and job.update_time > self.history.update_time
 
     @property
     def ready(self):
         return self.job.state == Job.states.OK
 
     @property
     def preparing(self):
@@ -2152,71 +2274,81 @@
     def create_for_history(history, job, sa_session, object_store, compressed):
         # Create dataset that will serve as archive.
         archive_dataset = Dataset()
         sa_session.add(archive_dataset)
         sa_session.flush()  # ensure job.id and archive_dataset.id are available
         object_store.create(archive_dataset)  # set the object store id, create dataset (if applicable)
         # Add association for keeping track of job, history, archive relationship.
-        jeha = JobExportHistoryArchive(
-            job=job, history=history,
-            dataset=archive_dataset,
-            compressed=compressed
-        )
+        jeha = JobExportHistoryArchive(job=job, history=history, dataset=archive_dataset, compressed=compressed)
         sa_session.add(jeha)
 
         #
         # Create attributes/metadata files for export.
         #
         jeha.dataset.create_extra_files_path()
         temp_output_dir = jeha.dataset.extra_files_path
 
         history_attrs_filename = os.path.join(temp_output_dir, jeha.ATTRS_FILENAME_HISTORY)
         jeha.history_attrs_filename = history_attrs_filename
         return jeha
 
     def to_dict(self):
         return {
-            'id': self.id,
-            'job_id': self.job.id,
-            'ready': self.ready,
-            'preparing': self.preparing,
-            'up_to_date': self.up_to_date,
+            "id": self.id,
+            "job_id": self.job.id,
+            "ready": self.ready,
+            "preparing": self.preparing,
+            "up_to_date": self.up_to_date,
         }
 
 
 class JobImportHistoryArchive(Base, RepresentById):
-    __tablename__ = 'job_import_history_archive'
+    __tablename__ = "job_import_history_archive"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
     archive_dir = Column(TEXT)
-    job = relationship('Job')
-    history = relationship('History')
+    job = relationship("Job")
+    history = relationship("History")
+
+
+class StoreExportAssociation(Base, RepresentById):
+    __tablename__ = "store_export_association"
+    __table_args__ = (Index("ix_store_export_object", "object_id", "object_type"),)
+
+    id = Column(Integer, primary_key=True)
+    task_uuid = Column(UUIDType(), index=True, unique=True)
+    create_time = Column(DateTime, default=now)
+    object_type = Column(TrimmedString(32))
+    object_id = Column(Integer)
+    export_metadata = Column(JSONType)
 
 
 class JobContainerAssociation(Base, RepresentById):
-    __tablename__ = 'job_container_association'
+    __tablename__ = "job_container_association"
 
     id = Column(Integer, primary_key=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True)
     container_type = Column(TEXT)
     container_name = Column(TEXT)
     container_info = Column(MutableJSONType, nullable=True)
     created_time = Column(DateTime, default=now)
     modified_time = Column(DateTime, default=now, onupdate=now)
-    job = relationship('Job', back_populates='container')
+    job = relationship("Job", back_populates="container")
 
     def __init__(self, **kwd):
+        if "job" in kwd:
+            add_object_to_object_session(self, kwd["job"])
         super().__init__(**kwd)
         self.container_info = self.container_info or {}
 
 
 class InteractiveToolEntryPoint(Base, Dictifiable, RepresentById):
-    __tablename__ = 'interactivetool_entry_point'
+    __tablename__ = "interactivetool_entry_point"
 
     id = Column(Integer, primary_key=True)
     job_id = Column(Integer, ForeignKey("job.id"), index=True)
     name = Column(TEXT)
     token = Column(TEXT)
     tool_port = Column(Integer)
     host = Column(TEXT)
@@ -2225,216 +2357,275 @@
     entry_url = Column(TEXT)
     requires_domain = Column(Boolean, default=True)
     info = Column(MutableJSONType, nullable=True)
     configured = Column(Boolean, default=False)
     deleted = Column(Boolean, default=False)
     created_time = Column(DateTime, default=now)
     modified_time = Column(DateTime, default=now, onupdate=now)
-    job = relationship('Job', back_populates='interactivetool_entry_points', uselist=False)
+    job = relationship("Job", back_populates="interactivetool_entry_points", uselist=False)
 
-    dict_collection_visible_keys = ['id', 'name', 'active', 'created_time', 'modified_time']
-    dict_element_visible_keys = ['id', 'name', 'active', 'created_time', 'modified_time']
+    dict_collection_visible_keys = [
+        "id",
+        "job_id",
+        "name",
+        "active",
+        "created_time",
+        "modified_time",
+        "output_datasets_ids",
+    ]
+    dict_element_visible_keys = [
+        "id",
+        "job_id",
+        "name",
+        "active",
+        "created_time",
+        "modified_time",
+        "output_datasets_ids",
+    ]
 
-    def __init__(self, requires_domain=True, configured=False, deleted=False, **kwd):
+    def __init__(self, requires_domain=True, configured=False, deleted=False, short_token=False, **kwd):
         super().__init__(**kwd)
         self.requires_domain = requires_domain
         self.configured = configured
         self.deleted = deleted
-        self.token = self.token or uuid4().hex
+        if short_token:
+            self.token = (self.token or uuid4().hex)[:10]
+        else:
+            self.token = self.token or uuid4().hex
         self.info = self.info or {}
 
     @property
     def active(self):
         if self.configured and not self.deleted:
             # FIXME: don't included queued?
             return not self.job.finished
         return False
 
+    @property
+    def output_datasets_ids(self):
+        return [da.dataset.id for da in self.job.output_datasets]
+
 
 class GenomeIndexToolData(Base, RepresentById):  # TODO: params arg is lost
-    __tablename__ = 'genome_index_tool_data'
+    __tablename__ = "genome_index_tool_data"
 
     id = Column(Integer, primary_key=True)
     job_id = Column(Integer, ForeignKey("job.id"), index=True)
-    dataset_id = Column(Integer, ForeignKey('dataset.id'), index=True)
+    dataset_id = Column(Integer, ForeignKey("dataset.id"), index=True)
     fasta_path = Column(String(255))
     created_time = Column(DateTime, default=now)
     modified_time = Column(DateTime, default=now, onupdate=now)
     indexer = Column(String(64))
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    job = relationship('Job')
-    dataset = relationship('Dataset')
-    user = relationship('User')
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    job = relationship("Job")
+    dataset = relationship("Dataset")
+    user = relationship("User")
 
 
 class Group(Base, Dictifiable, RepresentById):
-    __tablename__ = 'galaxy_group'
+    __tablename__ = "galaxy_group"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     name = Column(String(255), index=True, unique=True)
     deleted = Column(Boolean, index=True, default=False)
-    quotas = relationship('GroupQuotaAssociation', back_populates='group')
-    roles = relationship('GroupRoleAssociation', back_populates='group')
-    users = relationship('UserGroupAssociation', back_populates='group')
+    quotas = relationship("GroupQuotaAssociation", back_populates="group")
+    roles = relationship("GroupRoleAssociation", back_populates="group")
+    users = relationship("UserGroupAssociation", back_populates="group")
 
-    dict_collection_visible_keys = ['id', 'name']
-    dict_element_visible_keys = ['id', 'name']
+    dict_collection_visible_keys = ["id", "name"]
+    dict_element_visible_keys = ["id", "name"]
 
     def __init__(self, name=None):
         self.name = name
         self.deleted = False
 
 
 class UserGroupAssociation(Base, RepresentById):
-    __tablename__ = 'user_group_association'
+    __tablename__ = "user_group_association"
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    group_id = Column(Integer, ForeignKey('galaxy_group.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    group_id = Column(Integer, ForeignKey("galaxy_group.id"), index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    user = relationship('User', back_populates='groups')
-    group = relationship('Group', back_populates='users')
+    user = relationship("User", back_populates="groups")
+    group = relationship("Group", back_populates="users")
 
     def __init__(self, user, group):
+        add_object_to_object_session(self, user)
         self.user = user
         self.group = group
 
 
 def is_hda(d):
     return isinstance(d, HistoryDatasetAssociation)
 
 
 class HistoryAudit(Base, RepresentById):
-    __tablename__ = 'history_audit'
-    __table_args__ = (
-        PrimaryKeyConstraint(sqlite_on_conflict='IGNORE'),
-    )
+    __tablename__ = "history_audit"
+    __table_args__ = (PrimaryKeyConstraint(sqlite_on_conflict="IGNORE"),)
 
-    history_id = Column(Integer, ForeignKey('history.id'), primary_key=True, nullable=False)
+    history_id = Column(Integer, ForeignKey("history.id"), primary_key=True, nullable=False)
     update_time = Column(DateTime, default=now, primary_key=True, nullable=False)
 
     # This class should never be instantiated.
     # See https://github.com/galaxyproject/galaxy/pull/11914 for details.
     __init__ = None  # type: ignore[assignment]
 
     @classmethod
     def prune(cls, sa_session):
-        latest_subq = sa_session.query(
-            cls.history_id,
-            func.max(cls.update_time).label('max_update_time')).group_by(cls.history_id).subquery()
-        not_latest_query = sa_session.query(
-            cls.history_id, cls.update_time
-        ).select_from(latest_subq).join(
-            cls, and_(
-                cls.update_time < latest_subq.columns.max_update_time,
-                cls.history_id == latest_subq.columns.history_id)).subquery()
+        latest_subq = (
+            sa_session.query(cls.history_id, func.max(cls.update_time).label("max_update_time"))
+            .group_by(cls.history_id)
+            .subquery()
+        )
+        not_latest_query = (
+            sa_session.query(cls.history_id, cls.update_time)
+            .select_from(latest_subq)
+            .join(
+                cls,
+                and_(
+                    cls.update_time < latest_subq.columns.max_update_time,
+                    cls.history_id == latest_subq.columns.history_id,
+                ),
+            )
+            .subquery()
+        )
         q = cls.__table__.delete().where(tuple_(cls.history_id, cls.update_time).in_(select(not_latest_query)))
         sa_session.execute(q)
 
 
 class History(Base, HasTags, Dictifiable, UsesAnnotations, HasName, Serializable):
-    __tablename__ = 'history'
-    __table_args__ = (
-        Index('ix_history_slug', 'slug', mysql_length=200),
-    )
+    __tablename__ = "history"
+    __table_args__ = (Index("ix_history_slug", "slug", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    _update_time = Column('update_time', DateTime, index=True, default=now, onupdate=now)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    _update_time = Column("update_time", DateTime, index=True, default=now, onupdate=now)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     name = Column(TrimmedString(255))
     hid_counter = Column(Integer, default=1)
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
     importing = Column(Boolean, index=True, default=False)
     genome_build = Column(TrimmedString(40))
     importable = Column(Boolean, default=False)
     slug = Column(TEXT)
     published = Column(Boolean, index=True, default=False)
 
-    datasets = relationship('HistoryDatasetAssociation',
-        back_populates='history',
-        order_by=lambda: asc(HistoryDatasetAssociation.hid))  # type: ignore[has-type]
-    exports = relationship('JobExportHistoryArchive',
-        back_populates='history',
+    datasets = relationship(
+        "HistoryDatasetAssociation", back_populates="history", cascade_backrefs=False, order_by=lambda: asc(HistoryDatasetAssociation.hid)  # type: ignore[has-type]
+    )
+    exports = relationship(
+        "JobExportHistoryArchive",
+        back_populates="history",
         primaryjoin=lambda: JobExportHistoryArchive.history_id == History.id,
-        order_by=lambda: desc(JobExportHistoryArchive.id))
-    active_datasets = relationship('HistoryDatasetAssociation',
+        order_by=lambda: desc(JobExportHistoryArchive.id),
+    )
+    active_datasets = relationship(
+        "HistoryDatasetAssociation",
         primaryjoin=(
-            lambda: and_(HistoryDatasetAssociation.history_id  # type: ignore[attr-defined]
-                == History.id, not_(HistoryDatasetAssociation.deleted))  # type: ignore[has-type]
+            lambda: and_(
+                HistoryDatasetAssociation.history_id == History.id,  # type: ignore[attr-defined]
+                not_(HistoryDatasetAssociation.deleted),  # type: ignore[has-type]
+            )
         ),
         order_by=lambda: asc(HistoryDatasetAssociation.hid),  # type: ignore[has-type]
-        viewonly=True)
-    dataset_collections = relationship('HistoryDatasetCollectionAssociation', back_populates='history')
-    active_dataset_collections = relationship('HistoryDatasetCollectionAssociation',
+        viewonly=True,
+    )
+    dataset_collections = relationship("HistoryDatasetCollectionAssociation", back_populates="history")
+    active_dataset_collections = relationship(
+        "HistoryDatasetCollectionAssociation",
         primaryjoin=(
-            lambda: (and_(HistoryDatasetCollectionAssociation.history_id == History.id,  # type: ignore[has-type]
-             not_(HistoryDatasetCollectionAssociation.deleted)))  # type: ignore[has-type]
+            lambda: (
+                and_(
+                    HistoryDatasetCollectionAssociation.history_id == History.id,  # type: ignore[has-type]
+                    not_(HistoryDatasetCollectionAssociation.deleted),  # type: ignore[has-type]
+                )
+            )
         ),
         order_by=lambda: asc(HistoryDatasetCollectionAssociation.hid),  # type: ignore[has-type]
-        viewonly=True)
-    visible_datasets = relationship('HistoryDatasetAssociation',
+        viewonly=True,
+    )
+    visible_datasets = relationship(
+        "HistoryDatasetAssociation",
         primaryjoin=(
-            lambda: and_(HistoryDatasetAssociation.history_id == History.id,  # type: ignore[attr-defined]
-             not_(HistoryDatasetAssociation.deleted), HistoryDatasetAssociation.visible)  # type: ignore[has-type]
+            lambda: and_(
+                HistoryDatasetAssociation.history_id == History.id,  # type: ignore[attr-defined]
+                not_(HistoryDatasetAssociation.deleted),  # type: ignore[has-type]
+                HistoryDatasetAssociation.visible,  # type: ignore[has-type]
+            )
         ),
         order_by=lambda: asc(HistoryDatasetAssociation.hid),  # type: ignore[has-type]
-        viewonly=True)
-    visible_dataset_collections = relationship('HistoryDatasetCollectionAssociation',
+        viewonly=True,
+    )
+    visible_dataset_collections = relationship(
+        "HistoryDatasetCollectionAssociation",
         primaryjoin=(
             lambda: and_(
                 HistoryDatasetCollectionAssociation.history_id == History.id,  # type: ignore[has-type]
                 not_(HistoryDatasetCollectionAssociation.deleted),  # type: ignore[has-type]
-                HistoryDatasetCollectionAssociation.visible)  # type: ignore[has-type]
+                HistoryDatasetCollectionAssociation.visible,  # type: ignore[has-type]
+            )
         ),
         order_by=lambda: asc(HistoryDatasetCollectionAssociation.hid),  # type: ignore[has-type]
-        viewonly=True)
-    tags = relationship('HistoryTagAssociation',
-        order_by=lambda: HistoryTagAssociation.id,
-        back_populates='history')
-    annotations = relationship('HistoryAnnotationAssociation',
-        order_by=lambda: HistoryAnnotationAssociation.id,
-        back_populates='history')
-    ratings = relationship('HistoryRatingAssociation',
+        viewonly=True,
+    )
+    tags = relationship("HistoryTagAssociation", order_by=lambda: HistoryTagAssociation.id, back_populates="history")
+    annotations = relationship(
+        "HistoryAnnotationAssociation", order_by=lambda: HistoryAnnotationAssociation.id, back_populates="history"
+    )
+    ratings = relationship(
+        "HistoryRatingAssociation",
         order_by=lambda: HistoryRatingAssociation.id,  # type: ignore[has-type]
-        back_populates='history')
-    default_permissions = relationship('DefaultHistoryPermissions', back_populates='history')
-    users_shared_with = relationship('HistoryUserShareAssociation', back_populates='history')
-    galaxy_sessions = relationship('GalaxySessionToHistoryAssociation', back_populates='history')
-    workflow_invocations = relationship('WorkflowInvocation', back_populates='history')
-    user = relationship('User', back_populates='histories')
-    jobs = relationship('Job', back_populates='history')
+        back_populates="history",
+    )
+    default_permissions = relationship("DefaultHistoryPermissions", back_populates="history")
+    users_shared_with = relationship("HistoryUserShareAssociation", back_populates="history")
+    galaxy_sessions = relationship("GalaxySessionToHistoryAssociation", back_populates="history")
+    workflow_invocations = relationship("WorkflowInvocation", back_populates="history")
+    user = relationship("User", back_populates="histories")
+    jobs = relationship("Job", back_populates="history")
 
     update_time = column_property(
         select(func.max(HistoryAudit.update_time)).where(HistoryAudit.history_id == id).scalar_subquery(),
     )
     users_shared_with_count: column_property  # defined at the end of this module
     average_rating: column_property  # defined at the end of this module
 
     # Set up proxy so that
     #   History.users_shared_with
     # returns a list of users that history is shared with.
-    users_shared_with_dot_users = association_proxy('users_shared_with', 'user')
+    users_shared_with_dot_users = association_proxy("users_shared_with", "user")
 
-    dict_collection_visible_keys = ['id', 'name', 'published', 'deleted']
-    dict_element_visible_keys = ['id', 'name', 'genome_build', 'deleted', 'purged', 'update_time',
-                                 'published', 'importable', 'slug', 'empty']
-    default_name = 'Unnamed history'
+    dict_collection_visible_keys = ["id", "name", "published", "deleted"]
+    dict_element_visible_keys = [
+        "id",
+        "name",
+        "genome_build",
+        "deleted",
+        "purged",
+        "update_time",
+        "published",
+        "importable",
+        "slug",
+        "empty",
+    ]
+    default_name = "Unnamed history"
 
     def __init__(self, id=None, name=None, user=None):
         self.id = id
         self.name = name or History.default_name
         self.deleted = False
         self.purged = False
         self.importing = False
         self.published = False
+        add_object_to_object_session(self, user)
         self.user = user
         # Objects to eventually add to history
         self._pending_additions = []
         self._item_by_hid_cache = None
 
     @reconstructor
     def init_on_load(self):
@@ -2449,18 +2640,24 @@
                 item.history_id = history_id
             self._pending_additions.append(item)
 
     @property
     def empty(self):
         return self.hid_counter is None or self.hid_counter == 1
 
+    @property
+    def count(self):
+        return self.hid_counter - 1
+
     def add_pending_items(self, set_output_hid=True):
         # These are assumed to be either copies of existing datasets or new, empty datasets,
         # so we don't need to set the quota.
-        self.add_datasets(object_session(self), self._pending_additions, set_hid=set_output_hid, quota=False, flush=False)
+        self.add_datasets(
+            object_session(self), self._pending_additions, set_hid=set_output_hid, quota=False, flush=False
+        )
         self._pending_additions = []
 
     def _next_hid(self, n=1):
         """
         Generate next_hid from the database in a concurrency safe way:
         1. Retrieve hid_counter from database
         2. Increment hid_counter by n and store in database
@@ -2472,62 +2669,67 @@
         session = object_session(self)
         engine = session.bind
         table = self.__table__
         history_id = cached_id(self)
         update_stmt = update(table).where(table.c.id == history_id).values(hid_counter=table.c.hid_counter + n)
 
         with engine.begin() as conn:
-            if engine.name in ['postgres', 'postgresql']:
+            if engine.name in ["postgres", "postgresql"]:
                 stmt = update_stmt.returning(table.c.hid_counter)
                 updated_hid = conn.execute(stmt).scalar()
                 hid = updated_hid - n
             else:
                 select_stmt = select(table.c.hid_counter).where(table.c.id == history_id).with_for_update()
                 hid = conn.execute(select_stmt).scalar()
                 conn.execute(update_stmt)
 
-        session.expire(self, ['hid_counter'])
+        session.expire(self, ["hid_counter"])
         return hid
 
     def add_galaxy_session(self, galaxy_session, association=None):
         if association is None:
             self.galaxy_sessions.append(GalaxySessionToHistoryAssociation(galaxy_session, self))
         else:
             self.galaxy_sessions.append(association)
 
     def add_dataset(self, dataset, parent_id=None, genome_build=None, set_hid=True, quota=True):
         if isinstance(dataset, Dataset):
             dataset = HistoryDatasetAssociation(dataset=dataset)
             object_session(self).add(dataset)
             object_session(self).flush()
         elif not isinstance(dataset, (HistoryDatasetAssociation, HistoryDatasetCollectionAssociation)):
-            raise TypeError("You can only add Dataset and HistoryDatasetAssociation instances to a history"
-                            + f" ( you tried to add {str(dataset)} ).")
+            raise TypeError(
+                "You can only add Dataset and HistoryDatasetAssociation instances to a history"
+                + f" ( you tried to add {str(dataset)} )."
+            )
         is_dataset = is_hda(dataset)
         if parent_id:
             for data in self.datasets:
                 if data.id == parent_id:
                     dataset.hid = data.hid
                     break
             else:
                 if set_hid:
                     dataset.hid = self._next_hid()
         else:
             if set_hid:
                 dataset.hid = self._next_hid()
+        add_object_to_object_session(dataset, self)
         if quota and is_dataset and self.user:
             self.user.adjust_total_disk_usage(dataset.quota_amount(self.user))
         dataset.history = self
-        if is_dataset and genome_build not in [None, '?']:
+        if is_dataset and genome_build not in [None, "?"]:
             self.genome_build = genome_build
         dataset.history_id = self.id
         return dataset
 
-    def add_datasets(self, sa_session, datasets, parent_id=None, genome_build=None, set_hid=True, quota=True, flush=False):
-        """ Optimized version of add_dataset above that minimizes database
+    def add_datasets(
+        self, sa_session, datasets, parent_id=None, genome_build=None, set_hid=True, quota=True, flush=False
+    ):
+        """Optimized version of add_dataset above that minimizes database
         interactions when adding many datasets and collections to history at once.
         """
         optimize = len(datasets) > 1 and parent_id is None and set_hid
         if optimize:
             self.__add_datasets_optimized(datasets, genome_build=genome_build)
             if quota and self.user:
                 disk_usage = sum(d.get_total_size() for d in datasets if is_hda(d))
@@ -2539,33 +2741,34 @@
             for dataset in datasets:
                 self.add_dataset(dataset, parent_id=parent_id, genome_build=genome_build, set_hid=set_hid, quota=quota)
                 sa_session.add(dataset)
                 if flush:
                     sa_session.flush()
 
     def __add_datasets_optimized(self, datasets, genome_build=None):
-        """ Optimized version of add_dataset above that minimizes database
+        """Optimized version of add_dataset above that minimizes database
         interactions when adding many datasets to history at once under
         certain circumstances.
         """
         n = len(datasets)
 
         base_hid = self._next_hid(n=n)
-        set_genome = genome_build not in [None, '?']
+        set_genome = genome_build not in [None, "?"]
         for i, dataset in enumerate(datasets):
             dataset.hid = base_hid + i
             dataset.history = self
             dataset.history_id = cached_id(self)
             if set_genome and is_hda(dataset):
                 self.genome_build = genome_build
         return datasets
 
     def add_dataset_collection(self, history_dataset_collection, set_hid=True):
         if set_hid:
             history_dataset_collection.hid = self._next_hid()
+        add_object_to_object_session(history_dataset_collection, self)
         history_dataset_collection.history = self
         # TODO: quota?
         self.dataset_collections.append(history_dataset_collection)
         return history_dataset_collection
 
     def copy(self, name=None, target_user=None, activatable=False, all_datasets=False):
         """
@@ -2633,35 +2836,33 @@
 
     @property
     def activatable_datasets(self):
         # This needs to be a list
         return [hda for hda in self.datasets if not hda.dataset.deleted]
 
     def _serialize(self, id_encoder, serialization_options):
-
         history_attrs = dict_for(
             self,
             create_time=self.create_time.__str__(),
             update_time=self.update_time.__str__(),
             name=unicodify(self.name),
             hid_counter=self.hid_counter,
             genome_build=self.genome_build,
             annotation=unicodify(get_item_annotation_str(object_session(self), self.user, self)),
             tags=self.make_tag_string_list(),
         )
         serialization_options.attach_identifier(id_encoder, self, history_attrs)
         return history_attrs
 
-    def to_dict(self, view='collection', value_mapper=None):
-
+    def to_dict(self, view="collection", value_mapper=None):
         # Get basic value.
         rval = super().to_dict(view=view, value_mapper=value_mapper)
 
-        if view == 'element':
-            rval['size'] = int(self.disk_size)
+        if view == "element":
+            rval["size"] = int(self.disk_size)
 
         return rval
 
     @property
     def latest_export(self):
         exports = self.exports
         return exports and exports[0]
@@ -2677,153 +2878,168 @@
         if job is not None:
             # We'll flush once if there was a paused job
             object_session(job).flush()
 
     @property
     def paused_jobs(self):
         db_session = object_session(self)
-        return db_session.query(Job).filter(Job.history_id == self.id,
-                                            Job.state == Job.states.PAUSED).all()
+        return db_session.query(Job).filter(Job.history_id == self.id, Job.state == Job.states.PAUSED).all()
 
     @hybrid.hybrid_property
     def disk_size(self):
         """
         Return the size in bytes of this history by summing the 'total_size's of
         all non-purged, unique datasets within it.
         """
         # non-.expression part of hybrid.hybrid_property: called when an instance is the namespace (not the class)
         db_session = object_session(self)
         rval = db_session.query(
-            func.sum(db_session.query(HistoryDatasetAssociation.dataset_id, Dataset.total_size).join(Dataset)
-                    .filter(HistoryDatasetAssociation.table.c.history_id == self.id)
-                    .filter(HistoryDatasetAssociation.purged != true())
-                    .filter(Dataset.purged != true())
-                    # unique datasets only
-                    .distinct().subquery().c.total_size)).first()[0]
+            func.sum(
+                db_session.query(HistoryDatasetAssociation.dataset_id, Dataset.total_size)
+                .join(Dataset)
+                .filter(HistoryDatasetAssociation.table.c.history_id == self.id)
+                .filter(HistoryDatasetAssociation.purged != true())
+                .filter(Dataset.purged != true())
+                # unique datasets only
+                .distinct()
+                .subquery()
+                .c.total_size
+            )
+        ).first()[0]
         if rval is None:
             rval = 0
         return rval
 
     @disk_size.expression  # type: ignore[no-redef]
     def disk_size(cls):
         """
         Return a query scalar that will get any history's size in bytes by summing
         the 'total_size's of all non-purged, unique datasets within it.
         """
         # .expression acts as a column_property and should return a scalar
         # first, get the distinct datasets within a history that are not purged
-        hda_to_dataset_join = join(HistoryDatasetAssociation, Dataset,
-            HistoryDatasetAssociation.table.c.dataset_id == Dataset.table.c.id)
+        hda_to_dataset_join = join(
+            HistoryDatasetAssociation, Dataset, HistoryDatasetAssociation.table.c.dataset_id == Dataset.table.c.id
+        )
         distinct_datasets = (
-            select([
-                # use labels here to better access from the query above
-                HistoryDatasetAssociation.table.c.history_id.label('history_id'),
-                Dataset.total_size.label('dataset_size'),
-                Dataset.id.label('dataset_id')
-            ])
+            select(
+                [
+                    # use labels here to better access from the query above
+                    HistoryDatasetAssociation.table.c.history_id.label("history_id"),
+                    Dataset.total_size.label("dataset_size"),
+                    Dataset.id.label("dataset_id"),
+                ]
+            )
             .where(HistoryDatasetAssociation.table.c.purged != true())
             .where(Dataset.table.c.purged != true())
             .select_from(hda_to_dataset_join)
             # TODO: slow (in general) but most probably here - index total_size for easier sorting/distinct?
             .distinct()
         )
         # postgres needs an alias on FROM
-        distinct_datasets_alias = aliased(distinct_datasets, name="datasets")
+        distinct_datasets_alias = aliased(distinct_datasets.subquery(), name="datasets")
         # then, bind as property of history using the cls.id
         size_query = (
-            select([
-                func.coalesce(func.sum(distinct_datasets_alias.c.dataset_size), 0)
-            ])
+            select([func.coalesce(func.sum(distinct_datasets_alias.c.dataset_size), 0)])
             .select_from(distinct_datasets_alias)
             .where(distinct_datasets_alias.c.history_id == cls.id)
         )
         # label creates a scalar
-        return size_query.label('disk_size')
+        return size_query.label("disk_size")
 
     @property
     def disk_nice_size(self):
         """Returns human readable size of history on disk."""
         return galaxy.util.nice_size(self.disk_size)
 
     @property
     def active_dataset_and_roles_query(self):
         db_session = object_session(self)
-        return (db_session.query(HistoryDatasetAssociation)
+        return (
+            db_session.query(HistoryDatasetAssociation)
             .filter(HistoryDatasetAssociation.table.c.history_id == self.id)
             .filter(not_(HistoryDatasetAssociation.deleted))
             .order_by(HistoryDatasetAssociation.table.c.hid.asc())
-            .options(joinedload("dataset"),
-                     joinedload("dataset.actions"),
-                     joinedload("dataset.actions.role"),
-                     joinedload("tags")))
+            .options(
+                joinedload(HistoryDatasetAssociation.dataset)
+                .joinedload(Dataset.actions)
+                .joinedload(DatasetPermissions.role),
+                joinedload(HistoryDatasetAssociation.tags),
+            )
+        )
 
     @property
     def active_datasets_and_roles(self):
-        if not hasattr(self, '_active_datasets_and_roles'):
+        if not hasattr(self, "_active_datasets_and_roles"):
             self._active_datasets_and_roles = self.active_dataset_and_roles_query.all()
         return self._active_datasets_and_roles
 
     @property
     def active_visible_datasets_and_roles(self):
-        if not hasattr(self, '_active_visible_datasets_and_roles'):
-            self._active_visible_datasets_and_roles = self.active_dataset_and_roles_query.filter(HistoryDatasetAssociation.visible).all()
+        if not hasattr(self, "_active_visible_datasets_and_roles"):
+            self._active_visible_datasets_and_roles = self.active_dataset_and_roles_query.filter(
+                HistoryDatasetAssociation.visible
+            ).all()
         return self._active_visible_datasets_and_roles
 
     @property
     def active_visible_dataset_collections(self):
-        if not hasattr(self, '_active_visible_dataset_collections'):
+        if not hasattr(self, "_active_visible_dataset_collections"):
             db_session = object_session(self)
-            query = (db_session.query(HistoryDatasetCollectionAssociation)
+            query = (
+                db_session.query(HistoryDatasetCollectionAssociation)
                 .filter(HistoryDatasetCollectionAssociation.table.c.history_id == self.id)
                 .filter(not_(HistoryDatasetCollectionAssociation.deleted))
                 .filter(HistoryDatasetCollectionAssociation.visible)
                 .order_by(HistoryDatasetCollectionAssociation.table.c.hid.asc())
-                .options(joinedload("collection"),
-                         joinedload("tags")))
+                .options(
+                    joinedload(HistoryDatasetCollectionAssociation.collection),
+                    joinedload(HistoryDatasetCollectionAssociation.tags),
+                )
+            )
             self._active_visible_dataset_collections = query.all()
         return self._active_visible_dataset_collections
 
     @property
     def active_contents(self):
-        """ Return all active contents ordered by hid.
-        """
+        """Return all active contents ordered by hid."""
         return self.contents_iter(types=["dataset", "dataset_collection"], deleted=False, visible=True)
 
     def contents_iter(self, **kwds):
         """
         Fetch filtered list of contents of history.
         """
         default_contents_types = [
-            'dataset',
+            "dataset",
         ]
-        types = kwds.get('types', default_contents_types)
+        types = kwds.get("types", default_contents_types)
         iters = []
-        if 'dataset' in types:
+        if "dataset" in types:
             iters.append(self.__dataset_contents_iter(**kwds))
-        if 'dataset_collection' in types:
+        if "dataset_collection" in types:
             iters.append(self.__collection_contents_iter(**kwds))
         return galaxy.util.merge_sorted_iterables(operator.attrgetter("hid"), *iters)
 
     def __dataset_contents_iter(self, **kwds):
         return self.__filter_contents(HistoryDatasetAssociation, **kwds)
 
     def __filter_contents(self, content_class, **kwds):
         db_session = object_session(self)
         assert db_session is not None
         query = db_session.query(content_class).filter(content_class.table.c.history_id == self.id)
         query = query.order_by(content_class.table.c.hid.asc())
-        deleted = galaxy.util.string_as_bool_or_none(kwds.get('deleted', None))
+        deleted = galaxy.util.string_as_bool_or_none(kwds.get("deleted", None))
         if deleted is not None:
             query = query.filter(content_class.deleted == deleted)
-        visible = galaxy.util.string_as_bool_or_none(kwds.get('visible', None))
+        visible = galaxy.util.string_as_bool_or_none(kwds.get("visible", None))
         if visible is not None:
             query = query.filter(content_class.visible == visible)
-        if 'ids' in kwds:
-            ids = kwds['ids']
-            max_in_filter_length = kwds.get('max_in_filter_length', MAX_IN_FILTER_LENGTH)
+        if "ids" in kwds:
+            ids = kwds["ids"]
+            max_in_filter_length = kwds.get("max_in_filter_length", MAX_IN_FILTER_LENGTH)
             if len(ids) < max_in_filter_length:
                 query = query.filter(content_class.id.in_(ids))
             else:
                 query = (content for content in query if content.id in ids)
         return query
 
     def __collection_contents_iter(self, **kwds):
@@ -2831,144 +3047,157 @@
 
 
 class UserShareAssociation(RepresentById):
     user: Optional[User]
 
 
 class HistoryUserShareAssociation(Base, UserShareAssociation):
-    __tablename__ = 'history_user_share_association'
+    __tablename__ = "history_user_share_association"
 
     id = Column(Integer, primary_key=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    user = relationship('User')
-    history = relationship('History', back_populates='users_shared_with')
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    user = relationship("User")
+    history = relationship("History", back_populates="users_shared_with")
 
 
 class UserRoleAssociation(Base, RepresentById):
-    __tablename__ = 'user_role_association'
+    __tablename__ = "user_role_association"
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
 
-    user = relationship('User', back_populates="roles")
-    role = relationship('Role', back_populates="users")
+    user = relationship("User", back_populates="roles")
+    role = relationship("Role", back_populates="users")
 
     def __init__(self, user, role):
+        add_object_to_object_session(self, user)
         self.user = user
         self.role = role
 
 
 class GroupRoleAssociation(Base, RepresentById):
-    __tablename__ = 'group_role_association'
+    __tablename__ = "group_role_association"
 
     id = Column(Integer, primary_key=True)
-    group_id = Column(Integer, ForeignKey('galaxy_group.id'), index=True)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
+    group_id = Column(Integer, ForeignKey("galaxy_group.id"), index=True)
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    group = relationship('Group', back_populates='roles')
-    role = relationship('Role', back_populates='groups')
+    group = relationship("Group", back_populates="roles")
+    role = relationship("Role", back_populates="groups")
 
     def __init__(self, group, role):
         self.group = group
         self.role = role
 
 
 class Role(Base, Dictifiable, RepresentById):
-    __tablename__ = 'role'
+    __tablename__ = "role"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     name = Column(String(255), index=True, unique=True)
     description = Column(TEXT)
     type = Column(String(40), index=True)
     deleted = Column(Boolean, index=True, default=False)
-    dataset_actions = relationship('DatasetPermissions', back_populates='role')
-    groups = relationship('GroupRoleAssociation', back_populates='role')
-    users = relationship('UserRoleAssociation', back_populates='role')
+    dataset_actions = relationship("DatasetPermissions", back_populates="role")
+    groups = relationship("GroupRoleAssociation", back_populates="role")
+    users = relationship("UserRoleAssociation", back_populates="role")
 
-    dict_collection_visible_keys = ['id', 'name']
-    dict_element_visible_keys = ['id', 'name', 'description', 'type']
+    dict_collection_visible_keys = ["id", "name"]
+    dict_element_visible_keys = ["id", "name", "description", "type"]
     private_id = None
 
     class types(str, Enum):
-        PRIVATE = 'private'
-        SYSTEM = 'system'
-        USER = 'user'
-        ADMIN = 'admin'
-        SHARING = 'sharing'
+        PRIVATE = "private"
+        SYSTEM = "system"
+        USER = "user"
+        ADMIN = "admin"
+        SHARING = "sharing"
 
     def __init__(self, name=None, description=None, type=types.SYSTEM, deleted=False):
         self.name = name
         self.description = description
         self.type = type
         self.deleted = deleted
 
 
 class UserQuotaAssociation(Base, Dictifiable, RepresentById):
-    __tablename__ = 'user_quota_association'
+    __tablename__ = "user_quota_association"
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    quota_id = Column(Integer, ForeignKey('quota.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    quota_id = Column(Integer, ForeignKey("quota.id"), index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    user = relationship('User', back_populates='quotas')
-    quota = relationship('Quota', back_populates='users')
+    user = relationship("User", back_populates="quotas")
+    quota = relationship("Quota", back_populates="users")
 
-    dict_element_visible_keys = ['user']
+    dict_element_visible_keys = ["user"]
 
     def __init__(self, user, quota):
+        add_object_to_object_session(self, user)
         self.user = user
         self.quota = quota
 
 
 class GroupQuotaAssociation(Base, Dictifiable, RepresentById):
-    __tablename__ = 'group_quota_association'
+    __tablename__ = "group_quota_association"
 
     id = Column(Integer, primary_key=True)
     group_id = Column(Integer, ForeignKey("galaxy_group.id"), index=True)
-    quota_id = Column(Integer, ForeignKey('quota.id'), index=True)
+    quota_id = Column(Integer, ForeignKey("quota.id"), index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    group = relationship('Group', back_populates='quotas')
-    quota = relationship('Quota', back_populates='groups')
+    group = relationship("Group", back_populates="quotas")
+    quota = relationship("Quota", back_populates="groups")
 
-    dict_element_visible_keys = ['group']
+    dict_element_visible_keys = ["group"]
 
     def __init__(self, group, quota):
+        add_object_to_object_session(self, group)
         self.group = group
         self.quota = quota
 
 
 class Quota(Base, Dictifiable, RepresentById):
-    __tablename__ = 'quota'
+    __tablename__ = "quota"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     name = Column(String(255), index=True, unique=True)
     description = Column(TEXT)
     bytes = Column(BigInteger)
     operation = Column(String(8))
     deleted = Column(Boolean, index=True, default=False)
-    default = relationship('DefaultQuotaAssociation', back_populates='quota')
-    groups = relationship('GroupQuotaAssociation', back_populates='quota')
-    users = relationship('UserQuotaAssociation', back_populates='quota')
-
-    dict_collection_visible_keys = ['id', 'name']
-    dict_element_visible_keys = ['id', 'name', 'description', 'bytes', 'operation', 'display_amount', 'default', 'users', 'groups']
-    valid_operations = ('+', '-', '=')
+    default = relationship("DefaultQuotaAssociation", back_populates="quota")
+    groups = relationship("GroupQuotaAssociation", back_populates="quota")
+    users = relationship("UserQuotaAssociation", back_populates="quota")
+
+    dict_collection_visible_keys = ["id", "name"]
+    dict_element_visible_keys = [
+        "id",
+        "name",
+        "description",
+        "bytes",
+        "operation",
+        "display_amount",
+        "default",
+        "users",
+        "groups",
+    ]
+    valid_operations = ("+", "-", "=")
 
-    def __init__(self, name=None, description=None, amount=0, operation='='):
+    def __init__(self, name=None, description=None, amount=0, operation="="):
         self.name = name
         self.description = description
         if amount is None:
             self.bytes = -1
         else:
             self.bytes = amount
         self.operation = operation
@@ -2979,248 +3208,321 @@
         return self.bytes
 
     def set_amount(self, amount):
         if amount is None:
             self.bytes = -1
         else:
             self.bytes = amount
+
     amount = property(get_amount, set_amount)
 
     @property
     def display_amount(self):
         if self.bytes == -1:
             return "unlimited"
         else:
             return galaxy.util.nice_size(self.bytes)
 
 
 class DefaultQuotaAssociation(Base, Dictifiable, RepresentById):
-    __tablename__ = 'default_quota_association'
+    __tablename__ = "default_quota_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     type = Column(String(32), index=True, unique=True)
-    quota_id = Column(Integer, ForeignKey('quota.id'), index=True)
-    quota = relationship('Quota', back_populates='default')
+    quota_id = Column(Integer, ForeignKey("quota.id"), index=True)
+    quota = relationship("Quota", back_populates="default")
 
-    dict_element_visible_keys = ['type']
+    dict_element_visible_keys = ["type"]
 
     class types(str, Enum):
-        UNREGISTERED = 'unregistered'
-        REGISTERED = 'registered'
+        UNREGISTERED = "unregistered"
+        REGISTERED = "registered"
 
     def __init__(self, type, quota):
-        assert type in self.types.__members__.values(), 'Invalid type'
+        assert type in self.types.__members__.values(), "Invalid type"
         self.type = type
         self.quota = quota
 
 
 class DatasetPermissions(Base, RepresentById):
-    __tablename__ = 'dataset_permissions'
+    __tablename__ = "dataset_permissions"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     action = Column(TEXT)
-    dataset_id = Column(Integer, ForeignKey('dataset.id'), index=True)
+    dataset_id = Column(Integer, ForeignKey("dataset.id"), index=True)
     role_id = Column(Integer, ForeignKey("role.id"), index=True)
-    dataset = relationship('Dataset', back_populates='actions')
-    role = relationship('Role', back_populates='dataset_actions')
+    dataset = relationship("Dataset", back_populates="actions")
+    role = relationship("Role", back_populates="dataset_actions")
 
     def __init__(self, action, dataset, role=None, role_id=None):
         self.action = action
+        add_object_to_object_session(self, dataset)
         self.dataset = dataset
         if role is not None:
             self.role = role
         else:
             self.role_id = role_id
 
 
 class LibraryPermissions(Base, RepresentById):
-    __tablename__ = 'library_permissions'
+    __tablename__ = "library_permissions"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     action = Column(TEXT)
-    library_id = Column(Integer, ForeignKey('library.id'), nullable=True, index=True)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
-    library = relationship('Library', back_populates='actions')
-    role = relationship('Role')
+    library_id = Column(Integer, ForeignKey("library.id"), nullable=True, index=True)
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
+    library = relationship("Library", back_populates="actions")
+    role = relationship("Role")
 
     def __init__(self, action, library_item, role):
         self.action = action
         if isinstance(library_item, Library):
             self.library = library_item
         else:
             raise Exception(f"Invalid Library specified: {library_item.__class__.__name__}")
         self.role = role
 
 
 class LibraryFolderPermissions(Base, RepresentById):
-    __tablename__ = 'library_folder_permissions'
+    __tablename__ = "library_folder_permissions"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     action = Column(TEXT)
-    library_folder_id = Column(Integer, ForeignKey('library_folder.id'), nullable=True, index=True)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
-    folder = relationship('LibraryFolder', back_populates='actions')
-    role = relationship('Role')
+    library_folder_id = Column(Integer, ForeignKey("library_folder.id"), nullable=True, index=True)
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
+    folder = relationship("LibraryFolder", back_populates="actions")
+    role = relationship("Role")
 
     def __init__(self, action, library_item, role):
         self.action = action
         if isinstance(library_item, LibraryFolder):
             self.folder = library_item
         else:
             raise Exception(f"Invalid LibraryFolder specified: {library_item.__class__.__name__}")
         self.role = role
 
 
 class LibraryDatasetPermissions(Base, RepresentById):
-    __tablename__ = 'library_dataset_permissions'
+    __tablename__ = "library_dataset_permissions"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     action = Column(TEXT)
-    library_dataset_id = Column(Integer, ForeignKey('library_dataset.id'), nullable=True, index=True)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
-    library_dataset = relationship('LibraryDataset', back_populates='actions')
-    role = relationship('Role')
+    library_dataset_id = Column(Integer, ForeignKey("library_dataset.id"), nullable=True, index=True)
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
+    library_dataset = relationship("LibraryDataset", back_populates="actions")
+    role = relationship("Role")
 
     def __init__(self, action, library_item, role):
         self.action = action
         if isinstance(library_item, LibraryDataset):
             self.library_dataset = library_item
         else:
             raise Exception(f"Invalid LibraryDataset specified: {library_item.__class__.__name__}")
         self.role = role
 
 
 class LibraryDatasetDatasetAssociationPermissions(Base, RepresentById):
-    __tablename__ = 'library_dataset_dataset_association_permissions'
+    __tablename__ = "library_dataset_dataset_association_permissions"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     action = Column(TEXT)
     library_dataset_dataset_association_id = Column(
-        Integer, ForeignKey('library_dataset_dataset_association.id'), nullable=True, index=True)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
-    library_dataset_dataset_association = relationship('LibraryDatasetDatasetAssociation',
-        back_populates='actions')
-    role = relationship('Role')
+        Integer, ForeignKey("library_dataset_dataset_association.id"), nullable=True, index=True
+    )
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
+    library_dataset_dataset_association = relationship("LibraryDatasetDatasetAssociation", back_populates="actions")
+    role = relationship("Role")
 
     def __init__(self, action, library_item, role):
         self.action = action
         if isinstance(library_item, LibraryDatasetDatasetAssociation):
+            add_object_to_object_session(self, library_item)
             self.library_dataset_dataset_association = library_item
         else:
             raise Exception(f"Invalid LibraryDatasetDatasetAssociation specified: {library_item.__class__.__name__}")
         self.role = role
 
 
 class DefaultUserPermissions(Base, RepresentById):
-    __tablename__ = 'default_user_permissions'
+    __tablename__ = "default_user_permissions"
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     action = Column(TEXT)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
-    user = relationship('User', back_populates='default_permissions')
-    role = relationship('Role')
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
+    user = relationship("User", back_populates="default_permissions")
+    role = relationship("Role")
 
     def __init__(self, user, action, role):
+        add_object_to_object_session(self, user)
         self.user = user
         self.action = action
         self.role = role
 
 
 class DefaultHistoryPermissions(Base, RepresentById):
-    __tablename__ = 'default_history_permissions'
+    __tablename__ = "default_history_permissions"
 
     id = Column(Integer, primary_key=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
     action = Column(TEXT)
-    role_id = Column(Integer, ForeignKey('role.id'), index=True)
-    history = relationship('History', back_populates='default_permissions')
-    role = relationship('Role')
+    role_id = Column(Integer, ForeignKey("role.id"), index=True)
+    history = relationship("History", back_populates="default_permissions")
+    role = relationship("Role")
 
     def __init__(self, history, action, role):
+        add_object_to_object_session(self, history)
         self.history = history
         self.action = action
         self.role = role
 
 
 class StorableObject:
-
     def flush(self):
         sa_session = object_session(self)
         if sa_session:
             sa_session.flush()
 
 
-class Dataset(StorableObject, Serializable, _HasTable):
+class Dataset(Base, StorableObject, Serializable):
+    __tablename__ = "dataset"
+
+    id = Column(Integer, primary_key=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True, nullable=True)
+    create_time = Column(DateTime, default=now)
+    update_time = Column(DateTime, index=True, default=now, onupdate=now)
+    state = Column(TrimmedString(64), index=True)
+    deleted = Column(Boolean, index=True, default=False)
+    purged = Column(Boolean, index=True, default=False)
+    purgable = Column(Boolean, default=True)
+    object_store_id = Column(TrimmedString(255), index=True)
+    external_filename = Column(TEXT)
+    _extra_files_path = Column(TEXT)
+    created_from_basename = Column(TEXT)
+    file_size = Column(Numeric(15, 0))
+    total_size = Column(Numeric(15, 0))
+    uuid = Column(UUIDType())
+
+    actions = relationship("DatasetPermissions", back_populates="dataset")
+    job = relationship(Job, primaryjoin=(lambda: Dataset.job_id == Job.id))
+    active_history_associations = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(
+            lambda: and_(
+                Dataset.id == HistoryDatasetAssociation.dataset_id,  # type: ignore[attr-defined]
+                HistoryDatasetAssociation.deleted == false(),  # type: ignore[has-type]
+                HistoryDatasetAssociation.purged == false(),  # type: ignore[attr-defined]
+            )
+        ),
+        viewonly=True,
+    )
+    purged_history_associations = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(
+            lambda: and_(
+                Dataset.id == HistoryDatasetAssociation.dataset_id,  # type: ignore[attr-defined]
+                HistoryDatasetAssociation.purged == true(),  # type: ignore[attr-defined]
+            )
+        ),
+        viewonly=True,
+    )
+    active_library_associations = relationship(
+        "LibraryDatasetDatasetAssociation",
+        primaryjoin=(
+            lambda: and_(
+                Dataset.id == LibraryDatasetDatasetAssociation.dataset_id,  # type: ignore[attr-defined]
+                LibraryDatasetDatasetAssociation.deleted == false(),  # type: ignore[has-type]
+            )
+        ),
+        viewonly=True,
+    )
+    hashes = relationship("DatasetHash", back_populates="dataset")
+    sources = relationship("DatasetSource", back_populates="dataset")
+    history_associations = relationship("HistoryDatasetAssociation", back_populates="dataset")
+    library_associations = relationship(
+        "LibraryDatasetDatasetAssociation",
+        primaryjoin=(lambda: LibraryDatasetDatasetAssociation.table.c.dataset_id == Dataset.id),
+        back_populates="dataset",
+    )
 
     class states(str, Enum):
-        NEW = 'new'
-        UPLOAD = 'upload'
-        QUEUED = 'queued'
-        RUNNING = 'running'
-        OK = 'ok'
-        EMPTY = 'empty'
-        ERROR = 'error'
-        DISCARDED = 'discarded'
-        PAUSED = 'paused'
-        SETTING_METADATA = 'setting_metadata'
-        FAILED_METADATA = 'failed_metadata'
+        NEW = "new"
+        UPLOAD = "upload"
+        QUEUED = "queued"
+        RUNNING = "running"
+        OK = "ok"
+        EMPTY = "empty"
+        ERROR = "error"
+        PAUSED = "paused"
+        SETTING_METADATA = "setting_metadata"
+        FAILED_METADATA = "failed_metadata"
+        # Non-deleted, non-purged datasets that don't have physical files.
+        # These shouldn't have objectstores attached -
+        # 'deferred' can be materialized for jobs using
+        # attached DatasetSource objects but 'discarded'
+        # cannot (e.g. imported histories). These should still
+        # be able to have history contents associated (normal HDAs?)
+        DEFERRED = "deferred"
+        DISCARDED = "discarded"
 
         @classmethod
         def values(self):
             return self.__members__.values()
+
     # failed_metadata is only valid as DatasetInstance state currently
 
-    non_ready_states = (
-        states.NEW,
-        states.UPLOAD,
-        states.QUEUED,
-        states.RUNNING,
-        states.SETTING_METADATA
-    )
+    non_ready_states = (states.NEW, states.UPLOAD, states.QUEUED, states.RUNNING, states.SETTING_METADATA)
     ready_states = tuple(set(states.__members__.values()) - set(non_ready_states))
-    valid_input_states = tuple(
-        set(states.__members__.values()) - {states.ERROR, states.DISCARDED}
-    )
+    valid_input_states = tuple(set(states.__members__.values()) - {states.ERROR, states.DISCARDED})
     terminal_states = (
         states.OK,
         states.EMPTY,
         states.ERROR,
+        states.DEFERRED,
         states.DISCARDED,
         states.FAILED_METADATA,
     )
 
     class conversion_messages(str, Enum):
         PENDING = "pending"
         NO_DATA = "no data"
         NO_CHROMOSOME = "no chromosome"
         NO_CONVERTER = "no converter"
         NO_TOOL = "no tool"
         DATA = "data"
         ERROR = "error"
         OK = "ok"
 
-    permitted_actions = get_permitted_actions(filter='DATASET')
+    permitted_actions = get_permitted_actions(filter="DATASET")
     file_path = "/tmp/"
-    object_store = None  # This get initialized in mapping.py (method init) by app.py
+    object_store: Optional[ObjectStore] = None  # This get initialized in mapping.py (method init) by app.py
     engine = None
 
-    def __init__(self, id=None, state=None, external_filename=None, extra_files_path=None, file_size=None, purgable=True, uuid=None):
+    def __init__(
+        self,
+        id=None,
+        state=None,
+        external_filename=None,
+        extra_files_path=None,
+        file_size=None,
+        purgable=True,
+        uuid=None,
+    ):
         self.id = id
         self.uuid = get_uuid(uuid)
         self.state = state
         self.deleted = False
         self.purged = False
         self.purgable = purgable
         self.external_filename = external_filename
@@ -3236,21 +3538,21 @@
 
     def in_ready_state(self):
         return self.state in self.ready_states
 
     def get_file_name(self):
         if self.purged:
             log.warning(f"Attempt to get file name of purged dataset {self.id}")
-            return ''
+            return ""
         if not self.external_filename:
             assert self.object_store is not None, f"Object Store has not been initialized for dataset {self.id}"
             if self.object_store.exists(self):
                 file_name = self.object_store.get_filename(self)
             else:
-                file_name = ''
+                file_name = ""
             if not file_name and self.state not in (self.states.NEW, self.states.QUEUED):
                 # Queued datasets can be assigned an object store and have a filename, but they aren't guaranteed to.
                 # Anything after queued should have a file name.
                 log.warning(f"Failed to determine file name for dataset {self.id}")
             return file_name
         else:
             filename = self.external_filename
@@ -3258,36 +3560,38 @@
         return os.path.abspath(filename)
 
     def set_file_name(self, filename):
         if not filename:
             self.external_filename = None
         else:
             self.external_filename = filename
+
     file_name = property(get_file_name, set_file_name)
 
     def get_extra_files_path(self):
         # Unlike get_file_name - external_extra_files_path is not backed by an
         # actual database column so if SA instantiates this object - the
         # attribute won't exist yet.
         if not getattr(self, "external_extra_files_path", None):
             if self.object_store.exists(self, dir_only=True, extra_dir=self._extra_files_rel_path):
                 return self.object_store.get_filename(self, dir_only=True, extra_dir=self._extra_files_rel_path)
-            return ''
+            return ""
         else:
             return os.path.abspath(self.external_extra_files_path)
 
     def create_extra_files_path(self):
         if not self.extra_files_path_exists():
             self.object_store.create(self, dir_only=True, extra_dir=self._extra_files_rel_path)
 
     def set_extra_files_path(self, extra_files_path):
         if not extra_files_path:
             self.external_extra_files_path = None
         else:
             self.external_extra_files_path = extra_files_path
+
     extra_files_path = property(get_extra_files_path, set_extra_files_path)
 
     def extra_files_path_exists(self):
         return self.object_store.exists(self, extra_dir=self._extra_files_rel_path, dir_only=True)
 
     @property
     def store_by(self):
@@ -3314,26 +3618,29 @@
             try:
                 return os.path.getsize(self.external_filename)
             except OSError:
                 return 0
         else:
             return self.object_store.size(self)
 
-    def get_size(self, nice_size=False):
+    def get_size(self, nice_size=False, calculate_size=True):
         """Returns the size of the data on disk"""
         if self.file_size:
             if nice_size:
                 return galaxy.util.nice_size(self.file_size)
             else:
                 return self.file_size
-        else:
+        elif calculate_size:
+            # Hopefully we only reach this branch in sessionless mode
             if nice_size:
                 return galaxy.util.nice_size(self._calculate_size())
             else:
                 return self._calculate_size()
+        else:
+            return self.file_size or 0
 
     def set_size(self, no_extra_files=False):
         """Sets the size of the data on disk.
 
         If the caller is sure there are no extra files, pass no_extra_files as True to optimize subsequent
         calls to get_total_size or set_total_size - potentially avoiding both a database flush and check against
         the file system.
@@ -3356,15 +3663,19 @@
         if self.file_size is None:
             self.set_size()
         self.total_size = self.file_size or 0
         rel_path = self._extra_files_rel_path
         if rel_path is not None:
             if self.object_store.exists(self, extra_dir=rel_path, dir_only=True):
                 for root, _, files in os.walk(self.extra_files_path):
-                    self.total_size += sum(os.path.getsize(os.path.join(root, file)) for file in files if os.path.exists(os.path.join(root, file)))
+                    self.total_size += sum(
+                        os.path.getsize(os.path.join(root, file))
+                        for file in files
+                        if os.path.exists(os.path.join(root, file))
+                    )
         return self.total_size
 
     def has_data(self):
         """Detects whether there is any data"""
         return not self.is_new and self.get_size() > 0
 
     def mark_deleted(self):
@@ -3373,17 +3684,19 @@
     # FIXME: sqlalchemy will replace this
     def _delete(self):
         """Remove the file that corresponds to this data"""
         self.object_store.delete(self)
 
     @property
     def user_can_purge(self):
-        return self.purged is False \
-            and not bool(self.library_associations) \
+        return (
+            self.purged is False
+            and not bool(self.library_associations)
             and len(self.history_associations) == len(self.purged_history_associations)
+        )
 
     def full_delete(self):
         """Remove the file and extra files, marks deleted and purged"""
         # os.unlink( self.file_name )
         try:
             self.object_store.delete(self)
         except galaxy.exceptions.ObjectNotFound:
@@ -3416,159 +3729,216 @@
                 return True
         return False
 
     def _serialize(self, id_encoder, serialization_options):
         # serialize Dataset objects only for jobs that can actually modify these models.
         assert serialization_options.serialize_dataset_objects
 
-        def to_int(n):
-            return int(n) if n is not None else 0
+        def to_int(n) -> Optional[int]:
+            return int(n) if n is not None else None
 
         rval = dict_for(
             self,
             state=self.state,
             deleted=self.deleted,
             purged=self.purged,
             external_filename=self.external_filename,
             _extra_files_path=self._extra_files_path,
             file_size=to_int(self.file_size),
             object_store_id=self.object_store_id,
             total_size=to_int(self.total_size),
             created_from_basename=self.created_from_basename,
-            uuid=str(self.uuid or '') or None,
+            uuid=str(self.uuid or "") or None,
             hashes=list(map(lambda h: h.serialize(id_encoder, serialization_options), self.hashes)),
             sources=list(map(lambda s: s.serialize(id_encoder, serialization_options), self.sources)),
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
 
 class DatasetSource(Base, Dictifiable, Serializable):
-    __tablename__ = 'dataset_source'
+    __tablename__ = "dataset_source"
 
     id = Column(Integer, primary_key=True)
-    dataset_id = Column(Integer, ForeignKey('dataset.id'), index=True)
+    dataset_id = Column(Integer, ForeignKey("dataset.id"), index=True)
     source_uri = Column(TEXT)
     extra_files_path = Column(TEXT)
     transform = Column(MutableJSONType)
-    dataset = relationship('Dataset', back_populates='sources')
-    hashes = relationship('DatasetSourceHash', back_populates='source')
-    dict_collection_visible_keys = ['id', 'source_uri', 'extra_files_path', "transform"]
-    dict_element_visible_keys = ['id', 'source_uri', 'extra_files_path', 'transform']  # TODO: implement to_dict and add hashes...
+    dataset = relationship("Dataset", back_populates="sources")
+    hashes = relationship("DatasetSourceHash", back_populates="source")
+    dict_collection_visible_keys = ["id", "source_uri", "extra_files_path", "transform"]
+    dict_element_visible_keys = [
+        "id",
+        "source_uri",
+        "extra_files_path",
+        "transform",
+    ]  # TODO: implement to_dict and add hashes...
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             source_uri=self.source_uri,
             extra_files_path=self.extra_files_path,
             transform=self.transform,
             hashes=[h.serialize(id_encoder, serialization_options) for h in self.hashes],
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
+    def copy(self) -> "DatasetSource":
+        new_source = DatasetSource()
+        new_source.source_uri = self.source_uri
+        new_source.extra_files_path = self.extra_files_path
+        new_source.transform = self.transform
+        new_source.hashes = [h.copy() for h in self.hashes]
+        return new_source
+
 
 class DatasetSourceHash(Base, Serializable):
-    __tablename__ = 'dataset_source_hash'
+    __tablename__ = "dataset_source_hash"
 
     id = Column(Integer, primary_key=True)
-    dataset_source_id = Column(Integer, ForeignKey('dataset_source.id'), index=True)
+    dataset_source_id = Column(Integer, ForeignKey("dataset_source.id"), index=True)
     hash_function = Column(TEXT)
     hash_value = Column(TEXT)
-    source = relationship('DatasetSource', back_populates='hashes')
+    source = relationship("DatasetSource", back_populates="hashes")
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             hash_function=self.hash_function,
             hash_value=self.hash_value,
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
+    def copy(self) -> "DatasetSourceHash":
+        new_hash = DatasetSourceHash()
+        new_hash.hash_function = self.hash_function
+        new_hash.hash_value = self.hash_value
+        return new_hash
+
 
 class DatasetHash(Base, Dictifiable, Serializable):
-    __tablename__ = 'dataset_hash'
+    __tablename__ = "dataset_hash"
 
     id = Column(Integer, primary_key=True)
-    dataset_id = Column(Integer, ForeignKey('dataset.id'), index=True)
+    dataset_id = Column(Integer, ForeignKey("dataset.id"), index=True)
     hash_function = Column(TEXT)
     hash_value = Column(TEXT)
     extra_files_path = Column(TEXT)
-    dataset = relationship('Dataset', back_populates='hashes')
-    dict_collection_visible_keys = ['id', 'hash_function', 'hash_value', 'extra_files_path']
-    dict_element_visible_keys = ['id', 'hash_function', 'hash_value', 'extra_files_path']
+    dataset = relationship("Dataset", back_populates="hashes")
+    dict_collection_visible_keys = ["id", "hash_function", "hash_value", "extra_files_path"]
+    dict_element_visible_keys = ["id", "hash_function", "hash_value", "extra_files_path"]
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             hash_function=self.hash_function,
             hash_value=self.hash_value,
             extra_files_path=self.extra_files_path,
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
+    def copy(self) -> "DatasetHash":
+        new_hash = DatasetHash()
+        new_hash.hash_function = self.hash_function
+        new_hash.hash_value = self.hash_value
+        new_hash.extra_files_path = self.extra_files_path
+        return new_hash
+
 
 def datatype_for_extension(extension, datatypes_registry=None) -> "Data":
     if extension is not None:
         extension = extension.lower()
     if datatypes_registry is None:
         datatypes_registry = _get_datatypes_registry()
-    if not extension or extension == 'auto' or extension == '_sniff_':
-        extension = 'data'
+    if not extension or extension == "auto" or extension == "_sniff_":
+        extension = "data"
     ret = datatypes_registry.get_datatype_by_extension(extension)
     if ret is None:
         log.warning(f"Datatype class not found for extension '{extension}'")
-        return datatypes_registry.get_datatype_by_extension('data')
+        return datatypes_registry.get_datatype_by_extension("data")
     return ret
 
 
 class DatasetInstance(UsesCreateAndUpdateTime, _HasTable):
     """A base class for all 'dataset instances', HDAs, LDAs, etc"""
+
     states = Dataset.states
+    _state: str
     conversion_messages = Dataset.conversion_messages
     permitted_actions = Dataset.permitted_actions
+    purged: bool
+    creating_job_associations: List[Union[JobToOutputDatasetCollectionAssociation, JobToOutputDatasetAssociation]]
 
     class validated_states(str, Enum):
-        UNKNOWN = 'unknown'
-        INVALID = 'invalid'
-        OK = 'ok'
-
-    def __init__(self, id=None, hid=None, name=None, info=None, blurb=None, peek=None, tool_version=None,
-                 extension=None, dbkey=None, metadata=None, history=None, dataset=None, deleted=False,
-                 designation=None, parent_id=None, validated_state='unknown', validated_state_message=None,
-                 visible=True, create_dataset=False, sa_session=None, extended_metadata=None, flush=True,
-                 creating_job_id=None):
+        UNKNOWN = "unknown"
+        INVALID = "invalid"
+        OK = "ok"
+
+    def __init__(
+        self,
+        id=None,
+        hid=None,
+        name=None,
+        info=None,
+        blurb=None,
+        peek=None,
+        tool_version=None,
+        extension=None,
+        dbkey=None,
+        metadata=None,
+        history=None,
+        dataset=None,
+        deleted=False,
+        designation=None,
+        parent_id=None,
+        validated_state="unknown",
+        validated_state_message=None,
+        visible=True,
+        create_dataset=False,
+        sa_session=None,
+        extended_metadata=None,
+        flush=True,
+        metadata_deferred=False,
+        creating_job_id=None,
+    ):
         self.name = name or "Unnamed dataset"
         self.id = id
         self.info = info
         self.blurb = blurb
         self.peek = peek
         self.tool_version = tool_version
         self.extension = extension
         self.designation = designation
         # set private variable to None here, since the attribute may be needed in by MetadataCollection.__init__
         self._metadata = None
         self.metadata = metadata or dict()
+        self.metadata_deferred = metadata_deferred
         self.extended_metadata = extended_metadata
-        if dbkey:  # dbkey is stored in metadata, only set if non-zero, or else we could clobber one supplied by input 'metadata'
-            self._metadata['dbkey'] = dbkey
+        if (
+            dbkey
+        ):  # dbkey is stored in metadata, only set if non-zero, or else we could clobber one supplied by input 'metadata'
+            self._metadata["dbkey"] = listify(dbkey)
         self.deleted = deleted
         self.visible = visible
         self.validated_state = validated_state
         self.validated_state_message = validated_state_message
         # Relationships
         if not dataset and create_dataset:
             # Had to pass the sqlalchemy session in order to create a new dataset
             dataset = Dataset(state=Dataset.states.NEW)
             dataset.job_id = creating_job_id
             if flush:
                 sa_session.add(dataset)
                 sa_session.flush()
+        elif dataset:
+            add_object_to_object_session(self, dataset)
         self.dataset = dataset
         self.parent_id = parent_id
 
     @property
     def peek(self):
         return self._peek
 
@@ -3576,14 +3946,18 @@
     def peek(self, peek):
         self._peek = unicodify(peek, strip_null=True)
 
     @property
     def ext(self):
         return self.extension
 
+    @property
+    def has_deferred_data(self):
+        return self.get_dataset_state() == Dataset.states.DEFERRED
+
     def get_dataset_state(self):
         # self._state is currently only used when setting metadata externally
         # leave setting the state as-is, we'll currently handle this specially in the external metadata code
         if self._state:
             return self._state
         return self.dataset.state
 
@@ -3596,23 +3970,25 @@
 
     def set_dataset_state(self, state):
         if self.raw_set_dataset_state(state):
             sa_session = object_session(self)
             if sa_session:
                 object_session(self).add(self.dataset)
                 object_session(self).flush()  # flush here, because hda.flush() won't flush the Dataset object
+
     state = property(get_dataset_state, set_dataset_state)
 
     def get_file_name(self) -> str:
         if self.dataset.purged:
             return ""
         return self.dataset.get_file_name()
 
     def set_file_name(self, filename: str):
         return self.dataset.set_file_name(filename)
+
     file_name = property(get_file_name, set_file_name)
 
     def link_to(self, path):
         self.file_name = os.path.abspath(path)
         # Since we are not copying the file into Galaxy's managed
         # default file location, the dataset should never be purgable.
         self.dataset.purgable = False
@@ -3627,25 +4003,26 @@
     @property
     def datatype(self) -> "Data":
         return datatype_for_extension(self.extension)
 
     def get_metadata(self):
         # using weakref to store parent (to prevent circ ref),
         #   does a Session.clear() cause parent to be invalidated, while still copying over this non-database attribute?
-        if not hasattr(self, '_metadata_collection') or self._metadata_collection.parent != self:
+        if not hasattr(self, "_metadata_collection") or self._metadata_collection.parent != self:
             self._metadata_collection = galaxy.model.metadata.MetadataCollection(self)
         return self._metadata_collection
 
     @property
     def set_metadata_requires_flush(self):
         return self.metadata.requires_dataset_id
 
     def set_metadata(self, bunch):
         # Needs to accept a MetadataCollection, a bunch, or a dict
         self._metadata = self.metadata.make_dict_copy(bunch)
+
     metadata = property(get_metadata, set_metadata)
 
     @property
     def has_metadata_files(self):
         return len(self.metadata_file_types) > 0
 
     @property
@@ -3678,36 +4055,38 @@
             return "?"
         return dbkey[0]
 
     def set_dbkey(self, value):
         if "dbkey" in self.datatype.metadata_spec:
             if not isinstance(value, list):
                 self.metadata.dbkey = [value]
+
     dbkey = property(get_dbkey, set_dbkey)
 
     def ok_to_edit_metadata(self):
         # prevent modifying metadata when dataset is queued or running as input/output
         # This code could be more efficient, i.e. by using mappers, but to prevent slowing down loading a History panel, we'll leave the code here for now
         sa_session = object_session(self)
-        for job_to_dataset_association in sa_session.query(
-                JobToInputDatasetAssociation).filter_by(dataset_id=self.id).all() \
-                + sa_session.query(JobToOutputDatasetAssociation).filter_by(dataset_id=self.id).all():
+        for job_to_dataset_association in (
+            sa_session.query(JobToInputDatasetAssociation).filter_by(dataset_id=self.id).all()
+            + sa_session.query(JobToOutputDatasetAssociation).filter_by(dataset_id=self.id).all()
+        ):
             if job_to_dataset_association.job.state not in Job.terminal_states:
                 return False
         return True
 
     def change_datatype(self, new_ext):
         self.clear_associated_files()
         _get_datatypes_registry().change_datatype(self, new_ext)
 
-    def get_size(self, nice_size=False):
+    def get_size(self, nice_size=False, calculate_size=True):
         """Returns the size of the data on disk"""
         if nice_size:
-            return galaxy.util.nice_size(self.dataset.get_size())
-        return self.dataset.get_size()
+            return galaxy.util.nice_size(self.dataset.get_size(calculate_size=calculate_size))
+        return self.dataset.get_size(calculate_size=calculate_size)
 
     def set_size(self, **kwds):
         """Sets and gets the size of the data on disk"""
         return self.dataset.set_size(**kwds)
 
     def get_total_size(self):
         return self.dataset.get_total_size()
@@ -3733,25 +4112,21 @@
     def sources(self):
         return self.dataset.sources
 
     @property
     def hashes(self):
         return self.dataset.hashes
 
-    def get_raw_data(self):
-        """Returns the full data. To stream it open the file_name and read/write as needed"""
-        return self.datatype.get_raw_data(self)
-
     def get_mime(self):
         """Returns the mime type of the data"""
         try:
             return _get_datatypes_registry().get_mimetype_by_extension(self.extension.lower())
         except AttributeError:
             # extension is None
-            return 'data'
+            return "data"
 
     def set_peek(self, **kwd):
         return self.datatype.set_peek(self, **kwd)
 
     def init_meta(self, copy_from=None):
         return self.datatype.init_meta(self, copy_from=copy_from)
 
@@ -3828,18 +4203,33 @@
                     # Pending
                     return None
                 deps[dependency] = dep_dataset
         except NoConverterException:
             raise NoConverterException(f"A dependency ({dependency}) is missing a converter.")
         except KeyError:
             pass  # No deps
-        new_dataset = next(iter(self.datatype.convert_dataset(trans, self, target_ext, return_output=True, visible=False, deps=deps, target_context=target_context, history=history).values()))
+        new_dataset = next(
+            iter(
+                self.datatype.convert_dataset(
+                    trans,
+                    self,
+                    target_ext,
+                    return_output=True,
+                    visible=False,
+                    deps=deps,
+                    target_context=target_context,
+                    history=history,
+                ).values()
+            )
+        )
         new_dataset.name = self.name
         self.copy_attributes(new_dataset)
-        assoc = ImplicitlyConvertedDatasetAssociation(parent=self, file_type=target_ext, dataset=new_dataset, metadata_safe=False)
+        assoc = ImplicitlyConvertedDatasetAssociation(
+            parent=self, file_type=target_ext, dataset=new_dataset, metadata_safe=False
+        )
         session = trans.sa_session
         session.add(new_dataset)
         session.add(assoc)
         session.flush()
         return new_dataset
 
     def copy_attributes(self, new_dataset):
@@ -3851,15 +4241,15 @@
         """
         Returns an HDA that points to a metadata file which contains a
         converted data with the requested extension.
         """
         for name, value in self.metadata.items():
             # HACK: MetadataFile objects do not have a type/ext, so need to use metadata name
             # to determine type.
-            if dataset_ext == 'bai' and name == 'bam_index' and isinstance(value, MetadataFile):
+            if dataset_ext == "bai" and name == "bam_index" and isinstance(value, MetadataFile):
                 # HACK: MetadataFile objects cannot be used by tools, so return
                 # a fake HDA that points to metadata file.
                 fake_dataset = Dataset(state=Dataset.states.OK, external_filename=value.file_name)
                 fake_hda = HistoryDatasetAssociation(dataset=fake_dataset)
                 return fake_hda
 
     def clear_associated_files(self, metadata_safe=False, purge=False):
@@ -3902,17 +4292,21 @@
         return self.state == self.states.OK
 
     @property
     def is_pending(self):
         """
         Return true if the dataset is neither ready nor in error
         """
-        return self.state in (self.states.NEW, self.states.UPLOAD,
-                              self.states.QUEUED, self.states.RUNNING,
-                              self.states.SETTING_METADATA)
+        return self.state in (
+            self.states.NEW,
+            self.states.UPLOAD,
+            self.states.QUEUED,
+            self.states.RUNNING,
+            self.states.SETTING_METADATA,
+        )
 
     @property
     def source_library_dataset(self):
         def get_source(dataset):
             if isinstance(dataset, LibraryDatasetDatasetAssociation):
                 if dataset.library_dataset:
                     return (dataset, dataset.library_dataset)
@@ -3921,14 +4315,15 @@
                 if source:
                     return source
             if dataset.copied_from_history_dataset_association:
                 source = get_source(dataset.copied_from_history_dataset_association)
                 if source:
                     return source
             return (None, None)
+
         return get_source(self)
 
     @property
     def source_dataset_chain(self):
         def _source_dataset_chain(dataset, lst):
             try:
                 cp_from_ldda = dataset.copied_from_library_dataset_dataset_association
@@ -3941,14 +4336,15 @@
                 cp_from_hda = dataset.copied_from_history_dataset_association
                 if cp_from_hda:
                     lst.append((cp_from_hda, cp_from_hda.history.name))
                     return _source_dataset_chain(cp_from_hda, lst)
             except Exception as e:
                 log.warning(e)
             return lst
+
         return _source_dataset_chain(self, [])
 
     @property
     def creating_job(self):
         # TODO this should work with `return self.dataset.job` (revise failing unit tests)
         creating_job_associations = None
         if self.creating_job_associations:
@@ -3960,17 +4356,14 @@
         if creating_job_associations:
             return creating_job_associations[0].job
         return None
 
     def get_display_applications(self, trans):
         return self.datatype.get_display_applications_by_dataset(self, trans)
 
-    def get_visualizations(self):
-        return self.datatype.get_visualizations(self)
-
     def get_datasources(self, trans):
         """
         Returns datasources for dataset; if datasources are not available
         due to indexing, indexing is started. Return value is a dictionary
         with entries of type
         (<datasource_type> : {<datasource_name>, <indexing_message>}).
         """
@@ -4010,23 +4403,27 @@
 
         # Get converted dataset; this will start the conversion if necessary.
         try:
             converted_dataset = self.get_converted_dataset(trans, target_type)
         except NoConverterException:
             return self.conversion_messages.NO_CONVERTER
         except ConverterDependencyException as dep_error:
-            return {'kind': self.conversion_messages.ERROR, 'message': dep_error.value}
+            return {"kind": self.conversion_messages.ERROR, "message": dep_error.value}
 
         # Check dataset state and return any messages.
         msg = None
         if converted_dataset and converted_dataset.state == Dataset.states.ERROR:
-            job_id = trans.sa_session.query(JobToOutputDatasetAssociation) \
-                .filter_by(dataset_id=converted_dataset.id).first().job_id
+            job_id = (
+                trans.sa_session.query(JobToOutputDatasetAssociation)
+                .filter_by(dataset_id=converted_dataset.id)
+                .first()
+                .job_id
+            )
             job = trans.sa_session.query(Job).get(job_id)
-            msg = {'kind': self.conversion_messages.ERROR, 'message': job.stderr}
+            msg = {"kind": self.conversion_messages.ERROR, "message": job.stderr}
         elif not converted_dataset or converted_dataset.state != Dataset.states.OK:
             msg = self.conversion_messages.PENDING
 
         return msg
 
     def _serialize(self, id_encoder, serialization_options):
         metadata = _prepare_metadata_for_serialization(id_encoder, serialization_options, self.metadata)
@@ -4066,39 +4463,45 @@
             sources = dataset.sources
             if sources:
                 file_metadata["sources"] = [s.serialize(id_encoder, serialization_options) for s in sources]
 
             rval["file_metadata"] = file_metadata
 
 
-class HistoryDatasetAssociation(DatasetInstance, HasTags, Dictifiable, UsesAnnotations,
-                                HasName, Serializable):
+class HistoryDatasetAssociation(DatasetInstance, HasTags, Dictifiable, UsesAnnotations, HasName, Serializable):
     """
     Resource class that creates a relation between a dataset and a user history.
     """
 
-    def __init__(self,
-                 hid=None,
-                 history=None,
-                 copied_from_history_dataset_association=None,
-                 copied_from_library_dataset_dataset_association=None,
-                 sa_session=None,
-                 **kwd):
+    def __init__(
+        self,
+        hid=None,
+        history=None,
+        copied_from_history_dataset_association=None,
+        copied_from_library_dataset_dataset_association=None,
+        sa_session=None,
+        **kwd,
+    ):
         """
         Create a a new HDA and associate it with the given history.
         """
         # FIXME: sa_session is must be passed to DataSetInstance if the create_dataset
         # parameter is True so that the new object can be flushed.  Is there a better way?
         DatasetInstance.__init__(self, sa_session=sa_session, **kwd)
         self.hid = hid
         # Relationships
         self.history = history
         self.copied_from_history_dataset_association = copied_from_history_dataset_association
         self.copied_from_library_dataset_dataset_association = copied_from_library_dataset_dataset_association
 
+    @property
+    def user(self):
+        if self.history:
+            return self.history.user
+
     def __create_version__(self, session):
         state = inspect(self)
         changes = {}
 
         for attr in state.mapper.columns:
             # We only create a new version if columns of the HDA table have changed, and ignore relationships.
             hist = state.get_history(attr.key, True)
@@ -4107,79 +4510,79 @@
                 continue
 
             # hist.deleted holds old value(s)
             changes[attr.key] = hist.deleted
         if self.update_time and self.state == self.states.OK and not self.deleted:
             # We only record changes to HDAs that exist in the database and have a update_time
             new_values = {}
-            new_values['name'] = changes.get('name', self.name)
-            new_values['dbkey'] = changes.get('dbkey', self.dbkey)
-            new_values['extension'] = changes.get('extension', self.extension)
-            new_values['extended_metadata_id'] = changes.get('extended_metadata_id', self.extended_metadata_id)
+            new_values["name"] = changes.get("name", self.name)
+            new_values["dbkey"] = changes.get("dbkey", self.dbkey)
+            new_values["extension"] = changes.get("extension", self.extension)
+            new_values["extended_metadata_id"] = changes.get("extended_metadata_id", self.extended_metadata_id)
             for k, v in new_values.items():
                 if isinstance(v, list):
                     new_values[k] = v[0]
-            new_values['update_time'] = self.update_time
-            new_values['version'] = self.version or 1
-            new_values['metadata'] = self._metadata
-            past_hda = HistoryDatasetAssociationHistory(history_dataset_association_id=self.id,
-                                                        **new_values)
+            new_values["update_time"] = self.update_time
+            new_values["version"] = self.version or 1
+            new_values["metadata"] = self._metadata
+            past_hda = HistoryDatasetAssociationHistory(history_dataset_association_id=self.id, **new_values)
             self.version = self.version + 1 if self.version else 1
             session.add(past_hda)
 
-    def copy_from(self, other_hda):
+    def copy_from(self, other_hda, new_dataset=None, include_tags=True, include_metadata=False):
         # This deletes the old dataset, so make sure to only call this on new things
         # in the history (e.g. during job finishing).
         old_dataset = self.dataset
-        self._metadata = None
-        self.metadata = other_hda.metadata
+        if include_metadata:
+            self._metadata = other_hda._metadata
+        self.metadata_deferred = other_hda.metadata_deferred
         self.info = other_hda.info
         self.blurb = other_hda.blurb
         self.peek = other_hda.peek
         self.extension = other_hda.extension
         self.designation = other_hda.designation
         self.deleted = other_hda.deleted
         self.visible = other_hda.visible
         self.validated_state = other_hda.validated_state
         self.validated_state_message = other_hda.validated_state_message
-        self.copy_tags_from(self.history.user, other_hda)
-        self.dataset = other_hda.dataset
-        old_dataset.full_delete()
+        if include_tags and self.history:
+            self.copy_tags_from(self.user, other_hda)
+        self.dataset = new_dataset or other_hda.dataset
+        if old_dataset:
+            old_dataset.full_delete()
 
     def copy(self, parent_id=None, copy_tags=None, flush=True, copy_hid=True, new_name=None):
         """
         Create a copy of this HDA.
         """
         hid = None
         if copy_hid:
             hid = self.hid
-        hda = HistoryDatasetAssociation(hid=hid,
-                                        name=new_name or self.name,
-                                        info=self.info,
-                                        blurb=self.blurb,
-                                        peek=self.peek,
-                                        tool_version=self.tool_version,
-                                        extension=self.extension,
-                                        dbkey=self.dbkey,
-                                        dataset=self.dataset,
-                                        visible=self.visible,
-                                        deleted=self.deleted,
-                                        parent_id=parent_id,
-                                        copied_from_history_dataset_association=self,
-                                        flush=False)
+        hda = HistoryDatasetAssociation(
+            hid=hid,
+            name=new_name or self.name,
+            info=self.info,
+            blurb=self.blurb,
+            peek=self.peek,
+            tool_version=self.tool_version,
+            extension=self.extension,
+            dbkey=self.dbkey,
+            dataset=self.dataset,
+            visible=self.visible,
+            deleted=self.deleted,
+            parent_id=parent_id,
+            copied_from_history_dataset_association=self,
+            flush=False,
+        )
         # update init non-keywords as well
         hda.purged = self.purged
 
         hda.copy_tags_to(copy_tags)
         object_session(self).add(hda)
         hda.metadata = self.metadata
-        # In some instances peek relies on dataset_id, i.e. gmaj.zip for viewing MAFs
-        if not self.datatype.copy_safe_peek:
-            object_session(self).flush([self])
-            hda.set_peek()
         if flush:
             object_session(self).flush()
         return hda
 
     def copy_tags_to(self, copy_tags=None):
         if copy_tags is not None:
             if isinstance(copy_tags, dict):
@@ -4187,91 +4590,93 @@
             for tag in copy_tags:
                 copied_tag = tag.copy(cls=HistoryDatasetAssociationTagAssociation)
                 self.tags.append(copied_tag)
 
     def copy_attributes(self, new_dataset):
         new_dataset.hid = self.hid
 
-    def to_library_dataset_dataset_association(self, trans, target_folder, replace_dataset=None,
-                                               parent_id=None, roles=None, ldda_message='', element_identifier=None):
+    def to_library_dataset_dataset_association(
+        self,
+        trans,
+        target_folder,
+        replace_dataset=None,
+        parent_id=None,
+        roles=None,
+        ldda_message="",
+        element_identifier=None,
+    ):
         """
         Copy this HDA to a library optionally replacing an existing LDDA.
         """
         if replace_dataset:
             # The replace_dataset param ( when not None ) refers to a LibraryDataset that
             #   is being replaced with a new version.
             library_dataset = replace_dataset
         else:
             # If replace_dataset is None, the Library level permissions will be taken from the folder and
             #   applied to the new LibraryDataset, and the current user's DefaultUserPermissions will be applied
             #   to the associated Dataset.
             library_dataset = LibraryDataset(folder=target_folder, name=self.name, info=self.info)
         user = trans.user or self.history.user
-        ldda = LibraryDatasetDatasetAssociation(name=element_identifier or self.name,
-                                                info=self.info,
-                                                blurb=self.blurb,
-                                                peek=self.peek,
-                                                tool_version=self.tool_version,
-                                                extension=self.extension,
-                                                dbkey=self.dbkey,
-                                                dataset=self.dataset,
-                                                library_dataset=library_dataset,
-                                                visible=self.visible,
-                                                deleted=self.deleted,
-                                                parent_id=parent_id,
-                                                copied_from_history_dataset_association=self,
-                                                user=user)
+        ldda = LibraryDatasetDatasetAssociation(
+            name=element_identifier or self.name,
+            info=self.info,
+            blurb=self.blurb,
+            peek=self.peek,
+            tool_version=self.tool_version,
+            extension=self.extension,
+            dbkey=self.dbkey,
+            dataset=self.dataset,
+            library_dataset=library_dataset,
+            visible=self.visible,
+            deleted=self.deleted,
+            parent_id=parent_id,
+            copied_from_history_dataset_association=self,
+            user=user,
+        )
         library_dataset.library_dataset_dataset_association = ldda
         object_session(self).add(library_dataset)
         # If roles were selected on the upload form, restrict access to the Dataset to those roles
         roles = roles or []
         for role in roles:
-            dp = trans.model.DatasetPermissions(trans.app.security_agent.permitted_actions.DATASET_ACCESS.action,
-                                                ldda.dataset, role)
+            dp = trans.model.DatasetPermissions(
+                trans.app.security_agent.permitted_actions.DATASET_ACCESS.action, ldda.dataset, role
+            )
             trans.sa_session.add(dp)
         # Must set metadata after ldda flushed, as MetadataFiles require ldda.id
-        flushed = False
         if self.set_metadata_requires_flush:
-            flushed = True
             object_session(self).flush()
         ldda.metadata = self.metadata
         # TODO: copy #tags from history
         if ldda_message:
             ldda.message = ldda_message
         if not replace_dataset:
             target_folder.add_library_dataset(library_dataset, genome_build=ldda.dbkey)
             object_session(self).add(target_folder)
         object_session(self).add(library_dataset)
-        if not self.datatype.copy_safe_peek:
-            # In some instances peek relies on dataset_id, i.e. gmaj.zip for viewing MAFs
-            if not flushed:
-                object_session(self).flush()
-            ldda.set_peek()
         object_session(self).flush()
         return ldda
 
     def clear_associated_files(self, metadata_safe=False, purge=False):
-        """
-        """
+        """ """
         # metadata_safe = True means to only clear when assoc.metadata_safe == False
         for assoc in self.implicitly_converted_datasets:
             if not assoc.deleted and (not metadata_safe or not assoc.metadata_safe):
                 assoc.clear(purge=purge)
         for assoc in self.implicitly_converted_parent_datasets:
             assoc.clear(purge=purge, delete_dataset=False)
 
     def get_access_roles(self, security_agent):
         """
         Return The access roles associated with this HDA's dataset.
         """
         return self.dataset.get_access_roles(security_agent)
 
     def purge_usage_from_quota(self, user):
-        """Remove this HDA's quota_amount from user's quota.
-        """
+        """Remove this HDA's quota_amount from user's quota."""
         if user:
             user.adjust_total_disk_usage(-self.quota_amount(user))
 
     def quota_amount(self, user):
         """
         Return the disk space used for this HDA relevant to user quotas.
 
@@ -4284,82 +4689,86 @@
             return rval
         # Gets an HDA disk usage, if the user does not already
         #   have an association of the same dataset
         if not self.dataset.library_associations and not self.purged and not self.dataset.purged:
             for hda in self.dataset.history_associations:
                 if hda.id == self.id:
                     continue
-                if not hda.purged and hda.history and hda.history.user and hda.history.user == user:
+                if not hda.purged and hda.history and hda.user and hda.user == user:
                     break
             else:
                 rval += self.get_total_size()
         return rval
 
     def _serialize(self, id_encoder, serialization_options):
         rval = super()._serialize(id_encoder, serialization_options)
-        rval['state'] = self.state
+        rval["state"] = self.state
         rval["hid"] = self.hid
-        rval["annotation"] = unicodify(getattr(self, 'annotation', ''))
+        rval["annotation"] = unicodify(getattr(self, "annotation", ""))
         rval["tags"] = self.make_tag_string_list()
-        rval['tool_version'] = self.tool_version
+        rval["tool_version"] = self.tool_version
         if self.history:
             rval["history_encoded_id"] = serialization_options.get_identifier(id_encoder, self.history)
 
         # Handle copied_from_history_dataset_association information...
         copied_from_history_dataset_association_chain = []
         src_hda = self
         while src_hda.copied_from_history_dataset_association:
             src_hda = src_hda.copied_from_history_dataset_association
-            copied_from_history_dataset_association_chain.append(serialization_options.get_identifier(id_encoder, src_hda))
+            copied_from_history_dataset_association_chain.append(
+                serialization_options.get_identifier(id_encoder, src_hda)
+            )
         rval["copied_from_history_dataset_association_id_chain"] = copied_from_history_dataset_association_chain
         self._handle_serialize_files(id_encoder, serialization_options, rval)
         return rval
 
-    def to_dict(self, view='collection', expose_dataset_path=False):
+    def to_dict(self, view="collection", expose_dataset_path=False):
         """
         Return attributes of this HDA that are exposed using the API.
         """
         # Since this class is a proxy to rather complex attributes we want to
         # display in other objects, we can't use the simpler method used by
         # other model classes.
         original_rval = super().to_dict(view=view)
         hda = self
-        rval = dict(id=hda.id,
-                    hda_ldda='hda',
-                    uuid=(lambda uuid: str(uuid) if uuid else None)(hda.dataset.uuid),
-                    hid=hda.hid,
-                    file_ext=hda.ext,
-                    peek=unicodify(hda.display_peek()) if hda.peek and hda.peek != 'no peek' else None,
-                    model_class=self.__class__.__name__,
-                    name=hda.name,
-                    deleted=hda.deleted,
-                    purged=hda.purged,
-                    visible=hda.visible,
-                    state=hda.state,
-                    history_content_type=hda.history_content_type,
-                    file_size=int(hda.get_size()),
-                    create_time=hda.create_time.isoformat(),
-                    update_time=hda.update_time.isoformat(),
-                    data_type=f"{hda.datatype.__class__.__module__}.{hda.datatype.__class__.__name__}",
-                    genome_build=hda.dbkey,
-                    validated_state=hda.validated_state,
-                    validated_state_message=hda.validated_state_message,
-                    misc_info=hda.info.strip() if isinstance(hda.info, str) else hda.info,
-                    misc_blurb=hda.blurb)
+        rval = dict(
+            id=hda.id,
+            hda_ldda="hda",
+            uuid=(lambda uuid: str(uuid) if uuid else None)(hda.dataset.uuid),
+            hid=hda.hid,
+            file_ext=hda.ext,
+            peek=unicodify(hda.display_peek()) if hda.peek and hda.peek != "no peek" else None,
+            model_class=self.__class__.__name__,
+            name=hda.name,
+            deleted=hda.deleted,
+            purged=hda.purged,
+            visible=hda.visible,
+            state=hda.state,
+            history_content_type=hda.history_content_type,
+            file_size=int(hda.get_size()),
+            create_time=hda.create_time.isoformat(),
+            update_time=hda.update_time.isoformat(),
+            data_type=f"{hda.datatype.__class__.__module__}.{hda.datatype.__class__.__name__}",
+            genome_build=hda.dbkey,
+            validated_state=hda.validated_state,
+            validated_state_message=hda.validated_state_message,
+            misc_info=hda.info.strip() if isinstance(hda.info, str) else hda.info,
+            misc_blurb=hda.blurb,
+        )
 
         rval.update(original_rval)
 
         if hda.copied_from_library_dataset_dataset_association is not None:
-            rval['copied_from_ldda_id'] = hda.copied_from_library_dataset_dataset_association.id
+            rval["copied_from_ldda_id"] = hda.copied_from_library_dataset_dataset_association.id
 
         if hda.history is not None:
-            rval['history_id'] = hda.history.id
+            rval["history_id"] = hda.history.id
 
         if hda.extended_metadata is not None:
-            rval['extended_metadata'] = hda.extended_metadata.data
+            rval["extended_metadata"] = hda.extended_metadata.data
 
         for name in hda.metadata.spec.keys():
             val = hda.metadata.get(name)
             if isinstance(val, MetadataFile):
                 # only when explicitly set: fetching filepaths can be expensive
                 if not expose_dataset_path:
                     continue
@@ -4375,130 +4784,131 @@
             self.state = self.states.NEW
             self.info = None
         jobs_to_unpause = jobs or set()
         for jtida in self.dependent_jobs:
             if jtida.job not in jobs_to_unpause:
                 jobs_to_unpause.add(jtida.job)
                 for jtoda in jtida.job.output_datasets:
-                    jobs_to_unpause.update(
-                        jtoda.dataset.unpause_dependent_jobs(jobs=jobs_to_unpause)
-                    )
+                    jobs_to_unpause.update(jtoda.dataset.unpause_dependent_jobs(jobs=jobs_to_unpause))
         return jobs_to_unpause
 
     @property
     def history_content_type(self):
         return "dataset"
 
     # TODO: down into DatasetInstance
-    content_type = 'dataset'
+    content_type = "dataset"
 
     @hybrid.hybrid_property
     def type_id(self):
-        return '-'.join((self.content_type, str(self.id)))
+        return "-".join((self.content_type, str(self.id)))
 
     @type_id.expression  # type: ignore[no-redef]
     def type_id(cls):
-        return ((type_coerce(cls.content_type, Unicode) + '-'
-                 + type_coerce(cls.id, Unicode)).label('type_id'))
+        return (type_coerce(cls.content_type, Unicode) + "-" + type_coerce(cls.id, Unicode)).label("type_id")
 
 
 class HistoryDatasetAssociationHistory(Base, Serializable):
-    __tablename__ = 'history_dataset_association_history'
+    __tablename__ = "history_dataset_association_history"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey("history_dataset_association.id"), index=True)
+    history_dataset_association_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
     update_time = Column(DateTime, default=now)
     version = Column(Integer)
     name = Column(TrimmedString(255))
     extension = Column(TrimmedString(64))
     _metadata = Column("metadata", MetadataType)
     extended_metadata_id = Column(Integer, ForeignKey("extended_metadata.id"), index=True)
 
-    def __init__(self,
-                 history_dataset_association_id,
-                 name,
-                 dbkey,
-                 update_time,
-                 version,
-                 extension,
-                 extended_metadata_id,
-                 metadata,
-                 ):
+    def __init__(
+        self,
+        history_dataset_association_id,
+        name,
+        dbkey,
+        update_time,
+        version,
+        extension,
+        extended_metadata_id,
+        metadata,
+    ):
         self.history_dataset_association_id = history_dataset_association_id
         self.name = name
         self.dbkey = dbkey
         self.update_time = update_time
         self.version = version
         self.extension = extension
         self.extended_metadata_id = extended_metadata_id
         self._metadata = metadata
 
 
 # hda read access permission given by a user to a specific site (gen. for external display applications)
 class HistoryDatasetAssociationDisplayAtAuthorization(Base, RepresentById):
-    __tablename__ = 'history_dataset_association_display_at_authorization'
+    __tablename__ = "history_dataset_association_display_at_authorization"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, index=True, default=now, onupdate=now)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_association_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     site = Column(TrimmedString(255))
-    history_dataset_association = relationship('HistoryDatasetAssociation')
-    user = relationship('User')
+    history_dataset_association = relationship("HistoryDatasetAssociation")
+    user = relationship("User")
 
     def __init__(self, hda=None, user=None, site=None):
         self.history_dataset_association = hda
         self.user = user
         self.site = site
 
 
 class HistoryDatasetAssociationSubset(Base, RepresentById):
-    __tablename__ = 'history_dataset_association_subset'
+    __tablename__ = "history_dataset_association_subset"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
-    history_dataset_association_subset_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
+    history_dataset_association_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    history_dataset_association_subset_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
     location = Column(Unicode(255), index=True)
 
-    hda = relationship('HistoryDatasetAssociation',
-        primaryjoin=(lambda: HistoryDatasetAssociationSubset.history_dataset_association_id
-            == HistoryDatasetAssociation.id))
-    subset = relationship('HistoryDatasetAssociation',
-        primaryjoin=(lambda: HistoryDatasetAssociationSubset.history_dataset_association_subset_id
-            == HistoryDatasetAssociation.id))
+    hda = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(
+            lambda: HistoryDatasetAssociationSubset.history_dataset_association_id == HistoryDatasetAssociation.id
+        ),
+    )
+    subset = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(
+            lambda: HistoryDatasetAssociationSubset.history_dataset_association_subset_id
+            == HistoryDatasetAssociation.id
+        ),
+    )
 
     def __init__(self, hda, subset, location):
         self.hda = hda
         self.subset = subset
         self.location = location
 
 
 class Library(Base, Dictifiable, HasName, Serializable):
-    __tablename__ = 'library'
+    __tablename__ = "library"
 
     id = Column(Integer, primary_key=True)
     root_folder_id = Column(Integer, ForeignKey("library_folder.id"), index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     name = Column(String(255), index=True)
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
     description = Column(TEXT)
     synopsis = Column(TEXT)
-    root_folder = relationship('LibraryFolder', back_populates='library_root')
-    actions = relationship('LibraryPermissions', back_populates='library')
+    root_folder = relationship("LibraryFolder", back_populates="library_root")
+    actions = relationship("LibraryPermissions", back_populates="library")
 
-    permitted_actions = get_permitted_actions(filter='LIBRARY')
-    dict_collection_visible_keys = ['id', 'name']
-    dict_element_visible_keys = ['id', 'deleted', 'name', 'description', 'synopsis', 'root_folder_id', 'create_time']
+    permitted_actions = get_permitted_actions(filter="LIBRARY")
+    dict_collection_visible_keys = ["id", "name"]
+    dict_element_visible_keys = ["id", "deleted", "name", "description", "synopsis", "root_folder_id", "create_time"]
 
     def __init__(self, name=None, description=None, synopsis=None, root_folder=None):
         self.name = name or "Unnamed library"
         self.description = description
         self.synopsis = synopsis
         self.root_folder = root_folder
 
@@ -4511,21 +4921,21 @@
         )
         if self.root_folder:
             rval["root_folder"] = self.root_folder.serialize(id_encoder, serialization_options)
 
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
-    def to_dict(self, view='collection', value_mapper=None):
+    def to_dict(self, view="collection", value_mapper=None):
         """
         We prepend an F to folders.
         """
         rval = super().to_dict(view=view, value_mapper=value_mapper)
-        if 'root_folder_id' in rval:
-            rval['root_folder_id'] = f"F{str(rval['root_folder_id'])}"
+        if "root_folder_id" in rval:
+            rval["root_folder_id"] = f"F{str(rval['root_folder_id'])}"
         return rval
 
     def get_active_folders(self, folder, folders=None):
         # TODO: should we make sure the library is not deleted?
         def sort_by_attr(seq, attr):
             """
             Sort the sequence of objects by object's attribute
@@ -4537,101 +4947,124 @@
             # Create the auxiliary list of tuples where every i-th tuple has form
             # (seq[i].attr, i, seq[i]) and sort it. The second item of tuple is needed not
             # only to provide stable sorting, but mainly to eliminate comparison of objects
             # (which can be expensive or prohibited) in case of equal attribute values.
             intermed = [(getattr(v, attr), i, v) for i, v in enumerate(seq)]
             intermed.sort()
             return [_[-1] for _ in intermed]
+
         if folders is None:
             active_folders = [folder]
         for active_folder in folder.active_folders:
             active_folders.extend(self.get_active_folders(active_folder, folders))
-        return sort_by_attr(active_folders, 'id')
+        return sort_by_attr(active_folders, "id")
 
     def get_access_roles(self, security_agent):
         roles = []
         for lp in self.actions:
             if lp.action == security_agent.permitted_actions.LIBRARY_ACCESS.action:
                 roles.append(lp.role)
         return roles
 
 
 class LibraryFolder(Base, Dictifiable, HasName, Serializable):
-    __tablename__ = 'library_folder'
-    __table_args__ = (
-        Index('ix_library_folder_name', 'name', mysql_length=200),
-    )
+    __tablename__ = "library_folder"
+    __table_args__ = (Index("ix_library_folder_name", "name", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    parent_id = Column(Integer, ForeignKey('library_folder.id'), nullable=True, index=True)
+    parent_id = Column(Integer, ForeignKey("library_folder.id"), nullable=True, index=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     name = Column(TEXT)
     description = Column(TEXT)
     order_id = Column(Integer)  # not currently being used, but for possible future use
     item_count = Column(Integer)
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
     genome_build = Column(TrimmedString(40))
 
-    folders = relationship('LibraryFolder',
+    folders = relationship(
+        "LibraryFolder",
         primaryjoin=(lambda: LibraryFolder.id == LibraryFolder.parent_id),
         order_by=asc(name),
-        back_populates='parent')
-    parent = relationship('LibraryFolder', back_populates='folders', remote_side=[id])
+        back_populates="parent",
+    )
+    parent = relationship("LibraryFolder", back_populates="folders", remote_side=[id])
 
-    active_folders = relationship('LibraryFolder',
-        primaryjoin=(
-            'and_(LibraryFolder.parent_id == LibraryFolder.id, not_(LibraryFolder.deleted))'),
+    active_folders = relationship(
+        "LibraryFolder",
+        primaryjoin=("and_(LibraryFolder.parent_id == LibraryFolder.id, not_(LibraryFolder.deleted))"),
         order_by=asc(name),
         # """sqlalchemy.exc.ArgumentError: Error creating eager relationship 'active_folders'
         # on parent class '<class 'galaxy.model.LibraryFolder'>' to child class '<class 'galaxy.model.LibraryFolder'>':
         # Cant use eager loading on a self referential relationship."""
         # TODO: This is no longer the case. Fix this: https://docs.sqlalchemy.org/en/14/orm/self_referential.html#configuring-self-referential-eager-loading
-        viewonly=True)
+        viewonly=True,
+    )
 
-    datasets = relationship('LibraryDataset',
-        primaryjoin=(lambda: LibraryDataset.folder_id == LibraryFolder.id and LibraryDataset.library_dataset_dataset_association_id.isnot(None)),
+    datasets = relationship(
+        "LibraryDataset",
+        primaryjoin=(
+            lambda: LibraryDataset.folder_id == LibraryFolder.id
+            and LibraryDataset.library_dataset_dataset_association_id.isnot(None)
+        ),
         order_by=(lambda: asc(LibraryDataset._name)),
-        viewonly=True)
+        viewonly=True,
+    )
 
-    active_datasets = relationship('LibraryDataset',
+    active_datasets = relationship(
+        "LibraryDataset",
         primaryjoin=(
-            'and_(LibraryDataset.folder_id == LibraryFolder.id, not_(LibraryDataset.deleted), LibraryDataset.library_dataset_dataset_association_id.isnot(None))'),
+            "and_(LibraryDataset.folder_id == LibraryFolder.id, not_(LibraryDataset.deleted), LibraryDataset.library_dataset_dataset_association_id.isnot(None))"
+        ),
         order_by=(lambda: asc(LibraryDataset._name)),
-        viewonly=True)
+        viewonly=True,
+    )
 
-    library_root = relationship('Library', back_populates='root_folder')
-    actions = relationship('LibraryFolderPermissions', back_populates='folder')
+    library_root = relationship("Library", back_populates="root_folder")
+    actions = relationship("LibraryFolderPermissions", back_populates="folder")
 
-    dict_element_visible_keys = ['id', 'parent_id', 'name', 'description', 'item_count', 'genome_build', 'update_time', 'deleted']
+    dict_element_visible_keys = [
+        "id",
+        "parent_id",
+        "name",
+        "description",
+        "item_count",
+        "genome_build",
+        "update_time",
+        "deleted",
+    ]
 
     def __init__(self, name=None, description=None, item_count=0, order_id=None, genome_build=None):
         self.name = name or "Unnamed folder"
         self.description = description
         self.item_count = item_count
         self.order_id = order_id
         self.genome_build = genome_build
 
     def add_library_dataset(self, library_dataset, genome_build=None):
         library_dataset.folder_id = self.id
         library_dataset.order_id = self.item_count
         self.item_count += 1
-        if genome_build not in [None, '?']:
+        if genome_build not in [None, "?"]:
             self.genome_build = genome_build
 
     def add_folder(self, folder):
         folder.parent_id = self.id
         folder.order_id = self.item_count
         self.item_count += 1
 
     @property
     def activatable_library_datasets(self):
         # This needs to be a list
-        return [ld for ld in self.datasets if ld.library_dataset_dataset_association and not ld.library_dataset_dataset_association.dataset.deleted]
+        return [
+            ld
+            for ld in self.datasets
+            if ld.library_dataset_dataset_association and not ld.library_dataset_dataset_association.dataset.deleted
+        ]
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             id=self.id,  # FIXME: serialize only in sessionless export mode
             name=self.name,
             description=self.description,
@@ -4644,22 +5077,22 @@
         folders = []
         for folder in self.folders:
             folders.append(folder.serialize(id_encoder, serialization_options))
         rval["folders"] = folders
         datasets = []
         for dataset in self.datasets:
             datasets.append(dataset.serialize(id_encoder, serialization_options))
-        rval['datasets'] = datasets
+        rval["datasets"] = datasets
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
-    def to_dict(self, view='collection', value_mapper=None):
+    def to_dict(self, view="collection", value_mapper=None):
         rval = super().to_dict(view=view, value_mapper=value_mapper)
-        rval['library_path'] = self.library_path
-        rval['parent_library_id'] = self.parent_library.id
+        rval["library_path"] = self.library_path
+        rval["parent_library_id"] = self.parent_library.id
         return rval
 
     @property
     def library_path(self):
         l_path = []
         f = self
         while f.parent:
@@ -4672,75 +5105,85 @@
         f = self
         while f.parent:
             f = f.parent
         return f.library_root[0]
 
 
 class LibraryDataset(Base, Serializable):
-    __tablename__ = 'library_dataset'
+    __tablename__ = "library_dataset"
 
     id = Column(Integer, primary_key=True)
     # current version of dataset, if null, there is not a current version selected
-    library_dataset_dataset_association_id = Column(Integer,
-        ForeignKey('library_dataset_dataset_association.id',
-            use_alter=True, name='library_dataset_dataset_association_id_fk'),
-        nullable=True, index=True)
-    folder_id = Column(Integer, ForeignKey('library_folder.id'), index=True)
+    library_dataset_dataset_association_id = Column(
+        Integer,
+        ForeignKey(
+            "library_dataset_dataset_association.id", use_alter=True, name="library_dataset_dataset_association_id_fk"
+        ),
+        nullable=True,
+        index=True,
+    )
+    folder_id = Column(Integer, ForeignKey("library_folder.id"), index=True)
     # not currently being used, but for possible future use
     order_id = Column(Integer)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     # when not None/null this will supercede display in library (but not when imported into user's history?)
-    _name = Column('name', TrimmedString(255), index=True)
+    _name = Column("name", TrimmedString(255), index=True)
     # when not None/null this will supercede display in library (but not when imported into user's history?)
-    _info = Column('info', TrimmedString(255))
+    _info = Column("info", TrimmedString(255))
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
-    folder = relationship('LibraryFolder')
-    library_dataset_dataset_association = relationship('LibraryDatasetDatasetAssociation',
-            foreign_keys=library_dataset_dataset_association_id,
-            post_update=True)
-    expired_datasets = relationship('LibraryDatasetDatasetAssociation',
+    folder = relationship("LibraryFolder")
+    library_dataset_dataset_association = relationship(
+        "LibraryDatasetDatasetAssociation", foreign_keys=library_dataset_dataset_association_id, post_update=True
+    )
+    expired_datasets = relationship(
+        "LibraryDatasetDatasetAssociation",
         foreign_keys=[id, library_dataset_dataset_association_id],
         primaryjoin=(
-            'and_(LibraryDataset.id == LibraryDatasetDatasetAssociation.library_dataset_id, \
-             not_(LibraryDataset.library_dataset_dataset_association_id == LibraryDatasetDatasetAssociation.id))'
+            "and_(LibraryDataset.id == LibraryDatasetDatasetAssociation.library_dataset_id, \
+             not_(LibraryDataset.library_dataset_dataset_association_id == LibraryDatasetDatasetAssociation.id))"
         ),
         viewonly=True,
-        uselist=True)
-    actions = relationship('LibraryDatasetPermissions', back_populates='library_dataset')
+        uselist=True,
+    )
+    actions = relationship("LibraryDatasetPermissions", back_populates="library_dataset")
 
     # This class acts as a proxy to the currently selected LDDA
-    upload_options = [('upload_file', 'Upload files'),
-                      ('upload_directory', 'Upload directory of files'),
-                      ('upload_paths', 'Upload files from filesystem paths'),
-                      ('import_from_history', 'Import datasets from your current history')]
+    upload_options = [
+        ("upload_file", "Upload files"),
+        ("upload_directory", "Upload directory of files"),
+        ("upload_paths", "Upload files from filesystem paths"),
+        ("import_from_history", "Import datasets from your current history"),
+    ]
 
     def get_info(self):
         if self.library_dataset_dataset_association:
             return self.library_dataset_dataset_association.info
         elif self._info:
             return self._info
         else:
-            return 'no info'
+            return "no info"
 
     def set_info(self, info):
         self._info = info
+
     info = property(get_info, set_info)
 
     def get_name(self):
         if self.library_dataset_dataset_association:
             return self.library_dataset_dataset_association.name
         elif self._name:
             return self._name
         else:
-            return 'Unnamed dataset'
+            return "Unnamed dataset"
 
     def set_name(self, name):
         self._name = name
+
     name = property(get_name, set_name)
 
     def display_name(self):
         self.library_dataset_dataset_association.display_name()
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
@@ -4749,129 +5192,131 @@
             info=self.info,
             order_id=self.order_id,
             ldda=self.library_dataset_dataset_association.serialize(id_encoder, serialization_options, for_link=True),
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
-    def to_dict(self, view='collection'):
+    def to_dict(self, view="collection"):
         # Since this class is a proxy to rather complex attributes we want to
         # display in other objects, we can't use the simpler method used by
         # other model classes.
         ldda = self.library_dataset_dataset_association
-        rval = dict(id=self.id,
-                    ldda_id=ldda.id,
-                    parent_library_id=self.folder.parent_library.id,
-                    folder_id=self.folder_id,
-                    model_class=self.__class__.__name__,
-                    state=ldda.state,
-                    name=ldda.name,
-                    file_name=ldda.file_name,
-                    created_from_basename=ldda.created_from_basename,
-                    uploaded_by=ldda.user and ldda.user.email,
-                    message=ldda.message,
-                    date_uploaded=ldda.create_time.isoformat(),
-                    update_time=ldda.update_time.isoformat(),
-                    file_size=int(ldda.get_size()),
-                    file_ext=ldda.ext,
-                    data_type=f"{ldda.datatype.__class__.__module__}.{ldda.datatype.__class__.__name__}",
-                    genome_build=ldda.dbkey,
-                    misc_info=ldda.info,
-                    misc_blurb=ldda.blurb,
-                    peek=(lambda ldda: ldda.display_peek() if ldda.peek and ldda.peek != 'no peek' else None)(ldda))
+        rval = dict(
+            id=self.id,
+            ldda_id=ldda.id,
+            parent_library_id=self.folder.parent_library.id,
+            folder_id=self.folder_id,
+            model_class=self.__class__.__name__,
+            state=ldda.state,
+            name=ldda.name,
+            file_name=ldda.file_name,
+            created_from_basename=ldda.created_from_basename,
+            uploaded_by=ldda.user and ldda.user.email,
+            message=ldda.message,
+            date_uploaded=ldda.create_time.isoformat(),
+            update_time=ldda.update_time.isoformat(),
+            file_size=int(ldda.get_size()),
+            file_ext=ldda.ext,
+            data_type=f"{ldda.datatype.__class__.__module__}.{ldda.datatype.__class__.__name__}",
+            genome_build=ldda.dbkey,
+            misc_info=ldda.info,
+            misc_blurb=ldda.blurb,
+            peek=(lambda ldda: ldda.display_peek() if ldda.peek and ldda.peek != "no peek" else None)(ldda),
+        )
         if ldda.dataset.uuid is None:
-            rval['uuid'] = None
+            rval["uuid"] = None
         else:
-            rval['uuid'] = str(ldda.dataset.uuid)
+            rval["uuid"] = str(ldda.dataset.uuid)
         for name in ldda.metadata.spec.keys():
             val = ldda.metadata.get(name)
             if isinstance(val, MetadataFile):
                 val = val.file_name
             elif isinstance(val, list):
-                val = ', '.join(str(v) for v in val)
+                val = ", ".join(str(v) for v in val)
             rval[f"metadata_{name}"] = val
         return rval
 
 
 class LibraryDatasetDatasetAssociation(DatasetInstance, HasName, Serializable):
-
-    def __init__(self,
-                 copied_from_history_dataset_association=None,
-                 copied_from_library_dataset_dataset_association=None,
-                 library_dataset=None,
-                 user=None,
-                 sa_session=None,
-                 **kwd):
+    def __init__(
+        self,
+        copied_from_history_dataset_association=None,
+        copied_from_library_dataset_dataset_association=None,
+        library_dataset=None,
+        user=None,
+        sa_session=None,
+        **kwd,
+    ):
         # FIXME: sa_session is must be passed to DataSetInstance if the create_dataset
         # parameter in kwd is True so that the new object can be flushed.  Is there a better way?
         DatasetInstance.__init__(self, sa_session=sa_session, **kwd)
         if copied_from_history_dataset_association:
             self.copied_from_history_dataset_association_id = copied_from_history_dataset_association.id
         if copied_from_library_dataset_dataset_association:
             self.copied_from_library_dataset_dataset_association_id = copied_from_library_dataset_dataset_association.id
         self.library_dataset = library_dataset
         self.user = user
 
     def to_history_dataset_association(self, target_history, parent_id=None, add_to_history=False, visible=None):
         sa_session = object_session(self)
-        hda = HistoryDatasetAssociation(name=self.name,
-                                        info=self.info,
-                                        blurb=self.blurb,
-                                        peek=self.peek,
-                                        tool_version=self.tool_version,
-                                        extension=self.extension,
-                                        dbkey=self.dbkey,
-                                        dataset=self.dataset,
-                                        visible=visible if visible is not None else self.visible,
-                                        deleted=self.deleted,
-                                        parent_id=parent_id,
-                                        copied_from_library_dataset_dataset_association=self,
-                                        history=target_history)
+        hda = HistoryDatasetAssociation(
+            name=self.name,
+            info=self.info,
+            blurb=self.blurb,
+            peek=self.peek,
+            tool_version=self.tool_version,
+            extension=self.extension,
+            dbkey=self.dbkey,
+            dataset=self.dataset,
+            visible=visible if visible is not None else self.visible,
+            deleted=self.deleted,
+            parent_id=parent_id,
+            copied_from_library_dataset_dataset_association=self,
+            history=target_history,
+        )
 
         tag_manager = galaxy.model.tags.GalaxyTagHandler(sa_session)
         src_ldda_tags = tag_manager.get_tags_str(self.tags)
         tag_manager.apply_item_tags(user=self.user, item=hda, tags_str=src_ldda_tags)
 
         sa_session.add(hda)
         sa_session.flush()
         hda.metadata = self.metadata  # need to set after flushed, as MetadataFiles require dataset.id
         if add_to_history and target_history:
             target_history.add_dataset(hda)
-        if not self.datatype.copy_safe_peek:
-            hda.set_peek()  # in some instances peek relies on dataset_id, i.e. gmaj.zip for viewing MAFs
         sa_session.flush()
         return hda
 
     def copy(self, parent_id=None, target_folder=None):
         sa_session = object_session(self)
-        ldda = LibraryDatasetDatasetAssociation(name=self.name,
-                                                info=self.info,
-                                                blurb=self.blurb,
-                                                peek=self.peek,
-                                                tool_version=self.tool_version,
-                                                extension=self.extension,
-                                                dbkey=self.dbkey,
-                                                dataset=self.dataset,
-                                                visible=self.visible,
-                                                deleted=self.deleted,
-                                                parent_id=parent_id,
-                                                copied_from_library_dataset_dataset_association=self,
-                                                folder=target_folder)
+        ldda = LibraryDatasetDatasetAssociation(
+            name=self.name,
+            info=self.info,
+            blurb=self.blurb,
+            peek=self.peek,
+            tool_version=self.tool_version,
+            extension=self.extension,
+            dbkey=self.dbkey,
+            dataset=self.dataset,
+            visible=self.visible,
+            deleted=self.deleted,
+            parent_id=parent_id,
+            copied_from_library_dataset_dataset_association=self,
+            folder=target_folder,
+        )
 
         tag_manager = galaxy.model.tags.GalaxyTagHandler(sa_session)
         src_ldda_tags = tag_manager.get_tags_str(self.tags)
         tag_manager.apply_item_tags(user=self.user, item=ldda, tags_str=src_ldda_tags)
 
         sa_session.add(ldda)
         sa_session.flush()
         # Need to set after flushed, as MetadataFiles require dataset.id
         ldda.metadata = self.metadata
-        if not self.datatype.copy_safe_peek:
-            # In some instances peek relies on dataset_id, i.e. gmaj.zip for viewing MAFs
-            ldda.set_peek()
         sa_session.flush()
         return ldda
 
     def clear_associated_files(self, metadata_safe=False, purge=False):
         return
 
     def get_access_roles(self, security_agent):
@@ -4884,49 +5329,51 @@
         return self.dataset.has_manage_permissions_roles(security_agent)
 
     def _serialize(self, id_encoder, serialization_options):
         rval = super()._serialize(id_encoder, serialization_options)
         self._handle_serialize_files(id_encoder, serialization_options, rval)
         return rval
 
-    def to_dict(self, view='collection'):
+    def to_dict(self, view="collection"):
         # Since this class is a proxy to rather complex attributes we want to
         # display in other objects, we can't use the simpler method used by
         # other model classes.
         ldda = self
         try:
             file_size = int(ldda.get_size())
         except OSError:
             file_size = 0
 
         # TODO: render tags here
-        rval = dict(id=ldda.id,
-                    hda_ldda='ldda',
-                    model_class=self.__class__.__name__,
-                    name=ldda.name,
-                    deleted=ldda.deleted,
-                    visible=ldda.visible,
-                    state=ldda.state,
-                    library_dataset_id=ldda.library_dataset_id,
-                    file_size=file_size,
-                    file_name=ldda.file_name,
-                    update_time=ldda.update_time.isoformat(),
-                    file_ext=ldda.ext,
-                    data_type=f"{ldda.datatype.__class__.__module__}.{ldda.datatype.__class__.__name__}",
-                    genome_build=ldda.dbkey,
-                    misc_info=ldda.info,
-                    misc_blurb=ldda.blurb,
-                    created_from_basename=ldda.created_from_basename)
+        rval = dict(
+            id=ldda.id,
+            hda_ldda="ldda",
+            model_class=self.__class__.__name__,
+            name=ldda.name,
+            deleted=ldda.deleted,
+            visible=ldda.visible,
+            state=ldda.state,
+            library_dataset_id=ldda.library_dataset_id,
+            file_size=file_size,
+            file_name=ldda.file_name,
+            update_time=ldda.update_time.isoformat(),
+            file_ext=ldda.ext,
+            data_type=f"{ldda.datatype.__class__.__module__}.{ldda.datatype.__class__.__name__}",
+            genome_build=ldda.dbkey,
+            misc_info=ldda.info,
+            misc_blurb=ldda.blurb,
+            created_from_basename=ldda.created_from_basename,
+        )
         if ldda.dataset.uuid is None:
-            rval['uuid'] = None
+            rval["uuid"] = None
         else:
-            rval['uuid'] = str(ldda.dataset.uuid)
-        rval['parent_library_id'] = ldda.library_dataset.folder.parent_library.id
+            rval["uuid"] = str(ldda.dataset.uuid)
+        rval["parent_library_id"] = ldda.library_dataset.folder.parent_library.id
         if ldda.extended_metadata is not None:
-            rval['extended_metadata'] = ldda.extended_metadata.data
+            rval["extended_metadata"] = ldda.extended_metadata.data
         for name in ldda.metadata.spec.keys():
             val = ldda.metadata.get(name)
             if isinstance(val, MetadataFile):
                 val = val.file_name
             # If no value for metadata, look in datatype for metadata.
             elif val is None and hasattr(ldda.datatype, name):
                 val = getattr(ldda.datatype, name)
@@ -4934,15 +5381,15 @@
         return rval
 
     def update_parent_folder_update_times(self):
         # sets the update_time for all continaing folders up the tree
         ldda = self
 
         sql = text(
-            '''
+            """
                 WITH RECURSIVE parent_folders_of(folder_id) AS
                     (SELECT folder_id
                     FROM library_dataset
                     WHERE id = :library_dataset_id
                     UNION ALL
                     SELECT library_folder.parent_id
                     FROM library_folder, parent_folders_of
@@ -4950,181 +5397,204 @@
                 UPDATE library_folder
                 SET update_time =
                     (SELECT update_time
                     FROM library_dataset_dataset_association
                     WHERE id = :ldda_id)
                 WHERE exists (SELECT 1 FROM parent_folders_of
                     WHERE library_folder.id = parent_folders_of.folder_id)
-            ''').execution_options(autocommit=True)
-        ret = object_session(self).execute(sql, {'library_dataset_id': ldda.library_dataset_id, 'ldda_id': ldda.id})
+            """
+        ).execution_options(autocommit=True)
+        ret = object_session(self).execute(sql, {"library_dataset_id": ldda.library_dataset_id, "ldda_id": ldda.id})
         if ret.rowcount < 1:
-            log.warning(f'Attempt to updated parent folder times failed: {ret.rowcount} records updated.')
+            log.warning(f"Attempt to updated parent folder times failed: {ret.rowcount} records updated.")
 
 
 class ExtendedMetadata(Base, RepresentById):
-    __tablename__ = 'extended_metadata'
+    __tablename__ = "extended_metadata"
 
     id = Column(Integer, primary_key=True)
     data = Column(MutableJSONType)
-    children = relationship('ExtendedMetadataIndex', back_populates='extended_metadata')
+    children = relationship("ExtendedMetadataIndex", back_populates="extended_metadata")
 
     def __init__(self, data):
         self.data = data
 
 
 class ExtendedMetadataIndex(Base, RepresentById):
-    __tablename__ = 'extended_metadata_index'
+    __tablename__ = "extended_metadata_index"
 
     id = Column(Integer, primary_key=True)
-    extended_metadata_id = Column(Integer,
-        ForeignKey('extended_metadata.id', onupdate='CASCADE', ondelete='CASCADE'), index=True)
+    extended_metadata_id = Column(
+        Integer, ForeignKey("extended_metadata.id", onupdate="CASCADE", ondelete="CASCADE"), index=True
+    )
     path = Column(String(255))
     value = Column(TEXT)
-    extended_metadata = relationship('ExtendedMetadata', back_populates='children')
+    extended_metadata = relationship("ExtendedMetadata", back_populates="children")
 
     def __init__(self, extended_metadata, path, value):
         self.extended_metadata = extended_metadata
         self.path = path
         self.value = value
 
 
 class LibraryInfoAssociation(Base, RepresentById):
-    __tablename__ = 'library_info_association'
+    __tablename__ = "library_info_association"
 
     id = Column(Integer, primary_key=True)
-    library_id = Column(Integer, ForeignKey('library.id'), index=True)
-    form_definition_id = Column(Integer, ForeignKey('form_definition.id'), index=True)
-    form_values_id = Column(Integer, ForeignKey('form_values.id'), index=True)
+    library_id = Column(Integer, ForeignKey("library.id"), index=True)
+    form_definition_id = Column(Integer, ForeignKey("form_definition.id"), index=True)
+    form_values_id = Column(Integer, ForeignKey("form_values.id"), index=True)
     inheritable = Column(Boolean, index=True, default=False)
     deleted = Column(Boolean, index=True, default=False)
 
-    library = relationship('Library',
+    library = relationship(
+        "Library",
         primaryjoin=(
-            lambda: and_(
-                LibraryInfoAssociation.library_id == Library.id,
-                not_(LibraryInfoAssociation.deleted))
-        ))
-    template = relationship('FormDefinition',
-        primaryjoin=lambda: LibraryInfoAssociation.form_definition_id == FormDefinition.id)
-    info = relationship('FormValues',
-        primaryjoin=lambda: LibraryInfoAssociation.form_values_id == FormValues.id)  # type: ignore[has-type]
+            lambda: and_(LibraryInfoAssociation.library_id == Library.id, not_(LibraryInfoAssociation.deleted))
+        ),
+    )
+    template = relationship(
+        "FormDefinition", primaryjoin=lambda: LibraryInfoAssociation.form_definition_id == FormDefinition.id
+    )
+    info = relationship(
+        "FormValues", primaryjoin=lambda: LibraryInfoAssociation.form_values_id == FormValues.id  # type: ignore[has-type]
+    )
 
     def __init__(self, library, form_definition, info, inheritable=False):
         self.library = library
         self.template = form_definition
         self.info = info
         self.inheritable = inheritable
 
 
 class LibraryFolderInfoAssociation(Base, RepresentById):
-    __tablename__ = 'library_folder_info_association'
+    __tablename__ = "library_folder_info_association"
 
     id = Column(Integer, primary_key=True)
-    library_folder_id = Column(Integer, ForeignKey('library_folder.id'), nullable=True, index=True)
-    form_definition_id = Column(Integer, ForeignKey('form_definition.id'), index=True)
-    form_values_id = Column(Integer, ForeignKey('form_values.id'), index=True)
+    library_folder_id = Column(Integer, ForeignKey("library_folder.id"), nullable=True, index=True)
+    form_definition_id = Column(Integer, ForeignKey("form_definition.id"), index=True)
+    form_values_id = Column(Integer, ForeignKey("form_values.id"), index=True)
     inheritable = Column(Boolean, index=True, default=False)
     deleted = Column(Boolean, index=True, default=False)
 
-    folder = relationship('LibraryFolder',
-        primaryjoin=(lambda:
-            (LibraryFolderInfoAssociation.library_folder_id == LibraryFolder.id)
-            & (not_(LibraryFolderInfoAssociation.deleted))))
-    template = relationship('FormDefinition',
-        primaryjoin=(lambda: LibraryFolderInfoAssociation.form_definition_id == FormDefinition.id))
-    info = relationship('FormValues',
-        primaryjoin=(lambda: LibraryFolderInfoAssociation.form_values_id == FormValues.id))  # type: ignore[has-type]
+    folder = relationship(
+        "LibraryFolder",
+        primaryjoin=(
+            lambda: (LibraryFolderInfoAssociation.library_folder_id == LibraryFolder.id)
+            & (not_(LibraryFolderInfoAssociation.deleted))
+        ),
+    )
+    template = relationship(
+        "FormDefinition", primaryjoin=(lambda: LibraryFolderInfoAssociation.form_definition_id == FormDefinition.id)
+    )
+    info = relationship(
+        "FormValues", primaryjoin=(lambda: LibraryFolderInfoAssociation.form_values_id == FormValues.id)  # type: ignore[has-type]
+    )
 
     def __init__(self, folder, form_definition, info, inheritable=False):
         self.folder = folder
         self.template = form_definition
         self.info = info
         self.inheritable = inheritable
 
 
 class LibraryDatasetDatasetInfoAssociation(Base, RepresentById):
-    __tablename__ = 'library_dataset_dataset_info_association'
+    __tablename__ = "library_dataset_dataset_info_association"
 
     id = Column(Integer, primary_key=True)
-    library_dataset_dataset_association_id = Column(Integer,
-        ForeignKey('library_dataset_dataset_association.id'), nullable=True, index=True)
-    form_definition_id = Column(Integer, ForeignKey('form_definition.id'), index=True)
-    form_values_id = Column(Integer, ForeignKey('form_values.id'), index=True)
+    library_dataset_dataset_association_id = Column(
+        Integer, ForeignKey("library_dataset_dataset_association.id"), nullable=True, index=True
+    )
+    form_definition_id = Column(Integer, ForeignKey("form_definition.id"), index=True)
+    form_values_id = Column(Integer, ForeignKey("form_values.id"), index=True)
     deleted = Column(Boolean, index=True, default=False)
 
-    library_dataset_dataset_association = relationship('LibraryDatasetDatasetAssociation',
-        primaryjoin=(lambda:
-            (LibraryDatasetDatasetInfoAssociation.library_dataset_dataset_association_id
-             == LibraryDatasetDatasetAssociation.id)
+    library_dataset_dataset_association = relationship(
+        "LibraryDatasetDatasetAssociation",
+        primaryjoin=(
+            lambda: (
+                LibraryDatasetDatasetInfoAssociation.library_dataset_dataset_association_id
+                == LibraryDatasetDatasetAssociation.id
+            )
             & (not_(LibraryDatasetDatasetInfoAssociation.deleted))
-        ))
-    template = relationship('FormDefinition',
-        primaryjoin=(lambda:
-            LibraryDatasetDatasetInfoAssociation.form_definition_id == FormDefinition.id))
-    info = relationship('FormValues',
-        primaryjoin=(lambda:
-            LibraryDatasetDatasetInfoAssociation.form_values_id == FormValues.id))  # type: ignore[has-type]
+        ),
+    )
+    template = relationship(
+        "FormDefinition",
+        primaryjoin=(lambda: LibraryDatasetDatasetInfoAssociation.form_definition_id == FormDefinition.id),
+    )
+    info = relationship(
+        "FormValues", primaryjoin=(lambda: LibraryDatasetDatasetInfoAssociation.form_values_id == FormValues.id)  # type: ignore[has-type]
+    )
 
     def __init__(self, library_dataset_dataset_association, form_definition, info):
         # TODO: need to figure out if this should be inheritable to the associated LibraryDataset
         self.library_dataset_dataset_association = library_dataset_dataset_association
         self.template = form_definition
         self.info = info
 
     @property
     def inheritable(self):
         return True  # always allow inheriting, used for replacement
 
 
 class ImplicitlyConvertedDatasetAssociation(Base, RepresentById):
-    __tablename__ = 'implicitly_converted_dataset_association'
+    __tablename__ = "implicitly_converted_dataset_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    hda_id = Column(Integer, ForeignKey('history_dataset_association.id'), index=True, nullable=True)
-    ldda_id = Column(Integer,
-        ForeignKey('library_dataset_dataset_association.id'), index=True, nullable=True)
-    hda_parent_id = Column(Integer, ForeignKey('history_dataset_association.id'), index=True)
-    ldda_parent_id = Column(Integer, ForeignKey('library_dataset_dataset_association.id'), index=True)
+    hda_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True, nullable=True)
+    ldda_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True, nullable=True)
+    hda_parent_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    ldda_parent_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True)
     deleted = Column(Boolean, index=True, default=False)
     metadata_safe = Column(Boolean, index=True, default=True)
     type = Column(TrimmedString(255))
 
-    parent_hda = relationship('HistoryDatasetAssociation',
-        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_parent_id
-            == HistoryDatasetAssociation.id),
-        back_populates='implicitly_converted_datasets')
-    dataset_ldda = relationship('LibraryDatasetDatasetAssociation',
-        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.ldda_id
-            == LibraryDatasetDatasetAssociation.id),
-        back_populates='implicitly_converted_parent_datasets')
-    dataset = relationship('HistoryDatasetAssociation',
-        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_id
-            == HistoryDatasetAssociation.id),
-        back_populates='implicitly_converted_parent_datasets')
-    parent_ldda = relationship('LibraryDatasetDatasetAssociation',
-        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.ldda_parent_id
-            == LibraryDatasetDatasetAssociation.table.c.id),
-        back_populates='implicitly_converted_datasets')
+    parent_hda = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_parent_id == HistoryDatasetAssociation.id),
+        back_populates="implicitly_converted_datasets",
+    )
+    dataset_ldda = relationship(
+        "LibraryDatasetDatasetAssociation",
+        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.ldda_id == LibraryDatasetDatasetAssociation.id),
+        back_populates="implicitly_converted_parent_datasets",
+    )
+    dataset = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_id == HistoryDatasetAssociation.id),
+        back_populates="implicitly_converted_parent_datasets",
+    )
+    parent_ldda = relationship(
+        "LibraryDatasetDatasetAssociation",
+        primaryjoin=(
+            lambda: ImplicitlyConvertedDatasetAssociation.ldda_parent_id == LibraryDatasetDatasetAssociation.table.c.id
+        ),
+        back_populates="implicitly_converted_datasets",
+    )
 
-    def __init__(self, id=None, parent=None, dataset=None, file_type=None, deleted=False, purged=False, metadata_safe=True):
+    def __init__(
+        self, id=None, parent=None, dataset=None, file_type=None, deleted=False, purged=False, metadata_safe=True
+    ):
         self.id = id
+        add_object_to_object_session(self, dataset)
         if isinstance(dataset, HistoryDatasetAssociation):
             self.dataset = dataset
         elif isinstance(dataset, LibraryDatasetDatasetAssociation):
             self.dataset_ldda = dataset
         else:
-            raise AttributeError(f'Unknown dataset type provided for dataset: {type(dataset)}')
+            raise AttributeError(f"Unknown dataset type provided for dataset: {type(dataset)}")
         if isinstance(parent, HistoryDatasetAssociation):
             self.parent_hda = parent
         elif isinstance(parent, LibraryDatasetDatasetAssociation):
             self.parent_ldda = parent
         else:
-            raise AttributeError(f'Unknown dataset type provided for parent: {type(parent)}')
+            raise AttributeError(f"Unknown dataset type provided for parent: {type(parent)}")
         self.type = file_type
         self.deleted = deleted
         self.purged = purged
         self.metadata_safe = metadata_safe
 
     def clear(self, purge=False, delete_dataset=True):
         self.deleted = True
@@ -5150,59 +5620,65 @@
     expected_value: Union[str, int, float, bool]
 
     def produce_filter(self, table):
         return self.operator_function(getattr(table, self.column), self.expected_value)
 
 
 class DatasetCollection(Base, Dictifiable, UsesAnnotations, Serializable):
-    __tablename__ = 'dataset_collection'
+    __tablename__ = "dataset_collection"
 
     id = Column(Integer, primary_key=True)
     collection_type = Column(Unicode(255), nullable=False)
-    populated_state = Column(TrimmedString(64), default='ok', nullable=False)
+    populated_state = Column(TrimmedString(64), default="ok", nullable=False)
     populated_state_message = Column(TEXT)
     element_count = Column(Integer, nullable=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
 
-    elements = relationship('DatasetCollectionElement',
+    elements = relationship(
+        "DatasetCollectionElement",
         primaryjoin=(lambda: DatasetCollection.id == DatasetCollectionElement.dataset_collection_id),  # type: ignore[has-type]
-        back_populates='collection',
-        order_by=lambda: DatasetCollectionElement.element_index)  # type: ignore[has-type]
+        back_populates="collection",
+        order_by=lambda: DatasetCollectionElement.element_index,  # type: ignore[has-type]
+    )
 
-    dict_collection_visible_keys = ['id', 'collection_type']
-    dict_element_visible_keys = ['id', 'collection_type']
+    dict_collection_visible_keys = ["id", "collection_type"]
+    dict_element_visible_keys = ["id", "collection_type"]
 
     class populated_states(str, Enum):
-        NEW = 'new'  # New dataset collection, unpopulated elements
-        OK = 'ok'  # Collection elements populated (HDAs may or may not have errors)
-        FAILED = 'failed'  # some problem populating state, won't be populated
+        NEW = "new"  # New dataset collection, unpopulated elements
+        OK = "ok"  # Collection elements populated (HDAs may or may not have errors)
+        FAILED = "failed"  # some problem populating state, won't be populated
 
-    def __init__(
-        self,
-        id=None,
-        collection_type=None,
-        populated=True,
-        element_count=None
-    ):
+    def __init__(self, id=None, collection_type=None, populated=True, element_count=None):
         self.id = id
         self.collection_type = collection_type
         if not populated:
             self.populated_state = DatasetCollection.populated_states.NEW
         self.element_count = element_count
 
     def _get_nested_collection_attributes(
         self,
         collection_attributes: Optional[Iterable[str]] = None,
         element_attributes: Optional[Iterable[str]] = None,
         hda_attributes: Optional[Iterable[str]] = None,
         dataset_attributes: Optional[Iterable[str]] = None,
         dataset_permission_attributes: Optional[Iterable[str]] = None,
-        return_entities: Optional[Iterable[Union[Type[HistoryDatasetAssociation], Type[Dataset], Type[DatasetPermissions], Type['DatasetCollection'], Type['DatasetCollectionElement']]]] = None,
-        inner_filter: Optional[InnerCollectionFilter] = None
+        return_entities: Optional[
+            Iterable[
+                Union[
+                    Type[HistoryDatasetAssociation],
+                    Type[Dataset],
+                    Type[DatasetPermissions],
+                    Type["DatasetCollection"],
+                    Type["DatasetCollectionElement"],
+                ]
+            ]
+        ] = None,
+        inner_filter: Optional[InnerCollectionFilter] = None,
     ):
         collection_attributes = collection_attributes or ()
         element_attributes = element_attributes or ()
         hda_attributes = hda_attributes or ()
         dataset_attributes = dataset_attributes or ()
         dataset_permission_attributes = dataset_permission_attributes or ()
         return_entities = return_entities or ()
@@ -5215,86 +5691,109 @@
         order_by_columns = [dce.c.element_index]
         nesting_level = 0
 
         def attribute_columns(column_collection, attributes, nesting_level=None):
             label_fragment = f"_{nesting_level}" if nesting_level is not None else ""
             return [getattr(column_collection, a).label(f"{a}{label_fragment}") for a in attributes]
 
-        q = db_session.query(
-            *attribute_columns(dce.c, element_attributes, nesting_level),
-            *attribute_columns(dc.c, collection_attributes, nesting_level),
-        ).select_from(
-            dce, dc
-        ).join(
-            dce, dce.c.dataset_collection_id == dc.c.id
-        ).filter(dc.c.id == dataset_collection.id)
-        while ':' in depth_collection_type:
+        q = (
+            db_session.query(
+                *attribute_columns(dce.c, element_attributes, nesting_level),
+                *attribute_columns(dc.c, collection_attributes, nesting_level),
+            )
+            .select_from(dce, dc)
+            .join(dce, dce.c.dataset_collection_id == dc.c.id)
+            .filter(dc.c.id == dataset_collection.id)
+        )
+        while ":" in depth_collection_type:
             nesting_level += 1
             inner_dc = alias(DatasetCollection)
             inner_dce = alias(DatasetCollectionElement)
             order_by_columns.append(inner_dce.c.element_index)
-            q = q.join(
-                inner_dc, inner_dc.c.id == dce.c.child_collection_id
-            ).outerjoin(
+            q = q.join(inner_dc, inner_dc.c.id == dce.c.child_collection_id).outerjoin(
                 inner_dce, inner_dce.c.dataset_collection_id == inner_dc.c.id
             )
             q = q.add_columns(
                 *attribute_columns(inner_dce.c, element_attributes, nesting_level),
                 *attribute_columns(inner_dc.c, collection_attributes, nesting_level),
             )
             dce = inner_dce
             dc = inner_dc
             depth_collection_type = depth_collection_type.split(":", 1)[1]
         if inner_filter:
             q = q.filter(inner_filter.produce_filter(dc.c))
 
-        if hda_attributes or dataset_attributes or dataset_permission_attributes or return_entities and not return_entities == (DatasetCollectionElement,):
+        if (
+            hda_attributes
+            or dataset_attributes
+            or dataset_permission_attributes
+            or return_entities
+            and not return_entities == (DatasetCollectionElement,)
+        ):
             q = q.join(HistoryDatasetAssociation).join(Dataset)
         if dataset_permission_attributes:
             q = q.join(DatasetPermissions)
-        q = q.add_columns(
-            *attribute_columns(HistoryDatasetAssociation, hda_attributes)
-        ).add_columns(
-            *attribute_columns(Dataset, dataset_attributes)
-        ).add_columns(
-            *attribute_columns(DatasetPermissions, dataset_permission_attributes)
+        q = (
+            q.add_columns(*attribute_columns(HistoryDatasetAssociation, hda_attributes))
+            .add_columns(*attribute_columns(Dataset, dataset_attributes))
+            .add_columns(*attribute_columns(DatasetPermissions, dataset_permission_attributes))
         )
         for entity in return_entities:
             q = q.add_entity(entity)
             if entity == DatasetCollectionElement:
                 q = q.filter(entity.id == dce.c.id)
         return q.distinct().order_by(*order_by_columns)
 
     @property
     def dataset_states_and_extensions_summary(self):
-        if not hasattr(self, '_dataset_states_and_extensions_summary'):
-            q = self._get_nested_collection_attributes(
-                hda_attributes=('extension',),
-                dataset_attributes=('state',)
-            )
+        if not hasattr(self, "_dataset_states_and_extensions_summary"):
+            q = self._get_nested_collection_attributes(hda_attributes=("extension",), dataset_attributes=("state",))
             extensions = set()
             states = set()
             for extension, state in q:
                 states.add(state)
                 extensions.add(extension)
 
             self._dataset_states_and_extensions_summary = (states, extensions)
 
         return self._dataset_states_and_extensions_summary
 
     @property
+    def has_deferred_data(self):
+        if not hasattr(self, "_has_deferred_data"):
+            has_deferred_data = False
+            if object_session(self):
+                # TODO: Optimize by just querying without returning the states...
+                q = self._get_nested_collection_attributes(dataset_attributes=("state",))
+                for (state,) in q:
+                    if state == Dataset.states.DEFERRED:
+                        has_deferred_data = True
+                        break
+            else:
+                # This will be in a remote tool evaluation context, so can't query database
+                for dataset_element in self.dataset_elements_and_identifiers():
+                    if dataset_element.hda.state == Dataset.states.DEFERRED:
+                        has_deferred_data = True
+                        break
+            self._has_deferred_data = has_deferred_data
+
+        return self._has_deferred_data
+
+    @property
     def populated_optimized(self):
-        if not hasattr(self, '_populated_optimized'):
+        if not hasattr(self, "_populated_optimized"):
             _populated_optimized = True
             if ":" not in self.collection_type:
                 _populated_optimized = self.populated_state == DatasetCollection.populated_states.OK
             else:
                 q = self._get_nested_collection_attributes(
-                    collection_attributes=('populated_state',),
-                    inner_filter=InnerCollectionFilter('populated_state', operator.__ne__, DatasetCollection.populated_states.OK)
+                    collection_attributes=("populated_state",),
+                    inner_filter=InnerCollectionFilter(
+                        "populated_state", operator.__ne__, DatasetCollection.populated_states.OK
+                    ),
                 )
                 _populated_optimized = q.session.query(~exists(q.subquery())).scalar()
 
             self._populated_optimized = _populated_optimized
 
         return self._populated_optimized
 
@@ -5303,59 +5802,62 @@
         top_level_populated = self.populated_state == DatasetCollection.populated_states.OK
         if top_level_populated and self.has_subcollections:
             return all(e.child_collection and e.child_collection.populated for e in self.elements)
         return top_level_populated
 
     @property
     def dataset_action_tuples(self):
-        if not hasattr(self, '_dataset_action_tuples'):
-            q = self._get_nested_collection_attributes(
-                dataset_permission_attributes=('action', 'role_id')
-            )
+        if not hasattr(self, "_dataset_action_tuples"):
+            q = self._get_nested_collection_attributes(dataset_permission_attributes=("action", "role_id"))
             _dataset_action_tuples = []
             for _dataset_action_tuple in q:
                 if _dataset_action_tuple[0] is None:
                     continue
                 _dataset_action_tuples.append(_dataset_action_tuple)
 
             self._dataset_action_tuples = _dataset_action_tuples
 
         return self._dataset_action_tuples
 
     @property
     def element_identifiers_extensions_and_paths(self):
         q = self._get_nested_collection_attributes(
-            element_attributes=('element_identifier',),
-            hda_attributes=('extension',),
-            return_entities=(Dataset,)
+            element_attributes=("element_identifier",), hda_attributes=("extension",), return_entities=(Dataset,)
         )
         return [(row[:-2], row.extension, row.Dataset.file_name) for row in q]
 
     @property
     def element_identifiers_extensions_paths_and_metadata_files(
         self,
     ) -> List[List[Any]]:
         results = []
         if object_session(self):
             q = self._get_nested_collection_attributes(
-                element_attributes=('element_identifier',),
-                hda_attributes=('extension',),
-                return_entities=(HistoryDatasetAssociation, Dataset)
+                element_attributes=("element_identifier",),
+                hda_attributes=("extension",),
+                return_entities=(HistoryDatasetAssociation, Dataset),
             )
             # element_identifiers, extension, path
             for row in q:
                 result = [row[:-3], row.extension, row.Dataset.file_name]
                 hda = row.HistoryDatasetAssociation
                 result.append(hda.get_metadata_file_paths_and_extensions())
                 results.append(result)
         else:
             # This will be in a remote tool evaluation context, so can't query database
             for dataset_element in self.dataset_elements_and_identifiers():
                 # Let's pretend name is element identifier
-                results.append([dataset_element._identifiers, dataset_element.hda.extension, dataset_element.hda.file_name, dataset_element.hda.get_metadata_file_paths_and_extensions()])
+                results.append(
+                    [
+                        dataset_element._identifiers,
+                        dataset_element.hda.extension,
+                        dataset_element.hda.file_name,
+                        dataset_element.hda.get_metadata_file_paths_and_extensions(),
+                    ]
+                )
         return results
 
     @property
     def waiting_for_elements(self):
         top_level_waiting = self.populated_state == DatasetCollection.populated_states.NEW
         if not top_level_waiting and self.has_subcollections:
             return any(e.child_collection.waiting_for_elements for e in self.elements)
@@ -5430,15 +5932,15 @@
             else:
                 return element
         return None
 
     @property
     def state(self):
         # TODO: DatasetCollection state handling...
-        return 'ok'
+        return "ok"
 
     def validate(self):
         if self.collection_type is None:
             raise Exception("Each dataset collection must define a collection type.")
 
     def __getitem__(self, key):
         if isinstance(key, int):
@@ -5451,19 +5953,23 @@
             for element in self.elements:
                 if element.element_identifier == key:
                     return element
         get_by_attribute = "element_index" if isinstance(key, int) else "element_identifier"
         error_message = f"Dataset collection has no {get_by_attribute} with key {key}."
         raise KeyError(error_message)
 
-    def copy(self, destination=None, element_destination=None, dataset_instance_attributes=None, flush=True, minimize_copies=False):
-        new_collection = DatasetCollection(
-            collection_type=self.collection_type,
-            element_count=self.element_count
-        )
+    def copy(
+        self,
+        destination=None,
+        element_destination=None,
+        dataset_instance_attributes=None,
+        flush=True,
+        minimize_copies=False,
+    ):
+        new_collection = DatasetCollection(collection_type=self.collection_type, element_count=self.element_count)
         for element in self.elements:
             element.copy_to_collection(
                 new_collection,
                 destination=destination,
                 element_destination=element_destination,
                 dataset_instance_attributes=dataset_instance_attributes,
                 flush=flush,
@@ -5471,15 +5977,17 @@
             )
         object_session(self).add(new_collection)
         if flush:
             object_session(self).flush()
         return new_collection
 
     def replace_failed_elements(self, replacements):
-        hda_id_to_element = dict(self._get_nested_collection_attributes(return_entities=[DatasetCollectionElement], hda_attributes=['id']))
+        hda_id_to_element = dict(
+            self._get_nested_collection_attributes(return_entities=[DatasetCollectionElement], hda_attributes=["id"])
+        )
         for failed, replacement in replacements.items():
             element = hda_id_to_element.get(failed.id)
             if element:
                 element.hda = replacement
 
     def set_from_dict(self, new_data):
         # Nothing currently editable in this class.
@@ -5491,22 +5999,21 @@
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             type=self.collection_type,
             populated_state=self.populated_state,
             populated_state_message=self.populated_state_message,
-            elements=list(map(lambda e: e.serialize(id_encoder, serialization_options), self.elements))
+            elements=list(map(lambda e: e.serialize(id_encoder, serialization_options), self.elements)),
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
 
 class DatasetCollectionInstance(HasName, UsesCreateAndUpdateTime):
-
     @property
     def state(self):
         return self.collection.state
 
     @property
     def populated(self):
         return self.collection.populated
@@ -5518,19 +6025,21 @@
     def display_name(self):
         return self.get_display_name()
 
     def _base_to_dict(self, view):
         return dict(
             id=self.id,
             name=self.name,
+            collection_id=self.collection_id,
             collection_type=self.collection.collection_type,
             populated=self.populated,
             populated_state=self.collection.populated_state,
             populated_state_message=self.collection.populated_state_message,
             element_count=self.collection.element_count,
+            elements_datatypes=list(self.dataset_dbkeys_and_extensions_summary[1]),
             type="collection",  # contents type (distinguished from file or folder (in case of library))
         )
 
     def set_from_dict(self, new_data):
         """
         Set object attributes to the values in dictionary new_data limiting
         to only those keys in dict_element_visible_keys.
@@ -5548,91 +6057,102 @@
                 continue
 
             self.__setattr__(key, new_val)
             changed[key] = new_val
 
         return changed
 
+    @property
+    def has_deferred_data(self):
+        return self.collection.has_deferred_data
+
 
 class HistoryDatasetCollectionAssociation(
     Base,
     DatasetCollectionInstance,
     HasTags,
     Dictifiable,
     UsesAnnotations,
     Serializable,
 ):
-    """ Associates a DatasetCollection with a History. """
-    __tablename__ = 'history_dataset_collection_association'
+    """Associates a DatasetCollection with a History."""
+
+    __tablename__ = "history_dataset_collection_association"
 
     id = Column(Integer, primary_key=True)
-    collection_id = Column(Integer, ForeignKey('dataset_collection.id'), index=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
+    collection_id = Column(Integer, ForeignKey("dataset_collection.id"), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
     name = Column(TrimmedString(255))
     hid = Column(Integer)
     visible = Column(Boolean)
     deleted = Column(Boolean, default=False)
-    copied_from_history_dataset_collection_association_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id'), nullable=True)
+    copied_from_history_dataset_collection_association_id = Column(
+        Integer, ForeignKey("history_dataset_collection_association.id"), nullable=True
+    )
     implicit_output_name = Column(Unicode(255), nullable=True)
-    job_id = Column(ForeignKey('job.id'), index=True, nullable=True)
-    implicit_collection_jobs_id = Column(
-        ForeignKey('implicit_collection_jobs.id'), index=True, nullable=True)
+    job_id = Column(ForeignKey("job.id"), index=True, nullable=True)
+    implicit_collection_jobs_id = Column(ForeignKey("implicit_collection_jobs.id"), index=True, nullable=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now, index=True)
 
-    collection = relationship('DatasetCollection')
-    history = relationship('History', back_populates='dataset_collections')
+    collection = relationship("DatasetCollection")
+    history = relationship("History", back_populates="dataset_collections")
 
     copied_from_history_dataset_collection_association = relationship(
-        'HistoryDatasetCollectionAssociation',
+        "HistoryDatasetCollectionAssociation",
         primaryjoin=copied_from_history_dataset_collection_association_id == id,
         remote_side=[id],
         uselist=False,
-        back_populates='copied_to_history_dataset_collection_association',
+        back_populates="copied_to_history_dataset_collection_association",
     )
     copied_to_history_dataset_collection_association = relationship(
-        'HistoryDatasetCollectionAssociation',
-        back_populates='copied_from_history_dataset_collection_association',
+        "HistoryDatasetCollectionAssociation",
+        back_populates="copied_from_history_dataset_collection_association",
     )
-    implicit_input_collections = relationship('ImplicitlyCreatedDatasetCollectionInput',
-        primaryjoin=(lambda: HistoryDatasetCollectionAssociation.id
-                == ImplicitlyCreatedDatasetCollectionInput.dataset_collection_id)
+    implicit_input_collections = relationship(
+        "ImplicitlyCreatedDatasetCollectionInput",
+        primaryjoin=(
+            lambda: HistoryDatasetCollectionAssociation.id
+            == ImplicitlyCreatedDatasetCollectionInput.dataset_collection_id
+        ),
     )
-    implicit_collection_jobs = relationship('ImplicitCollectionJobs', uselist=False)
+    implicit_collection_jobs = relationship("ImplicitCollectionJobs", uselist=False)
     job = relationship(
-        'Job',
-        back_populates='history_dataset_collection_associations',
+        "Job",
+        back_populates="history_dataset_collection_associations",
         uselist=False,
     )
-    job_state_summary = relationship(HistoryDatasetCollectionJobStateSummary,
-        primaryjoin=(lambda: HistoryDatasetCollectionAssociation.id
-            == HistoryDatasetCollectionJobStateSummary.__table__.c.hdca_id),
+    job_state_summary = relationship(
+        HistoryDatasetCollectionJobStateSummary,
+        primaryjoin=(
+            lambda: HistoryDatasetCollectionAssociation.id
+            == HistoryDatasetCollectionJobStateSummary.__table__.c.hdca_id
+        ),
         foreign_keys=HistoryDatasetCollectionJobStateSummary.__table__.c.hdca_id,
         uselist=False,
     )
     tags = relationship(
-        'HistoryDatasetCollectionTagAssociation',
+        "HistoryDatasetCollectionTagAssociation",
         order_by=lambda: HistoryDatasetCollectionTagAssociation.id,
-        back_populates='dataset_collection',
+        back_populates="dataset_collection",
     )
     annotations = relationship(
-        'HistoryDatasetCollectionAssociationAnnotationAssociation',
+        "HistoryDatasetCollectionAssociationAnnotationAssociation",
         order_by=lambda: HistoryDatasetCollectionAssociationAnnotationAssociation.id,
-        back_populates='history_dataset_collection',
+        back_populates="history_dataset_collection",
     )
     ratings = relationship(
-        'HistoryDatasetCollectionRatingAssociation',
+        "HistoryDatasetCollectionRatingAssociation",
         order_by=lambda: HistoryDatasetCollectionRatingAssociation.id,  # type: ignore[has-type]
-        back_populates='dataset_collection',
+        back_populates="dataset_collection",
     )
-    creating_job_associations = relationship('JobToOutputDatasetCollectionAssociation', viewonly=True)
+    creating_job_associations = relationship("JobToOutputDatasetCollectionAssociation", viewonly=True)
 
-    dict_dbkeysandextensions_visible_keys = ['dbkeys', 'extensions']
-    editable_keys = ('name', 'deleted', 'visible')
+    dict_dbkeysandextensions_visible_keys = ["dbkeys", "extensions"]
+    editable_keys = ("name", "deleted", "visible")
 
     def __init__(self, deleted=False, visible=True, **kwd):
         super().__init__(**kwd)
         # Since deleted property is shared between history and dataset collections,
         # it could be on either table - some places in the code however it is convient
         # it is on instance instead of collection.
         self.deleted = deleted
@@ -5640,56 +6160,71 @@
         self.implicit_input_collections = self.implicit_input_collections or []
 
     @property
     def history_content_type(self):
         return "dataset_collection"
 
     # TODO: down into DatasetCollectionInstance
-    content_type = 'dataset_collection'
+    content_type = "dataset_collection"
 
     @hybrid.hybrid_property
     def type_id(self):
-        return '-'.join((self.content_type, str(self.id)))
+        return "-".join((self.content_type, str(self.id)))
 
     @type_id.expression  # type: ignore[no-redef]
     def type_id(cls):
-        return ((type_coerce(cls.content_type, Unicode) + '-'
-                 + type_coerce(cls.id, Unicode)).label('type_id'))
+        return (type_coerce(cls.content_type, Unicode) + "-" + type_coerce(cls.id, Unicode)).label("type_id")
 
     @property
     def job_source_type(self):
         if self.implicit_collection_jobs_id:
             return "ImplicitCollectionJobs"
         elif self.job_id:
             return "Job"
         else:
             return None
 
     @property
+    def job_state_summary_dict(self):
+        if self.job_state_summary:
+            states = self.job_state_summary.__dict__.copy()
+            del states["_sa_instance_state"]
+            del states["hdca_id"]
+            return states
+
+    @property
     def dataset_dbkeys_and_extensions_summary(self):
-        if not hasattr(self, '_dataset_dbkeys_and_extensions_summary'):
-            rows = self.collection._get_nested_collection_attributes(hda_attributes=('_metadata', 'extension'))
+        if not hasattr(self, "_dataset_dbkeys_and_extensions_summary"):
+            rows = self.collection._get_nested_collection_attributes(hda_attributes=("_metadata", "extension"))
             extensions = set()
             dbkeys = set()
             for row in rows:
                 if row is not None:
-                    dbkey_field = row._metadata.get('dbkey')
+                    dbkey_field = row._metadata.get("dbkey")
                     if isinstance(dbkey_field, list):
                         for dbkey in dbkey_field:
                             dbkeys.add(dbkey)
                     else:
                         dbkeys.add(dbkey_field)
                     extensions.add(row.extension)
             self._dataset_dbkeys_and_extensions_summary = (dbkeys, extensions)
         return self._dataset_dbkeys_and_extensions_summary
 
     @property
     def job_source_id(self):
         return self.implicit_collection_jobs_id or self.job_id
 
+    def touch(self):
+        # cause an update to be emitted, so that e.g. update_time is incremented and triggers are notified
+        if getattr(self, "name", None):
+            # attribute to flag doesn't really matter as long as it's not null (and primary key also doesn't work)
+            flag_modified(self, "name")
+            if self.collection:
+                flag_modified(self.collection, "collection_type")
+
     def to_hda_representative(self, multiple=False):
         rval = []
         for dataset in self.collection.dataset_elements:
             rval.append(dataset.dataset_instance)
             if multiple is False:
                 break
         if len(rval) > 0:
@@ -5706,69 +6241,85 @@
         )
         if self.history:
             rval["history_encoded_id"] = serialization_options.get_identifier(id_encoder, self.history)
 
         implicit_input_collections = []
         for implicit_input_collection in self.implicit_input_collections:
             input_hdca = implicit_input_collection.input_dataset_collection
-            implicit_input_collections.append({
-                "name": implicit_input_collection.name,
-                "input_dataset_collection": serialization_options.get_identifier(id_encoder, input_hdca)
-            })
+            implicit_input_collections.append(
+                {
+                    "name": implicit_input_collection.name,
+                    "input_dataset_collection": serialization_options.get_identifier(id_encoder, input_hdca),
+                }
+            )
         if implicit_input_collections:
             rval["implicit_input_collections"] = implicit_input_collections
 
         # Handle copied_from_history_dataset_association information...
         copied_from_history_dataset_collection_association_chain = []
         src_hdca = self
         while src_hdca.copied_from_history_dataset_collection_association:
             src_hdca = src_hdca.copied_from_history_dataset_collection_association
-            copied_from_history_dataset_collection_association_chain.append(serialization_options.get_identifier(id_encoder, src_hdca))
-        rval["copied_from_history_dataset_collection_association_id_chain"] = copied_from_history_dataset_collection_association_chain
+            copied_from_history_dataset_collection_association_chain.append(
+                serialization_options.get_identifier(id_encoder, src_hdca)
+            )
+        rval[
+            "copied_from_history_dataset_collection_association_id_chain"
+        ] = copied_from_history_dataset_collection_association_chain
         serialization_options.attach_identifier(id_encoder, self, rval)
         return rval
 
-    def to_dict(self, view='collection'):
+    def to_dict(self, view="collection"):
         original_dict_value = super().to_dict(view=view)
-        if (view == 'dbkeysandextensions'):
+        if view == "dbkeysandextensions":
             (dbkeys, extensions) = self.dataset_dbkeys_and_extensions_summary
             dict_value = dict(
                 dbkey=dbkeys.pop() if len(dbkeys) == 1 else "?",
-                extension=extensions.pop() if len(extensions) == 1 else "auto"
+                extension=extensions.pop() if len(extensions) == 1 else "auto",
             )
         else:
             dict_value = dict(
                 hid=self.hid,
                 history_id=self.history.id,
                 history_content_type=self.history_content_type,
                 visible=self.visible,
                 deleted=self.deleted,
                 job_source_id=self.job_source_id,
                 job_source_type=self.job_source_type,
+                job_state_summary=self.job_state_summary_dict,
                 create_time=self.create_time.isoformat(),
                 update_time=self.update_time.isoformat(),
-                **self._base_to_dict(view=view)
+                **self._base_to_dict(view=view),
             )
 
         dict_value.update(original_dict_value)
 
         return dict_value
 
     def add_implicit_input_collection(self, name, history_dataset_collection):
-        self.implicit_input_collections.append(ImplicitlyCreatedDatasetCollectionInput(name, history_dataset_collection))
+        self.implicit_input_collections.append(
+            ImplicitlyCreatedDatasetCollectionInput(name, history_dataset_collection)
+        )
 
     def find_implicit_input_collection(self, name):
         matching_collection = None
         for implicit_input_collection in self.implicit_input_collections:
             if implicit_input_collection.name == name:
                 matching_collection = implicit_input_collection.input_dataset_collection
                 break
         return matching_collection
 
-    def copy(self, element_destination=None, dataset_instance_attributes=None, flush=True, set_hid=True, minimize_copies=False):
+    def copy(
+        self,
+        element_destination=None,
+        dataset_instance_attributes=None,
+        flush=True,
+        set_hid=True,
+        minimize_copies=False,
+    ):
         """
         Create a copy of this history dataset collection association. Copy
         underlying collection.
         """
         hdca = HistoryDatasetCollectionAssociation(
             hid=self.hid,
             collection=None,
@@ -5816,107 +6367,117 @@
             return True
 
         sa_session = object_session(self)
         DCE = DatasetCollectionElement
         HDCA = HistoryDatasetCollectionAssociation
 
         # non-recursive part of the cte (starting point)
-        parents_cte = Query(DCE.dataset_collection_id) \
-            .filter(or_(DCE.child_collection_id == collection_id, DCE.dataset_collection_id == collection_id)) \
+        parents_cte = (
+            Query(DCE.dataset_collection_id)
+            .filter(or_(DCE.child_collection_id == collection_id, DCE.dataset_collection_id == collection_id))
             .cte(name="element_parents", recursive="True")
+        )
         ep = aliased(parents_cte, name="ep")
 
         # add the recursive part of the cte expression
         dce = aliased(DCE, name="dce")
-        rec = Query(dce.dataset_collection_id.label('dataset_collection_id')) \
-            .filter(dce.child_collection_id == ep.c.dataset_collection_id)
+        rec = Query(dce.dataset_collection_id.label("dataset_collection_id")).filter(
+            dce.child_collection_id == ep.c.dataset_collection_id
+        )
         parents_cte = parents_cte.union(rec)
 
         # join parents to hdca, look for matching hdca_id
         hdca = aliased(HDCA, name="hdca")
         jointohdca = parents_cte.join(hdca, hdca.collection_id == parents_cte.c.dataset_collection_id)
         qry = Query(hdca.id).select_entity_from(jointohdca).filter(hdca.id == self.id)
 
         results = qry.with_session(sa_session).all()
         return len(results) > 0
 
 
 class LibraryDatasetCollectionAssociation(Base, DatasetCollectionInstance, RepresentById):
-    """ Associates a DatasetCollection with a library folder. """
-    __tablename__ = 'library_dataset_collection_association'
+    """Associates a DatasetCollection with a library folder."""
+
+    __tablename__ = "library_dataset_collection_association"
 
     id = Column(Integer, primary_key=True)
-    collection_id = Column(Integer, ForeignKey('dataset_collection.id'), index=True)
-    folder_id = Column(Integer, ForeignKey('library_folder.id'), index=True)
+    collection_id = Column(Integer, ForeignKey("dataset_collection.id"), index=True)
+    folder_id = Column(Integer, ForeignKey("library_folder.id"), index=True)
     name = Column(TrimmedString(255))
     deleted = Column(Boolean, default=False)
 
-    collection = relationship('DatasetCollection')
-    folder = relationship('LibraryFolder')
+    collection = relationship("DatasetCollection")
+    folder = relationship("LibraryFolder")
 
-    tags = relationship('LibraryDatasetCollectionTagAssociation',
+    tags = relationship(
+        "LibraryDatasetCollectionTagAssociation",
         order_by=lambda: LibraryDatasetCollectionTagAssociation.id,
-        back_populates='dataset_collection')
-    annotations = relationship('LibraryDatasetCollectionAnnotationAssociation',
+        back_populates="dataset_collection",
+    )
+    annotations = relationship(
+        "LibraryDatasetCollectionAnnotationAssociation",
         order_by=lambda: LibraryDatasetCollectionAnnotationAssociation.id,
-        back_populates="dataset_collection")
-    ratings = relationship('LibraryDatasetCollectionRatingAssociation',
+        back_populates="dataset_collection",
+    )
+    ratings = relationship(
+        "LibraryDatasetCollectionRatingAssociation",
         order_by=lambda: LibraryDatasetCollectionRatingAssociation.id,  # type: ignore[has-type]
-        back_populates="dataset_collection")
+        back_populates="dataset_collection",
+    )
 
-    editable_keys = ('name', 'deleted')
+    editable_keys = ("name", "deleted")
 
     def __init__(self, deleted=False, **kwd):
         super().__init__(**kwd)
         # Since deleted property is shared between history and dataset collections,
         # it could be on either table - some places in the code however it is convient
         # it is on instance instead of collection.
         self.deleted = deleted
 
-    def to_dict(self, view='collection'):
-        dict_value = dict(
-            folder_id=self.folder.id,
-            **self._base_to_dict(view=view)
-        )
+    def to_dict(self, view="collection"):
+        dict_value = dict(folder_id=self.folder.id, **self._base_to_dict(view=view))
         return dict_value
 
 
 class DatasetCollectionElement(Base, Dictifiable, Serializable):
-    """ Associates a DatasetInstance (hda or ldda) with a DatasetCollection. """
-    __tablename__ = 'dataset_collection_element'
+    """Associates a DatasetInstance (hda or ldda) with a DatasetCollection."""
+
+    __tablename__ = "dataset_collection_element"
 
     id = Column(Integer, primary_key=True)
     # Parent collection id describing what collection this element belongs to.
-    dataset_collection_id = Column(Integer,
-        ForeignKey('dataset_collection.id'), index=True, nullable=False)
+    dataset_collection_id = Column(Integer, ForeignKey("dataset_collection.id"), index=True, nullable=False)
     # Child defined by this association - HDA, LDDA, or another dataset association...
-    hda_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True, nullable=True)
-    ldda_id = Column(Integer,
-        ForeignKey('library_dataset_dataset_association.id'), index=True, nullable=True)
-    child_collection_id = Column(Integer,
-        ForeignKey('dataset_collection.id'), index=True, nullable=True)
+    hda_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True, nullable=True)
+    ldda_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True, nullable=True)
+    child_collection_id = Column(Integer, ForeignKey("dataset_collection.id"), index=True, nullable=True)
     # Element index and identifier to define this parent-child relationship.
     element_index = Column(Integer)
     element_identifier = Column(Unicode(255))
 
-    hda = relationship('HistoryDatasetAssociation',
-        primaryjoin=(lambda: DatasetCollectionElement.hda_id == HistoryDatasetAssociation.id))
-    ldda = relationship('LibraryDatasetDatasetAssociation',
-        primaryjoin=(lambda: DatasetCollectionElement.ldda_id == LibraryDatasetDatasetAssociation.id))
-    child_collection = relationship('DatasetCollection',
-        primaryjoin=(lambda: DatasetCollectionElement.child_collection_id == DatasetCollection.id))
-    collection = relationship('DatasetCollection',
+    hda = relationship(
+        "HistoryDatasetAssociation",
+        primaryjoin=(lambda: DatasetCollectionElement.hda_id == HistoryDatasetAssociation.id),
+    )
+    ldda = relationship(
+        "LibraryDatasetDatasetAssociation",
+        primaryjoin=(lambda: DatasetCollectionElement.ldda_id == LibraryDatasetDatasetAssociation.id),
+    )
+    child_collection = relationship(
+        "DatasetCollection", primaryjoin=(lambda: DatasetCollectionElement.child_collection_id == DatasetCollection.id)
+    )
+    collection = relationship(
+        "DatasetCollection",
         primaryjoin=(lambda: DatasetCollection.id == DatasetCollectionElement.dataset_collection_id),
-        back_populates='elements',
+        back_populates="elements",
     )
 
     # actionable dataset id needs to be available via API...
-    dict_collection_visible_keys = ['id', 'element_type', 'element_index', 'element_identifier']
-    dict_element_visible_keys = ['id', 'element_type', 'element_index', 'element_identifier']
+    dict_collection_visible_keys = ["id", "element_type", "element_index", "element_identifier"]
+    dict_element_visible_keys = ["id", "element_type", "element_index", "element_identifier"]
 
     UNINITIALIZED_ELEMENT = object()
 
     def __init__(
         self,
         id=None,
         collection=None,
@@ -5927,17 +6488,18 @@
         if isinstance(element, HistoryDatasetAssociation):
             self.hda = element
         elif isinstance(element, LibraryDatasetDatasetAssociation):
             self.ldda = element
         elif isinstance(element, DatasetCollection):
             self.child_collection = element
         elif element != self.UNINITIALIZED_ELEMENT:
-            raise AttributeError(f'Unknown element type provided: {type(element)}')
+            raise AttributeError(f"Unknown element type provided: {type(element)}")
 
         self.id = id
+        add_object_to_object_session(self, collection)
         self.collection = collection
         self.element_index = element_index
         self.element_identifier = element_identifier or str(element_index)
 
     @property
     def element_type(self):
         if self.hda:
@@ -5987,15 +6549,27 @@
     def dataset_instances(self):
         element_object = self.element_object
         if isinstance(element_object, DatasetCollection):
             return element_object.dataset_instances
         else:
             return [element_object]
 
-    def copy_to_collection(self, collection, destination=None, element_destination=None, dataset_instance_attributes=None, flush=True, minimize_copies=False):
+    @property
+    def has_deferred_data(self):
+        return self.element_object.has_deferred_data
+
+    def copy_to_collection(
+        self,
+        collection,
+        destination=None,
+        element_destination=None,
+        dataset_instance_attributes=None,
+        flush=True,
+        minimize_copies=False,
+    ):
         dataset_instance_attributes = dataset_instance_attributes or {}
         element_object = self.element_object
         if element_destination:
             if self.is_collection:
                 element_object = element_object.copy(
                     destination=destination,
                     element_destination=element_destination,
@@ -6003,15 +6577,19 @@
                     flush=flush,
                     minimize_copies=minimize_copies,
                 )
             else:
                 new_element_object = None
                 if minimize_copies:
                     new_element_object = element_destination.get_dataset_by_hid(element_object.hid)
-                if new_element_object and new_element_object.dataset and new_element_object.dataset.id == element_object.dataset_id:
+                if (
+                    new_element_object
+                    and new_element_object.dataset
+                    and new_element_object.dataset.id == element_object.dataset_id
+                ):
                     element_object = new_element_object
                 else:
                     new_element_object = element_object.copy(flush=flush, copy_tags=element_object.tags)
                     for attribute, value in dataset_instance_attributes.items():
                         setattr(new_element_object, attribute, value)
 
                     new_element_object.visible = False
@@ -6032,97 +6610,99 @@
         return new_element
 
     def _serialize(self, id_encoder, serialization_options):
         rval = dict_for(
             self,
             element_type=self.element_type,
             element_index=self.element_index,
-            element_identifier=self.element_identifier
+            element_identifier=self.element_identifier,
         )
         serialization_options.attach_identifier(id_encoder, self, rval)
         element_obj = self.element_object
         if isinstance(element_obj, HistoryDatasetAssociation):
             rval["hda"] = element_obj.serialize(id_encoder, serialization_options, for_link=True)
         else:
             rval["child_collection"] = element_obj.serialize(id_encoder, serialization_options)
         return rval
 
 
 class Event(Base, RepresentById):
-    __tablename__ = 'event'
+    __tablename__ = "event"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True, nullable=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True, nullable=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True, nullable=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True, nullable=True)
     message = Column(TrimmedString(1024))
-    session_id = Column(Integer, ForeignKey('galaxy_session.id'), index=True, nullable=True)
+    session_id = Column(Integer, ForeignKey("galaxy_session.id"), index=True, nullable=True)
     tool_id = Column(String(255))
 
-    history = relationship('History')
-    user = relationship('User')
-    galaxy_session = relationship('GalaxySession')
+    history = relationship("History")
+    user = relationship("User")
+    galaxy_session = relationship("GalaxySession")
 
 
 class GalaxySession(Base, RepresentById):
-    __tablename__ = 'galaxy_session'
+    __tablename__ = "galaxy_session"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True, nullable=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True, nullable=True)
     remote_host = Column(String(255))
     remote_addr = Column(String(255))
     referer = Column(TEXT)
-    current_history_id = Column(Integer, ForeignKey('history.id'), nullable=True)
+    current_history_id = Column(Integer, ForeignKey("history.id"), nullable=True)
     # unique 128 bit random number coerced to a string
     session_key = Column(TrimmedString(255), index=True, unique=True)
     is_valid = Column(Boolean, default=False)
     # saves a reference to the previous session so we have a way to chain them together
     prev_session_id = Column(Integer)
     disk_usage = Column(Numeric(15, 0), index=True)
     last_action = Column(DateTime)
-    current_history = relationship('History')
-    histories = relationship('GalaxySessionToHistoryAssociation', back_populates='galaxy_session')
-    user = relationship('User', back_populates='galaxy_sessions')
+    current_history = relationship("History")
+    histories = relationship("GalaxySessionToHistoryAssociation", back_populates="galaxy_session")
+    user = relationship("User", back_populates="galaxy_sessions")
 
     def __init__(self, is_valid=False, **kwd):
         super().__init__(**kwd)
         self.is_valid = is_valid
-        self.last_action = self.last_action or datetime.now()
+        self.last_action = self.last_action or now()
 
     def add_history(self, history, association=None):
         if association is None:
             self.histories.append(GalaxySessionToHistoryAssociation(self, history))
         else:
             self.histories.append(association)
 
     def get_disk_usage(self):
         if self.disk_usage is None:
             return 0
         return self.disk_usage
 
     def set_disk_usage(self, bytes):
         self.disk_usage = bytes
+
     total_disk_usage = property(get_disk_usage, set_disk_usage)
 
 
 class GalaxySessionToHistoryAssociation(Base, RepresentById):
-    __tablename__ = 'galaxy_session_to_history'
+    __tablename__ = "galaxy_session_to_history"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    session_id = Column(Integer, ForeignKey('galaxy_session.id'), index=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    galaxy_session = relationship('GalaxySession', back_populates='histories')
-    history = relationship('History', back_populates='galaxy_sessions')
+    session_id = Column(Integer, ForeignKey("galaxy_session.id"), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    galaxy_session = relationship("GalaxySession", back_populates="histories")
+    history = relationship("History", back_populates="galaxy_sessions")
 
     def __init__(self, galaxy_session, history):
         self.galaxy_session = galaxy_session
+        add_object_to_object_session(self, history)
         self.history = history
 
 
 class UCI:
     def __init__(self):
         self.id = None
         self.user = None
@@ -6132,76 +6712,118 @@
     """
     StoredWorkflow represents the root node of a tree of objects that compose a workflow, including workflow revisions, steps, and subworkflows.
     It is responsible for the metadata associated with a workflow including owner, name, published, and create/update time.
 
     Each time a workflow is modified a revision is created, represented by a new :class:`galaxy.model.Workflow` instance.
     See :class:`galaxy.model.Workflow` for more information
     """
-    __tablename__ = 'stored_workflow'
-    __table_args__ = (
-        Index('ix_stored_workflow_slug', 'slug', mysql_length=200),
-    )
+
+    __tablename__ = "stored_workflow"
+    __table_args__ = (Index("ix_stored_workflow_slug", "slug", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now, index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True, nullable=False)
-    latest_workflow_id = Column(Integer,
-        ForeignKey('workflow.id', use_alter=True, name='stored_workflow_latest_workflow_id_fk'),
-        index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True, nullable=False)
+    latest_workflow_id = Column(
+        Integer, ForeignKey("workflow.id", use_alter=True, name="stored_workflow_latest_workflow_id_fk"), index=True
+    )
     name = Column(TEXT)
     deleted = Column(Boolean, default=False)
     hidden = Column(Boolean, default=False)
     importable = Column(Boolean, default=False)
     slug = Column(TEXT)
     from_path = Column(TEXT)
     published = Column(Boolean, index=True, default=False)
 
-    user = relationship('User',
-        primaryjoin=(lambda: User.id == StoredWorkflow.user_id),
-        back_populates='stored_workflows')
-    workflows = relationship('Workflow',
-        back_populates='stored_workflow',
+    user = relationship(
+        "User", primaryjoin=(lambda: User.id == StoredWorkflow.user_id), back_populates="stored_workflows"
+    )
+    workflows = relationship(
+        "Workflow",
+        back_populates="stored_workflow",
         cascade="all, delete-orphan",
         primaryjoin=(lambda: StoredWorkflow.id == Workflow.stored_workflow_id),  # type: ignore[has-type]
-        order_by=lambda: -Workflow.id)  # type: ignore[has-type]
-    latest_workflow = relationship('Workflow',
+        order_by=lambda: -Workflow.id,  # type: ignore[has-type]
+    )
+    latest_workflow = relationship(
+        "Workflow",
         post_update=True,
         primaryjoin=(lambda: StoredWorkflow.latest_workflow_id == Workflow.id),  # type: ignore[has-type]
-        lazy=False)
-    tags = relationship('StoredWorkflowTagAssociation',
+        lazy=False,
+    )
+    tags = relationship(
+        "StoredWorkflowTagAssociation",
         order_by=lambda: StoredWorkflowTagAssociation.id,
-        back_populates="stored_workflow")
-    owner_tags = relationship('StoredWorkflowTagAssociation',
-        primaryjoin=(lambda:
-            and_(StoredWorkflow.id == StoredWorkflowTagAssociation.stored_workflow_id,
-                StoredWorkflow.user_id == StoredWorkflowTagAssociation.user_id)
+        back_populates="stored_workflow",
+    )
+    owner_tags = relationship(
+        "StoredWorkflowTagAssociation",
+        primaryjoin=(
+            lambda: and_(
+                StoredWorkflow.id == StoredWorkflowTagAssociation.stored_workflow_id,
+                StoredWorkflow.user_id == StoredWorkflowTagAssociation.user_id,
+            )
         ),
         viewonly=True,
-        order_by=lambda: StoredWorkflowTagAssociation.id)
-    annotations = relationship('StoredWorkflowAnnotationAssociation',
+        order_by=lambda: StoredWorkflowTagAssociation.id,
+    )
+    annotations = relationship(
+        "StoredWorkflowAnnotationAssociation",
         order_by=lambda: StoredWorkflowAnnotationAssociation.id,
-        back_populates="stored_workflow")
-    ratings = relationship('StoredWorkflowRatingAssociation',
+        back_populates="stored_workflow",
+    )
+    ratings = relationship(
+        "StoredWorkflowRatingAssociation",
         order_by=lambda: StoredWorkflowRatingAssociation.id,  # type: ignore[has-type]
-        back_populates="stored_workflow")
-    users_shared_with = relationship('StoredWorkflowUserShareAssociation',
-        back_populates='stored_workflow')
+        back_populates="stored_workflow",
+    )
+    users_shared_with = relationship("StoredWorkflowUserShareAssociation", back_populates="stored_workflow")
 
     average_rating: column_property
 
     # Set up proxy so that
     #   StoredWorkflow.users_shared_with
     # returns a list of users that workflow is shared with.
-    users_shared_with_dot_users = association_proxy('users_shared_with', 'user')
+    users_shared_with_dot_users = association_proxy("users_shared_with", "user")
 
-    dict_collection_visible_keys = ['id', 'name', 'create_time', 'update_time', 'published', 'deleted', 'hidden']
-    dict_element_visible_keys = ['id', 'name', 'create_time', 'update_time', 'published', 'deleted', 'hidden']
+    dict_collection_visible_keys = [
+        "id",
+        "name",
+        "create_time",
+        "update_time",
+        "published",
+        "importable",
+        "deleted",
+        "hidden",
+    ]
+    dict_element_visible_keys = [
+        "id",
+        "name",
+        "create_time",
+        "update_time",
+        "published",
+        "importable",
+        "deleted",
+        "hidden",
+    ]
 
-    def __init__(self, user=None, name=None, slug=None, create_time=None, update_time=None, published=False, latest_workflow_id=None, workflow=None, hidden=False):
+    def __init__(
+        self,
+        user=None,
+        name=None,
+        slug=None,
+        create_time=None,
+        update_time=None,
+        published=False,
+        latest_workflow_id=None,
+        workflow=None,
+        hidden=False,
+    ):
+        add_object_to_object_session(self, user)
         self.user = user
         self.name = name
         self.slug = slug
         self.create_time = create_time
         self.update_time = update_time
         self.published = published
         self.latest_workflow = workflow
@@ -6213,102 +6835,115 @@
             return self.latest_workflow
         if len(self.workflows) <= version:
             raise Exception("Version does not exist")
         return list(reversed(self.workflows))[version]
 
     def show_in_tool_panel(self, user_id):
         sa_session = object_session(self)
-        return bool(sa_session.query(StoredWorkflowMenuEntry).filter(
-            StoredWorkflowMenuEntry.stored_workflow_id == self.id,
-            StoredWorkflowMenuEntry.user_id == user_id,
-        ).count())
+        return bool(
+            sa_session.query(StoredWorkflowMenuEntry)
+            .filter(
+                StoredWorkflowMenuEntry.stored_workflow_id == self.id,
+                StoredWorkflowMenuEntry.user_id == user_id,
+            )
+            .count()
+        )
 
     def copy_tags_from(self, target_user, source_workflow):
         # Override to only copy owner tags.
         for src_swta in source_workflow.owner_tags:
             new_swta = src_swta.copy()
             new_swta.user = target_user
             self.tags.append(new_swta)
 
-    def to_dict(self, view='collection', value_mapper=None):
+    def to_dict(self, view="collection", value_mapper=None):
         rval = super().to_dict(view=view, value_mapper=value_mapper)
-        rval['latest_workflow_uuid'] = (lambda uuid: str(uuid) if self.latest_workflow.uuid else None)(self.latest_workflow.uuid)
+        rval["latest_workflow_uuid"] = (lambda uuid: str(uuid) if self.latest_workflow.uuid else None)(
+            self.latest_workflow.uuid
+        )
         return rval
 
 
 class Workflow(Base, Dictifiable, RepresentById):
     """
     Workflow represents a revision of a :class:`galaxy.model.StoredWorkflow`.
     A new instance is created for each workflow revision and provides a common parent for the workflow steps.
 
     See :class:`galaxy.model.WorkflowStep` for more information
     """
-    __tablename__ = 'workflow'
+
+    __tablename__ = "workflow"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     # workflows will belong to either a stored workflow or a parent/nesting workflow.
-    stored_workflow_id = Column(Integer, ForeignKey('stored_workflow.id'), index=True, nullable=True)
-    parent_workflow_id = Column(Integer, ForeignKey('workflow.id'), index=True, nullable=True)
+    stored_workflow_id = Column(Integer, ForeignKey("stored_workflow.id"), index=True, nullable=True)
+    parent_workflow_id = Column(Integer, ForeignKey("workflow.id"), index=True, nullable=True)
     name = Column(TEXT)
     has_cycles = Column(Boolean)
     has_errors = Column(Boolean)
-    reports_config = Column(MutableJSONType)
-    creator_metadata = Column(MutableJSONType)
+    reports_config = Column(JSONType)
+    creator_metadata = Column(JSONType)
     license = Column(TEXT)
+    source_metadata = Column(JSONType)
     uuid = Column(UUIDType, nullable=True)
 
-    steps = relationship('WorkflowStep',
-        back_populates='workflow',
+    steps: List["WorkflowStep"] = relationship(
+        "WorkflowStep",
+        back_populates="workflow",
         primaryjoin=(lambda: Workflow.id == WorkflowStep.workflow_id),  # type: ignore[has-type]
         order_by=lambda: asc(WorkflowStep.order_index),  # type: ignore[has-type]
         cascade="all, delete-orphan",
-        lazy=False)
+        lazy=False,
+    )
     parent_workflow_steps = relationship(
-        'WorkflowStep',
+        "WorkflowStep",
         primaryjoin=(lambda: Workflow.id == WorkflowStep.subworkflow_id),  # type: ignore[has-type]
-        back_populates='subworkflow')
-    stored_workflow = relationship('StoredWorkflow',
+        back_populates="subworkflow",
+    )
+    stored_workflow = relationship(
+        "StoredWorkflow",
         primaryjoin=(lambda: StoredWorkflow.id == Workflow.stored_workflow_id),
-        back_populates='workflows')
+        back_populates="workflows",
+    )
 
     step_count: column_property
 
-    dict_collection_visible_keys = ['name', 'has_cycles', 'has_errors']
-    dict_element_visible_keys = ['name', 'has_cycles', 'has_errors']
-    input_step_types = ['data_input', 'data_collection_input', 'parameter_input']
+    dict_collection_visible_keys = ["name", "has_cycles", "has_errors"]
+    dict_element_visible_keys = ["name", "has_cycles", "has_errors"]
+    input_step_types = ["data_input", "data_collection_input", "parameter_input"]
 
     def __init__(self, uuid=None):
         self.user = None
         self.uuid = get_uuid(uuid)
 
     def has_outputs_defined(self):
         """
         Returns true or false indicating whether or not a workflow has outputs defined.
         """
         for step in self.steps:
             if step.workflow_outputs:
                 return True
         return False
 
-    def to_dict(self, view='collection', value_mapper=None):
+    def to_dict(self, view="collection", value_mapper=None):
         rval = super().to_dict(view=view, value_mapper=value_mapper)
-        rval['uuid'] = (lambda uuid: str(uuid) if uuid else None)(self.uuid)
+        rval["uuid"] = (lambda uuid: str(uuid) if uuid else None)(self.uuid)
         return rval
 
     @property
     def steps_by_id(self):
         steps = {}
         for step in self.steps:
             step_id = step.id
             steps[step_id] = step
         return steps
 
-    def step_by_index(self, order_index):
+    def step_by_index(self, order_index: int):
         for step in self.steps:
             if order_index == step.order_index:
                 return step
         raise KeyError(f"Workflow has no step with order_index '{order_index}'")
 
     def step_by_label(self, label):
         for step in self.steps:
@@ -6340,36 +6975,36 @@
         names = []
         for workflow_output in self.workflow_outputs:
             names.append(workflow_output.label)
         return names
 
     @property
     def top_level_workflow(self):
-        """ If this workflow is not attached to stored workflow directly,
+        """If this workflow is not attached to stored workflow directly,
         recursively grab its parents until it is the top level workflow
         which must have a stored workflow associated with it.
         """
         top_level_workflow = self
         if self.stored_workflow is None:
             # TODO: enforce this at creation...
             assert len({w.uuid for w in self.parent_workflow_steps}) == 1
             return self.parent_workflow_steps[0].workflow.top_level_workflow
         return top_level_workflow
 
     @property
     def top_level_stored_workflow(self):
-        """ If this workflow is not attached to stored workflow directly,
+        """If this workflow is not attached to stored workflow directly,
         recursively grab its parents until it is the top level workflow
         which must have a stored workflow associated with it and then
         grab that stored workflow.
         """
         return self.top_level_workflow.stored_workflow
 
     def copy(self, user=None):
-        """ Copy a workflow for a new StoredWorkflow object.
+        """Copy a workflow for a new StoredWorkflow object.
 
         Pass user if user-specific information needed.
         """
         copied_workflow = Workflow()
         copied_workflow.name = self.name
         copied_workflow.has_cycles = self.has_cycles
         copied_workflow.has_errors = self.has_errors
@@ -6402,85 +7037,124 @@
 
 class WorkflowStep(Base, RepresentById):
     """
     WorkflowStep represents a tool or subworkflow, its inputs, annotations, and any outputs that are flagged as workflow outputs.
 
     See :class:`galaxy.model.WorkflowStepInput` and :class:`galaxy.model.WorkflowStepConnection` for more information.
     """
-    __tablename__ = 'workflow_step'
+
+    __tablename__ = "workflow_step"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    workflow_id = Column(Integer, ForeignKey('workflow.id'), index=True, nullable=False)
-    subworkflow_id = Column(Integer, ForeignKey('workflow.id'), index=True, nullable=True)
-    dynamic_tool_id = Column(Integer, ForeignKey('dynamic_tool.id'), index=True, nullable=True)
-    type = Column(String(64))
+    workflow_id = Column(Integer, ForeignKey("workflow.id"), index=True, nullable=False)
+    subworkflow_id = Column(Integer, ForeignKey("workflow.id"), index=True, nullable=True)
+    dynamic_tool_id = Column(Integer, ForeignKey("dynamic_tool.id"), index=True, nullable=True)
+    type: str = Column(String(64))
     tool_id = Column(TEXT)
     tool_version = Column(TEXT)
     tool_inputs = Column(JSONType)
     tool_errors = Column(JSONType)
     position = Column(MutableJSONType)
     config = Column(JSONType)
-    order_index = Column(Integer)
+    order_index: int = Column(Integer)
+    when_expression = Column(JSONType)
     uuid = Column(UUIDType)
     label = Column(Unicode(255))
     temp_input_connections: Optional[InputConnDictType]
 
-    subworkflow = relationship('Workflow',
+    subworkflow: Optional[Workflow] = relationship(
+        "Workflow",
         primaryjoin=(lambda: Workflow.id == WorkflowStep.subworkflow_id),
-        back_populates='parent_workflow_steps')
-    dynamic_tool = relationship('DynamicTool',
-        primaryjoin=(lambda: DynamicTool.id == WorkflowStep.dynamic_tool_id))
-    tags = relationship('WorkflowStepTagAssociation',
-        order_by=lambda: WorkflowStepTagAssociation.id,
-        back_populates='workflow_step')
-    annotations = relationship('WorkflowStepAnnotationAssociation',
+        back_populates="parent_workflow_steps",
+    )
+    dynamic_tool = relationship("DynamicTool", primaryjoin=(lambda: DynamicTool.id == WorkflowStep.dynamic_tool_id))
+    tags = relationship(
+        "WorkflowStepTagAssociation", order_by=lambda: WorkflowStepTagAssociation.id, back_populates="workflow_step"
+    )
+    annotations = relationship(
+        "WorkflowStepAnnotationAssociation",
         order_by=lambda: WorkflowStepAnnotationAssociation.id,
-        back_populates="workflow_step")
-    post_job_actions = relationship('PostJobAction', back_populates='workflow_step')
-    inputs = relationship('WorkflowStepInput', back_populates='workflow_step')
-    workflow_outputs = relationship('WorkflowOutput', back_populates='workflow_step')
-    output_connections = relationship('WorkflowStepConnection',
-        primaryjoin=(lambda: WorkflowStepConnection.output_step_id == WorkflowStep.id)
-    )
-    workflow = relationship('Workflow',
-        primaryjoin=(lambda: Workflow.id == WorkflowStep.workflow_id),
-        back_populates='steps'
+        back_populates="workflow_step",
     )
+    post_job_actions = relationship("PostJobAction", back_populates="workflow_step")
+    inputs = relationship("WorkflowStepInput", back_populates="workflow_step")
+    workflow_outputs = relationship("WorkflowOutput", back_populates="workflow_step")
+    output_connections = relationship(
+        "WorkflowStepConnection", primaryjoin=(lambda: WorkflowStepConnection.output_step_id == WorkflowStep.id)
+    )
+    workflow = relationship(
+        "Workflow", primaryjoin=(lambda: Workflow.id == WorkflowStep.workflow_id), back_populates="steps"
+    )
+
+    # Injected attributes
+    # TODO: code using these should be refactored to not depend on these non-persistent fields
+    module: Optional["WorkflowModule"]
+    state: Optional["DefaultToolState"]
+    upgrade_messages: Optional[Dict]
 
     STEP_TYPE_TO_INPUT_TYPE = {
         "data_input": "dataset",
         "data_collection_input": "dataset_collection",
         "parameter_input": "parameter",
     }
     DEFAULT_POSITION = {"left": 0, "top": 0}
 
     def __init__(self):
         self.uuid = uuid4()
         self._input_connections_by_name = None
+        self._inputs_by_name = None
+
+    @reconstructor
+    def init_on_load(self):
+        self._input_connections_by_name = None
+        self._inputs_by_name = None
 
     @property
     def tool_uuid(self):
         return self.dynamic_tool and self.dynamic_tool.uuid
 
     @property
     def input_type(self):
-        assert self.type and self.type in self.STEP_TYPE_TO_INPUT_TYPE, "step.input_type can only be called on input step types"
+        assert (
+            self.type and self.type in self.STEP_TYPE_TO_INPUT_TYPE
+        ), "step.input_type can only be called on input step types"
         return self.STEP_TYPE_TO_INPUT_TYPE[self.type]
 
     @property
     def input_default_value(self):
-        tool_inputs = self.tool_inputs
-        tool_state = tool_inputs
+        tool_state = self.tool_inputs
         default_value = tool_state.get("default")
         if default_value:
             default_value = json.loads(default_value)["value"]
         return default_value
 
+    @property
+    def input_optional(self):
+        tool_state = self.tool_inputs
+        return tool_state.get("optional") or False
+
+    def setup_inputs_by_name(self):
+        # Ensure input_connections has already been set.
+
+        # Make connection information available on each step by input name.
+        inputs_by_name = {}
+        for step_input in self.inputs:
+            input_name = step_input.name
+            assert input_name not in inputs_by_name
+            inputs_by_name[input_name] = step_input
+        self._inputs_by_name = inputs_by_name
+
+    @property
+    def inputs_by_name(self):
+        if self._inputs_by_name is None:
+            self.setup_inputs_by_name()
+        return self._inputs_by_name
+
     def get_input(self, input_name):
         for step_input in self.inputs:
             if step_input.name == input_name:
                 return step_input
 
         return None
 
@@ -6494,17 +7168,37 @@
 
     def add_connection(self, input_name, output_name, output_step, input_subworkflow_step_index=None):
         step_input = self.get_or_add_input(input_name)
 
         conn = WorkflowStepConnection()
         conn.input_step_input = step_input
         conn.output_name = output_name
+        add_object_to_object_session(conn, output_step)
         conn.output_step = output_step
-        if input_subworkflow_step_index is not None:
-            input_subworkflow_step = self.subworkflow.step_by_index(input_subworkflow_step_index)
+        if self.subworkflow:
+            if input_subworkflow_step_index is not None:
+                input_subworkflow_step = self.subworkflow.step_by_index(input_subworkflow_step_index)
+            else:
+                input_subworkflow_steps = [step for step in self.subworkflow.input_steps if step.label == input_name]
+                if not input_subworkflow_steps:
+                    inferred_order_index = input_name.split(":", 1)[0]
+                    if inferred_order_index.isdigit():
+                        input_subworkflow_steps = [self.subworkflow.step_by_index(int(inferred_order_index))]
+                if len(input_subworkflow_steps) != 1:
+                    # `when` expression inputs don't need to be passed into subworkflow
+                    # In the absence of formal extra step inputs this seems like the best we can do.
+                    # A better way to do these validations is to validate that all required subworkflow inputs
+                    # are connected.
+                    if input_name not in (self.when_expression or ""):
+                        raise galaxy.exceptions.MessageException(
+                            f"Invalid subworkflow connection at step index {self.order_index + 1}"
+                        )
+                    else:
+                        input_subworkflow_steps = [None]
+                input_subworkflow_step = input_subworkflow_steps[0]
             conn.input_subworkflow_step = input_subworkflow_step
         return conn
 
     @property
     def input_connections(self):
         connections = [_ for step_input in self.inputs for _ in step_input.connections]
         return connections
@@ -6605,61 +7299,74 @@
 
         for old_conn, new_conn in zip(self.input_connections, copied_step.input_connections):
             new_conn.input_step_input = copied_step.get_or_add_input(old_conn.input_name)
             new_conn.output_step = step_mapping[old_conn.output_step_id]
             if old_conn.input_subworkflow_step_id:
                 new_conn.input_subworkflow_step = subworkflow_step_mapping[old_conn.input_subworkflow_step_id]
         for orig_pja in self.post_job_actions:
-            PostJobAction(orig_pja.action_type,
-                          copied_step,
-                          output_name=orig_pja.output_name,
-                          action_arguments=orig_pja.action_arguments)
+            PostJobAction(
+                orig_pja.action_type,
+                copied_step,
+                output_name=orig_pja.output_name,
+                action_arguments=orig_pja.action_arguments,
+            )
         copied_step.workflow_outputs = copy_list(self.workflow_outputs, copied_step)
 
     def log_str(self):
-        return "WorkflowStep[index=%d,type=%s]" % (self.order_index, self.type)
+        return (
+            f"WorkflowStep[index={self.order_index},type={self.type},label={self.label},uuid={self.uuid},id={self.id}]"
+        )
 
     def clear_module_extras(self):
         # the module code adds random dynamic state to the step, this
         # attempts to clear that.
         for module_attribute in ["module"]:
             try:
                 delattr(self, module_attribute)
             except AttributeError:
                 pass
 
 
 class WorkflowStepInput(Base, RepresentById):
-    __tablename__ = 'workflow_step_input'
+    __tablename__ = "workflow_step_input"
     __table_args__ = (
-        Index('ix_workflow_step_input_workflow_step_id_name_unique',
-            'workflow_step_id', 'name', unique=True, mysql_length={'name': 200}),
+        Index(
+            "ix_workflow_step_input_workflow_step_id_name_unique",
+            "workflow_step_id",
+            "name",
+            unique=True,
+            mysql_length={"name": 200},
+        ),
     )
 
     id = Column(Integer, primary_key=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True)
     name = Column(TEXT)
     merge_type = Column(TEXT)
     scatter_type = Column(TEXT)
     value_from = Column(MutableJSONType)
     value_from_type = Column(TEXT)
     default_value = Column(MutableJSONType)
     default_value_set = Column(Boolean, default=False)
     runtime_value = Column(Boolean, default=False)
 
-    workflow_step = relationship('WorkflowStep',
-        back_populates='inputs',
-        cascade='all',
-        primaryjoin=(lambda: WorkflowStepInput.workflow_step_id == WorkflowStep.id))
+    workflow_step = relationship(
+        "WorkflowStep",
+        back_populates="inputs",
+        cascade="all",
+        primaryjoin=(lambda: WorkflowStepInput.workflow_step_id == WorkflowStep.id),
+    )
     connections = relationship(
-        'WorkflowStepConnection',
-        back_populates='input_step_input',
-        primaryjoin=(lambda: WorkflowStepConnection.input_step_input_id == WorkflowStepInput.id))
+        "WorkflowStepConnection",
+        back_populates="input_step_input",
+        primaryjoin=(lambda: WorkflowStepConnection.input_step_input_id == WorkflowStepInput.id),
+    )
 
     def __init__(self, workflow_step):
+        add_object_to_object_session(self, workflow_step)
         self.workflow_step = workflow_step
         self.default_value_set = False
 
     def copy(self, copied_step):
         copied_step_input = WorkflowStepInput(copied_step)
         copied_step_input.name = self.name
         copied_step_input.default_value = self.default_value
@@ -6668,43 +7375,48 @@
         copied_step_input.scatter_type = self.scatter_type
 
         copied_step_input.connections = copy_list(self.connections)
         return copied_step_input
 
 
 class WorkflowStepConnection(Base, RepresentById):
-    __tablename__ = 'workflow_step_connection'
+    __tablename__ = "workflow_step_connection"
 
     id = Column(Integer, primary_key=True)
-    output_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True)
-    input_step_input_id = Column(Integer, ForeignKey('workflow_step_input.id'), index=True)
+    output_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True)
+    input_step_input_id = Column(Integer, ForeignKey("workflow_step_input.id"), index=True)
     output_name = Column(TEXT)
-    input_subworkflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True)
+    input_subworkflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True)
 
-    input_step_input = relationship('WorkflowStepInput',
-        back_populates='connections',
-        cascade='all',
-        primaryjoin=(lambda: WorkflowStepConnection.input_step_input_id == WorkflowStepInput.id))
-    input_subworkflow_step = relationship('WorkflowStep',
-        primaryjoin=(lambda: WorkflowStepConnection.input_subworkflow_step_id == WorkflowStep.id))
-    output_step = relationship('WorkflowStep',
-        back_populates='output_connections',
-        cascade='all',
-        primaryjoin=(lambda: WorkflowStepConnection.output_step_id == WorkflowStep.id))
+    input_step_input = relationship(
+        "WorkflowStepInput",
+        back_populates="connections",
+        cascade="all",
+        primaryjoin=(lambda: WorkflowStepConnection.input_step_input_id == WorkflowStepInput.id),
+    )
+    input_subworkflow_step = relationship(
+        "WorkflowStep", primaryjoin=(lambda: WorkflowStepConnection.input_subworkflow_step_id == WorkflowStep.id)
+    )
+    output_step = relationship(
+        "WorkflowStep",
+        back_populates="output_connections",
+        cascade="all",
+        primaryjoin=(lambda: WorkflowStepConnection.output_step_id == WorkflowStep.id),
+    )
 
     # Constant used in lieu of output_name and input_name to indicate an
     # implicit connection between two steps that is not dependent on a dataset
     # or a dataset collection. Allowing for instance data manager steps to setup
     # index data before a normal tool runs or for workflows that manage data
     # outside of Galaxy.
     NON_DATA_CONNECTION = "__NO_INPUT_OUTPUT_NAME__"
 
     @property
     def non_data_connection(self):
-        return (self.output_name == self.input_name == WorkflowStepConnection.NON_DATA_CONNECTION)
+        return self.output_name == self.input_name == WorkflowStepConnection.NON_DATA_CONNECTION
 
     @property
     def input_name(self):
         return self.input_step_input.name
 
     @property
     def input_step(self) -> Optional[WorkflowStep]:
@@ -6718,127 +7430,161 @@
     def copy(self):
         # TODO: handle subworkflow ids...
         copied_connection = WorkflowStepConnection()
         copied_connection.output_name = self.output_name
         return copied_connection
 
 
-class WorkflowOutput(Base, RepresentById):
-    __tablename__ = 'workflow_output'
+class WorkflowOutput(Base, Serializable):
+    __tablename__ = "workflow_output"
 
     id = Column(Integer, primary_key=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True, nullable=False)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True, nullable=False)
     output_name = Column(String(255), nullable=True)
     label = Column(Unicode(255))
     uuid = Column(UUIDType)
-    workflow_step = relationship('WorkflowStep',
-        back_populates='workflow_outputs',
-        primaryjoin=(lambda: WorkflowStep.id == WorkflowOutput.workflow_step_id))
+    workflow_step = relationship(
+        "WorkflowStep",
+        back_populates="workflow_outputs",
+        primaryjoin=(lambda: WorkflowStep.id == WorkflowOutput.workflow_step_id),
+    )
 
     def __init__(self, workflow_step, output_name=None, label=None, uuid=None):
         self.workflow_step = workflow_step
         self.output_name = output_name
         self.label = label
         self.uuid = get_uuid(uuid)
 
     def copy(self, copied_step):
         copied_output = WorkflowOutput(copied_step)
         copied_output.output_name = self.output_name
         copied_output.label = self.label
         return copied_output
 
+    def _serialize(self, id_encoder, serialization_options):
+        return dict_for(
+            self,
+            output_name=self.output_name,
+            label=self.label,
+            uuid=str(self.uuid),
+        )
+
 
 class StoredWorkflowUserShareAssociation(Base, UserShareAssociation):
-    __tablename__ = 'stored_workflow_user_share_connection'
+    __tablename__ = "stored_workflow_user_share_connection"
 
     id = Column(Integer, primary_key=True)
-    stored_workflow_id = Column(Integer, ForeignKey('stored_workflow.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    user = relationship('User')
-    stored_workflow = relationship('StoredWorkflow', back_populates='users_shared_with')
+    stored_workflow_id = Column(Integer, ForeignKey("stored_workflow.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    user = relationship("User")
+    stored_workflow = relationship("StoredWorkflow", back_populates="users_shared_with")
 
 
 class StoredWorkflowMenuEntry(Base, RepresentById):
-    __tablename__ = 'stored_workflow_menu_entry'
+    __tablename__ = "stored_workflow_menu_entry"
 
     id = Column(Integer, primary_key=True)
-    stored_workflow_id = Column(Integer, ForeignKey('stored_workflow.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    stored_workflow_id = Column(Integer, ForeignKey("stored_workflow.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     order_index = Column(Integer)
 
-    stored_workflow = relationship('StoredWorkflow')
-    user = relationship('User', back_populates='stored_workflow_menu_entries',
-        primaryjoin=(lambda:
-            (StoredWorkflowMenuEntry.user_id == User.id)
+    stored_workflow = relationship("StoredWorkflow")
+    user = relationship(
+        "User",
+        back_populates="stored_workflow_menu_entries",
+        primaryjoin=(
+            lambda: (StoredWorkflowMenuEntry.user_id == User.id)
             & (StoredWorkflowMenuEntry.stored_workflow_id == StoredWorkflow.id)
-            & not_(StoredWorkflow.deleted))
+            & not_(StoredWorkflow.deleted)
+        ),
     )
 
 
-class WorkflowInvocation(Base, UsesCreateAndUpdateTime, Dictifiable, RepresentById):
-    __tablename__ = 'workflow_invocation'
+class WorkflowInvocation(Base, UsesCreateAndUpdateTime, Dictifiable, Serializable):
+    __tablename__ = "workflow_invocation"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now, index=True)
-    workflow_id = Column(Integer, ForeignKey('workflow.id'), index=True, nullable=False)
+    workflow_id = Column(Integer, ForeignKey("workflow.id"), index=True, nullable=False)
     state = Column(TrimmedString(64), index=True)
     scheduler = Column(TrimmedString(255), index=True)
     handler = Column(TrimmedString(255), index=True)
     uuid = Column(UUIDType())
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
 
-    history = relationship('History', back_populates='workflow_invocations')
-    input_parameters = relationship('WorkflowRequestInputParameter',
-        back_populates='workflow_invocation')
-    step_states = relationship('WorkflowRequestStepState', back_populates='workflow_invocation')
-    input_step_parameters = relationship('WorkflowRequestInputStepParameter',
-        back_populates='workflow_invocation')
-    input_datasets = relationship('WorkflowRequestToInputDatasetAssociation',
-        back_populates='workflow_invocation')
-    input_dataset_collections = relationship('WorkflowRequestToInputDatasetCollectionAssociation',
-        back_populates='workflow_invocation')
-    subworkflow_invocations = relationship('WorkflowInvocationToSubworkflowInvocationAssociation',
-        primaryjoin=(lambda:
-            WorkflowInvocationToSubworkflowInvocationAssociation.workflow_invocation_id
-                == WorkflowInvocation.id),
-        back_populates='parent_workflow_invocation',
+    history = relationship("History", back_populates="workflow_invocations")
+    input_parameters = relationship("WorkflowRequestInputParameter", back_populates="workflow_invocation")
+    step_states = relationship("WorkflowRequestStepState", back_populates="workflow_invocation")
+    input_step_parameters = relationship("WorkflowRequestInputStepParameter", back_populates="workflow_invocation")
+    input_datasets = relationship("WorkflowRequestToInputDatasetAssociation", back_populates="workflow_invocation")
+    input_dataset_collections = relationship(
+        "WorkflowRequestToInputDatasetCollectionAssociation", back_populates="workflow_invocation"
+    )
+    subworkflow_invocations = relationship(
+        "WorkflowInvocationToSubworkflowInvocationAssociation",
+        primaryjoin=(
+            lambda: WorkflowInvocationToSubworkflowInvocationAssociation.workflow_invocation_id == WorkflowInvocation.id
+        ),
+        back_populates="parent_workflow_invocation",
         uselist=True,
     )
-    steps = relationship('WorkflowInvocationStep', back_populates='workflow_invocation')
-    workflow = relationship('Workflow')
-    output_dataset_collections = relationship('WorkflowInvocationOutputDatasetCollectionAssociation',
-        back_populates='workflow_invocation')
-    output_datasets = relationship('WorkflowInvocationOutputDatasetAssociation',
-        back_populates='workflow_invocation')
-    output_values = relationship('WorkflowInvocationOutputValue', back_populates='workflow_invocation')
-
-    dict_collection_visible_keys = ['id', 'update_time', 'create_time', 'workflow_id', 'history_id', 'uuid', 'state']
-    dict_element_visible_keys = ['id', 'update_time', 'create_time', 'workflow_id', 'history_id', 'uuid', 'state']
+    steps = relationship(
+        "WorkflowInvocationStep",
+        back_populates="workflow_invocation",
+        order_by=lambda: WorkflowInvocationStep.order_index,
+    )
+    workflow: Workflow = relationship("Workflow")
+    output_dataset_collections = relationship(
+        "WorkflowInvocationOutputDatasetCollectionAssociation", back_populates="workflow_invocation"
+    )
+    output_datasets = relationship("WorkflowInvocationOutputDatasetAssociation", back_populates="workflow_invocation")
+    output_values = relationship("WorkflowInvocationOutputValue", back_populates="workflow_invocation")
+    messages = relationship("WorkflowInvocationMessage", back_populates="workflow_invocation")
+
+    dict_collection_visible_keys = [
+        "id",
+        "update_time",
+        "create_time",
+        "workflow_id",
+        "history_id",
+        "uuid",
+        "state",
+    ]
+    dict_element_visible_keys = [
+        "id",
+        "update_time",
+        "create_time",
+        "workflow_id",
+        "history_id",
+        "uuid",
+        "state",
+    ]
 
     class states(str, Enum):
-        NEW = 'new'  # Brand new workflow invocation... maybe this should be same as READY
-        READY = 'ready'  # Workflow ready for another iteration of scheduling.
-        SCHEDULED = 'scheduled'  # Workflow has been scheduled.
-        CANCELLED = 'cancelled'
-        FAILED = 'failed'
+        NEW = "new"  # Brand new workflow invocation... maybe this should be same as READY
+        READY = "ready"  # Workflow ready for another iteration of scheduling.
+        SCHEDULED = "scheduled"  # Workflow has been scheduled.
+        CANCELLED = "cancelled"
+        FAILED = "failed"
 
     non_terminal_states = [states.NEW, states.READY]
 
     def create_subworkflow_invocation_for_step(self, step):
         assert step.type == "subworkflow"
         subworkflow_invocation = WorkflowInvocation()
         self.attach_subworkflow_invocation_for_step(step, subworkflow_invocation)
         return subworkflow_invocation
 
     def attach_subworkflow_invocation_for_step(self, step, subworkflow_invocation):
         assert step.type == "subworkflow"
         assoc = WorkflowInvocationToSubworkflowInvocationAssociation()
         assoc.workflow_invocation = self
         assoc.workflow_step = step
+        add_object_to_object_session(subworkflow_invocation, self.history)
         subworkflow_invocation.history = self.history
         subworkflow_invocation.workflow = step.subworkflow
         assoc.subworkflow_invocation = subworkflow_invocation
         self.subworkflow_invocations.append(assoc)
         return assoc
 
     def get_subworkflow_invocation_for_step(self, step):
@@ -6852,15 +7598,15 @@
             if subworkflow_invocation.workflow_step == step:
                 assoc = subworkflow_invocation
                 break
         return assoc
 
     @property
     def active(self):
-        """ Indicates the workflow invocation is somehow active - and in
+        """Indicates the workflow invocation is somehow active - and in
         particular valid actions may be performed on its
         WorkflowInvocationSteps.
         """
         states = WorkflowInvocation.states
         return self.state in [states.NEW, states.READY]
 
     def cancel(self):
@@ -6884,15 +7630,15 @@
         step_invocations = {}
         for invocation_step in self.steps:
             step_id = invocation_step.workflow_step_id
             assert step_id not in step_invocations
             step_invocations[step_id] = invocation_step
         return step_invocations
 
-    def step_invocation_for_step_id(self, step_id):
+    def step_invocation_for_step_id(self, step_id: int) -> Optional["WorkflowInvocationStep"]:
         target_invocation_step = None
         for invocation_step in self.steps:
             if step_id == invocation_step.workflow_step_id:
                 target_invocation_step = invocation_step
         return target_invocation_step
 
     def step_invocation_for_label(self, label):
@@ -6902,41 +7648,41 @@
                 target_invocation_step = invocation_step
         return target_invocation_step
 
     @staticmethod
     def poll_unhandled_workflow_ids(sa_session):
         and_conditions = [
             WorkflowInvocation.state == WorkflowInvocation.states.NEW,
-            WorkflowInvocation.handler.is_(None)
+            WorkflowInvocation.handler.is_(None),
         ]
-        query = sa_session.query(
-            WorkflowInvocation.id
-        ).filter(and_(*and_conditions)).order_by(WorkflowInvocation.table.c.id.asc())
+        query = (
+            sa_session.query(WorkflowInvocation.id)
+            .filter(and_(*and_conditions))
+            .order_by(WorkflowInvocation.table.c.id.asc())
+        )
         return [wid for wid in query.all()]
 
     @staticmethod
-    def poll_active_workflow_ids(
-        sa_session,
-        scheduler=None,
-        handler=None
-    ):
+    def poll_active_workflow_ids(sa_session, scheduler=None, handler=None):
         and_conditions = [
             or_(
                 WorkflowInvocation.state == WorkflowInvocation.states.NEW,
-                WorkflowInvocation.state == WorkflowInvocation.states.READY
+                WorkflowInvocation.state == WorkflowInvocation.states.READY,
             ),
         ]
         if scheduler is not None:
             and_conditions.append(WorkflowInvocation.scheduler == scheduler)
         if handler is not None:
             and_conditions.append(WorkflowInvocation.handler == handler)
 
-        query = sa_session.query(
-            WorkflowInvocation.id
-        ).filter(and_(*and_conditions)).order_by(WorkflowInvocation.table.c.id.asc())
+        query = (
+            sa_session.query(WorkflowInvocation.id)
+            .filter(and_(*and_conditions))
+            .order_by(WorkflowInvocation.table.c.id.asc())
+        )
         # Immediately just load all ids into memory so time slicing logic
         # is relatively intutitive.
         return [wid for wid in query.all()]
 
     def add_output(self, workflow_output, step, output_object):
         if not hasattr(output_object, "history_content_type"):
             # assuming this is a simple type, just JSON-ify it and stick in the database. In the future
@@ -6971,17 +7717,21 @@
                 return output_dataset_assoc.dataset
         for output_dataset_collection_assoc in self.output_dataset_collections:
             if output_dataset_collection_assoc.workflow_output.label == label:
                 return output_dataset_collection_assoc.dataset_collection
         # That probably isn't good.
         workflow_output = self.workflow.workflow_output_for(label)
         if workflow_output:
-            raise Exception(f"Failed to find workflow output named [{label}], one was defined but none registered during execution.")
+            raise Exception(
+                f"Failed to find workflow output named [{label}], one was defined but none registered during execution."
+            )
         else:
-            raise Exception(f"Failed to find workflow output named [{label}], workflow doesn't define output by that name - valid names are {self.workflow.workflow_output_labels}.")
+            raise Exception(
+                f"Failed to find workflow output named [{label}], workflow doesn't define output by that name - valid names are {self.workflow.workflow_output_labels}."
+            )
 
     def get_input_object(self, label):
         for input_dataset_assoc in self.input_datasets:
             if input_dataset_assoc.workflow_step.label == label:
                 return input_dataset_assoc.dataset
         for input_dataset_collection_assoc in self.input_dataset_collections:
             if input_dataset_collection_assoc.workflow_step.label == label:
@@ -7002,106 +7752,160 @@
         inputs = []
         for input_dataset_assoc in self.input_datasets:
             inputs.append(input_dataset_assoc)
         for input_dataset_collection_assoc in self.input_dataset_collections:
             inputs.append(input_dataset_collection_assoc)
         return inputs
 
-    def to_dict(self, view='collection', value_mapper=None, step_details=False, legacy_job_state=False):
+    def _serialize(self, id_encoder, serialization_options):
+        invocation_attrs = dict_for(self)
+        invocation_attrs["state"] = self.state
+        invocation_attrs["create_time"] = self.create_time.__str__()
+        invocation_attrs["update_time"] = self.update_time.__str__()
+
+        steps = []
+        for step in self.steps:
+            steps.append(step.serialize(id_encoder, serialization_options))
+        invocation_attrs["steps"] = steps
+
+        input_parameters = []
+        for input_parameter in self.input_parameters:
+            input_parameters.append(input_parameter.serialize(id_encoder, serialization_options))
+        invocation_attrs["input_parameters"] = input_parameters
+
+        step_states = []
+        for step_state in self.step_states:
+            step_states.append(step_state.serialize(id_encoder, serialization_options))
+        invocation_attrs["step_states"] = step_states
+
+        input_step_parameters = []
+        for input_step_parameter in self.input_step_parameters:
+            input_step_parameters.append(input_step_parameter.serialize(id_encoder, serialization_options))
+        invocation_attrs["input_step_parameters"] = input_step_parameters
+
+        input_datasets = []
+        for input_dataset in self.input_datasets:
+            input_datasets.append(input_dataset.serialize(id_encoder, serialization_options))
+        invocation_attrs["input_datasets"] = input_datasets
+
+        input_dataset_collections = []
+        for input_dataset_collection in self.input_dataset_collections:
+            input_dataset_collections.append(input_dataset_collection.serialize(id_encoder, serialization_options))
+        invocation_attrs["input_dataset_collections"] = input_dataset_collections
+
+        output_dataset_collections = []
+        for output_dataset_collection in self.output_dataset_collections:
+            output_dataset_collections.append(output_dataset_collection.serialize(id_encoder, serialization_options))
+        invocation_attrs["output_dataset_collections"] = output_dataset_collections
+
+        output_datasets = []
+        for output_dataset in self.output_datasets:
+            output_datasets.append(output_dataset.serialize(id_encoder, serialization_options))
+        invocation_attrs["output_datasets"] = output_datasets
+
+        output_values = []
+        for output_value in self.output_values:
+            output_values.append(output_value.serialize(id_encoder, serialization_options))
+        invocation_attrs["output_values"] = output_values
+
+        serialization_options.attach_identifier(id_encoder, self, invocation_attrs)
+        return invocation_attrs
+
+    def to_dict(self, view="collection", value_mapper=None, step_details=False, legacy_job_state=False):
         rval = super().to_dict(view=view, value_mapper=value_mapper)
-        if view == 'element':
+        if view == "element":
             steps = []
             for step in self.steps:
                 if step_details:
-                    v = step.to_dict(view='element')
+                    v = step.to_dict(view="element")
                 else:
-                    v = step.to_dict(view='collection')
+                    v = step.to_dict(view="collection")
                 if legacy_job_state:
                     step_jobs = step.jobs
                     if step_jobs:
                         for step_job in step_jobs:
                             v_clone = v.copy()
                             v_clone["state"] = step_job.state
                             v_clone["job_id"] = step_job.id
                             steps.append(v_clone)
                     else:
                         v["state"] = None
                         steps.append(v)
                 else:
                     steps.append(v)
-            rval['steps'] = steps
+            rval["steps"] = steps
 
             inputs = {}
             for input_item_association in self.input_datasets + self.input_dataset_collections:
-                if input_item_association.history_content_type == 'dataset':
-                    src = 'hda'
+                if input_item_association.history_content_type == "dataset":
+                    src = "hda"
                     item = input_item_association.dataset
-                elif input_item_association.history_content_type == 'dataset_collection':
-                    src = 'hdca'
+                elif input_item_association.history_content_type == "dataset_collection":
+                    src = "hdca"
                     item = input_item_association.dataset_collection
                 else:
                     # TODO: LDDAs are not implemented in workflow_request_to_input_dataset table
                     raise Exception(f"Unknown history content type '{input_item_association.history_content_type}'")
                 # Should this maybe also be by label ? Would break backwards compatibility though
                 inputs[str(input_item_association.workflow_step.order_index)] = {
-                    'id': item.id,
-                    'src': src,
-                    'label': input_item_association.workflow_step.label,
-                    'workflow_step_id': input_item_association.workflow_step_id,
+                    "id": item.id,
+                    "src": src,
+                    "label": input_item_association.workflow_step.label,
+                    "workflow_step_id": input_item_association.workflow_step_id,
                 }
 
-            rval['inputs'] = inputs
+            rval["inputs"] = inputs
 
             input_parameters = {}
             for input_step_parameter in self.input_step_parameters:
                 label = input_step_parameter.workflow_step.label
                 if not label:
                     continue
                 input_parameters[label] = {
-                    'parameter_value': input_step_parameter.parameter_value,
-                    'label': label,
-                    'workflow_step_id': input_step_parameter.workflow_step_id,
+                    "parameter_value": input_step_parameter.parameter_value,
+                    "label": label,
+                    "workflow_step_id": input_step_parameter.workflow_step_id,
                 }
-            rval['input_step_parameters'] = input_parameters
+            rval["input_step_parameters"] = input_parameters
 
             outputs = {}
             for output_assoc in self.output_datasets:
                 # TODO: does this work correctly if outputs are mapped over?
                 label = output_assoc.workflow_output.label
                 if not label:
                     continue
 
                 outputs[label] = {
-                    'src': 'hda',
-                    'id': output_assoc.dataset_id,
-                    'workflow_step_id': output_assoc.workflow_step_id,
+                    "src": "hda",
+                    "id": output_assoc.dataset_id,
+                    "workflow_step_id": output_assoc.workflow_step_id,
                 }
 
             output_collections = {}
             for output_assoc in self.output_dataset_collections:
                 label = output_assoc.workflow_output.label
                 if not label:
                     continue
 
                 output_collections[label] = {
-                    'src': 'hdca',
-                    'id': output_assoc.dataset_collection_id,
-                    'workflow_step_id': output_assoc.workflow_step_id,
+                    "src": "hdca",
+                    "id": output_assoc.dataset_collection_id,
+                    "workflow_step_id": output_assoc.workflow_step_id,
                 }
 
-            rval['outputs'] = outputs
-            rval['output_collections'] = output_collections
+            rval["outputs"] = outputs
+            rval["output_collections"] = output_collections
 
             output_values = {}
             for output_param in self.output_values:
                 label = output_param.workflow_output.label
                 if not label:
                     continue
                 output_values[label] = output_param.value
-            rval['output_values'] = output_values
+            rval["output_values"] = output_values
 
         return rval
 
     def add_input(self, content, step_id=None, step=None):
         assert step_id is not None or step is not None
 
         def attach_step(request_to_content):
@@ -7123,14 +7927,27 @@
             self.input_dataset_collections.append(request_to_content)
         else:
             request_to_content = WorkflowRequestInputStepParameter()
             request_to_content.parameter_value = content
             attach_step(request_to_content)
             self.input_step_parameters.append(request_to_content)
 
+    def add_message(self, message: "InvocationMessageUnion"):
+        self.messages.append(
+            WorkflowInvocationMessage(
+                workflow_invocation_id=self.id,
+                **message.dict(
+                    exclude_unset=True,
+                    exclude={
+                        "history_id"
+                    },  # history_id comes in through workflow_invocation and isn't persisted in database
+                ),
+            )
+        )
+
     @property
     def resource_parameters(self):
         resource_type = WorkflowRequestInputParameter.types.RESOURCE_PARAMETERS
         _resource_parameters = {}
         for input_parameter in self.input_parameters:
             if input_parameter.type == resource_type:
                 _resource_parameters[input_parameter.name] = input_parameter.value
@@ -7156,86 +7973,136 @@
             extra += f"id={safe_id}"
         else:
             extra += "unflushed"
         return f"{self.__class__.__name__}[{extra}]"
 
 
 class WorkflowInvocationToSubworkflowInvocationAssociation(Base, Dictifiable, RepresentById):
-    __tablename__ = 'workflow_invocation_to_subworkflow_invocation_association'
+    __tablename__ = "workflow_invocation_to_subworkflow_invocation_association"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_id = Column(Integer,
-        ForeignKey('workflow_invocation.id', name='fk_wfi_swi_wfi'), index=True)
-    subworkflow_invocation_id = Column(Integer,
-        ForeignKey('workflow_invocation.id', name='fk_wfi_swi_swi'), index=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id', name='fk_wfi_swi_ws'))
-
-    subworkflow_invocation = relationship('WorkflowInvocation',
-        primaryjoin=(lambda:
-            WorkflowInvocationToSubworkflowInvocationAssociation.subworkflow_invocation_id
-                == WorkflowInvocation.id),
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id", name="fk_wfi_swi_wfi"), index=True)
+    subworkflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id", name="fk_wfi_swi_swi"), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id", name="fk_wfi_swi_ws"))
+
+    subworkflow_invocation = relationship(
+        "WorkflowInvocation",
+        primaryjoin=(
+            lambda: WorkflowInvocationToSubworkflowInvocationAssociation.subworkflow_invocation_id
+            == WorkflowInvocation.id
+        ),
         uselist=False,
     )
-    workflow_step = relationship('WorkflowStep')
-    parent_workflow_invocation = relationship('WorkflowInvocation',
-        primaryjoin=(lambda:
-            WorkflowInvocationToSubworkflowInvocationAssociation.workflow_invocation_id
-                == WorkflowInvocation.id),
-        back_populates='subworkflow_invocations',
+    workflow_step = relationship("WorkflowStep")
+    parent_workflow_invocation = relationship(
+        "WorkflowInvocation",
+        primaryjoin=(
+            lambda: WorkflowInvocationToSubworkflowInvocationAssociation.workflow_invocation_id == WorkflowInvocation.id
+        ),
+        back_populates="subworkflow_invocations",
         uselist=False,
     )
-    dict_collection_visible_keys = ['id', 'workflow_step_id', 'workflow_invocation_id', 'subworkflow_invocation_id']
-    dict_element_visible_keys = ['id', 'workflow_step_id', 'workflow_invocation_id', 'subworkflow_invocation_id']
+    dict_collection_visible_keys = ["id", "workflow_step_id", "workflow_invocation_id", "subworkflow_invocation_id"]
+    dict_element_visible_keys = ["id", "workflow_step_id", "workflow_invocation_id", "subworkflow_invocation_id"]
+
+
+class WorkflowInvocationMessage(Base, Dictifiable, Serializable):
+    __tablename__ = "workflow_invocation_message"
+    id = Column(Integer, primary_key=True)
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True, nullable=False)
+    reason = Column(String(32))
+    details = Column(TrimmedString(255), nullable=True)
+    output_name = Column(String(255), nullable=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), nullable=True)
+    dependent_workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), nullable=True)
+    job_id = Column(Integer, ForeignKey("job.id"), nullable=True)
+    hda_id = Column(Integer, ForeignKey("history_dataset_association.id"), nullable=True)
+    hdca_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), nullable=True)
+
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="messages", lazy=True)
+    workflow_step = relationship("WorkflowStep", foreign_keys=workflow_step_id, lazy=True)
+    dependent_workflow_step = relationship("WorkflowStep", foreign_keys=dependent_workflow_step_id, lazy=True)
+
+    @property
+    def workflow_step_index(self):
+        return self.workflow_step and self.workflow_step.order_index
+
+    @property
+    def dependent_workflow_step_index(self):
+        return self.dependent_workflow_step and self.dependent_workflow_step.order_index
+
+    @property
+    def history_id(self):
+        return self.workflow_invocation.history_id
 
 
-class WorkflowInvocationStep(Base, Dictifiable, RepresentById):
-    __tablename__ = 'workflow_invocation_step'
+class WorkflowInvocationStep(Base, Dictifiable, Serializable):
+    __tablename__ = "workflow_invocation_step"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    workflow_invocation_id = Column(Integer,
-        ForeignKey('workflow_invocation.id'), index=True, nullable=False)
-    workflow_step_id = Column(Integer,
-        ForeignKey('workflow_step.id'), index=True, nullable=False)
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True, nullable=False)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True, nullable=False)
     state = Column(TrimmedString(64), index=True)
-    job_id = Column(Integer, ForeignKey('job.id'), index=True, nullable=True)
-    implicit_collection_jobs_id = Column(Integer,
-        ForeignKey('implicit_collection_jobs.id'), index=True, nullable=True)
+    job_id = Column(Integer, ForeignKey("job.id"), index=True, nullable=True)
+    implicit_collection_jobs_id = Column(Integer, ForeignKey("implicit_collection_jobs.id"), index=True, nullable=True)
     action = Column(MutableJSONType, nullable=True)
 
-    workflow_step = relationship('WorkflowStep')
-    job = relationship('Job', back_populates='workflow_invocation_step', uselist=False)
-    implicit_collection_jobs = relationship('ImplicitCollectionJobs', uselist=False)
+    workflow_step = relationship("WorkflowStep")
+    job = relationship("Job", back_populates="workflow_invocation_step", uselist=False)
+    implicit_collection_jobs = relationship("ImplicitCollectionJobs", uselist=False)
     output_dataset_collections = relationship(
-        'WorkflowInvocationStepOutputDatasetCollectionAssociation',
-        back_populates='workflow_invocation_step')
-    output_datasets = relationship('WorkflowInvocationStepOutputDatasetAssociation',
-        back_populates='workflow_invocation_step')
-    workflow_invocation = relationship('WorkflowInvocation', back_populates='steps')
-    output_value = relationship('WorkflowInvocationOutputValue',
-        foreign_keys='[WorkflowInvocationStep.workflow_invocation_id, WorkflowInvocationStep.workflow_step_id]',
-        primaryjoin=(lambda: and_(
-            WorkflowInvocationStep.workflow_invocation_id
-            == WorkflowInvocationOutputValue.workflow_invocation_id,
-            WorkflowInvocationStep.workflow_step_id == WorkflowInvocationOutputValue.workflow_step_id,
-        )),
-        back_populates='workflow_invocation_step',
-        viewonly=True
+        "WorkflowInvocationStepOutputDatasetCollectionAssociation", back_populates="workflow_invocation_step"
+    )
+    output_datasets = relationship(
+        "WorkflowInvocationStepOutputDatasetAssociation", back_populates="workflow_invocation_step"
+    )
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="steps")
+    output_value = relationship(
+        "WorkflowInvocationOutputValue",
+        foreign_keys="[WorkflowInvocationStep.workflow_invocation_id, WorkflowInvocationStep.workflow_step_id]",
+        primaryjoin=(
+            lambda: and_(
+                WorkflowInvocationStep.workflow_invocation_id == WorkflowInvocationOutputValue.workflow_invocation_id,
+                WorkflowInvocationStep.workflow_step_id == WorkflowInvocationOutputValue.workflow_step_id,
+            )
+        ),
+        back_populates="workflow_invocation_step",
+        viewonly=True,
+    )
+    order_index = column_property(
+        select([WorkflowStep.order_index]).where(WorkflowStep.id == workflow_step_id).scalar_subquery()
     )
 
     subworkflow_invocation_id: column_property
 
-    dict_collection_visible_keys = ['id', 'update_time', 'job_id', 'workflow_step_id', 'subworkflow_invocation_id', 'state', 'action']
-    dict_element_visible_keys = ['id', 'update_time', 'job_id', 'workflow_step_id', 'subworkflow_invocation_id', 'state', 'action']
+    dict_collection_visible_keys = [
+        "id",
+        "update_time",
+        "job_id",
+        "workflow_step_id",
+        "subworkflow_invocation_id",
+        "state",
+        "action",
+    ]
+    dict_element_visible_keys = [
+        "id",
+        "update_time",
+        "job_id",
+        "workflow_step_id",
+        "subworkflow_invocation_id",
+        "state",
+        "action",
+    ]
 
     class states(str, Enum):
-        NEW = 'new'  # Brand new workflow invocation step
-        READY = 'ready'  # Workflow invocation step ready for another iteration of scheduling.
-        SCHEDULED = 'scheduled'  # Workflow invocation step has been scheduled.
+        NEW = "new"  # Brand new workflow invocation step
+        READY = "ready"  # Workflow invocation step ready for another iteration of scheduling.
+        SCHEDULED = "scheduled"  # Workflow invocation step has been scheduled.
         # CANCELLED = 'cancelled',  TODO: implement and expose
         # FAILED = 'failed',  TODO: implement and expose
 
     @property
     def is_new(self):
         return self.state == self.states.NEW
 
@@ -7260,443 +8127,589 @@
         if self.job:
             return [self.job]
         elif self.implicit_collection_jobs:
             return self.implicit_collection_jobs.job_list
         else:
             return []
 
-    def to_dict(self, view='collection', value_mapper=None):
+    def _serialize(self, id_encoder, serialization_options):
+        step_attrs = dict_for(self)
+        step_attrs["state"] = self.state
+        step_attrs["create_time"] = self.create_time.__str__()
+        step_attrs["update_time"] = self.update_time.__str__()
+        step_attrs["order_index"] = self.workflow_step.order_index
+        step_attrs["action"] = self.action
+        if self.job:
+            step_attrs["job"] = self.job.serialize(id_encoder, serialization_options, for_link=True)
+        elif self.implicit_collection_jobs:
+            step_attrs["implicit_collection_jobs"] = self.implicit_collection_jobs.serialize(
+                id_encoder, serialization_options, for_link=True
+            )
+
+        outputs = []
+        for output_dataset_assoc in self.output_datasets:
+            output = dict(
+                output_name=output_dataset_assoc.output_name,
+            )
+            dataset = output_dataset_assoc.dataset
+            if dataset:
+                output["dataset"] = dataset.serialize(id_encoder, serialization_options, for_link=True)
+            outputs.append(output)
+        step_attrs["outputs"] = outputs
+
+        output_collections = []
+        for output_dataset_collection_assoc in self.output_dataset_collections:
+            output_collection = dict(
+                output_name=output_dataset_collection_assoc.output_name,
+            )
+            dataset_collection = output_dataset_collection_assoc.dataset_collection
+            if dataset_collection:
+                output_collection["dataset_collection"] = dataset_collection.serialize(
+                    id_encoder, serialization_options, for_link=True
+                )
+            output_collections.append(output_collection)
+        step_attrs["output_collections"] = output_collections
+
+        return step_attrs
+
+    def to_dict(self, view="collection", value_mapper=None):
         rval = super().to_dict(view=view, value_mapper=value_mapper)
-        rval['order_index'] = self.workflow_step.order_index
-        rval['workflow_step_label'] = self.workflow_step.label
-        rval['workflow_step_uuid'] = str(self.workflow_step.uuid)
+        rval["order_index"] = self.workflow_step.order_index
+        rval["workflow_step_label"] = self.workflow_step.label
+        rval["workflow_step_uuid"] = str(self.workflow_step.uuid)
         # Following no longer makes sense...
         # rval['state'] = self.job.state if self.job is not None else None
-        if view == 'element':
+        if view == "element":
             jobs = []
             for job in self.jobs:
                 jobs.append(job.to_dict())
 
             outputs = {}
             for output_assoc in self.output_datasets:
                 name = output_assoc.output_name
                 outputs[name] = {
-                    'src': 'hda',
-                    'id': output_assoc.dataset.id,
-                    'uuid': str(output_assoc.dataset.dataset.uuid) if output_assoc.dataset.dataset.uuid is not None else None
+                    "src": "hda",
+                    "id": output_assoc.dataset.id,
+                    "uuid": str(output_assoc.dataset.dataset.uuid)
+                    if output_assoc.dataset.dataset.uuid is not None
+                    else None,
                 }
 
             output_collections = {}
             for output_assoc in self.output_dataset_collections:
                 name = output_assoc.output_name
                 output_collections[name] = {
-                    'src': 'hdca',
-                    'id': output_assoc.dataset_collection.id,
+                    "src": "hdca",
+                    "id": output_assoc.dataset_collection.id,
                 }
 
-            rval['outputs'] = outputs
-            rval['output_collections'] = output_collections
-            rval['jobs'] = jobs
+            rval["outputs"] = outputs
+            rval["output_collections"] = output_collections
+            rval["jobs"] = jobs
         return rval
 
 
-class WorkflowRequestInputParameter(Base, Dictifiable, RepresentById):
-    """ Workflow-related parameters not tied to steps or inputs.
-    """
-    __tablename__ = 'workflow_request_input_parameters'
+class WorkflowRequestInputParameter(Base, Dictifiable, Serializable):
+    """Workflow-related parameters not tied to steps or inputs."""
+
+    __tablename__ = "workflow_request_input_parameters"
 
     id = Column(Integer, primary_key=True)
     workflow_invocation_id = Column(
-        Integer, ForeignKey('workflow_invocation.id', onupdate='CASCADE', ondelete='CASCADE'))
+        Integer, ForeignKey("workflow_invocation.id", onupdate="CASCADE", ondelete="CASCADE"), index=True
+    )
     name = Column(Unicode(255))
     value = Column(TEXT)
     type = Column(Unicode(255))
-    workflow_invocation = relationship('WorkflowInvocation', back_populates='input_parameters')
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="input_parameters")
 
-    dict_collection_visible_keys = ['id', 'name', 'value', 'type']
+    dict_collection_visible_keys = ["id", "name", "value", "type"]
 
     class types(str, Enum):
-        REPLACEMENT_PARAMETERS = 'replacements'
-        STEP_PARAMETERS = 'step'
-        META_PARAMETERS = 'meta'
-        RESOURCE_PARAMETERS = 'resource'
+        REPLACEMENT_PARAMETERS = "replacements"
+        STEP_PARAMETERS = "step"
+        META_PARAMETERS = "meta"
+        RESOURCE_PARAMETERS = "resource"
 
+    def _serialize(self, id_encoder, serialization_options):
+        request_input_parameter_attrs = dict_for(self)
+        request_input_parameter_attrs["name"] = self.name
+        request_input_parameter_attrs["value"] = self.value
+        request_input_parameter_attrs["type"] = self.type
+        return request_input_parameter_attrs
 
-class WorkflowRequestStepState(Base, Dictifiable, RepresentById):
-    """ Workflow step value parameters.
-    """
-    __tablename__ = 'workflow_request_step_states'
+
+class WorkflowRequestStepState(Base, Dictifiable, Serializable):
+    """Workflow step value parameters."""
+
+    __tablename__ = "workflow_request_step_states"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_id = Column(Integer,
-        ForeignKey('workflow_invocation.id', onupdate='CASCADE', ondelete='CASCADE'))
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'))
+    workflow_invocation_id = Column(
+        Integer, ForeignKey("workflow_invocation.id", onupdate="CASCADE", ondelete="CASCADE"), index=True
+    )
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"))
     value = Column(MutableJSONType)
-    workflow_step = relationship('WorkflowStep')
-    workflow_invocation = relationship('WorkflowInvocation', back_populates='step_states')
+    workflow_step = relationship("WorkflowStep")
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="step_states")
+
+    dict_collection_visible_keys = ["id", "name", "value", "workflow_step_id"]
 
-    dict_collection_visible_keys = ['id', 'name', 'value', 'workflow_step_id']
+    def _serialize(self, id_encoder, serialization_options):
+        request_step_state = dict_for(self)
+        request_step_state["value"] = self.value
+        request_step_state["order_index"] = self.workflow_step.order_index
+        return request_step_state
 
 
-class WorkflowRequestToInputDatasetAssociation(Base, Dictifiable, RepresentById):
-    """ Workflow step input dataset parameters.
-    """
-    __tablename__ = 'workflow_request_to_input_dataset'
+class WorkflowRequestToInputDatasetAssociation(Base, Dictifiable, Serializable):
+    """Workflow step input dataset parameters."""
+
+    __tablename__ = "workflow_request_to_input_dataset"
 
     id = Column(Integer, primary_key=True)
     name = Column(String(255))
     workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True)
     workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"))
     dataset_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
 
-    workflow_step = relationship('WorkflowStep')
-    dataset = relationship('HistoryDatasetAssociation')
-    workflow_invocation = relationship('WorkflowInvocation', back_populates='input_datasets')
+    workflow_step = relationship("WorkflowStep")
+    dataset = relationship("HistoryDatasetAssociation")
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="input_datasets")
 
     history_content_type = "dataset"
-    dict_collection_visible_keys = ['id', 'workflow_invocation_id', 'workflow_step_id', 'dataset_id', 'name']
+    dict_collection_visible_keys = ["id", "workflow_invocation_id", "workflow_step_id", "dataset_id", "name"]
 
+    def _serialize(self, id_encoder, serialization_options):
+        request_input_dataset_attrs = dict_for(self)
+        request_input_dataset_attrs["name"] = self.name
+        request_input_dataset_attrs["dataset"] = self.dataset.serialize(
+            id_encoder, serialization_options, for_link=True
+        )
+        request_input_dataset_attrs["order_index"] = self.workflow_step.order_index
+        return request_input_dataset_attrs
 
-class WorkflowRequestToInputDatasetCollectionAssociation(Base, Dictifiable, RepresentById):
-    """ Workflow step input dataset collection parameters.
-    """
-    __tablename__ = 'workflow_request_to_input_collection_dataset'
+
+class WorkflowRequestToInputDatasetCollectionAssociation(Base, Dictifiable, Serializable):
+    """Workflow step input dataset collection parameters."""
+
+    __tablename__ = "workflow_request_to_input_collection_dataset"
 
     id = Column(Integer, primary_key=True)
     name = Column(String(255))
-    workflow_invocation_id = Column(Integer, ForeignKey('workflow_invocation.id'), index=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'))
-    dataset_collection_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id'), index=True)
-    workflow_step = relationship('WorkflowStep')
-    dataset_collection = relationship('HistoryDatasetCollectionAssociation')
-    workflow_invocation = relationship('WorkflowInvocation',
-        back_populates='input_dataset_collections')
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"))
+    dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
+    workflow_step = relationship("WorkflowStep")
+    dataset_collection = relationship("HistoryDatasetCollectionAssociation")
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="input_dataset_collections")
 
     history_content_type = "dataset_collection"
-    dict_collection_visible_keys = ['id', 'workflow_invocation_id', 'workflow_step_id', 'dataset_collection_id', 'name']
+    dict_collection_visible_keys = ["id", "workflow_invocation_id", "workflow_step_id", "dataset_collection_id", "name"]
+
+    def _serialize(self, id_encoder, serialization_options):
+        request_input_collection_attrs = dict_for(self)
+        request_input_collection_attrs["name"] = self.name
+        request_input_collection_attrs["dataset_collection"] = self.dataset_collection.serialize(
+            id_encoder, serialization_options, for_link=True
+        )
+        request_input_collection_attrs["order_index"] = self.workflow_step.order_index
+        return request_input_collection_attrs
 
 
-class WorkflowRequestInputStepParameter(Base, Dictifiable, RepresentById):
-    """ Workflow step parameter inputs.
-    """
-    __tablename__ = 'workflow_request_input_step_parameter'
+class WorkflowRequestInputStepParameter(Base, Dictifiable, Serializable):
+    """Workflow step parameter inputs."""
+
+    __tablename__ = "workflow_request_input_step_parameter"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_id = Column(Integer, ForeignKey('workflow_invocation.id'), index=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'))
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"))
     parameter_value = Column(MutableJSONType)
 
-    workflow_step = relationship('WorkflowStep')
-    workflow_invocation = relationship('WorkflowInvocation', back_populates='input_step_parameters')
+    workflow_step = relationship("WorkflowStep")
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="input_step_parameters")
 
-    dict_collection_visible_keys = ['id', 'workflow_invocation_id', 'workflow_step_id', 'parameter_value']
+    dict_collection_visible_keys = ["id", "workflow_invocation_id", "workflow_step_id", "parameter_value"]
+
+    def _serialize(self, id_encoder, serialization_options):
+        request_input_step_parameter_attrs = dict_for(self)
+        request_input_step_parameter_attrs["parameter_value"] = self.parameter_value
+        request_input_step_parameter_attrs["order_index"] = self.workflow_step.order_index
+        return request_input_step_parameter_attrs
 
 
-class WorkflowInvocationOutputDatasetAssociation(Base, Dictifiable, RepresentById):
+class WorkflowInvocationOutputDatasetAssociation(Base, Dictifiable, Serializable):
     """Represents links to output datasets for the workflow."""
-    __tablename__ = 'workflow_invocation_output_dataset_association'
+
+    __tablename__ = "workflow_invocation_output_dataset_association"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_id = Column(Integer, ForeignKey('workflow_invocation.id'), index=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True)
-    dataset_id = Column(Integer, ForeignKey('history_dataset_association.id'), index=True)
-    workflow_output_id = Column(Integer, ForeignKey('workflow_output.id'), index=True)
-
-    workflow_invocation = relationship('WorkflowInvocation', back_populates='output_datasets')
-    workflow_step = relationship('WorkflowStep')
-    dataset = relationship('HistoryDatasetAssociation')
-    workflow_output = relationship('WorkflowOutput')
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True)
+    dataset_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    workflow_output_id = Column(Integer, ForeignKey("workflow_output.id"), index=True)
+
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="output_datasets")
+    workflow_step = relationship("WorkflowStep")
+    dataset = relationship("HistoryDatasetAssociation")
+    workflow_output = relationship("WorkflowOutput")
 
     history_content_type = "dataset"
-    dict_collection_visible_keys = ['id', 'workflow_invocation_id', 'workflow_step_id', 'dataset_id', 'name']
+    dict_collection_visible_keys = ["id", "workflow_invocation_id", "workflow_step_id", "dataset_id", "name"]
+
+    def _serialize(self, id_encoder, serialization_options):
+        output_dataset_attrs = dict_for(self)
+        output_dataset_attrs["dataset"] = self.dataset.serialize(id_encoder, serialization_options, for_link=True)
+        output_dataset_attrs["order_index"] = self.workflow_step.order_index
+        output_dataset_attrs["workflow_output"] = self.workflow_output.serialize(id_encoder, serialization_options)
+        return output_dataset_attrs
 
 
-class WorkflowInvocationOutputDatasetCollectionAssociation(Base, Dictifiable, RepresentById):
+class WorkflowInvocationOutputDatasetCollectionAssociation(Base, Dictifiable, Serializable):
     """Represents links to output dataset collections for the workflow."""
-    __tablename__ = 'workflow_invocation_output_dataset_collection_association'
+
+    __tablename__ = "workflow_invocation_output_dataset_collection_association"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_id = Column(Integer,
-        ForeignKey('workflow_invocation.id', name='fk_wiodca_wii'), index=True)
-    workflow_step_id = Column(Integer,
-        ForeignKey('workflow_step.id', name='fk_wiodca_wsi'), index=True)
-    dataset_collection_id = Column(Integer,
-        ForeignKey('history_dataset_collection_association.id', name='fk_wiodca_dci'), index=True)
-    workflow_output_id = Column(Integer,
-        ForeignKey('workflow_output.id', name='fk_wiodca_woi'), index=True)
-
-    workflow_invocation = relationship('WorkflowInvocation',
-        back_populates='output_dataset_collections')
-    workflow_step = relationship('WorkflowStep')
-    dataset_collection = relationship('HistoryDatasetCollectionAssociation')
-    workflow_output = relationship('WorkflowOutput')
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id", name="fk_wiodca_wii"), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id", name="fk_wiodca_wsi"), index=True)
+    dataset_collection_id = Column(
+        Integer, ForeignKey("history_dataset_collection_association.id", name="fk_wiodca_dci"), index=True
+    )
+    workflow_output_id = Column(Integer, ForeignKey("workflow_output.id", name="fk_wiodca_woi"), index=True)
+
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="output_dataset_collections")
+    workflow_step = relationship("WorkflowStep")
+    dataset_collection = relationship("HistoryDatasetCollectionAssociation")
+    workflow_output = relationship("WorkflowOutput")
 
     history_content_type = "dataset_collection"
-    dict_collection_visible_keys = ['id', 'workflow_invocation_id', 'workflow_step_id', 'dataset_collection_id', 'name']
+    dict_collection_visible_keys = ["id", "workflow_invocation_id", "workflow_step_id", "dataset_collection_id", "name"]
+
+    def _serialize(self, id_encoder, serialization_options):
+        output_collection_attrs = dict_for(self)
+        output_collection_attrs["dataset_collection"] = self.dataset_collection.serialize(
+            id_encoder, serialization_options, for_link=True
+        )
+        output_collection_attrs["order_index"] = self.workflow_step.order_index
+        output_collection_attrs["workflow_output"] = self.workflow_output.serialize(id_encoder, serialization_options)
+        return output_collection_attrs
 
 
-class WorkflowInvocationOutputValue(Base, Dictifiable, RepresentById):
+class WorkflowInvocationOutputValue(Base, Dictifiable, Serializable):
     """Represents a link to a specified or computed workflow parameter."""
-    __tablename__ = 'workflow_invocation_output_value'
+
+    __tablename__ = "workflow_invocation_output_value"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_id = Column(Integer, ForeignKey('workflow_invocation.id'), index=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'))
-    workflow_output_id = Column(Integer, ForeignKey('workflow_output.id'), index=True)
+    workflow_invocation_id = Column(Integer, ForeignKey("workflow_invocation.id"), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"))
+    workflow_output_id = Column(Integer, ForeignKey("workflow_output.id"), index=True)
     value = Column(MutableJSONType)
 
-    workflow_invocation = relationship('WorkflowInvocation', back_populates="output_values")
+    workflow_invocation = relationship("WorkflowInvocation", back_populates="output_values")
 
-    workflow_invocation_step = relationship('WorkflowInvocationStep',
-        foreign_keys='[WorkflowInvocationStep.workflow_invocation_id, WorkflowInvocationStep.workflow_step_id]',
-        primaryjoin=(lambda: and_(
-            WorkflowInvocationStep.workflow_invocation_id == WorkflowInvocationOutputValue.workflow_invocation_id,
-            WorkflowInvocationStep.workflow_step_id == WorkflowInvocationOutputValue.workflow_step_id,
-        )),
-        back_populates='output_value',
-        viewonly=True
+    workflow_invocation_step = relationship(
+        "WorkflowInvocationStep",
+        foreign_keys="[WorkflowInvocationStep.workflow_invocation_id, WorkflowInvocationStep.workflow_step_id]",
+        primaryjoin=(
+            lambda: and_(
+                WorkflowInvocationStep.workflow_invocation_id == WorkflowInvocationOutputValue.workflow_invocation_id,
+                WorkflowInvocationStep.workflow_step_id == WorkflowInvocationOutputValue.workflow_step_id,
+            )
+        ),
+        back_populates="output_value",
+        viewonly=True,
     )
 
-    workflow_step = relationship('WorkflowStep')
-    workflow_output = relationship('WorkflowOutput')
+    workflow_step = relationship("WorkflowStep")
+    workflow_output = relationship("WorkflowOutput")
 
-    dict_collection_visible_keys = ['id', 'workflow_invocation_id', 'workflow_step_id', 'value']
+    dict_collection_visible_keys = ["id", "workflow_invocation_id", "workflow_step_id", "value"]
+
+    def _serialize(self, id_encoder, serialization_options):
+        output_value_attrs = dict_for(self)
+        output_value_attrs["value"] = self.value
+        output_value_attrs["order_index"] = self.workflow_step.order_index
+        output_value_attrs["workflow_output"] = self.workflow_output.serialize(id_encoder, serialization_options)
+        return output_value_attrs
 
 
 class WorkflowInvocationStepOutputDatasetAssociation(Base, Dictifiable, RepresentById):
     """Represents links to output datasets for the workflow."""
-    __tablename__ = 'workflow_invocation_step_output_dataset_association'
+
+    __tablename__ = "workflow_invocation_step_output_dataset_association"
 
     id = Column(Integer, primary_key=True)
-    workflow_invocation_step_id = Column(Integer,
-        ForeignKey('workflow_invocation_step.id'), index=True)
-    dataset_id = Column(Integer, ForeignKey('history_dataset_association.id'), index=True)
+    workflow_invocation_step_id = Column(Integer, ForeignKey("workflow_invocation_step.id"), index=True)
+    dataset_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
     output_name = Column(String(255), nullable=True)
-    workflow_invocation_step = relationship('WorkflowInvocationStep',
-        back_populates='output_datasets')
-    dataset = relationship('HistoryDatasetAssociation')
+    workflow_invocation_step = relationship("WorkflowInvocationStep", back_populates="output_datasets")
+    dataset = relationship("HistoryDatasetAssociation")
 
-    dict_collection_visible_keys = ['id', 'workflow_invocation_step_id', 'dataset_id', 'output_name']
+    dict_collection_visible_keys = ["id", "workflow_invocation_step_id", "dataset_id", "output_name"]
 
 
 class WorkflowInvocationStepOutputDatasetCollectionAssociation(Base, Dictifiable, RepresentById):
     """Represents links to output dataset collections for the workflow."""
-    __tablename__ = 'workflow_invocation_step_output_dataset_collection_association'
+
+    __tablename__ = "workflow_invocation_step_output_dataset_collection_association"
 
     id = Column(Integer, primary_key=True)
     workflow_invocation_step_id = Column(
-        Integer, ForeignKey('workflow_invocation_step.id', name='fk_wisodca_wisi'), index=True)
-    workflow_step_id = Column(
-        Integer, ForeignKey('workflow_step.id', name='fk_wisodca_wsi'), index=True)
+        Integer, ForeignKey("workflow_invocation_step.id", name="fk_wisodca_wisi"), index=True
+    )
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id", name="fk_wisodca_wsi"), index=True)
     dataset_collection_id = Column(
-        Integer,
-        ForeignKey('history_dataset_collection_association.id', name='fk_wisodca_dci'), index=True)
+        Integer, ForeignKey("history_dataset_collection_association.id", name="fk_wisodca_dci"), index=True
+    )
     output_name = Column(String(255), nullable=True)
 
-    workflow_invocation_step = relationship('WorkflowInvocationStep',
-        back_populates='output_dataset_collections')
-    dataset_collection = relationship('HistoryDatasetCollectionAssociation')
+    workflow_invocation_step = relationship("WorkflowInvocationStep", back_populates="output_dataset_collections")
+    dataset_collection = relationship("HistoryDatasetCollectionAssociation")
 
-    dict_collection_visible_keys = ['id', 'workflow_invocation_step_id', 'dataset_collection_id', 'output_name']
+    dict_collection_visible_keys = ["id", "workflow_invocation_step_id", "dataset_collection_id", "output_name"]
 
 
 class MetadataFile(Base, StorableObject, Serializable):
-    __tablename__ = 'metadata_file'
+    __tablename__ = "metadata_file"
 
     id = Column(Integer, primary_key=True)
     name = Column(TEXT)
-    hda_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True, nullable=True)
-    lda_id = Column(Integer,
-        ForeignKey('library_dataset_dataset_association.id'), index=True, nullable=True)
+    hda_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True, nullable=True)
+    lda_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True, nullable=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, index=True, default=now, onupdate=now)
     object_store_id = Column(TrimmedString(255), index=True)
     uuid = Column(UUIDType(), index=True)
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
 
-    history_dataset = relationship('HistoryDatasetAssociation')
-    library_dataset = relationship('LibraryDatasetDatasetAssociation')
+    history_dataset = relationship("HistoryDatasetAssociation")
+    library_dataset = relationship("LibraryDatasetDatasetAssociation")
 
     def __init__(self, dataset=None, name=None, uuid=None):
         self.uuid = get_uuid(uuid)
         if isinstance(dataset, HistoryDatasetAssociation):
             self.history_dataset = dataset
         elif isinstance(dataset, LibraryDatasetDatasetAssociation):
             self.library_dataset = dataset
         self.name = name
 
     @property
+    def dataset(self) -> Optional[Dataset]:
+        da = self.history_dataset or self.library_dataset
+        return da and da.dataset
+
+    def update_from_file(self, file_name):
+        if not self.dataset:
+            raise Exception("Attempted to write MetadataFile, but no DatasetAssociation set")
+        self.dataset.object_store.update_from_file(
+            self,
+            file_name=file_name,
+            extra_dir="_metadata_files",
+            extra_dir_at_root=True,
+            alt_name=os.path.basename(self.file_name),
+        )
+
+    @property
     def file_name(self):
         # Ensure the directory structure and the metadata file object exist
         try:
             da = self.history_dataset or self.library_dataset
             if self.object_store_id is None and da is not None:
                 self.object_store_id = da.dataset.object_store_id
             object_store = da.dataset.object_store
             store_by = object_store.get_store_by(da.dataset)
-            if store_by == 'id' and self.id is None:
+            if store_by == "id" and self.id is None:
                 self.flush()
             identifier = getattr(self, store_by)
             alt_name = f"metadata_{identifier}.dat"
-            if not object_store.exists(self, extra_dir='_metadata_files', extra_dir_at_root=True, alt_name=alt_name):
-                object_store.create(self, extra_dir='_metadata_files', extra_dir_at_root=True, alt_name=alt_name)
-            path = object_store.get_filename(self, extra_dir='_metadata_files', extra_dir_at_root=True, alt_name=alt_name)
+            if not object_store.exists(self, extra_dir="_metadata_files", extra_dir_at_root=True, alt_name=alt_name):
+                object_store.create(self, extra_dir="_metadata_files", extra_dir_at_root=True, alt_name=alt_name)
+            path = object_store.get_filename(
+                self, extra_dir="_metadata_files", extra_dir_at_root=True, alt_name=alt_name
+            )
             return path
         except AttributeError:
-            assert self.id is not None, "ID must be set before MetadataFile used without an HDA/LDDA (commit the object)"
+            assert (
+                self.id is not None
+            ), "ID must be set before MetadataFile used without an HDA/LDDA (commit the object)"
             # In case we're not working with the history_dataset
-            path = os.path.join(Dataset.file_path, '_metadata_files', *directory_hash_id(self.id))
+            path = os.path.join(Dataset.file_path, "_metadata_files", *directory_hash_id(self.id))
             # Create directory if it does not exist
             try:
                 os.makedirs(path)
             except OSError as e:
                 # File Exists is okay, otherwise reraise
                 if e.errno != errno.EEXIST:
                     raise
             # Return filename inside hashed directory
             return os.path.abspath(os.path.join(path, "metadata_%d.dat" % self.id))
 
     def _serialize(self, id_encoder, serialization_options):
         as_dict = dict_for(self)
         serialization_options.attach_identifier(id_encoder, self, as_dict)
-        as_dict["uuid"] = str(self.uuid or '') or None
+        as_dict["uuid"] = str(self.uuid or "") or None
         return as_dict
 
 
 class FormDefinition(Base, Dictifiable, RepresentById):
-    __tablename__ = 'form_definition'
+    __tablename__ = "form_definition"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     name = Column(TrimmedString(255), nullable=False)
     desc = Column(TEXT)
-    form_definition_current_id = Column(Integer,
-        ForeignKey('form_definition_current.id', use_alter=True), index=True, nullable=False)
+    form_definition_current_id = Column(
+        Integer, ForeignKey("form_definition_current.id", use_alter=True), index=True, nullable=False
+    )
     fields = Column(MutableJSONType)
     type = Column(TrimmedString(255), index=True)
     layout = Column(MutableJSONType)
     form_definition_current = relationship(
-        'FormDefinitionCurrent',
-        back_populates='forms',
-        primaryjoin=(lambda: FormDefinitionCurrent.id == FormDefinition.form_definition_current_id))  # type: ignore[has-type]
+        "FormDefinitionCurrent",
+        back_populates="forms",
+        primaryjoin=(lambda: FormDefinitionCurrent.id == FormDefinition.form_definition_current_id),  # type: ignore[has-type]
+    )
 
     # The following form_builder classes are supported by the FormDefinition class.
-    supported_field_types = [AddressField, CheckboxField, PasswordField, SelectField, TextArea, TextField, WorkflowField, WorkflowMappingField, HistoryField]
+    supported_field_types = [
+        AddressField,
+        CheckboxField,
+        PasswordField,
+        SelectField,
+        TextArea,
+        TextField,
+        WorkflowField,
+        WorkflowMappingField,
+        HistoryField,
+    ]
 
     class types(str, Enum):
-        USER_INFO = 'User Information'
+        USER_INFO = "User Information"
 
-    dict_collection_visible_keys = ['id', 'name']
-    dict_element_visible_keys = ['id', 'name', 'desc', 'form_definition_current_id', 'fields', 'layout']
+    dict_collection_visible_keys = ["id", "name"]
+    dict_element_visible_keys = ["id", "name", "desc", "form_definition_current_id", "fields", "layout"]
 
     def to_dict(self, user=None, values=None, security=None):
         values = values or {}
-        form_def = {'id': security.encode_id(self.id) if security else self.id, 'name': self.name, 'inputs': []}
+        form_def = {"id": security.encode_id(self.id) if security else self.id, "name": self.name, "inputs": []}
         for field in self.fields:
-            FieldClass = ({'AddressField': AddressField,
-                           'CheckboxField': CheckboxField,
-                           'HistoryField': HistoryField,
-                           'PasswordField': PasswordField,
-                           'SelectField': SelectField,
-                           'TextArea': TextArea,
-                           'TextField': TextField,
-                           'WorkflowField': WorkflowField}).get(field['type'], TextField)
-            form_def['inputs'].append(FieldClass(user=user, value=values.get(field['name'], field['default']), security=security, **field).to_dict())
+            FieldClass = (
+                {
+                    "AddressField": AddressField,
+                    "CheckboxField": CheckboxField,
+                    "HistoryField": HistoryField,
+                    "PasswordField": PasswordField,
+                    "SelectField": SelectField,
+                    "TextArea": TextArea,
+                    "TextField": TextField,
+                    "WorkflowField": WorkflowField,
+                }
+            ).get(field["type"], TextField)
+            form_def["inputs"].append(
+                FieldClass(
+                    user=user, value=values.get(field["name"], field.get("default")), security=security, **field
+                ).to_dict()
+            )
         return form_def
 
     def grid_fields(self, grid_index):
         # Returns a dictionary whose keys are integers corresponding to field positions
         # on the grid and whose values are the field.
         gridfields = {}
         for i, f in enumerate(self.fields):
-            if str(f['layout']) == str(grid_index):
+            if str(f["layout"]) == str(grid_index):
                 gridfields[i] = f
         return gridfields
 
 
 class FormDefinitionCurrent(Base, RepresentById):
-    __tablename__ = 'form_definition_current'
+    __tablename__ = "form_definition_current"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    latest_form_id = Column(Integer, ForeignKey('form_definition.id'), index=True)
+    latest_form_id = Column(Integer, ForeignKey("form_definition.id"), index=True)
     deleted = Column(Boolean, index=True, default=False)
     forms = relationship(
-        'FormDefinition',
-        back_populates='form_definition_current',
-        cascade='all, delete-orphan',
-        primaryjoin=(lambda: FormDefinitionCurrent.id == FormDefinition.form_definition_current_id))
+        "FormDefinition",
+        back_populates="form_definition_current",
+        cascade="all, delete-orphan",
+        primaryjoin=(lambda: FormDefinitionCurrent.id == FormDefinition.form_definition_current_id),
+    )
     latest_form = relationship(
-        'FormDefinition',
+        "FormDefinition",
         post_update=True,
-        primaryjoin=(lambda: FormDefinitionCurrent.latest_form_id == FormDefinition.id))
+        primaryjoin=(lambda: FormDefinitionCurrent.latest_form_id == FormDefinition.id),
+    )
 
     def __init__(self, form_definition=None):
         self.latest_form = form_definition
 
 
 class FormValues(Base, RepresentById):
-    __tablename__ = 'form_values'
+    __tablename__ = "form_values"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    form_definition_id = Column(Integer, ForeignKey('form_definition.id'), index=True)
+    form_definition_id = Column(Integer, ForeignKey("form_definition.id"), index=True)
     content = Column(MutableJSONType)
     form_definition = relationship(
-        'FormDefinition',
-        primaryjoin=(lambda: FormValues.form_definition_id == FormDefinition.id))
+        "FormDefinition", primaryjoin=(lambda: FormValues.form_definition_id == FormDefinition.id)
+    )
 
     def __init__(self, form_def=None, content=None):
         self.form_definition = form_def
         self.content = content
 
 
 class UserAddress(Base, RepresentById):
-    __tablename__ = 'user_address'
+    __tablename__ = "user_address"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     desc = Column(TrimmedString(255))
     name = Column(TrimmedString(255), nullable=False)
     institution = Column(TrimmedString(255))
     address = Column(TrimmedString(255), nullable=False)
     city = Column(TrimmedString(255), nullable=False)
     state = Column(TrimmedString(255), nullable=False)
     postal_code = Column(TrimmedString(255), nullable=False)
     country = Column(TrimmedString(255), nullable=False)
     phone = Column(TrimmedString(255))
     deleted = Column(Boolean, index=True, default=False)
     purged = Column(Boolean, index=True, default=False)
     # `desc` needs to be fully qualified because it is shadowed by `desc` Column defined above
     # TODO: db migration to rename column, then use `desc`
-    user = relationship('User', back_populates='addresses', order_by=sqlalchemy.desc('update_time'))
+    user = relationship("User", back_populates="addresses", order_by=sqlalchemy.desc("update_time"))
 
     def to_dict(self, trans):
-        return {'id': trans.security.encode_id(self.id),
-                'name': sanitize_html(self.name),
-                'desc': sanitize_html(self.desc),
-                'institution': sanitize_html(self.institution),
-                'address': sanitize_html(self.address),
-                'city': sanitize_html(self.city),
-                'state': sanitize_html(self.state),
-                'postal_code': sanitize_html(self.postal_code),
-                'country': sanitize_html(self.country),
-                'phone': sanitize_html(self.phone)}
+        return {
+            "id": trans.security.encode_id(self.id),
+            "name": sanitize_html(self.name),
+            "desc": sanitize_html(self.desc),
+            "institution": sanitize_html(self.institution),
+            "address": sanitize_html(self.address),
+            "city": sanitize_html(self.city),
+            "state": sanitize_html(self.state),
+            "postal_code": sanitize_html(self.postal_code),
+            "country": sanitize_html(self.country),
+            "phone": sanitize_html(self.phone),
+        }
 
 
 class PSAAssociation(Base, AssociationMixin, RepresentById):
-    __tablename__ = 'psa_association'
+    __tablename__ = "psa_association"
 
     id = Column(Integer, primary_key=True)
     server_url = Column(VARCHAR(255))
     handle = Column(VARCHAR(255))
     secret = Column(VARCHAR(255))
     issued = Column(Integer)
     lifetime = Column(Integer)
@@ -7724,22 +8737,20 @@
 
     @classmethod
     def get(cls, *args, **kwargs):
         return cls.sa_session.query(cls).filter_by(*args, **kwargs)
 
     @classmethod
     def remove(cls, ids_to_delete):
-        cls.sa_session.query(cls).filter(cls.id.in_(ids_to_delete)).delete(synchronize_session='fetch')
+        cls.sa_session.query(cls).filter(cls.id.in_(ids_to_delete)).delete(synchronize_session="fetch")
 
 
 class PSACode(Base, CodeMixin, RepresentById):
-    __tablename__ = 'psa_code'
-    __table_args__ = (
-        UniqueConstraint('code', 'email'),
-    )
+    __tablename__ = "psa_code"
+    __table_args__ = (UniqueConstraint("code", "email"),)
 
     id = Column(Integer, primary_key=True)
     email = Column(VARCHAR(200))
     code = Column(VARCHAR(32))
 
     # This static property is set at: galaxy.authnz.psa_authnz.PSAAuthnz
     sa_session = None
@@ -7754,15 +8765,15 @@
 
     @classmethod
     def get_code(cls, code):
         return cls.sa_session.query(cls).filter(cls.code == code).first()
 
 
 class PSANonce(Base, NonceMixin, RepresentById):
-    __tablename__ = 'psa_nonce'
+    __tablename__ = "psa_nonce"
 
     id = Column(Integer, primary_key=True)
     server_url = Column(VARCHAR(255))
     timestamp = Column(Integer)
     salt = Column(VARCHAR(40))
 
     # This static property is set at: galaxy.authnz.psa_authnz.PSAAuthnz
@@ -7785,15 +8796,15 @@
             instance = cls(server_url=server_url, timestamp=timestamp, salt=salt)
             cls.sa_session.add(instance)
             cls.sa_session.flush()
             return instance
 
 
 class PSAPartial(Base, PartialMixin, RepresentById):
-    __tablename__ = 'psa_partial'
+    __tablename__ = "psa_partial"
 
     id = Column(Integer, primary_key=True)
     token = Column(VARCHAR(32))
     data = Column(TEXT)
     next_step = Column(Integer)
     backend = Column(VARCHAR(32))
 
@@ -7818,25 +8829,25 @@
     def destroy(cls, token):
         partial = cls.load(token)
         if partial:
             cls.sa_session.delete(partial)
 
 
 class UserAuthnzToken(Base, UserMixin, RepresentById):
-    __tablename__ = 'oidc_user_authnz_tokens'
-    __table_args__ = (UniqueConstraint('provider', 'uid'),)
+    __tablename__ = "oidc_user_authnz_tokens"
+    __table_args__ = (UniqueConstraint("provider", "uid"),)
 
     id = Column(Integer, primary_key=True)
     user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     uid = Column(VARCHAR(255))
     provider = Column(VARCHAR(32))
     extra_data = Column(MutableJSONType, nullable=True)
     lifetime = Column(Integer)
     assoc_type = Column(VARCHAR(64))
-    user = relationship('User', back_populates='social_auth')
+    user = relationship("User", back_populates="social_auth")
 
     # This static property is set at: galaxy.authnz.psa_authnz.PSAAuthnz
     sa_session = None
 
     def __init__(self, provider, uid, extra_data=None, lifetime=None, assoc_type=None, user=None):
         self.provider = provider
         self.uid = uid
@@ -7846,15 +8857,15 @@
         self.assoc_type = assoc_type
 
     def get_id_token(self, strategy):
         if self.access_token_expired():
             # Access and ID tokens have same expiration time;
             # hence, if one is expired, the other is expired too.
             self.refresh_token(strategy)
-        return self.extra_data.get('id_token', None) if self.extra_data is not None else None
+        return self.extra_data.get("id_token", None) if self.extra_data is not None else None
 
     def set_extra_data(self, extra_data=None):
         if super().set_extra_data(extra_data):
             self.sa_session.add(self)
             self.sa_session.flush()
 
     def save(self):
@@ -7882,15 +8893,15 @@
 
     @classmethod
     def user_exists(cls, *args, **kwargs):
         return cls.user_query().filter_by(*args, **kwargs).count() > 0
 
     @classmethod
     def get_username(cls, user):
-        return getattr(user, 'username', None)
+        return getattr(user, "username", None)
 
     @classmethod
     def create_user(cls, *args, **kwargs):
         """
         This is used by PSA authnz, do not use directly.
         Prefer using the user manager.
         """
@@ -7934,216 +8945,238 @@
         instance = cls(user=user, uid=uid, provider=provider)
         cls.sa_session.add(instance)
         cls.sa_session.flush()
         return instance
 
 
 class CustosAuthnzToken(Base, RepresentById):
-    __tablename__ = 'custos_authnz_token'
+    __tablename__ = "custos_authnz_token"
     __table_args__ = (
-        UniqueConstraint('user_id', 'external_user_id', 'provider'),
-        UniqueConstraint('external_user_id', 'provider'),
+        UniqueConstraint("user_id", "external_user_id", "provider"),
+        UniqueConstraint("external_user_id", "provider"),
     )
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'))
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"))
     external_user_id = Column(String(64))
     provider = Column(String(255))
     access_token = Column(Text)
     id_token = Column(Text)
     refresh_token = Column(Text)
     expiration_time = Column(DateTime)
     refresh_expiration_time = Column(DateTime)
-    user = relationship('User', back_populates='custos_auth')
+    user = relationship("User", back_populates="custos_auth")
 
 
-class CloudAuthz(Base, _HasTable):
-    __tablename__ = 'cloudauthz'
+class CloudAuthz(Base):
+    __tablename__ = "cloudauthz"
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     provider = Column(String(255))
     config = Column(MutableJSONType)
-    authn_id = Column(Integer, ForeignKey('oidc_user_authnz_tokens.id'), index=True)
+    authn_id = Column(Integer, ForeignKey("oidc_user_authnz_tokens.id"), index=True)
     tokens = Column(MutableJSONType)
     last_update = Column(DateTime)
     last_activity = Column(DateTime)
     description = Column(TEXT)
     create_time = Column(DateTime, default=now)
-    user = relationship('User', back_populates='cloudauthz')
-    authn = relationship('UserAuthnzToken')
+    user = relationship("User", back_populates="cloudauthz")
+    authn = relationship("UserAuthnzToken")
 
     def __init__(self, user_id, provider, config, authn_id, description=None):
         self.user_id = user_id
         self.provider = provider
         self.config = config
         self.authn_id = authn_id
-        self.last_update = datetime.now()
-        self.last_activity = datetime.now()
+        self.last_update = now()
+        self.last_activity = now()
         self.description = description
 
     def equals(self, user_id, provider, authn_id, config):
-        return (self.user_id == user_id
-                and self.provider == provider
-                and self.authn_id
-                and self.authn_id == authn_id
-                and len({k: self.config[k] for k in self.config if k in config
-                         and self.config[k] == config[k]}) == len(self.config))
+        return (
+            self.user_id == user_id
+            and self.provider == provider
+            and self.authn_id
+            and self.authn_id == authn_id
+            and len({k: self.config[k] for k in self.config if k in config and self.config[k] == config[k]})
+            == len(self.config)
+        )
 
 
-class Page(Base, Dictifiable, RepresentById):
-    __tablename__ = 'page'
-    __table_args__ = (
-        Index('ix_page_slug', 'slug', mysql_length=200),
-    )
+class Page(Base, HasTags, Dictifiable, RepresentById):
+    __tablename__ = "page"
+    __table_args__ = (Index("ix_page_slug", "slug", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True, nullable=False)
-    latest_revision_id = Column(Integer,
-        ForeignKey('page_revision.id', use_alter=True, name='page_latest_revision_id_fk'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True, nullable=False)
+    latest_revision_id = Column(
+        Integer, ForeignKey("page_revision.id", use_alter=True, name="page_latest_revision_id_fk"), index=True
+    )
     title = Column(TEXT)
     deleted = Column(Boolean, index=True, default=False)
     importable = Column(Boolean, index=True, default=False)
     slug = Column(TEXT)
     published = Column(Boolean, index=True, default=False)
-    user = relationship('User')
+    user = relationship("User")
     revisions = relationship(
-        'PageRevision',
+        "PageRevision",
         cascade="all, delete-orphan",
         primaryjoin=(lambda: Page.id == PageRevision.page_id),  # type: ignore[has-type]
-        back_populates='page')
+        back_populates="page",
+    )
     latest_revision = relationship(
-        'PageRevision',
+        "PageRevision",
         post_update=True,
         primaryjoin=(lambda: Page.latest_revision_id == PageRevision.id),  # type: ignore[has-type]
-        lazy=False)
-    tags = relationship(
-        'PageTagAssociation',
-        order_by=lambda: PageTagAssociation.id,
-        back_populates='page')
+        lazy=False,
+    )
+    tags = relationship("PageTagAssociation", order_by=lambda: PageTagAssociation.id, back_populates="page")
     annotations = relationship(
-        'PageAnnotationAssociation',
-        order_by=lambda: PageAnnotationAssociation.id,
-        back_populates='page')
+        "PageAnnotationAssociation", order_by=lambda: PageAnnotationAssociation.id, back_populates="page"
+    )
     ratings = relationship(
-        'PageRatingAssociation',
+        "PageRatingAssociation",
         order_by=lambda: PageRatingAssociation.id,  # type: ignore[has-type]
-        back_populates='page')
-    users_shared_with = relationship(
-        'PageUserShareAssociation',
-        back_populates='page')
+        back_populates="page",
+    )
+    users_shared_with = relationship("PageUserShareAssociation", back_populates="page")
 
     average_rating: column_property  # defined at the end of this module
 
     # Set up proxy so that
     #   Page.users_shared_with
     # returns a list of users that page is shared with.
-    users_shared_with_dot_users = association_proxy('users_shared_with', 'user')
+    users_shared_with_dot_users = association_proxy("users_shared_with", "user")
 
-    dict_element_visible_keys = ['id', 'title', 'latest_revision_id', 'slug', 'published', 'importable', 'deleted', 'username']
+    dict_element_visible_keys = [
+        "id",
+        "title",
+        "latest_revision_id",
+        "slug",
+        "published",
+        "importable",
+        "deleted",
+        "username",
+        "email_hash",
+    ]
 
-    def to_dict(self, view='element'):
+    def to_dict(self, view="element"):
         rval = super().to_dict(view=view)
         rev = []
         for a in self.revisions:
             rev.append(a.id)
-        rval['revision_ids'] = rev
+        rval["revision_ids"] = rev
         return rval
 
     # username needed for slug generation
     @property
     def username(self):
         return self.user.username
 
+    # email needed for hash generation
+    @property
+    def email_hash(self):
+        return md5_hash_str(self.user.email)
+
 
 class PageRevision(Base, Dictifiable, RepresentById):
-    __tablename__ = 'page_revision'
+    __tablename__ = "page_revision"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    page_id = Column(Integer, ForeignKey('page.id'), index=True, nullable=False)
+    page_id = Column(Integer, ForeignKey("page.id"), index=True, nullable=False)
     title = Column(TEXT)
     content = Column(TEXT)
     content_format = Column(TrimmedString(32))
-    page = relationship('Page',
-        primaryjoin=(lambda: Page.id == PageRevision.page_id))
-    DEFAULT_CONTENT_FORMAT = 'html'
-    dict_element_visible_keys = ['id', 'page_id', 'title', 'content', 'content_format']
+    page = relationship("Page", primaryjoin=(lambda: Page.id == PageRevision.page_id))
+    DEFAULT_CONTENT_FORMAT = "html"
+    dict_element_visible_keys = ["id", "page_id", "title", "content", "content_format"]
 
     def __init__(self):
         self.content_format = PageRevision.DEFAULT_CONTENT_FORMAT
 
-    def to_dict(self, view='element'):
+    def to_dict(self, view="element"):
         rval = super().to_dict(view=view)
-        rval['create_time'] = str(self.create_time)
-        rval['update_time'] = str(self.update_time)
+        rval["create_time"] = self.create_time.isoformat()
+        rval["update_time"] = self.update_time.isoformat()
         return rval
 
 
 class PageUserShareAssociation(Base, UserShareAssociation):
-    __tablename__ = 'page_user_share_association'
+    __tablename__ = "page_user_share_association"
 
     id = Column(Integer, primary_key=True)
     page_id = Column(Integer, ForeignKey("page.id"), index=True)
     user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
-    user = relationship('User')
-    page = relationship('Page', back_populates='users_shared_with')
+    user = relationship("User")
+    page = relationship("Page", back_populates="users_shared_with")
 
 
-class Visualization(Base, RepresentById):
-    __tablename__ = 'visualization'
+class Visualization(Base, HasTags, RepresentById):
+    __tablename__ = "visualization"
     __table_args__ = (
-        Index('ix_visualization_dbkey', 'dbkey', mysql_length=200),
-        Index('ix_visualization_slug', 'slug', mysql_length=200),
+        Index("ix_visualization_dbkey", "dbkey", mysql_length=200),
+        Index("ix_visualization_slug", "slug", mysql_length=200),
     )
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True, nullable=False)
-    latest_revision_id = Column(Integer,
-        ForeignKey('visualization_revision.id', use_alter=True, name='visualization_latest_revision_id_fk'),
-        index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True, nullable=False)
+    latest_revision_id = Column(
+        Integer,
+        ForeignKey("visualization_revision.id", use_alter=True, name="visualization_latest_revision_id_fk"),
+        index=True,
+    )
     title = Column(TEXT)
     type = Column(TEXT)
     dbkey = Column(TEXT)
     deleted = Column(Boolean, default=False, index=True)
     importable = Column(Boolean, default=False, index=True)
     slug = Column(TEXT)
     published = Column(Boolean, default=False, index=True)
 
-    user = relationship('User')
-    revisions = relationship('VisualizationRevision',
-        back_populates='visualization',
+    user = relationship("User")
+    revisions = relationship(
+        "VisualizationRevision",
+        back_populates="visualization",
         cascade="all, delete-orphan",
-        primaryjoin=(lambda: Visualization.id == VisualizationRevision.visualization_id))
-    latest_revision = relationship('VisualizationRevision',
+        primaryjoin=(lambda: Visualization.id == VisualizationRevision.visualization_id),
+    )
+    latest_revision = relationship(
+        "VisualizationRevision",
         post_update=True,
         primaryjoin=(lambda: Visualization.latest_revision_id == VisualizationRevision.id),
-        lazy=False)
-    tags = relationship('VisualizationTagAssociation',
-        order_by=lambda: VisualizationTagAssociation.id,
-        back_populates="visualization")
-    annotations = relationship('VisualizationAnnotationAssociation',
+        lazy=False,
+    )
+    tags = relationship(
+        "VisualizationTagAssociation", order_by=lambda: VisualizationTagAssociation.id, back_populates="visualization"
+    )
+    annotations = relationship(
+        "VisualizationAnnotationAssociation",
         order_by=lambda: VisualizationAnnotationAssociation.id,
-        back_populates="visualization")
-    ratings = relationship('VisualizationRatingAssociation',
+        back_populates="visualization",
+    )
+    ratings = relationship(
+        "VisualizationRatingAssociation",
         order_by=lambda: VisualizationRatingAssociation.id,  # type: ignore[has-type]
-        back_populates="visualization")
-    users_shared_with = relationship('VisualizationUserShareAssociation', back_populates='visualization')
+        back_populates="visualization",
+    )
+    users_shared_with = relationship("VisualizationUserShareAssociation", back_populates="visualization")
 
     average_rating: column_property  # defined at the end of this module
 
     # Set up proxy so that
     #   Visualization.users_shared_with
     # returns a list of users that visualization is shared with.
-    users_shared_with_dot_users = association_proxy('users_shared_with', 'user')
+    users_shared_with_dot_users = association_proxy("users_shared_with", "user")
 
     def __init__(self, **kwd):
         super().__init__(**kwd)
         if self.latest_revision:
             self.revisions.append(self.latest_revision)
 
     def copy(self, user=None, title=None):
@@ -8165,74 +9198,69 @@
         copy_viz = Visualization(user=user, type=self.type, title=title, dbkey=self.dbkey)
         copy_revision = self.latest_revision.copy(visualization=copy_viz)
         copy_viz.latest_revision = copy_revision
         return copy_viz
 
 
 class VisualizationRevision(Base, RepresentById):
-    __tablename__ = 'visualization_revision'
-    __table_args__ = (
-        Index('ix_visualization_revision_dbkey', 'dbkey', mysql_length=200),
-    )
+    __tablename__ = "visualization_revision"
+    __table_args__ = (Index("ix_visualization_revision_dbkey", "dbkey", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    visualization_id = Column(Integer, ForeignKey('visualization.id'), index=True, nullable=False)
+    visualization_id = Column(Integer, ForeignKey("visualization.id"), index=True, nullable=False)
     title = Column(TEXT)
     dbkey = Column(TEXT)
     config = Column(MutableJSONType)
-    visualization = relationship('Visualization',
-        back_populates='revisions',
-        primaryjoin=(lambda: Visualization.id == VisualizationRevision.visualization_id))
+    visualization = relationship(
+        "Visualization",
+        back_populates="revisions",
+        primaryjoin=(lambda: Visualization.id == VisualizationRevision.visualization_id),
+    )
 
     def copy(self, visualization=None):
         """
         Returns a copy of this object.
         """
         if not visualization:
             visualization = self.visualization
 
         return VisualizationRevision(
-            visualization=visualization,
-            title=self.title,
-            dbkey=self.dbkey,
-            config=self.config
+            visualization=visualization, title=self.title, dbkey=self.dbkey, config=self.config
         )
 
 
 class VisualizationUserShareAssociation(Base, UserShareAssociation):
-    __tablename__ = 'visualization_user_share_association'
+    __tablename__ = "visualization_user_share_association"
 
     id = Column(Integer, primary_key=True)
-    visualization_id = Column(Integer, ForeignKey('visualization.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    user = relationship('User')
-    visualization = relationship('Visualization', back_populates='users_shared_with')
+    visualization_id = Column(Integer, ForeignKey("visualization.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    user = relationship("User")
+    visualization = relationship("Visualization", back_populates="users_shared_with")
 
 
 class Tag(Base, RepresentById):
-    __tablename__ = 'tag'
-    __table_args__ = (
-        UniqueConstraint('name'),
-    )
+    __tablename__ = "tag"
+    __table_args__ = (UniqueConstraint("name"),)
 
     id = Column(Integer, primary_key=True)
     type = Column(Integer)
-    parent_id = Column(Integer, ForeignKey('tag.id'))
+    parent_id = Column(Integer, ForeignKey("tag.id"))
     name = Column(TrimmedString(255))
-    children = relationship('Tag', back_populates='parent')
-    parent = relationship('Tag', back_populates='children', remote_side=[id])
+    children = relationship("Tag", back_populates="parent")
+    parent = relationship("Tag", back_populates="children", remote_side=[id])
 
     def __str__(self):
         return "Tag(id=%s, type=%i, parent_id=%s, name=%s)" % (self.id, self.type or -1, self.parent_id, self.name)
 
 
 class ItemTagAssociation(Dictifiable):
-    dict_collection_visible_keys = ['id', 'user_tname', 'user_value']
+    dict_collection_visible_keys = ["id", "user_tname", "user_value"]
     dict_element_visible_keys = dict_collection_visible_keys
     associated_item_names: List[str] = []
     user_tname: Column
     user_value = Column(TrimmedString(255), index=True)
 
     def __init_subclass__(cls, **kwargs):
         super().__init_subclass__(**kwargs)
@@ -8247,276 +9275,256 @@
         new_ta.user_tname = self.user_tname
         new_ta.value = self.value
         new_ta.user_value = self.user_value
         return new_ta
 
 
 class HistoryTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'history_tag_association'
+    __tablename__ = "history_tag_association"
 
     id = Column(Integer, primary_key=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    history = relationship('History', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    history = relationship("History", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class HistoryDatasetAssociationTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'history_dataset_association_tag_association'
+    __tablename__ = "history_dataset_association_tag_association"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_association_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    history_dataset_association = relationship('HistoryDatasetAssociation', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    history_dataset_association = relationship("HistoryDatasetAssociation", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class LibraryDatasetDatasetAssociationTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'library_dataset_dataset_association_tag_association'
+    __tablename__ = "library_dataset_dataset_association_tag_association"
 
     id = Column(Integer, primary_key=True)
     library_dataset_dataset_association_id = Column(
-        Integer, ForeignKey('library_dataset_dataset_association.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+        Integer, ForeignKey("library_dataset_dataset_association.id"), index=True
+    )
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    library_dataset_dataset_association = relationship(
-        'LibraryDatasetDatasetAssociation', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    library_dataset_dataset_association = relationship("LibraryDatasetDatasetAssociation", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class PageTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'page_tag_association'
+    __tablename__ = "page_tag_association"
 
     id = Column(Integer, primary_key=True)
-    page_id = Column(Integer, ForeignKey('page.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    page_id = Column(Integer, ForeignKey("page.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    page = relationship('Page', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    page = relationship("Page", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class WorkflowStepTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'workflow_step_tag_association'
+    __tablename__ = "workflow_step_tag_association"
 
     id = Column(Integer, primary_key=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    workflow_step = relationship('WorkflowStep', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    workflow_step = relationship("WorkflowStep", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class StoredWorkflowTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'stored_workflow_tag_association'
+    __tablename__ = "stored_workflow_tag_association"
 
     id = Column(Integer, primary_key=True)
-    stored_workflow_id = Column(Integer, ForeignKey('stored_workflow.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    stored_workflow_id = Column(Integer, ForeignKey("stored_workflow.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    stored_workflow = relationship('StoredWorkflow', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    stored_workflow = relationship("StoredWorkflow", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class VisualizationTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'visualization_tag_association'
+    __tablename__ = "visualization_tag_association"
 
     id = Column(Integer, primary_key=True)
-    visualization_id = Column(Integer, ForeignKey('visualization.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    visualization_id = Column(Integer, ForeignKey("visualization.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    visualization = relationship('Visualization', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    visualization = relationship("Visualization", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class HistoryDatasetCollectionTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'history_dataset_collection_tag_association'
+    __tablename__ = "history_dataset_collection_tag_association"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_collection_id = Column(
-        Integer, ForeignKey('history_dataset_collection_association.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    dataset_collection = relationship('HistoryDatasetCollectionAssociation', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    dataset_collection = relationship("HistoryDatasetCollectionAssociation", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class LibraryDatasetCollectionTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'library_dataset_collection_tag_association'
+    __tablename__ = "library_dataset_collection_tag_association"
 
     id = Column(Integer, primary_key=True)
-    library_dataset_collection_id = Column(
-        Integer, ForeignKey('library_dataset_collection_association.id'), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    library_dataset_collection_id = Column(Integer, ForeignKey("library_dataset_collection_association.id"), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    dataset_collection = relationship('LibraryDatasetCollectionAssociation', back_populates='tags')
-    tag = relationship('Tag')
-    user = relationship('User')
+    dataset_collection = relationship("LibraryDatasetCollectionAssociation", back_populates="tags")
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 class ToolTagAssociation(Base, ItemTagAssociation, RepresentById):
-    __tablename__ = 'tool_tag_association'
+    __tablename__ = "tool_tag_association"
 
     id = Column(Integer, primary_key=True)
     tool_id = Column(TrimmedString(255), index=True)
-    tag_id = Column(Integer, ForeignKey('tag.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    tag_id = Column(Integer, ForeignKey("tag.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     user_tname = Column(TrimmedString(255), index=True)
     value = Column(TrimmedString(255), index=True)
-    tag = relationship('Tag')
-    user = relationship('User')
+    tag = relationship("Tag")
+    user = relationship("User")
 
 
 # Item annotation classes.
 class HistoryAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'history_annotation_association'
-    __table_args__ = (
-        Index('ix_history_anno_assoc_annotation', 'annotation', mysql_length=200),
-    )
+    __tablename__ = "history_annotation_association"
+    __table_args__ = (Index("ix_history_anno_assoc_annotation", "annotation", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    history = relationship('History', back_populates='annotations')
-    user = relationship('User')
+    history = relationship("History", back_populates="annotations")
+    user = relationship("User")
 
 
 class HistoryDatasetAssociationAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'history_dataset_association_annotation_association'
-    __table_args__ = (
-        Index('ix_history_dataset_anno_assoc_annotation', 'annotation', mysql_length=200),
-    )
+    __tablename__ = "history_dataset_association_annotation_association"
+    __table_args__ = (Index("ix_history_dataset_anno_assoc_annotation", "annotation", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_association_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    hda = relationship('HistoryDatasetAssociation', back_populates='annotations')
-    user = relationship('User')
+    hda = relationship("HistoryDatasetAssociation", back_populates="annotations")
+    user = relationship("User")
 
 
 class StoredWorkflowAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'stored_workflow_annotation_association'
-    __table_args__ = (
-        Index('ix_stored_workflow_ann_assoc_annotation', 'annotation', mysql_length=200),
-    )
+    __tablename__ = "stored_workflow_annotation_association"
+    __table_args__ = (Index("ix_stored_workflow_ann_assoc_annotation", "annotation", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    stored_workflow_id = Column(Integer, ForeignKey('stored_workflow.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    stored_workflow_id = Column(Integer, ForeignKey("stored_workflow.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    stored_workflow = relationship('StoredWorkflow', back_populates='annotations')
-    user = relationship('User')
+    stored_workflow = relationship("StoredWorkflow", back_populates="annotations")
+    user = relationship("User")
 
 
 class WorkflowStepAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'workflow_step_annotation_association'
-    __table_args__ = (
-        Index('ix_workflow_step_ann_assoc_annotation', 'annotation', mysql_length=200),
-    )
+    __tablename__ = "workflow_step_annotation_association"
+    __table_args__ = (Index("ix_workflow_step_ann_assoc_annotation", "annotation", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    workflow_step_id = Column(Integer, ForeignKey('workflow_step.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    workflow_step_id = Column(Integer, ForeignKey("workflow_step.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    workflow_step = relationship('WorkflowStep', back_populates='annotations')
-    user = relationship('User')
+    workflow_step = relationship("WorkflowStep", back_populates="annotations")
+    user = relationship("User")
 
 
 class PageAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'page_annotation_association'
-    __table_args__ = (
-        Index('ix_page_annotation_association_annotation', 'annotation', mysql_length=200),
-    )
+    __tablename__ = "page_annotation_association"
+    __table_args__ = (Index("ix_page_annotation_association_annotation", "annotation", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    page_id = Column(Integer, ForeignKey('page.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    page_id = Column(Integer, ForeignKey("page.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    page = relationship('Page', back_populates='annotations')
-    user = relationship('User')
+    page = relationship("Page", back_populates="annotations")
+    user = relationship("User")
 
 
 class VisualizationAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'visualization_annotation_association'
-    __table_args__ = (
-        Index('ix_visualization_annotation_association_annotation', 'annotation', mysql_length=200),
-    )
+    __tablename__ = "visualization_annotation_association"
+    __table_args__ = (Index("ix_visualization_annotation_association_annotation", "annotation", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
-    visualization_id = Column(Integer, ForeignKey('visualization.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    visualization_id = Column(Integer, ForeignKey("visualization.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    visualization = relationship('Visualization', back_populates='annotations')
-    user = relationship('User')
+    visualization = relationship("Visualization", back_populates="annotations")
+    user = relationship("User")
 
 
 class HistoryDatasetCollectionAssociationAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'history_dataset_collection_annotation_association'
+    __tablename__ = "history_dataset_collection_annotation_association"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_collection_id = Column(
-        Integer, ForeignKey('history_dataset_collection_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    history_dataset_collection = relationship('HistoryDatasetCollectionAssociation',
-        back_populates='annotations')
-    user = relationship('User')
+    history_dataset_collection = relationship("HistoryDatasetCollectionAssociation", back_populates="annotations")
+    user = relationship("User")
 
 
 class LibraryDatasetCollectionAnnotationAssociation(Base, RepresentById):
-    __tablename__ = 'library_dataset_collection_annotation_association'
+    __tablename__ = "library_dataset_collection_annotation_association"
 
     id = Column(Integer, primary_key=True)
-    library_dataset_collection_id = Column(
-        Integer, ForeignKey('library_dataset_collection_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    library_dataset_collection_id = Column(Integer, ForeignKey("library_dataset_collection_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     annotation = Column(TEXT)
-    dataset_collection = relationship('LibraryDatasetCollectionAssociation',
-        back_populates='annotations')
-    user = relationship('User')
+    dataset_collection = relationship("LibraryDatasetCollectionAssociation", back_populates="annotations")
+    user = relationship("User")
 
 
 class Vault(Base):
-    __tablename__ = 'vault'
+    __tablename__ = "vault"
 
     key = Column(Text, primary_key=True)
     parent_key = Column(Text, ForeignKey(key), index=True, nullable=True)
-    children = relationship('Vault', back_populates='parent')
-    parent = relationship('Vault', back_populates='children', remote_side=[key])
+    children = relationship("Vault", back_populates="parent")
+    parent = relationship("Vault", back_populates="children", remote_side=[key])
     value = Column(Text, nullable=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
 
 
 # Item rating classes.
 class ItemRatingAssociation(Base):
@@ -8524,193 +9532,196 @@
 
     def __init__(self, user, item, rating=0):
         self.user = user
         self.rating = rating
         self._set_item(item)
 
     def _set_item(self, item):
-        """ Set association's item. """
+        """Set association's item."""
         raise NotImplementedError()
 
 
 class HistoryRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'history_rating_association'
+    __tablename__ = "history_rating_association"
 
     id = Column(Integer, primary_key=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    history = relationship('History', back_populates='ratings')
-    user = relationship('User')
+    history = relationship("History", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, history):
+        add_object_to_object_session(self, history)
         self.history = history
 
 
 class HistoryDatasetAssociationRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'history_dataset_association_rating_association'
+    __tablename__ = "history_dataset_association_rating_association"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_association_id = Column(Integer,
-        ForeignKey('history_dataset_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_association_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    history_dataset_association = relationship('HistoryDatasetAssociation', back_populates='ratings')
-    user = relationship('User')
+    history_dataset_association = relationship("HistoryDatasetAssociation", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, history_dataset_association):
+        add_object_to_object_session(self, history_dataset_association)
         self.history_dataset_association = history_dataset_association
 
 
 class StoredWorkflowRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'stored_workflow_rating_association'
+    __tablename__ = "stored_workflow_rating_association"
 
     id = Column(Integer, primary_key=True)
-    stored_workflow_id = Column(Integer, ForeignKey('stored_workflow.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    stored_workflow_id = Column(Integer, ForeignKey("stored_workflow.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    stored_workflow = relationship('StoredWorkflow', back_populates='ratings')
-    user = relationship('User')
+    stored_workflow = relationship("StoredWorkflow", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, stored_workflow):
+        add_object_to_object_session(self, stored_workflow)
         self.stored_workflow = stored_workflow
 
 
 class PageRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'page_rating_association'
+    __tablename__ = "page_rating_association"
 
     id = Column(Integer, primary_key=True)
-    page_id = Column(Integer, ForeignKey('page.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    page_id = Column(Integer, ForeignKey("page.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    page = relationship('Page', back_populates='ratings')
-    user = relationship('User')
+    page = relationship("Page", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, page):
+        add_object_to_object_session(self, page)
         self.page = page
 
 
 class VisualizationRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'visualization_rating_association'
+    __tablename__ = "visualization_rating_association"
 
     id = Column(Integer, primary_key=True)
-    visualization_id = Column(Integer, ForeignKey('visualization.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    visualization_id = Column(Integer, ForeignKey("visualization.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    visualization = relationship('Visualization', back_populates='ratings')
-    user = relationship('User')
+    visualization = relationship("Visualization", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, visualization):
+        add_object_to_object_session(self, visualization)
         self.visualization = visualization
 
 
 class HistoryDatasetCollectionRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'history_dataset_collection_rating_association'
+    __tablename__ = "history_dataset_collection_rating_association"
 
     id = Column(Integer, primary_key=True)
-    history_dataset_collection_id = Column(
-        Integer, ForeignKey('history_dataset_collection_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    history_dataset_collection_id = Column(Integer, ForeignKey("history_dataset_collection_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    dataset_collection = relationship('HistoryDatasetCollectionAssociation', back_populates='ratings')
-    user = relationship('User')
+    dataset_collection = relationship("HistoryDatasetCollectionAssociation", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, dataset_collection):
+        add_object_to_object_session(self, dataset_collection)
         self.dataset_collection = dataset_collection
 
 
 class LibraryDatasetCollectionRatingAssociation(ItemRatingAssociation, RepresentById):
-    __tablename__ = 'library_dataset_collection_rating_association'
+    __tablename__ = "library_dataset_collection_rating_association"
 
     id = Column(Integer, primary_key=True)
-    library_dataset_collection_id = Column(
-        Integer, ForeignKey('library_dataset_collection_association.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    library_dataset_collection_id = Column(Integer, ForeignKey("library_dataset_collection_association.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     rating = Column(Integer, index=True)
-    dataset_collection = relationship('LibraryDatasetCollectionAssociation', back_populates='ratings')
-    user = relationship('User')
+    dataset_collection = relationship("LibraryDatasetCollectionAssociation", back_populates="ratings")
+    user = relationship("User")
 
     def _set_item(self, dataset_collection):
+        add_object_to_object_session(self, dataset_collection)
         self.dataset_collection = dataset_collection
 
 
 # Data manager classes.
 class DataManagerHistoryAssociation(Base, RepresentById):
-    __tablename__ = 'data_manager_history_association'
+    __tablename__ = "data_manager_history_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, index=True, default=now, onupdate=now)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
-    history = relationship('History')
-    user = relationship('User', back_populates='data_manager_histories')
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
+    history = relationship("History")
+    user = relationship("User", back_populates="data_manager_histories")
 
 
 class DataManagerJobAssociation(Base, RepresentById):
-    __tablename__ = 'data_manager_job_association'
-    __table_args__ = (
-        Index('ix_data_manager_job_association_data_manager_id', 'data_manager_id', mysql_length=200),
-    )
+    __tablename__ = "data_manager_job_association"
+    __table_args__ = (Index("ix_data_manager_job_association_data_manager_id", "data_manager_id", mysql_length=200),)
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, index=True, default=now, onupdate=now)
     job_id = Column(Integer, ForeignKey("job.id"), index=True)
     data_manager_id = Column(TEXT)
-    job = relationship('Job', back_populates='data_manager_association', uselist=False)
+    job = relationship("Job", back_populates="data_manager_association", uselist=False)
 
 
 class UserPreference(Base, RepresentById):
-    __tablename__ = 'user_preference'
+    __tablename__ = "user_preference"
 
     id = Column(Integer, primary_key=True)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     name = Column(Unicode(255), index=True)
     value = Column(Text)
 
     def __init__(self, name=None, value=None):
         # Do not remove this constructor: it is set as the creator for the User.preferences
         # AssociationProxy to which 2 args are passed.
         self.name = name
         self.value = value
 
 
 class UserAction(Base, RepresentById):
-    __tablename__ = 'user_action'
+    __tablename__ = "user_action"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     session_id = Column(Integer, ForeignKey("galaxy_session.id"), index=True)
     action = Column(Unicode(255))
     context = Column(Unicode(512))
     params = Column(Unicode(1024))
-    user = relationship('User')
+    user = relationship("User")
 
 
 class APIKeys(Base, RepresentById):
-    __tablename__ = 'api_keys'
+    __tablename__ = "api_keys"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    user_id = Column(Integer, ForeignKey('galaxy_user.id'), index=True)
+    user_id = Column(Integer, ForeignKey("galaxy_user.id"), index=True)
     key = Column(TrimmedString(32), index=True, unique=True)
-    user = relationship('User', back_populates='api_keys')
+    user = relationship("User", back_populates="api_keys")
+    deleted = Column(Boolean, index=True, default=False)
 
 
 def copy_list(lst, *args, **kwds):
     if lst is None:
         return lst
     else:
         return [el.copy(*args, **kwds) for el in lst]
 
 
 def _prepare_metadata_for_serialization(id_encoder, serialization_options, metadata):
-    """ Prepare metatdata for exporting. """
+    """Prepare metatdata for exporting."""
     processed_metadata = {}
     for name, value in metadata.items():
         # Metadata files are not needed for export because they can be
         # regenerated.
         if isinstance(value, MetadataFile):
             if serialization_options.strip_metadata_files:
                 continue
@@ -8720,394 +9731,449 @@
 
     return processed_metadata
 
 
 # The following CleanupEvent* models could be defined as tables only;
 # however making them models keeps things simple and consistent.
 
+
 class CleanupEvent(Base):
-    __tablename__ = 'cleanup_event'
+    __tablename__ = "cleanup_event"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     message = Column(TrimmedString(1024))
 
 
 class CleanupEventDatasetAssociation(Base):
-    __tablename__ = 'cleanup_event_dataset_association'
+    __tablename__ = "cleanup_event_dataset_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    dataset_id = Column(Integer, ForeignKey('dataset.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    dataset_id = Column(Integer, ForeignKey("dataset.id"), index=True)
 
 
 class CleanupEventMetadataFileAssociation(Base):
-    __tablename__ = 'cleanup_event_metadata_file_association'
+    __tablename__ = "cleanup_event_metadata_file_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    metadata_file_id = Column(Integer, ForeignKey('metadata_file.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    metadata_file_id = Column(Integer, ForeignKey("metadata_file.id"), index=True)
 
 
 class CleanupEventHistoryAssociation(Base):
-    __tablename__ = 'cleanup_event_history_association'
+    __tablename__ = "cleanup_event_history_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    history_id = Column(Integer, ForeignKey('history.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    history_id = Column(Integer, ForeignKey("history.id"), index=True)
 
 
 class CleanupEventHistoryDatasetAssociationAssociation(Base):
-    __tablename__ = 'cleanup_event_hda_association'
+    __tablename__ = "cleanup_event_hda_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    hda_id = Column(Integer, ForeignKey('history_dataset_association.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    hda_id = Column(Integer, ForeignKey("history_dataset_association.id"), index=True)
 
 
 class CleanupEventLibraryAssociation(Base):
-    __tablename__ = 'cleanup_event_library_association'
+    __tablename__ = "cleanup_event_library_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    library_id = Column(Integer, ForeignKey('library.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    library_id = Column(Integer, ForeignKey("library.id"), index=True)
 
 
 class CleanupEventLibraryFolderAssociation(Base):
-    __tablename__ = 'cleanup_event_library_folder_association'
+    __tablename__ = "cleanup_event_library_folder_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    library_folder_id = Column(Integer, ForeignKey('library_folder.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    library_folder_id = Column(Integer, ForeignKey("library_folder.id"), index=True)
 
 
 class CleanupEventLibraryDatasetAssociation(Base):
-    __tablename__ = 'cleanup_event_library_dataset_association'
+    __tablename__ = "cleanup_event_library_dataset_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    library_dataset_id = Column(Integer, ForeignKey('library_dataset.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    library_dataset_id = Column(Integer, ForeignKey("library_dataset.id"), index=True)
 
 
 class CleanupEventLibraryDatasetDatasetAssociationAssociation(Base):
-    __tablename__ = 'cleanup_event_ldda_association'
+    __tablename__ = "cleanup_event_ldda_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    ldda_id = Column(Integer, ForeignKey('library_dataset_dataset_association.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    ldda_id = Column(Integer, ForeignKey("library_dataset_dataset_association.id"), index=True)
 
 
 class CleanupEventImplicitlyConvertedDatasetAssociationAssociation(Base):
-    __tablename__ = 'cleanup_event_icda_association'
+    __tablename__ = "cleanup_event_icda_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
-    cleanup_event_id = Column(Integer, ForeignKey('cleanup_event.id'), index=True, nullable=True)
-    icda_id = Column(Integer, ForeignKey('implicitly_converted_dataset_association.id'), index=True)
+    cleanup_event_id = Column(Integer, ForeignKey("cleanup_event.id"), index=True, nullable=True)
+    icda_id = Column(Integer, ForeignKey("implicitly_converted_dataset_association.id"), index=True)
 
 
-# The following models (Dataset, HDA, LDDA) are mapped imperatively (for details see discussion in PR #12064)
+# The following models (HDA, LDDA) are mapped imperatively (for details see discussion in PR #12064)
 # TLDR: there are issues ('metadata' property, Galaxy object wrapping) that need to be addressed separately
 # before these models can be mapped declaratively. Keeping them in the mapping module breaks the auth package
 # tests (which import model directly bypassing the mapping module); fixing that is possible by importing
 # mapping into the test; however, having all models mapped in the same module is cleaner.
 
-Dataset.table = Table(
-    'dataset', mapper_registry.metadata,
-    Column('id', Integer, primary_key=True),
-    Column('job_id', Integer, ForeignKey('job.id'), index=True, nullable=True),
-    Column('create_time', DateTime, default=now),
-    Column('update_time', DateTime, index=True, default=now, onupdate=now),
-    Column('state', TrimmedString(64), index=True),
-    Column('deleted', Boolean, index=True, default=False),
-    Column('purged', Boolean, index=True, default=False),
-    Column('purgable', Boolean, default=True),
-    Column('object_store_id', TrimmedString(255), index=True),
-    Column('external_filename', TEXT),
-    Column('_extra_files_path', TEXT),
-    Column('created_from_basename', TEXT),
-    Column('file_size', Numeric(15, 0)),
-    Column('total_size', Numeric(15, 0)),
-    Column('uuid', UUIDType()))
-
 HistoryDatasetAssociation.table = Table(
-    'history_dataset_association', mapper_registry.metadata,
-    Column('id', Integer, primary_key=True),
-    Column('history_id', Integer, ForeignKey('history.id'), index=True),
-    Column('dataset_id', Integer, ForeignKey('dataset.id'), index=True),
-    Column('create_time', DateTime, default=now),
-    Column('update_time', DateTime, default=now, onupdate=now, index=True),
-    Column('state', TrimmedString(64), index=True, key='_state'),
-    Column('copied_from_history_dataset_association_id', Integer,
-           ForeignKey('history_dataset_association.id'), nullable=True),
-    Column('copied_from_library_dataset_dataset_association_id', Integer,
-           ForeignKey('library_dataset_dataset_association.id'), nullable=True),
-    Column('name', TrimmedString(255)),
-    Column('info', TrimmedString(255)),
-    Column('blurb', TrimmedString(255)),
-    Column('peek', TEXT, key='_peek'),
-    Column('tool_version', TEXT),
-    Column('extension', TrimmedString(64)),
+    "history_dataset_association",
+    mapper_registry.metadata,
+    Column("id", Integer, primary_key=True),
+    Column("history_id", Integer, ForeignKey("history.id"), index=True),
+    Column("dataset_id", Integer, ForeignKey("dataset.id"), index=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now, index=True),
+    Column("state", TrimmedString(64), index=True, key="_state"),
+    Column(
+        "copied_from_history_dataset_association_id",
+        Integer,
+        ForeignKey("history_dataset_association.id"),
+        nullable=True,
+    ),
+    Column(
+        "copied_from_library_dataset_dataset_association_id",
+        Integer,
+        ForeignKey("library_dataset_dataset_association.id"),
+        nullable=True,
+    ),
+    Column("name", TrimmedString(255)),
+    Column("info", TrimmedString(255)),
+    Column("blurb", TrimmedString(255)),
+    Column("peek", TEXT, key="_peek"),
+    Column("tool_version", TEXT),
+    Column("extension", TrimmedString(64)),
     Column("metadata", MetadataType, key="_metadata"),
-    Column('parent_id', Integer, ForeignKey('history_dataset_association.id'), nullable=True),
-    Column('designation', TrimmedString(255)),
-    Column('deleted', Boolean, index=True, default=False),
-    Column('visible', Boolean),
-    Column('extended_metadata_id', Integer, ForeignKey('extended_metadata.id'), index=True),
-    Column('version', Integer, default=1, nullable=True, index=True),
-    Column('hid', Integer),
-    Column('purged', Boolean, index=True, default=False),
-    Column('validated_state', TrimmedString(64), default='unvalidated', nullable=False),
-    Column('validated_state_message', TEXT),
-    Column('hidden_beneath_collection_instance_id',
-           ForeignKey('history_dataset_collection_association.id'), nullable=True))
+    Column("metadata_deferred", Boolean, key="metadata_deferred"),
+    Column("parent_id", Integer, ForeignKey("history_dataset_association.id"), nullable=True),
+    Column("designation", TrimmedString(255)),
+    Column("deleted", Boolean, index=True, default=False),
+    Column("visible", Boolean),
+    Column("extended_metadata_id", Integer, ForeignKey("extended_metadata.id"), index=True),
+    Column("version", Integer, default=1, nullable=True, index=True),
+    Column("hid", Integer),
+    Column("purged", Boolean, index=True, default=False),
+    Column("validated_state", TrimmedString(64), default="unvalidated", nullable=False),
+    Column("validated_state_message", TEXT),
+    Column(
+        "hidden_beneath_collection_instance_id", ForeignKey("history_dataset_collection_association.id"), nullable=True
+    ),
+)
 
 LibraryDatasetDatasetAssociation.table = Table(
-    'library_dataset_dataset_association', mapper_registry.metadata,
-    Column('id', Integer, primary_key=True),
-    Column('library_dataset_id', Integer, ForeignKey('library_dataset.id'), index=True),
-    Column('dataset_id', Integer, ForeignKey('dataset.id'), index=True),
-    Column('create_time', DateTime, default=now),
-    Column('update_time', DateTime, default=now, onupdate=now, index=True),
-    Column('state', TrimmedString(64), index=True, key='_state'),
-    Column('copied_from_history_dataset_association_id', Integer,
-        ForeignKey('history_dataset_association.id',
-            use_alter=True, name='history_dataset_association_dataset_id_fkey'),
-        nullable=True),
-    Column('copied_from_library_dataset_dataset_association_id', Integer,
-        ForeignKey('library_dataset_dataset_association.id',
-            use_alter=True, name='library_dataset_dataset_association_id_fkey'),
-        nullable=True),
-    Column('name', TrimmedString(255), index=True),
-    Column('info', TrimmedString(255)),
-    Column('blurb', TrimmedString(255)),
-    Column('peek', TEXT, key='_peek'),
-    Column('tool_version', TEXT),
-    Column('extension', TrimmedString(64)),
+    "library_dataset_dataset_association",
+    mapper_registry.metadata,
+    Column("id", Integer, primary_key=True),
+    Column("library_dataset_id", Integer, ForeignKey("library_dataset.id"), index=True),
+    Column("dataset_id", Integer, ForeignKey("dataset.id"), index=True),
+    Column("create_time", DateTime, default=now),
+    Column("update_time", DateTime, default=now, onupdate=now, index=True),
+    Column("state", TrimmedString(64), index=True, key="_state"),
+    Column(
+        "copied_from_history_dataset_association_id",
+        Integer,
+        ForeignKey(
+            "history_dataset_association.id", use_alter=True, name="history_dataset_association_dataset_id_fkey"
+        ),
+        nullable=True,
+    ),
+    Column(
+        "copied_from_library_dataset_dataset_association_id",
+        Integer,
+        ForeignKey(
+            "library_dataset_dataset_association.id", use_alter=True, name="library_dataset_dataset_association_id_fkey"
+        ),
+        nullable=True,
+    ),
+    Column("name", TrimmedString(255), index=True),
+    Column("info", TrimmedString(255)),
+    Column("blurb", TrimmedString(255)),
+    Column("peek", TEXT, key="_peek"),
+    Column("tool_version", TEXT),
+    Column("extension", TrimmedString(64)),
     Column("metadata", MetadataType, key="_metadata"),
-    Column('parent_id', Integer, ForeignKey('library_dataset_dataset_association.id'), nullable=True),
-    Column('designation', TrimmedString(255)),
-    Column('deleted', Boolean, index=True, default=False),
-    Column('validated_state', TrimmedString(64), default='unvalidated', nullable=False),
-    Column('validated_state_message', TEXT),
-    Column('visible', Boolean),
-    Column('extended_metadata_id', Integer, ForeignKey('extended_metadata.id'), index=True),
-    Column('user_id', Integer, ForeignKey('galaxy_user.id'), index=True),
-    Column('message', TrimmedString(255)))
-
-mapper_registry.map_imperatively(
-    Dataset,
-    Dataset.table,
-    properties=dict(
-        actions=relationship(DatasetPermissions, back_populates='dataset'),
-        job=relationship(Job, primaryjoin=(Dataset.table.c.job_id == Job.id)),
-        active_history_associations=relationship(HistoryDatasetAssociation,
-            primaryjoin=(
-                (Dataset.table.c.id == HistoryDatasetAssociation.table.c.dataset_id)
-                & (HistoryDatasetAssociation.table.c.deleted == false())
-                & (HistoryDatasetAssociation.table.c.purged == false())),
-            viewonly=True),
-        purged_history_associations=relationship(HistoryDatasetAssociation,
-            primaryjoin=(
-                (Dataset.table.c.id == HistoryDatasetAssociation.table.c.dataset_id)
-                & (HistoryDatasetAssociation.table.c.purged == true())),
-            viewonly=True),
-        active_library_associations=relationship(LibraryDatasetDatasetAssociation,
-            primaryjoin=(
-                (Dataset.table.c.id == LibraryDatasetDatasetAssociation.table.c.dataset_id)
-                & (LibraryDatasetDatasetAssociation.table.c.deleted == false())),
-            viewonly=True),
-        hashes=relationship(DatasetHash, back_populates='dataset'),
-        sources=relationship(DatasetSource, back_populates='dataset'),
-        history_associations=relationship(HistoryDatasetAssociation, back_populates='dataset'),
-        library_associations=relationship(LibraryDatasetDatasetAssociation,
-            primaryjoin=(LibraryDatasetDatasetAssociation.table.c.dataset_id == Dataset.table.c.id),
-            back_populates='dataset'),
-    )
+    Column("metadata_deferred", Boolean, key="metadata_deferred"),
+    Column("parent_id", Integer, ForeignKey("library_dataset_dataset_association.id"), nullable=True),
+    Column("designation", TrimmedString(255)),
+    Column("deleted", Boolean, index=True, default=False),
+    Column("validated_state", TrimmedString(64), default="unvalidated", nullable=False),
+    Column("validated_state_message", TEXT),
+    Column("visible", Boolean),
+    Column("extended_metadata_id", Integer, ForeignKey("extended_metadata.id"), index=True),
+    Column("user_id", Integer, ForeignKey("galaxy_user.id"), index=True),
+    Column("message", TrimmedString(255)),
 )
 
+
 mapper_registry.map_imperatively(
     HistoryDatasetAssociation,
     HistoryDatasetAssociation.table,
     properties=dict(
-        dataset=relationship(Dataset,
-            primaryjoin=(Dataset.table.c.id == HistoryDatasetAssociation.table.c.dataset_id),
+        dataset=relationship(
+            Dataset,
+            primaryjoin=(lambda: Dataset.id == HistoryDatasetAssociation.table.c.dataset_id),
             lazy="joined",
-            back_populates='history_associations'),
-        copied_from_history_dataset_association=relationship(HistoryDatasetAssociation,
-            primaryjoin=(HistoryDatasetAssociation.table.c.copied_from_history_dataset_association_id
-                == HistoryDatasetAssociation.table.c.id),
+            back_populates="history_associations",
+        ),
+        copied_from_history_dataset_association=relationship(
+            HistoryDatasetAssociation,
+            primaryjoin=(
+                HistoryDatasetAssociation.table.c.copied_from_history_dataset_association_id
+                == HistoryDatasetAssociation.table.c.id
+            ),
             remote_side=[HistoryDatasetAssociation.table.c.id],
             uselist=False,
-            back_populates='copied_to_history_dataset_associations'),
-        copied_from_library_dataset_dataset_association=relationship(LibraryDatasetDatasetAssociation,
-            primaryjoin=(LibraryDatasetDatasetAssociation.table.c.id
-                == HistoryDatasetAssociation.table.c.copied_from_library_dataset_dataset_association_id),
-            back_populates='copied_to_history_dataset_associations'),
-        copied_to_history_dataset_associations=relationship(HistoryDatasetAssociation,
-            primaryjoin=(HistoryDatasetAssociation.table.c.copied_from_history_dataset_association_id
-                == HistoryDatasetAssociation.table.c.id),
-            back_populates='copied_from_history_dataset_association'),
-        copied_to_library_dataset_dataset_associations=relationship(LibraryDatasetDatasetAssociation,
-            primaryjoin=(HistoryDatasetAssociation.table.c.id
-                == LibraryDatasetDatasetAssociation.table.c.copied_from_history_dataset_association_id),
-            back_populates='copied_from_history_dataset_association'),
-        tags=relationship(HistoryDatasetAssociationTagAssociation,
+            back_populates="copied_to_history_dataset_associations",
+        ),
+        copied_from_library_dataset_dataset_association=relationship(
+            LibraryDatasetDatasetAssociation,
+            primaryjoin=(
+                LibraryDatasetDatasetAssociation.table.c.id
+                == HistoryDatasetAssociation.table.c.copied_from_library_dataset_dataset_association_id
+            ),
+            back_populates="copied_to_history_dataset_associations",
+        ),
+        copied_to_history_dataset_associations=relationship(
+            HistoryDatasetAssociation,
+            primaryjoin=(
+                HistoryDatasetAssociation.table.c.copied_from_history_dataset_association_id
+                == HistoryDatasetAssociation.table.c.id
+            ),
+            back_populates="copied_from_history_dataset_association",
+        ),
+        copied_to_library_dataset_dataset_associations=relationship(
+            LibraryDatasetDatasetAssociation,
+            primaryjoin=(
+                HistoryDatasetAssociation.table.c.id
+                == LibraryDatasetDatasetAssociation.table.c.copied_from_history_dataset_association_id
+            ),
+            back_populates="copied_from_history_dataset_association",
+        ),
+        tags=relationship(
+            HistoryDatasetAssociationTagAssociation,
             order_by=HistoryDatasetAssociationTagAssociation.id,
-            back_populates='history_dataset_association'),
-        annotations=relationship(HistoryDatasetAssociationAnnotationAssociation,
+            back_populates="history_dataset_association",
+        ),
+        annotations=relationship(
+            HistoryDatasetAssociationAnnotationAssociation,
             order_by=HistoryDatasetAssociationAnnotationAssociation.id,
-            back_populates="hda"),
-        ratings=relationship(HistoryDatasetAssociationRatingAssociation,
+            back_populates="hda",
+        ),
+        ratings=relationship(
+            HistoryDatasetAssociationRatingAssociation,
             order_by=HistoryDatasetAssociationRatingAssociation.id,
-            back_populates="history_dataset_association"),
-        extended_metadata=relationship(ExtendedMetadata,
-            primaryjoin=(HistoryDatasetAssociation.table.c.extended_metadata_id
-                == ExtendedMetadata.id)),
-        hidden_beneath_collection_instance=relationship(HistoryDatasetCollectionAssociation,
-            primaryjoin=(HistoryDatasetAssociation.table.c.hidden_beneath_collection_instance_id
-                == HistoryDatasetCollectionAssociation.id),
-            uselist=False),
+            back_populates="history_dataset_association",
+        ),
+        extended_metadata=relationship(
+            ExtendedMetadata,
+            primaryjoin=(HistoryDatasetAssociation.table.c.extended_metadata_id == ExtendedMetadata.id),
+        ),
+        hidden_beneath_collection_instance=relationship(
+            HistoryDatasetCollectionAssociation,
+            primaryjoin=(
+                HistoryDatasetAssociation.table.c.hidden_beneath_collection_instance_id
+                == HistoryDatasetCollectionAssociation.id
+            ),
+            uselist=False,
+        ),
         _metadata=deferred(HistoryDatasetAssociation.table.c._metadata),
-        dependent_jobs=relationship(JobToInputDatasetAssociation, back_populates='dataset'),
-        creating_job_associations=relationship(
-            JobToOutputDatasetAssociation, back_populates='dataset'),
-        history=relationship(History, back_populates='datasets'),
-        implicitly_converted_datasets=relationship(ImplicitlyConvertedDatasetAssociation,
-            primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_parent_id
-                == HistoryDatasetAssociation.id),
-            back_populates='parent_hda'),
-        implicitly_converted_parent_datasets=relationship(ImplicitlyConvertedDatasetAssociation,
-            primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_id
-                == HistoryDatasetAssociation.id),
-            back_populates='dataset')
-    )
+        dependent_jobs=relationship(JobToInputDatasetAssociation, back_populates="dataset"),
+        creating_job_associations=relationship(JobToOutputDatasetAssociation, back_populates="dataset"),
+        history=relationship(History, back_populates="datasets", cascade_backrefs=False),
+        implicitly_converted_datasets=relationship(
+            ImplicitlyConvertedDatasetAssociation,
+            primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_parent_id == HistoryDatasetAssociation.id),
+            back_populates="parent_hda",
+        ),
+        implicitly_converted_parent_datasets=relationship(
+            ImplicitlyConvertedDatasetAssociation,
+            primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.hda_id == HistoryDatasetAssociation.id),
+            back_populates="dataset",
+        ),
+    ),
 )
 
 mapper_registry.map_imperatively(
     LibraryDatasetDatasetAssociation,
     LibraryDatasetDatasetAssociation.table,
     properties=dict(
-        dataset=relationship(Dataset,
-            primaryjoin=(LibraryDatasetDatasetAssociation.table.c.dataset_id == Dataset.table.c.id),
-            back_populates='library_associations'),
-        library_dataset=relationship(LibraryDataset,
-            foreign_keys=LibraryDatasetDatasetAssociation.table.c.library_dataset_id),
+        dataset=relationship(
+            Dataset,
+            primaryjoin=(lambda: LibraryDatasetDatasetAssociation.table.c.dataset_id == Dataset.id),
+            back_populates="library_associations",
+        ),
+        library_dataset=relationship(
+            LibraryDataset, foreign_keys=LibraryDatasetDatasetAssociation.table.c.library_dataset_id
+        ),
         user=relationship(User),
-        copied_from_library_dataset_dataset_association=relationship(LibraryDatasetDatasetAssociation,
+        copied_from_library_dataset_dataset_association=relationship(
+            LibraryDatasetDatasetAssociation,
             primaryjoin=(
                 LibraryDatasetDatasetAssociation.table.c.copied_from_library_dataset_dataset_association_id
-                == LibraryDatasetDatasetAssociation.table.c.id),
+                == LibraryDatasetDatasetAssociation.table.c.id
+            ),
             remote_side=[LibraryDatasetDatasetAssociation.table.c.id],
             uselist=False,
-            back_populates='copied_to_library_dataset_dataset_associations'),
-        copied_to_library_dataset_dataset_associations=relationship(LibraryDatasetDatasetAssociation,
+            back_populates="copied_to_library_dataset_dataset_associations",
+        ),
+        copied_to_library_dataset_dataset_associations=relationship(
+            LibraryDatasetDatasetAssociation,
             primaryjoin=(
                 LibraryDatasetDatasetAssociation.table.c.copied_from_library_dataset_dataset_association_id
-                == LibraryDatasetDatasetAssociation.table.c.id),
-            back_populates='copied_from_library_dataset_dataset_association'),
-        copied_to_history_dataset_associations=relationship(HistoryDatasetAssociation,
-            primaryjoin=(LibraryDatasetDatasetAssociation.table.c.id
-                == HistoryDatasetAssociation.table.c.copied_from_library_dataset_dataset_association_id),
-            back_populates='copied_from_library_dataset_dataset_association'),
-        implicitly_converted_datasets=relationship(ImplicitlyConvertedDatasetAssociation,
-            primaryjoin=(ImplicitlyConvertedDatasetAssociation.ldda_parent_id
-                         == LibraryDatasetDatasetAssociation.table.c.id),
-            back_populates='parent_ldda'),
-        tags=relationship(LibraryDatasetDatasetAssociationTagAssociation,
-                      order_by=LibraryDatasetDatasetAssociationTagAssociation.id,
-                      back_populates='library_dataset_dataset_association'),
-        extended_metadata=relationship(ExtendedMetadata,
-            primaryjoin=(LibraryDatasetDatasetAssociation.table.c.extended_metadata_id
-                == ExtendedMetadata.id)
+                == LibraryDatasetDatasetAssociation.table.c.id
+            ),
+            back_populates="copied_from_library_dataset_dataset_association",
+        ),
+        copied_to_history_dataset_associations=relationship(
+            HistoryDatasetAssociation,
+            primaryjoin=(
+                LibraryDatasetDatasetAssociation.table.c.id
+                == HistoryDatasetAssociation.table.c.copied_from_library_dataset_dataset_association_id
+            ),
+            back_populates="copied_from_library_dataset_dataset_association",
+        ),
+        implicitly_converted_datasets=relationship(
+            ImplicitlyConvertedDatasetAssociation,
+            primaryjoin=(
+                ImplicitlyConvertedDatasetAssociation.ldda_parent_id == LibraryDatasetDatasetAssociation.table.c.id
+            ),
+            back_populates="parent_ldda",
+        ),
+        tags=relationship(
+            LibraryDatasetDatasetAssociationTagAssociation,
+            order_by=LibraryDatasetDatasetAssociationTagAssociation.id,
+            back_populates="library_dataset_dataset_association",
+        ),
+        extended_metadata=relationship(
+            ExtendedMetadata,
+            primaryjoin=(LibraryDatasetDatasetAssociation.table.c.extended_metadata_id == ExtendedMetadata.id),
         ),
         _metadata=deferred(LibraryDatasetDatasetAssociation.table.c._metadata),
         actions=relationship(
-            LibraryDatasetDatasetAssociationPermissions,
-            back_populates='library_dataset_dataset_association'),
-        dependent_jobs=relationship(
-            JobToInputLibraryDatasetAssociation, back_populates='dataset'),
-        creating_job_associations=relationship(
-            JobToOutputLibraryDatasetAssociation, back_populates='dataset'),
-        implicitly_converted_parent_datasets=relationship(ImplicitlyConvertedDatasetAssociation,
-            primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.ldda_id
-                == LibraryDatasetDatasetAssociation.id),
-            back_populates='dataset_ldda'),
-        copied_from_history_dataset_association=relationship(HistoryDatasetAssociation,
-            primaryjoin=(HistoryDatasetAssociation.table.c.id
-                == LibraryDatasetDatasetAssociation.table.c.copied_from_history_dataset_association_id),
-            back_populates='copied_to_library_dataset_dataset_associations'),
-    )
+            LibraryDatasetDatasetAssociationPermissions, back_populates="library_dataset_dataset_association"
+        ),
+        dependent_jobs=relationship(JobToInputLibraryDatasetAssociation, back_populates="dataset"),
+        creating_job_associations=relationship(JobToOutputLibraryDatasetAssociation, back_populates="dataset"),
+        implicitly_converted_parent_datasets=relationship(
+            ImplicitlyConvertedDatasetAssociation,
+            primaryjoin=(lambda: ImplicitlyConvertedDatasetAssociation.ldda_id == LibraryDatasetDatasetAssociation.id),
+            back_populates="dataset_ldda",
+        ),
+        copied_from_history_dataset_association=relationship(
+            HistoryDatasetAssociation,
+            primaryjoin=(
+                HistoryDatasetAssociation.table.c.id
+                == LibraryDatasetDatasetAssociation.table.c.copied_from_history_dataset_association_id
+            ),
+            back_populates="copied_to_library_dataset_dataset_associations",
+        ),
+    ),
 )
 
 # ----------------------------------------------------------------------------------------
 # The following statements must not precede the mapped models defined above.
 
 Job.any_output_dataset_collection_instances_deleted = column_property(
-    exists(HistoryDatasetCollectionAssociation.id).where(and_(
-        Job.id == JobToOutputDatasetCollectionAssociation.job_id,
-        HistoryDatasetCollectionAssociation.id == JobToOutputDatasetCollectionAssociation.dataset_collection_id,
-        HistoryDatasetCollectionAssociation.deleted == true())
+    exists(HistoryDatasetCollectionAssociation.id).where(
+        and_(
+            Job.id == JobToOutputDatasetCollectionAssociation.job_id,
+            HistoryDatasetCollectionAssociation.id == JobToOutputDatasetCollectionAssociation.dataset_collection_id,
+            HistoryDatasetCollectionAssociation.deleted == true(),
+        )
     )
 )
 
 Job.any_output_dataset_deleted = column_property(
-    exists(HistoryDatasetAssociation).where(and_(
-        Job.id == JobToOutputDatasetAssociation.job_id,
-        HistoryDatasetAssociation.table.c.id == JobToOutputDatasetAssociation.dataset_id,
-        HistoryDatasetAssociation.table.c.deleted == true())
+    exists(HistoryDatasetAssociation).where(
+        and_(
+            Job.id == JobToOutputDatasetAssociation.job_id,
+            HistoryDatasetAssociation.table.c.id == JobToOutputDatasetAssociation.dataset_id,
+            HistoryDatasetAssociation.table.c.deleted == true(),
+        )
     )
 )
 
 History.average_rating = column_property(
-    select(func.avg(HistoryRatingAssociation.rating)).where(
-        HistoryRatingAssociation.history_id == History.id).scalar_subquery(),
-    deferred=True
+    select(func.avg(HistoryRatingAssociation.rating))
+    .where(HistoryRatingAssociation.history_id == History.id)
+    .scalar_subquery(),
+    deferred=True,
 )
 
 History.users_shared_with_count = column_property(
-    select(func.count(HistoryUserShareAssociation.id)).where(
-        History.id == HistoryUserShareAssociation.history_id).scalar_subquery(),
-    deferred=True
+    select(func.count(HistoryUserShareAssociation.id))
+    .where(History.id == HistoryUserShareAssociation.history_id)
+    .scalar_subquery(),
+    deferred=True,
 )
 
 Page.average_rating = column_property(
-    select(func.avg(PageRatingAssociation.rating)).where(
-        PageRatingAssociation.page_id == Page.id).scalar_subquery(),
-    deferred=True
+    select(func.avg(PageRatingAssociation.rating)).where(PageRatingAssociation.page_id == Page.id).scalar_subquery(),
+    deferred=True,
 )
 
 StoredWorkflow.average_rating = column_property(
-    select(func.avg(StoredWorkflowRatingAssociation.rating)).where(
-        StoredWorkflowRatingAssociation.stored_workflow_id == StoredWorkflow.id).scalar_subquery(),
-    deferred=True
+    select(func.avg(StoredWorkflowRatingAssociation.rating))
+    .where(StoredWorkflowRatingAssociation.stored_workflow_id == StoredWorkflow.id)
+    .scalar_subquery(),
+    deferred=True,
 )
 
 Visualization.average_rating = column_property(
-    select(func.avg(VisualizationRatingAssociation.rating)).where(
-        VisualizationRatingAssociation.visualization_id == Visualization.id).scalar_subquery(),
-    deferred=True
+    select(func.avg(VisualizationRatingAssociation.rating))
+    .where(VisualizationRatingAssociation.visualization_id == Visualization.id)
+    .scalar_subquery(),
+    deferred=True,
 )
 
 Workflow.step_count = column_property(
-    select(func.count(WorkflowStep.id)).where(Workflow.id == WorkflowStep.workflow_id).scalar_subquery(),
-    deferred=True
+    select(func.count(WorkflowStep.id)).where(Workflow.id == WorkflowStep.workflow_id).scalar_subquery(), deferred=True
 )
 
 WorkflowInvocationStep.subworkflow_invocation_id = column_property(
-    select(WorkflowInvocationToSubworkflowInvocationAssociation.subworkflow_invocation_id).where(and_(
-        WorkflowInvocationToSubworkflowInvocationAssociation.workflow_invocation_id == WorkflowInvocationStep.workflow_invocation_id,
-        WorkflowInvocationToSubworkflowInvocationAssociation.workflow_step_id == WorkflowInvocationStep.workflow_step_id,
-    )).scalar_subquery(),
+    select(WorkflowInvocationToSubworkflowInvocationAssociation.subworkflow_invocation_id)
+    .where(
+        and_(
+            WorkflowInvocationToSubworkflowInvocationAssociation.workflow_invocation_id
+            == WorkflowInvocationStep.workflow_invocation_id,
+            WorkflowInvocationToSubworkflowInvocationAssociation.workflow_step_id
+            == WorkflowInvocationStep.workflow_step_id,
+        )
+    )
+    .scalar_subquery(),
 )
 
 # Set up proxy so that this syntax is possible:
 # <user_obj>.preferences[pref_name] = pref_value
-User.preferences = association_proxy('_preferences', 'value', creator=UserPreference)
+User.preferences = association_proxy("_preferences", "value", creator=UserPreference)
+
+
+@event.listens_for(HistoryDatasetCollectionAssociation, "init")
+def receive_init(target, args, kwargs):
+    """
+    Listens for the 'init' event. This is not called when 'target' is loaded from the database.
+    https://docs.sqlalchemy.org/en/14/orm/events.html#sqlalchemy.orm.InstanceEvents.init
+
+    Addresses SQLAlchemy 2.0 compatibility issue: see inline documentation for
+    `add_object_to_object_session` in galaxy.model.orm.util.
+    """
+    for key in ("history", "copied_from_history_dataset_collection_association"):
+        obj = kwargs.get(key)
+        if obj:
+            add_object_to_object_session(target, obj)
+            return  # Once is enough.
```

### Comparing `galaxy-data-22.1.1/galaxy/model/base.py` & `galaxy-data-23.0.1/galaxy/model/base.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,38 +3,39 @@
 generalize to generic database connections.
 """
 import os
 import threading
 from contextvars import ContextVar
 from inspect import (
     getmembers,
-    isclass
+    isclass,
+)
+from typing import (
+    Dict,
+    Type,
 )
-from typing import Dict, Type
 
 from sqlalchemy import event
 from sqlalchemy.orm import (
     scoped_session,
-    sessionmaker
+    sessionmaker,
 )
 
 from galaxy.util.bunch import Bunch
 
-
 # Create a ContextVar with mutable state, this allows sync tasks in the context
 # of a request (which run within a threadpool) to see changes to the ContextVar
 # state. See https://github.com/tiangolo/fastapi/issues/953#issuecomment-586006249
 # for details
 _request_state: Dict[str, str] = {}
-REQUEST_ID = ContextVar('request_id', default=_request_state.copy())
+REQUEST_ID = ContextVar("request_id", default=_request_state.copy())
 
 
 # TODO: Refactor this to be a proper class, not a bunch.
 class ModelMapping(Bunch):
-
     def __init__(self, model_modules, engine):
         self.engine = engine
         SessionLocal = sessionmaker(autoflush=False, autocommit=True)
         versioned_session(SessionLocal)
         context = scoped_session(SessionLocal, scopefunc=self.request_scopefunc)
         # For backward compatibility with "context.current"
         # deprecated?
@@ -56,34 +57,34 @@
     def request_scopefunc(self):
         """
         Return a value that is used as dictionary key for sqlalchemy's ScopedRegistry.
 
         This ensures that threads or request contexts will receive a single identical session
         from the ScopedRegistry.
         """
-        return REQUEST_ID.get().get('request') or threading.get_ident()
+        return REQUEST_ID.get().get("request") or threading.get_ident()
 
     @staticmethod
     def set_request_id(request_id):
         # Set REQUEST_ID to a new dict.
         # This new ContextVar value will only be seen by the current asyncio context
         # and descendant threadpools, but not other threads or asyncio contexts.
-        return REQUEST_ID.set({'request': request_id})
+        return REQUEST_ID.set({"request": request_id})
 
     def unset_request_id(self, request_id):
         # Unconditionally calling self.gx_app.model.session.remove()
         # would create a new session if the session was not accessed
         # in a request, so we check if there is a sqlalchemy session
         # for the current request in the registry.
         if request_id in self.scoped_registry.registry:
             self.scoped_registry.registry[request_id].close()
             del self.scoped_registry.registry[request_id]
 
     @property
-    def context(self):
+    def context(self) -> scoped_session:
         return self.session
 
     @property
     def Session(self):
         """
         For backward compat., deprecated.
         """
@@ -92,44 +93,45 @@
 
 class SharedModelMapping(ModelMapping):
     """Model mapping containing references to classes shared between Galaxy and ToolShed.
 
     Generally things can be more strongly typed when importing models directly, but we need
     a way to do app.model.<CLASS> for common code shared by the tool shed and Galaxy.
     """
+
     User: Type
     GalaxySession: Type
     APIKeys: Type
     PasswordResetToken: Type
 
 
 def versioned_objects(iter):
     for obj in iter:
-        if hasattr(obj, '__create_version__'):
+        if hasattr(obj, "__create_version__"):
             yield obj
 
 
 def versioned_objects_strict(iter):
     for obj in iter:
-        if hasattr(obj, '__create_version__'):
+        if hasattr(obj, "__create_version__"):
             if obj.extension != "len":
                 # TODO: Custom builds (with .len extension) do not get a history or a HID.
                 # These should get some other type of permanent storage, perhaps UserDatasetAssociation ?
                 # Everything else needs to have a hid and a history
                 if not obj.history and not obj.history_id:
-                    raise Exception(f'HistoryDatsetAssociation {obj} without history detected, this is not valid')
+                    raise Exception(f"HistoryDatsetAssociation {obj} without history detected, this is not valid")
                 elif not obj.hid:
-                    raise Exception(f'HistoryDatsetAssociation {obj} without has no hid, this is not valid')
+                    raise Exception(f"HistoryDatsetAssociation {obj} without has no hid, this is not valid")
             yield obj
 
 
 if os.environ.get("GALAXY_TEST_RAISE_EXCEPTION_ON_HISTORYLESS_HDA"):
     versioned_objects = versioned_objects_strict  # noqa: F811
 
 
 def versioned_session(session):
-    @event.listens_for(session, 'before_flush')
+    @event.listens_for(session, "before_flush")
     def before_flush(session, flush_context, instances):
         for obj in versioned_objects(session.dirty):
             obj.__create_version__(session)
         for obj in versioned_objects(session.deleted):
             obj.__create_version__(session, deleted=True)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/custom_types.py` & `galaxy-data-23.0.1/galaxy/model/custom_types.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,20 +12,20 @@
 import sqlalchemy
 from sqlalchemy.ext.mutable import Mutable
 from sqlalchemy.inspection import inspect
 from sqlalchemy.types import (
     CHAR,
     LargeBinary,
     String,
-    TypeDecorator
+    TypeDecorator,
 )
 
 from galaxy.util import (
     smart_str,
-    unicodify
+    unicodify,
 )
 from galaxy.util.aliaspickler import AliasPickleModule
 
 log = logging.getLogger(__name__)
 
 
 class SafeJsonEncoder(json.JSONEncoder):
@@ -48,38 +48,38 @@
 
 
 def _sniffnfix_pg9_hex(value):
     """
     Sniff for and fix postgres 9 hex decoding issue
     """
     try:
-        if value[0] == 'x':
+        if value[0] == "x":
             return binascii.unhexlify(value[1:])
-        elif smart_str(value).startswith(b'\\x'):
+        elif smart_str(value).startswith(b"\\x"):
             return binascii.unhexlify(value[2:])
         else:
             return value
     except Exception:
         return value
 
 
 class GalaxyLargeBinary(LargeBinary):
-
     # This hack is necessary because the LargeBinary result processor
     # does not specify an encoding in the `bytes` call ,
     # likely because `result` should be binary.
     # This doesn't seem to be the case in galaxy.
     def result_processor(self, dialect, coltype):
         def process(value):
             if value is not None:
                 if isinstance(value, str):
-                    value = bytes(value, encoding='utf-8')
+                    value = bytes(value, encoding="utf-8")
                 else:
                     value = bytes(value)
             return value
+
         return process
 
 
 class JSONType(TypeDecorator):
     """
     Represents an immutable structure as a json-encoded string.
 
@@ -109,15 +109,15 @@
         else:
             return self.impl
 
     def copy_value(self, value):
         return copy.deepcopy(value)
 
     def compare_values(self, x, y):
-        return (x == y)
+        return x == y
 
 
 class MutableJSONType(JSONType):
     """Associated with MutationObj"""
 
 
 class MutationObj(Mutable):
@@ -169,28 +169,28 @@
             if isinstance(oldvalue, cls):
                 oldvalue._parents.pop(inspect(target), None)
             return value
 
         def pickle(state, state_dict):
             val = state.dict.get(key, None)
             if isinstance(val, cls):
-                if 'ext.mutable.values' not in state_dict:
-                    state_dict['ext.mutable.values'] = []
-                state_dict['ext.mutable.values'].append(val)
+                if "ext.mutable.values" not in state_dict:
+                    state_dict["ext.mutable.values"] = []
+                state_dict["ext.mutable.values"].append(val)
 
         def unpickle(state, state_dict):
-            if 'ext.mutable.values' in state_dict:
-                for val in state_dict['ext.mutable.values']:
+            if "ext.mutable.values" in state_dict:
+                for val in state_dict["ext.mutable.values"]:
                     val._parents[state] = key
 
-        sqlalchemy.event.listen(parent_cls, 'load', load, raw=True, propagate=True)
-        sqlalchemy.event.listen(parent_cls, 'refresh', load, raw=True, propagate=True)
-        sqlalchemy.event.listen(attribute, 'set', set, raw=True, retval=True, propagate=True)
-        sqlalchemy.event.listen(parent_cls, 'pickle', pickle, raw=True, propagate=True)
-        sqlalchemy.event.listen(parent_cls, 'unpickle', unpickle, raw=True, propagate=True)
+        sqlalchemy.event.listen(parent_cls, "load", load, raw=True, propagate=True)
+        sqlalchemy.event.listen(parent_cls, "refresh", load, raw=True, propagate=True)
+        sqlalchemy.event.listen(attribute, "set", set, raw=True, retval=True, propagate=True)
+        sqlalchemy.event.listen(parent_cls, "pickle", pickle, raw=True, propagate=True)
+        sqlalchemy.event.listen(parent_cls, "unpickle", unpickle, raw=True, propagate=True)
 
 
 class MutationDict(MutationObj, dict):
     @classmethod
     def coerce(cls, key, value):
         """Convert plain dictionary to MutationDict"""
         self = MutationDict((k, MutationObj.coerce(key, v)) for (k, v) in value.items())
@@ -275,21 +275,19 @@
         super().remove(value)
         self.changed()
 
 
 MutationObj.associate_with(MutableJSONType)
 
 
-metadata_pickler = AliasPickleModule({
-    ("cookbook.patterns", "Bunch"): ("galaxy.util.bunch", "Bunch")
-})
+metadata_pickler = AliasPickleModule({("cookbook.patterns", "Bunch"): ("galaxy.util.bunch", "Bunch")})
 
 
 def total_size(o, handlers=None, verbose=False):
-    """ Returns the approximate memory footprint an object and all of its contents.
+    """Returns the approximate memory footprint an object and all of its contents.
 
     Automatically finds the contents of the following builtin containers and
     their subclasses:  tuple, list, deque, dict, set and frozenset.
     To search other containers, add handlers to iterate over their contents:
 
         handlers = {SomeContainerClass: iter,
                     OtherContainerClass: OtherContainerClass.get_elements}
@@ -297,26 +295,21 @@
     Recipe from:  https://code.activestate.com/recipes/577504-compute-memory-footprint-of-an-object-and-its-cont/
     """
     handlers = handlers or {}
 
     def dict_handler(d):
         return chain.from_iterable(d.items())
 
-    all_handlers = {tuple: iter,
-                    list: iter,
-                    deque: iter,
-                    dict: dict_handler,
-                    set: iter,
-                    frozenset: iter}
-    all_handlers.update(handlers)     # user handlers take precedence
-    seen = set()                      # track which object id's have already been seen
-    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__
+    all_handlers = {tuple: iter, list: iter, deque: iter, dict: dict_handler, set: iter, frozenset: iter}
+    all_handlers.update(handlers)  # user handlers take precedence
+    seen = set()  # track which object id's have already been seen
+    default_size = getsizeof(0)  # estimate sizeof object without __sizeof__
 
     def sizeof(o):
-        if id(o) in seen:       # do not double count the same object
+        if id(o) in seen:  # do not double count the same object
             return 0
         seen.add(id(o))
         s = getsizeof(o, default_size)
 
         for typ, handler in all_handlers.items():
             if isinstance(o, typ):
                 s += sum(map(sizeof, handler(o)))
@@ -328,22 +321,24 @@
 
 class MetadataType(JSONType):
     """
     Backward compatible metadata type. Can read pickles or JSON, but always
     writes in JSON.
     """
 
+    cache_ok = True
+
     def process_bind_param(self, value, dialect):
         if value is not None:
             if MAX_METADATA_VALUE_SIZE is not None:
                 for k, v in list(value.items()):
                     sz = total_size(v)
                     if sz > MAX_METADATA_VALUE_SIZE:
                         del value[k]
-                        log.warning(f'Refusing to bind metadata key {k} due to size ({sz})')
+                        log.warning(f"Refusing to bind metadata key {k} due to size ({sz})")
             value = json_encoder.encode(value).encode()
         return value
 
     def process_result_value(self, value, dialect):
         if value is None:
             return None
         ret = None
@@ -364,14 +359,15 @@
     Platform-independent UUID type.
 
     Based on http://docs.sqlalchemy.org/en/rel_0_8/core/types.html#backend-agnostic-guid-type
     Changed to remove sqlalchemy 0.8 specific code
 
     CHAR(32), storing as stringified hex values.
     """
+
     impl = CHAR
     cache_ok = True
 
     def load_dialect_impl(self, dialect):
         return dialect.type_descriptor(CHAR(32))
 
     def process_bind_param(self, value, dialect):
@@ -392,9 +388,9 @@
 class TrimmedString(TypeDecorator):
     impl = String
     cache_ok = True
 
     def process_bind_param(self, value, dialect):
         """Automatically truncate string values"""
         if self.impl.length and value is not None:
-            value = unicodify(value)[0:self.impl.length]
+            value = unicodify(value)[0 : self.impl.length]
         return value
```

### Comparing `galaxy-data-22.1.1/galaxy/model/database_heartbeat.py` & `galaxy-data-23.0.1/galaxy/model/database_heartbeat.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 from galaxy.model import WorkerProcess
 from galaxy.model.orm.now import now
 
 log = logging.getLogger(__name__)
 
 
 class DatabaseHeartbeat:
-
     def __init__(self, application_stack, heartbeat_interval=60):
         self.application_stack = application_stack
         self.heartbeat_interval = heartbeat_interval
         self.hostname = socket.gethostname()
         self._is_config_watcher = False
         self._observers = []
         self.exit = threading.Event()
@@ -30,30 +29,32 @@
     @property
     def server_name(self):
         # Application stack manipulates server name after forking
         return self.application_stack.app.config.server_name
 
     def start(self):
         if not self.active:
-            self.thread = threading.Thread(target=self.send_database_heartbeat, name=f"database_heartbeart_{self.server_name}.thread")
+            self.thread = threading.Thread(
+                target=self.send_database_heartbeat, name=f"database_heartbeart_{self.server_name}.thread"
+            )
             self.thread.daemon = True
             self.active = True
             self.thread.start()
             self.pid = os.getpid()
 
     def shutdown(self):
         self.active = False
         self.exit.set()
         if self.thread:
             self.thread.join()
         worker_process = self.worker_process
         if worker_process:
             self.sa_session.delete(worker_process)
             self.sa_session.flush()
-            self.application_stack.app.queue_worker.send_control_task('reconfigure_watcher', noop_self=True)
+            self.application_stack.app.queue_worker.send_control_task("reconfigure_watcher", noop_self=True)
 
     def get_active_processes(self, last_seen_seconds=None):
         """Return all processes seen in ``last_seen_seconds`` seconds."""
         if last_seen_seconds is None:
             last_seen_seconds = self.heartbeat_interval
         seconds_ago = now() - datetime.timedelta(seconds=last_seen_seconds)
         return self.sa_session.query(WorkerProcess).filter(WorkerProcess.update_time > seconds_ago).all()
@@ -64,37 +65,43 @@
     @property
     def is_config_watcher(self):
         return self._is_config_watcher
 
     @is_config_watcher.setter
     def is_config_watcher(self, value):
         self._is_config_watcher = value
-        log.debug('%s %s config watcher', self.server_name, 'is' if self.is_config_watcher else 'is not')
+        log.debug("%s %s config watcher", self.server_name, "is" if self.is_config_watcher else "is not")
         for callback in self._observers:
             callback(self._is_config_watcher)
 
     @property
     def worker_process(self):
-        return self.sa_session.query(WorkerProcess).with_for_update(of=WorkerProcess).filter_by(
-            server_name=self.server_name,
-            hostname=self.hostname,
-        ).first()
+        return (
+            self.sa_session.query(WorkerProcess)
+            .with_for_update(of=WorkerProcess)
+            .filter_by(
+                server_name=self.server_name,
+                hostname=self.hostname,
+            )
+            .first()
+        )
 
     def update_watcher_designation(self):
         worker_process = self.worker_process
         if not worker_process:
             worker_process = WorkerProcess(server_name=self.server_name, hostname=self.hostname)
         worker_process.update_time = now()
         worker_process.pid = self.pid
         self.sa_session.add(worker_process)
         self.sa_session.flush()
         # We only want a single process watching the various config files on the file system.
         # We just pick the max server name for simplicity
         is_config_watcher = self.server_name == max(
-            p.server_name for p in self.get_active_processes(self.heartbeat_interval + 1))
+            p.server_name for p in self.get_active_processes(self.heartbeat_interval + 1)
+        )
         if is_config_watcher != self.is_config_watcher:
             self.is_config_watcher = is_config_watcher
 
     def send_database_heartbeat(self):
         if self.active:
             while not self.exit.is_set():
                 self.update_watcher_designation()
```

### Comparing `galaxy-data-22.1.1/galaxy/model/database_utils.py` & `galaxy-data-23.0.1/galaxy/model/database_utils.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import sqlite3
 from contextlib import contextmanager
+from typing import Optional
 
 from sqlalchemy import create_engine
 from sqlalchemy.engine.url import make_url
 from sqlalchemy.sql.compiler import IdentifierPreparer
 from sqlalchemy.sql.expression import text
 
 from galaxy.exceptions import ConfigurationError
@@ -14,15 +15,15 @@
 
     If database is None, use the database name from db_url.
     """
     dbm = DatabaseManager.make_manager(db_url, database)
     return dbm.exists()
 
 
-def create_database(db_url, database=None, encoding='utf8', template=None):
+def create_database(db_url, database=None, encoding="utf8", template=None):
     """Create database; connect with db_url.
 
     If database is None, use the database name from db_url.
     """
     dbm = DatabaseManager.make_manager(db_url, database)
     dbm.create(encoding, template)
 
@@ -33,82 +34,77 @@
     try:
         yield engine
     finally:
         engine.dispose()
 
 
 class DatabaseManager:
-
     @staticmethod
     def make_manager(db_url, database):
-        if db_url.startswith('postgres'):
+        if db_url.startswith("postgres"):
             return PosgresDatabaseManager(db_url, database)
-        elif db_url.startswith('sqlite'):
+        elif db_url.startswith("sqlite"):
             return SqliteDatabaseManager(db_url, database)
-        elif db_url.startswith('mysql'):
+        elif db_url.startswith("mysql"):
             return MySQLDatabaseManager(db_url, database)
         else:
-            raise ConfigurationError(f'Invalid database URL: {db_url}')
+            raise ConfigurationError(f"Invalid database URL: {db_url}")
 
     def __init__(self, db_url, database):
         self.url = make_url(db_url)
         self.database = database
         if not database:
             self._handle_no_database()
 
 
 class PosgresDatabaseManager(DatabaseManager):
-
     def _handle_no_database(self):
         self.database = self.url.database  # use database from db_url
-        self.url = self.url.set(database='postgres')
+        self.url = self.url.set(database="postgres")
 
     def exists(self):
         with sqlalchemy_engine(self.url) as engine:
-            stmt = text('SELECT 1 FROM pg_database WHERE datname=:database')
+            stmt = text("SELECT 1 FROM pg_database WHERE datname=:database")
             stmt = stmt.bindparams(database=self.database)
             with engine.connect() as conn:
                 return bool(conn.scalar(stmt))
 
     def create(self, encoding, template):
         with sqlalchemy_engine(self.url) as engine:
             preparer = IdentifierPreparer(engine.dialect)
-            template = template or 'template1'
+            template = template or "template1"
             database, template = preparer.quote(self.database), preparer.quote(template)
             stmt = f"CREATE DATABASE {database} ENCODING '{encoding}' TEMPLATE {template}"
-            with engine.connect().execution_options(isolation_level='AUTOCOMMIT') as conn:
+            with engine.connect().execution_options(isolation_level="AUTOCOMMIT") as conn:
                 conn.execute(stmt)
 
 
 class SqliteDatabaseManager(DatabaseManager):
-
     def _handle_no_database(self):
         self.database = self.url.database  # use database from db_url
 
     def exists(self):
-
         def can_connect_to_dbfile():
             try:
-                sqlite3.connect(f'file:{db}?mode=ro', uri=True)
+                sqlite3.connect(f"file:{db}?mode=ro", uri=True)
             except sqlite3.OperationalError:
                 return False
             else:
                 return True
 
         db = self.url.database
         # No database or ':memory:' creates an in-memory database
-        return not db or db == ':memory:' or can_connect_to_dbfile()
+        return not db or db == ":memory:" or can_connect_to_dbfile()
 
     def create(self, *args):
         # Ignore any args (encoding, template)
-        sqlite3.connect(f'file:{self.url.database}', uri=True)
+        sqlite3.connect(f"file:{self.url.database}", uri=True)
 
 
 class MySQLDatabaseManager(DatabaseManager):
-
     def _handle_no_database(self):
         self.database = self.url.database  # use database from db_url
 
     def exists(self):
         with sqlalchemy_engine(self.url) as engine:
             stmt = text("SELECT schema_name FROM information_schema.schemata WHERE schema_name=:database")
             stmt = stmt.bindparams(database=self.database)
@@ -117,9 +113,20 @@
 
     def create(self, encoding, *arg):
         # Ignore any args (template)
         with sqlalchemy_engine(self.url) as engine:
             preparer = IdentifierPreparer(engine.dialect)
             database = preparer.quote(self.database)
             stmt = f"CREATE DATABASE {database} CHARACTER SET = '{encoding}'"
-            with engine.connect().execution_options(isolation_level='AUTOCOMMIT') as conn:
+            with engine.connect().execution_options(isolation_level="AUTOCOMMIT") as conn:
                 conn.execute(stmt)
+
+
+def is_one_database(db1_url: str, db2_url: Optional[str]):
+    """
+    Check if the arguments refer to one database. This will be true
+    if only one argument is passed, or if the urls are the same.
+    URLs are strings, so sameness is determined via string comparison.
+    """
+    # TODO: Consider more aggressive check here that this is not the same
+    # database file under the hood.
+    return not (db1_url and db2_url and db1_url != db2_url)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/builder.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from galaxy import model
+from galaxy.model.orm.util import add_object_to_object_session
 from galaxy.util.oset import OrderedSet
 from .type_description import COLLECTION_TYPE_DESCRIPTION_FACTORY
 
 
 def build_collection(type, dataset_instances, collection=None, associated_identifiers=None):
     """
     Build DatasetCollection with populated DatasetcollectionElement objects
@@ -19,26 +20,27 @@
     new_element_keys = OrderedSet(dataset_instances.keys()) - associated_identifiers
     new_dataset_instances = {k: dataset_instances[k] for k in new_element_keys}
     dataset_collection.element_count = dataset_collection.element_count or 0
     element_index = dataset_collection.element_count
     elements = []
     for element in type.generate_elements(new_dataset_instances):
         element.element_index = element_index
+        add_object_to_object_session(element, dataset_collection)
         element.collection = dataset_collection
         elements.append(element)
 
         element_index += 1
         associated_identifiers.add(element.element_identifier)
 
     dataset_collection.element_count = element_index
     return dataset_collection
 
 
 class CollectionBuilder:
-    """ Purely functional builder pattern for building a dataset collection. """
+    """Purely functional builder pattern for building a dataset collection."""
 
     def __init__(self, collection_type_description):
         self._collection_type_description = collection_type_description
         self._current_elements = {}
         # Store collection here so we don't recreate the collection all the time
         self.collection = None
         self.associated_identifiers = set()
@@ -53,31 +55,30 @@
         elements = {}
         for element in template_collection.elements:
             if element.is_collection:
                 collection_builder = CollectionBuilder(
                     collection_type_description=self._collection_type_description.child_collection_type_description()
                 )
                 collection_builder.replace_elements_in_collection(
-                    template_collection=element.child_collection,
-                    replacement_dict=replacement_dict
+                    template_collection=element.child_collection, replacement_dict=replacement_dict
                 )
                 elements[element.element_identifier] = collection_builder
             else:
-                elements[element.element_identifier] = replacement_dict.get(element.element_object, element.element_object)
+                elements[element.element_identifier] = replacement_dict.get(
+                    element.element_object, element.element_object
+                )
         return elements
 
     def get_level(self, identifier):
         if not self._nested_collection:
             message_template = "Cannot add nested collection to collection of type [%s]"
             message = message_template % (self._collection_type_description)
             raise AssertionError(message)
         if identifier not in self._current_elements:
-            subcollection_builder = CollectionBuilder(
-                self._subcollection_type_description
-            )
+            subcollection_builder = CollectionBuilder(self._subcollection_type_description)
             self._current_elements[identifier] = subcollection_builder
 
         return self._current_elements[identifier]
 
     def add_dataset(self, identifier, dataset_instance):
         self._current_elements[identifier] = dataset_instance
 
@@ -90,29 +91,31 @@
             elements = new_elements
         else:
             self._current_elements = {}
         return elements
 
     def build(self):
         type_plugin = self._collection_type_description.rank_type_plugin()
-        self.collection = build_collection(type_plugin, self.build_elements(), self.collection, self.associated_identifiers)
+        self.collection = build_collection(
+            type_plugin, self.build_elements(), self.collection, self.associated_identifiers
+        )
         self.collection.collection_type = self._collection_type_description.collection_type
         return self.collection
 
     @property
     def _subcollection_type_description(self):
         return self._collection_type_description.subcollection_type_description()
 
     @property
     def _nested_collection(self):
         return self._collection_type_description.has_subcollections()
 
 
 class BoundCollectionBuilder(CollectionBuilder):
-    """ More stateful builder that is bound to a particular model object. """
+    """More stateful builder that is bound to a particular model object."""
 
     def __init__(self, dataset_collection):
         self.dataset_collection = dataset_collection
         if dataset_collection.populated:
             raise Exception("Cannot reset elements of an already populated dataset collection.")
         collection_type = dataset_collection.collection_type
         collection_type_description = COLLECTION_TYPE_DESCRIPTION_FACTORY.for_collection_type(collection_type)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/matching.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/matching.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,21 @@
+from typing import Optional
+
 from galaxy import exceptions
 from galaxy.util import bunch
 from .structure import (
     get_structure,
-    leaf
+    leaf,
 )
 
 CANNOT_MATCH_ERROR_MESSAGE = "Cannot match collection types."
 
 
 class CollectionsToMatch:
-    """ Structure representing a set of collections that need to be matched up
+    """Structure representing a set of collections that need to be matched up
     when running tools (possibly workflows in the future as well).
     """
 
     def __init__(self):
         self.collections = {}
 
     def add(self, input_name, hdca, subcollection_type=None, linked=True):
@@ -27,43 +29,45 @@
         return len(self.collections) > 0
 
     def items(self):
         return self.collections.items()
 
 
 class MatchingCollections:
-    """ Structure holding the result of matching a list of collections
+    """Structure holding the result of matching a list of collections
     together. This class being different than the class above and being
     created in the DatasetCollectionManager layer may seem like
     overkill but I suspect in the future plugins will be subtypable for
     instance so matching collections will need to make heavy use of the
     dataset collection type registry managed by the dataset collections
-    sevice - hence the complexity now.
+    service - hence the complexity now.
     """
 
     def __init__(self):
         self.linked_structure = None
         self.unlinked_structures = []
         self.collections = {}
         self.subcollection_types = {}
         self.action_tuples = {}
+        self.when_values = None
 
     def __attempt_add_to_linked_match(self, input_name, hdca, collection_type_description, subcollection_type):
         structure = get_structure(hdca, collection_type_description, leaf_subcollection_type=subcollection_type)
         if not self.linked_structure:
             self.linked_structure = structure
             self.collections[input_name] = hdca
             self.subcollection_types[input_name] = subcollection_type
         else:
             if not self.linked_structure.can_match(structure):
                 raise exceptions.MessageException(CANNOT_MATCH_ERROR_MESSAGE)
             self.collections[input_name] = hdca
             self.subcollection_types[input_name] = subcollection_type
 
     def slice_collections(self):
+        self.linked_structure.when_values = self.when_values
         return self.linked_structure.walk_collections(self.collections)
 
     def subcollection_mapping_type(self, input_name):
         return self.subcollection_types[input_name]
 
     @property
     def structure(self):
@@ -71,36 +75,41 @@
         effective_structure = leaf
         for unlinked_structure in self.unlinked_structures:
             effective_structure = effective_structure.multiply(unlinked_structure)
         linked_structure = self.linked_structure
         if linked_structure is None:
             linked_structure = leaf
         effective_structure = effective_structure.multiply(linked_structure)
+        effective_structure.when_values = self.when_values
         return None if effective_structure.is_leaf else effective_structure
 
     def map_over_action_tuples(self, input_name):
         if input_name not in self.action_tuples:
             collection_instance = self.collections[input_name]
             self.action_tuples[input_name] = collection_instance.collection.dataset_action_tuples
         return self.action_tuples[input_name]
 
     def is_mapped_over(self, input_name):
         return input_name in self.collections
 
     @staticmethod
-    def for_collections(collections_to_match, collection_type_descriptions):
+    def for_collections(collections_to_match, collection_type_descriptions) -> Optional["MatchingCollections"]:
         if not collections_to_match.has_collections():
             return None
 
         matching_collections = MatchingCollections()
         for input_key, to_match in sorted(collections_to_match.items()):
             hdca = to_match.hdca
-            collection_type_description = collection_type_descriptions.for_collection_type(hdca.collection.collection_type)
+            collection_type_description = collection_type_descriptions.for_collection_type(
+                hdca.collection.collection_type
+            )
             subcollection_type = to_match.subcollection_type
 
             if to_match.linked:
-                matching_collections.__attempt_add_to_linked_match(input_key, hdca, collection_type_description, subcollection_type)
+                matching_collections.__attempt_add_to_linked_match(
+                    input_key, hdca, collection_type_description, subcollection_type
+                )
             else:
                 structure = get_structure(hdca, collection_type_description, leaf_subcollection_type=subcollection_type)
                 matching_collections.unlinked_structures.append(structure)
 
         return matching_collections
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/registry.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/registry.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,27 +1,26 @@
 from galaxy import model
 from .types import (
     list,
-    paired
+    paired,
 )
 
 PLUGIN_CLASSES = [list.ListDatasetCollectionType, paired.PairedDatasetCollectionType]
 
 
 class DatasetCollectionTypesRegistry:
-
     def __init__(self):
         self.__plugins = {p.collection_type: p() for p in PLUGIN_CLASSES}
 
     def get(self, plugin_type):
         return self.__plugins[plugin_type]
 
     def prototype(self, plugin_type):
         plugin_type_object = self.get(plugin_type)
-        if not hasattr(plugin_type_object, 'prototype_elements'):
+        if not hasattr(plugin_type_object, "prototype_elements"):
             raise Exception(f"Cannot pre-determine structure for collection of type {plugin_type}")
 
         dataset_collection = model.DatasetCollection()
         for e in plugin_type_object.prototype_elements():
             e.collection = dataset_collection
         return dataset_collection
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/structure.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/structure.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 """ Module for reasoning about structure of and matching hierarchical collections of data.
 """
 import logging
 
-
 log = logging.getLogger(__name__)
 
 
 class Leaf:
     children_known = True
 
     def __len__(self):
@@ -29,15 +28,14 @@
         return "Leaf[]"
 
 
 leaf = Leaf()
 
 
 class BaseTree:
-
     def __init__(self, collection_type_description):
         self.collection_type_description = collection_type_description
 
 
 class UninitializedTree(BaseTree):
     children_known = False
 
@@ -61,45 +59,51 @@
     def __str__(self):
         return f"UninitializedTree[collection_type={self.collection_type_description}]"
 
 
 class Tree(BaseTree):
     children_known = True
 
-    def __init__(self, children, collection_type_description):
+    def __init__(self, children, collection_type_description, when_values=None):
         super().__init__(collection_type_description)
         self.children = children
+        self.when_values = when_values
 
     @staticmethod
     def for_dataset_collection(dataset_collection, collection_type_description):
         children = []
         for element in dataset_collection.elements:
             if collection_type_description.has_subcollections():
                 child_collection = element.child_collection
-                subcollection_type_description = collection_type_description.subcollection_type_description()  # Type description of children
-                tree = Tree.for_dataset_collection(child_collection, collection_type_description=subcollection_type_description)
+                subcollection_type_description = (
+                    collection_type_description.subcollection_type_description()
+                )  # Type description of children
+                tree = Tree.for_dataset_collection(
+                    child_collection, collection_type_description=subcollection_type_description
+                )
                 children.append((element.element_identifier, tree))
             else:
                 children.append((element.element_identifier, leaf))
         return Tree(children, collection_type_description)
 
     def walk_collections(self, hdca_dict):
         return self._walk_collections(dict_map(lambda hdca: hdca.collection, hdca_dict))
 
     def _walk_collections(self, collection_dict):
         for index, (_identifier, substructure) in enumerate(self.children):
-            def element(collection):
-                return collection[index]
+
+            def get_element(collection):
+                return collection[index]  # noqa: B023
 
             if substructure.is_leaf:
-                yield dict_map(element, collection_dict)
+                yield dict_map(get_element, collection_dict), self.when_values[index] if self.when_values else None
             else:
-                sub_collections = dict_map(lambda collection: element(collection).child_collection, collection_dict)
-                for element in substructure._walk_collections(sub_collections):
-                    yield element
+                sub_collections = dict_map(lambda collection: get_element(collection).child_collection, collection_dict)
+                for element, _when_value in substructure._walk_collections(sub_collections):
+                    yield element, self.when_values[index] if self.when_values else None
 
     @property
     def is_leaf(self):
         return False
 
     def can_match(self, other_structure):
         if not self.collection_type_description.can_match_type(other_structure.collection_type_description):
@@ -123,25 +127,25 @@
 
     def multiply(self, other_structure):
         if other_structure.is_leaf:
             return self.clone()
 
         new_collection_type = self.collection_type_description.multiply(other_structure.collection_type_description)
         new_children = []
-        for (identifier, structure) in self.children:
+        for identifier, structure in self.children:
             new_children.append((identifier, structure.multiply(other_structure)))
 
         return Tree(new_children, new_collection_type)
 
     def clone(self):
         cloned_children = [(_[0], _[1].clone()) for _ in self.children]
         return Tree(cloned_children, self.collection_type_description)
 
     def __str__(self):
-        return f"Tree[collection_type={self.collection_type_description},children={','.join(map(lambda identifier_and_element: '{}={}'.format(identifier_and_element[0], identifier_and_element[1]), self.children))}]"
+        return f"Tree[collection_type={self.collection_type_description},children={','.join(map(lambda identifier_and_element: f'{identifier_and_element[0]}={identifier_and_element[1]}', self.children))}]"
 
 
 def tool_output_to_structure(get_sliced_input_collection_structure, tool_output, collections_manager):
     if not tool_output.collection:
         tree = leaf
     else:
         collection_type_descriptions = collections_manager.collection_type_descriptions
@@ -154,15 +158,17 @@
             if collection_type and tree.collection_type_description.collection_type != collection_type:
                 # See tool paired_collection_map_over_structured_like - type should
                 # override structured_like if they disagree.
                 tree = UninitializedTree(collection_type_descriptions.for_collection_type(collection_type))
         else:
             # Can't pre-compute the structure in this case, see if we can find a collection type.
             if collection_type is None and tool_output.structure.collection_type_source:
-                collection_type = get_sliced_input_collection_structure(tool_output.structure.collection_type_source).collection_type_description.collection_type
+                collection_type = get_sliced_input_collection_structure(
+                    tool_output.structure.collection_type_source
+                ).collection_type_description.collection_type
 
             if not collection_type:
                 raise Exception(f"Failed to determine collection type for mapping over output {tool_output.name}")
 
             tree = UninitializedTree(collection_type_descriptions.for_collection_type(collection_type))
 
     if not tree.children_known and tree.collection_type_description.collection_type == "paired":
@@ -174,14 +180,20 @@
 
 def dict_map(func, input_dict):
     return {k: func(v) for k, v in input_dict.items()}
 
 
 def get_structure(dataset_collection_instance, collection_type_description, leaf_subcollection_type=None):
     if leaf_subcollection_type:
-        collection_type_description = collection_type_description.effective_collection_type_description(leaf_subcollection_type)
-        if hasattr(dataset_collection_instance, 'child_collection'):
-            collection_type_description = collection_type_description.collection_type_description_factory.for_collection_type(leaf_subcollection_type)
+        collection_type_description = collection_type_description.effective_collection_type_description(
+            leaf_subcollection_type
+        )
+        if hasattr(dataset_collection_instance, "child_collection"):
+            collection_type_description = (
+                collection_type_description.collection_type_description_factory.for_collection_type(
+                    leaf_subcollection_type
+                )
+            )
             return UninitializedTree(collection_type_description)
 
     collection = dataset_collection_instance.collection
     return Tree.for_dataset_collection(collection, collection_type_description)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/subcollections.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/subcollections.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from galaxy import exceptions
 
 
 def split_dataset_collection_instance(dataset_collection_instance, collection_type):
-    """ Split up collection into collection.
-    """
+    """Split up collection into collection."""
     return _split_dataset_collection(dataset_collection_instance.collection, collection_type)
 
 
 def _split_dataset_collection(dataset_collection, collection_type):
     this_collection_type = dataset_collection.collection_type
     if not this_collection_type.endswith(collection_type) or this_collection_type == collection_type:
         raise exceptions.MessageException("Cannot split collection in desired fashion.")
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/type_description.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/type_description.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,24 +1,25 @@
+from typing import Union
+
 from .registry import DATASET_COLLECTION_TYPES_REGISTRY
 
 
 class CollectionTypeDescriptionFactory:
-
     def __init__(self, type_registry=DATASET_COLLECTION_TYPES_REGISTRY):
         # taking in type_registry though not using it, because we will someday
         # I think.
         self.type_registry = type_registry
 
     def for_collection_type(self, collection_type):
         assert collection_type is not None
         return CollectionTypeDescription(collection_type, self)
 
 
 class CollectionTypeDescription:
-    """ Abstraction over dataset collection type that ties together string
+    """Abstraction over dataset collection type that ties together string
     reprentation in database/model with type registry.
 
     >>> factory = CollectionTypeDescriptionFactory(None)
     >>> nested_type_description = factory.for_collection_type("list:paired")
     >>> paired_type_description = factory.for_collection_type("paired")
     >>> nested_type_description.has_subcollections_of_type("list")
     False
@@ -40,76 +41,81 @@
     'list'
     >>> nested_type_description.effective_collection_type_description(paired_type_description).collection_type
     'list'
     >>> nested_type_description.child_collection_type()
     'paired'
     """
 
-    def __init__(self, collection_type, collection_type_description_factory):
-        self.collection_type = collection_type
+    collection_type: str
+
+    def __init__(self, collection_type: Union[str, "CollectionTypeDescription"], collection_type_description_factory):
+        if isinstance(collection_type, CollectionTypeDescription):
+            self.collection_type = collection_type.collection_type
+        else:
+            self.collection_type = collection_type
         self.collection_type_description_factory = collection_type_description_factory
         self.__has_subcollections = self.collection_type.find(":") > 0
 
     def child_collection_type(self):
         rank_collection_type = self.rank_collection_type()
-        return self.collection_type[len(rank_collection_type) + 1:]
+        return self.collection_type[len(rank_collection_type) + 1 :]
 
     def child_collection_type_description(self):
         child_collection_type = self.child_collection_type()
         return self.collection_type_description_factory.for_collection_type(child_collection_type)
 
     def effective_collection_type_description(self, subcollection_type):
         effective_collection_type = self.effective_collection_type(subcollection_type)
         return self.collection_type_description_factory.for_collection_type(effective_collection_type)
 
     def effective_collection_type(self, subcollection_type):
-        if hasattr(subcollection_type, 'collection_type'):
+        if hasattr(subcollection_type, "collection_type"):
             subcollection_type = subcollection_type.collection_type
 
         if not self.has_subcollections_of_type(subcollection_type):
             raise ValueError(f"Cannot compute effective subcollection type of {subcollection_type} over {self}")
 
-        return self.collection_type[:-(len(subcollection_type) + 1)]
+        return self.collection_type[: -(len(subcollection_type) + 1)]
 
     def has_subcollections_of_type(self, other_collection_type):
-        """ Take in another type (either flat string or another
+        """Take in another type (either flat string or another
         CollectionTypeDescription) and determine if this collection contains
         subcollections matching that type.
 
         The way this is used in map/reduce it seems to make the most sense
         for this to return True if these subtypes are proper (i.e. a type
         is not considered to have subcollections of its own type).
         """
-        if hasattr(other_collection_type, 'collection_type'):
+        if hasattr(other_collection_type, "collection_type"):
             other_collection_type = other_collection_type.collection_type
         collection_type = self.collection_type
         return collection_type.endswith(other_collection_type) and collection_type != other_collection_type
 
     def is_subcollection_of_type(self, other_collection_type):
-        if not hasattr(other_collection_type, 'collection_type'):
+        if not hasattr(other_collection_type, "collection_type"):
             other_collection_type = self.collection_type_description_factory.for_collection_type(other_collection_type)
         return other_collection_type.has_subcollections_of_type(self)
 
     def can_match_type(self, other_collection_type):
-        if hasattr(other_collection_type, 'collection_type'):
+        if hasattr(other_collection_type, "collection_type"):
             other_collection_type = other_collection_type.collection_type
         collection_type = self.collection_type
         return other_collection_type == collection_type
 
     def subcollection_type_description(self):
         if not self.__has_subcollections:
             raise ValueError(f"Cannot generate subcollection type description for flat type {self.collection_type}")
         subcollection_type = self.collection_type.split(":", 1)[1]
         return self.collection_type_description_factory.for_collection_type(subcollection_type)
 
     def has_subcollections(self):
         return self.__has_subcollections
 
     def rank_collection_type(self):
-        """ Return the top-level collection type corresponding to this
+        """Return the top-level collection type corresponding to this
         collection type. For instance the "rank" type of a list of paired
         data ("list:paired") is "list".
         """
         return self.collection_type.split(":")[0]
 
     def rank_type_plugin(self):
         return self.collection_type_description_factory.type_registry.get(self.rank_collection_type())
@@ -123,20 +129,20 @@
         return self.collection_type_description_factory.for_collection_type(collection_type)
 
     def __str__(self):
         return f"CollectionTypeDescription[{self.collection_type}]"
 
 
 def map_over_collection_type(mapped_over_collection_type, target_collection_type):
-    if hasattr(mapped_over_collection_type, 'collection_type'):
+    if hasattr(mapped_over_collection_type, "collection_type"):
         mapped_over_collection_type = mapped_over_collection_type.collection_type
 
     if not target_collection_type:
         return mapped_over_collection_type
     else:
-        if hasattr(target_collection_type, 'collection_type'):
+        if hasattr(target_collection_type, "collection_type"):
             target_collection_type = target_collection_type.collection_type
 
         return f"{mapped_over_collection_type}:{target_collection_type}"
 
 
 COLLECTION_TYPE_DESCRIPTION_FACTORY = CollectionTypeDescriptionFactory()
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/types/__init__.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/types/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,26 +1,23 @@
 import logging
 from abc import (
     ABCMeta,
-    abstractmethod
+    abstractmethod,
 )
 
-
 from galaxy import exceptions
 
 log = logging.getLogger(__name__)
 
 
 class DatasetCollectionType(metaclass=ABCMeta):
-
     @abstractmethod
     def generate_elements(self, dataset_instances):
-        """ Generate DatasetCollectionElements with corresponding
+        """Generate DatasetCollectionElements with corresponding
         to the supplied dataset instances or throw exception if
         this is not a valid collection of the specified type.
         """
 
 
 class BaseDatasetCollectionType(DatasetCollectionType):
-
     def _validation_failed(self, message):
         raise exceptions.ObjectAttributeInvalidException(message)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/types/list.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/types/list.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from galaxy.model import DatasetCollectionElement
-from ..types import BaseDatasetCollectionType
+from . import BaseDatasetCollectionType
 
 
 class ListDatasetCollectionType(BaseDatasetCollectionType):
-    """ A flat list of named elements.
-    """
+    """A flat list of named elements."""
+
     collection_type = "list"
 
     def __init__(self):
         pass
 
     def generate_elements(self, elements):
         for identifier, element in elements.items():
```

### Comparing `galaxy-data-22.1.1/galaxy/model/dataset_collections/types/paired.py` & `galaxy-data-23.0.1/galaxy/model/dataset_collections/types/paired.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,18 +1,22 @@
-from galaxy.model import DatasetCollectionElement, HistoryDatasetAssociation
-from ..types import BaseDatasetCollectionType
+from galaxy.model import (
+    DatasetCollectionElement,
+    HistoryDatasetAssociation,
+)
+from . import BaseDatasetCollectionType
 
 FORWARD_IDENTIFIER = "forward"
 REVERSE_IDENTIFIER = "reverse"
 
 
 class PairedDatasetCollectionType(BaseDatasetCollectionType):
     """
     Paired (left/right) datasets.
     """
+
     collection_type = "paired"
 
     def __init__(self):
         pass
 
     def generate_elements(self, elements):
         forward_dataset = elements.get(FORWARD_IDENTIFIER)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/item_attrs.py` & `galaxy-data-23.0.1/galaxy/model/item_attrs.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,24 +6,24 @@
 import galaxy
 
 log = logging.getLogger(__name__)
 
 
 class UsesItemRatings:
     """
-        Mixin for getting and setting item ratings.
+    Mixin for getting and setting item ratings.
 
-        Class makes two assumptions:
-        (1) item-rating association table is named <item_class>RatingAssocation
-        (2) item-rating association table has a column with a foreign key referencing
-        item table that contains the item's id.
+    Class makes two assumptions:
+    (1) item-rating association table is named <item_class>RatingAssocation
+    (2) item-rating association table has a column with a foreign key referencing
+    item table that contains the item's id.
     """
 
     def get_ave_item_rating_data(self, db_session, item, webapp_model=None):
-        """ Returns the average rating for an item."""
+        """Returns the average rating for an item."""
         if webapp_model is None:
             webapp_model = galaxy.model
         item_rating_assoc_class = self._get_item_rating_assoc_class(item, webapp_model=webapp_model)
         if not item_rating_assoc_class:
             raise Exception(f"Item does not have ratings: {item.__class__.__name__}")
         item_id_filter = self._get_item_id_filter_str(item, item_rating_assoc_class)
         ave_rating = db_session.query(func.avg(item_rating_assoc_class.rating)).filter(item_id_filter).scalar()
@@ -32,15 +32,15 @@
             ave_rating = float(ave_rating)
         else:
             ave_rating = 0
         num_ratings = int(db_session.query(func.count(item_rating_assoc_class.rating)).filter(item_id_filter).scalar())
         return (ave_rating, num_ratings)
 
     def rate_item(self, db_session, user, item, rating, webapp_model=None):
-        """ Rate an item. Return type is <item_class>RatingAssociation. """
+        """Rate an item. Return type is <item_class>RatingAssociation."""
         if webapp_model is None:
             webapp_model = galaxy.model
         item_rating = self.get_user_item_rating(db_session, user, item, webapp_model=webapp_model)
         if not item_rating:
             # User has not yet rated item; create rating.
             item_rating_assoc_class = self._get_item_rating_assoc_class(item, webapp_model=webapp_model)
             item_rating = item_rating_assoc_class(user, item, rating)
@@ -49,40 +49,40 @@
         elif item_rating.rating != rating:
             # User has rated item; update rating.
             item_rating.rating = rating
             db_session.flush()
         return item_rating
 
     def get_user_item_rating(self, db_session, user, item, webapp_model=None):
-        """ Returns user's rating for an item. Return type is <item_class>RatingAssociation. """
+        """Returns user's rating for an item. Return type is <item_class>RatingAssociation."""
         if webapp_model is None:
             webapp_model = galaxy.model
         item_rating_assoc_class = self._get_item_rating_assoc_class(item, webapp_model=webapp_model)
         if not item_rating_assoc_class:
             raise Exception(f"Item does not have ratings: {item.__class__.__name__}")
 
         # Query rating table by user and item id.
         item_id_filter = self._get_item_id_filter_str(item, item_rating_assoc_class)
         return db_session.query(item_rating_assoc_class).filter_by(user=user).filter(item_id_filter).first()
 
     def _get_item_rating_assoc_class(self, item, webapp_model=None):
-        """ Returns an item's item-rating association class. """
+        """Returns an item's item-rating association class."""
         if webapp_model is None:
             webapp_model = galaxy.model
-        item_rating_assoc_class = f'{item.__class__.__name__}RatingAssociation'
+        item_rating_assoc_class = f"{item.__class__.__name__}RatingAssociation"
         return getattr(webapp_model, item_rating_assoc_class, None)
 
     def _get_item_id_filter_str(self, item, item_rating_assoc_class):
         # Get foreign key in item-rating association table that references item table.
         item_fk = get_foreign_key(item_rating_assoc_class, item)
         return item_fk.parent == item.id
 
 
 class UsesAnnotations:
-    """ Mixin for getting and setting item annotations. """
+    """Mixin for getting and setting item annotations."""
 
     def get_item_annotation_str(self, db_session, user, item):
         return get_item_annotation_str(db_session, user, item)
 
     def get_item_annotation_obj(self, db_session, user, item):
         return get_item_annotation_obj(db_session, user, item)
 
@@ -92,15 +92,15 @@
     def delete_item_annotation(self, db_session, user, item):
         annotation_assoc = get_item_annotation_obj(db_session, user, item)
         if annotation_assoc:
             db_session.delete(annotation_assoc)
             db_session.flush()
 
     def copy_item_annotation(self, db_session, source_user, source_item, target_user, target_item):
-        """ Copy an annotation from a user/item source to a user/item target. """
+        """Copy an annotation from a user/item source to a user/item target."""
         if source_user and target_user:
             annotation_str = self.get_item_annotation_str(db_session, source_user, source_item)
             if annotation_str:
                 annotation = self.add_item_annotation(db_session, target_user, target_item, annotation_str)
                 return annotation
         return None
 
@@ -130,31 +130,31 @@
         annotation_assoc = annotation_assoc.filter_by(page=item)
     elif item.__class__ == galaxy.model.Visualization:
         annotation_assoc = annotation_assoc.filter_by(visualization=item)
     return annotation_assoc.first()
 
 
 def get_item_annotation_str(db_session, user, item):
-    """ Returns a user's annotation string for an item. """
-    if hasattr(item, 'annotations'):
+    """Returns a user's annotation string for an item."""
+    if hasattr(item, "annotations"):
         # If we already have an annotations object we use it.
         annotation_obj = None
         for annotation in item.annotations:
             if annotation.user == user:
                 annotation_obj = annotation
                 break
     else:
         annotation_obj = get_item_annotation_obj(db_session, user, item)
     if annotation_obj:
         return galaxy.util.unicodify(annotation_obj.annotation)
     return None
 
 
 def add_item_annotation(db_session, user, item, annotation):
-    """ Add or update an item's annotation; a user can only have a single annotation for an item. """
+    """Add or update an item's annotation; a user can only have a single annotation for an item."""
     # Get/create annotation association object.
     annotation_assoc = get_item_annotation_obj(db_session, user, item)
     if not annotation_assoc:
         annotation_assoc_class = _get_annotation_assoc_class(item)
         if not annotation_assoc_class:
             return None
         annotation_assoc = annotation_assoc_class()
@@ -162,29 +162,29 @@
         annotation_assoc.user = user
     # Set annotation.
     annotation_assoc.annotation = annotation
     return annotation_assoc
 
 
 def _get_annotation_assoc_class(item):
-    """ Returns an item's item-annotation association class. """
-    class_name = f'{item.__class__.__name__}AnnotationAssociation'
+    """Returns an item's item-annotation association class."""
+    class_name = f"{item.__class__.__name__}AnnotationAssociation"
     return getattr(galaxy.model, class_name, None)
 
 
 def get_foreign_key(source_class, target_class):
-    """ Returns foreign key in source class that references target class. """
+    """Returns foreign key in source class that references target class."""
     target_fk = None
     for fk in source_class.table.foreign_keys:
         if fk.references(target_class.table):
             target_fk = fk
             break
     if not target_fk:
         raise Exception("No foreign key found between objects: %s, %s" % source_class.table, target_class.table)
     return target_fk
 
 
 __all__ = (
-    'get_foreign_key',
-    'UsesAnnotations',
-    'UsesItemRatings',
+    "get_foreign_key",
+    "UsesAnnotations",
+    "UsesItemRatings",
 )
```

### Comparing `galaxy-data-22.1.1/galaxy/model/metadata.py` & `galaxy-data-23.0.1/galaxy/model/metadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,22 +2,27 @@
 Galaxy Metadata
 """
 
 import copy
 import json
 import logging
 import os
-import shutil
 import sys
 import tempfile
 import weakref
 from collections import OrderedDict
 from collections.abc import Mapping
 from os.path import abspath
-from typing import Any, Iterator, Optional, TYPE_CHECKING, Union
+from typing import (
+    Any,
+    Iterator,
+    Optional,
+    TYPE_CHECKING,
+    Union,
+)
 
 from sqlalchemy.orm import object_session
 from sqlalchemy.orm.attributes import flag_modified
 
 import galaxy.model
 from galaxy.model.scoped_session import galaxy_scoped_session
 from galaxy.security.object_wrapper import sanitize_lists_to_string
@@ -58,26 +63,32 @@
         statements = class_locals.setdefault(STATEMENTS, [])
         # add Statement containing info to populate a MetadataElementSpec
         statements.append((self, args, kwargs))
 
     @classmethod
     def process(cls, element):
         for statement, args, kwargs in getattr(element, STATEMENTS, []):
-            statement.target(element, *args, **kwargs)  # statement.target is MetadataElementSpec, element is a Datatype class
+            statement.target(
+                element, *args, **kwargs
+            )  # statement.target is MetadataElementSpec, element is a Datatype class
 
 
 class MetadataCollection(Mapping):
     """
     MetadataCollection is not a collection at all, but rather a proxy
     to the real metadata which is stored as a Dictionary. This class
     handles processing the metadata elements when they are set and
     retrieved, returning default values in cases when metadata is not set.
     """
 
-    def __init__(self, parent: Union["DatasetInstance", "NoneDataset"], session: Optional[Union[galaxy_scoped_session, 'SessionlessContext']] = None) -> None:
+    def __init__(
+        self,
+        parent: Union["DatasetInstance", "NoneDataset"],
+        session: Optional[Union[galaxy_scoped_session, "SessionlessContext"]] = None,
+    ) -> None:
         self.parent = parent
         self._session = session
         # initialize dict if needed
         if self.parent._metadata is None:
             self.parent._metadata = {}
 
     def get_parent(self):
@@ -86,14 +97,15 @@
         return None
 
     def set_parent(self, parent):
         # use weakref to prevent a circular reference interfering with garbage
         # collection: hda/lda (parent) <--> MetadataCollection (self) ; needs to be
         # hashable, so cannot use proxy.
         self.__dict__["_parent"] = weakref.ref(parent)
+
     parent = property(get_parent, set_parent)
 
     @property
     def spec(self):
         return self.parent.datatype.metadata_spec
 
     def _object_session(self, item):
@@ -121,14 +133,15 @@
         return len(self.spec)
 
     def __str__(self):
         return dict(self.items()).__str__()
 
     def __bool__(self):
         return bool(self.parent._metadata)
+
     __nonzero__ = __bool__
 
     def __getattr__(self, name):
         if name in self.spec:
             if name in self.parent._metadata:
                 return self.spec[name].wrap(self.parent._metadata[name], self._object_session(self.parent))
             return self.spec[name].wrap(self.spec[name].default, self._object_session(self.parent))
@@ -136,22 +149,22 @@
             return self.parent._metadata[name]
         # Instead of raising an AttributeError for non-existing metadata, we return None
         return None
 
     def __setattr__(self, name, value):
         if name == "parent":
             return self.set_parent(value)
-        elif name == '_session':
+        elif name == "_session":
             super().__setattr__(name, value)
         else:
             if name in self.spec:
                 self.parent._metadata[name] = self.spec[name].unwrap(value)
             else:
                 self.parent._metadata[name] = value
-            flag_modified(self.parent, '_metadata')
+            flag_modified(self.parent, "_metadata")
 
     def remove_key(self, name):
         if name in self.parent._metadata:
             del self.parent._metadata[name]
         else:
             log.info(f"Attempted to delete invalid key '{name}' from MetadataCollection")
 
@@ -195,19 +208,19 @@
                 return True
 
         return False
 
     def from_JSON_dict(self, filename=None, path_rewriter=None, json_dict=None):
         dataset = self.parent
         if filename is not None:
-            log.debug(f'loading metadata from file for: {dataset.__class__.__name__} {dataset.id}')
+            log.debug(f"loading metadata from file for: {dataset.__class__.__name__} {dataset.id}")
             with open(filename) as fh:
                 JSONified_dict = json.load(fh)
         elif json_dict is not None:
-            log.debug(f'loading metadata from dict for: {dataset.__class__.__name__} {dataset.id}')
+            log.debug(f"loading metadata from dict for: {dataset.__class__.__name__} {dataset.id}")
             if isinstance(json_dict, str):
                 JSONified_dict = json.loads(json_dict)
             elif isinstance(json_dict, dict):
                 JSONified_dict = json_dict
             else:
                 raise ValueError(f"json_dict must be either a dictionary or a string, got {type(json_dict)}.")
         else:
@@ -221,50 +234,50 @@
         metadata_name_value = {}
         for name, spec in self.spec.items():
             if name in JSONified_dict:
                 from_ext_kwds = {}
                 external_value = JSONified_dict[name]
                 param = spec.param
                 if isinstance(param, FileParameter):
-                    from_ext_kwds['path_rewriter'] = path_rewriter
+                    from_ext_kwds["path_rewriter"] = path_rewriter
                 value = param.from_external_value(external_value, dataset, **from_ext_kwds)
                 metadata_name_value[name] = value
             elif name in dataset._metadata:
                 # if the metadata value is not found in our externally set metadata but it has a value in the 'old'
                 # metadata associated with our dataset, we'll delete it from our dataset's metadata dict
                 del dataset._metadata[name]
         for name, value in metadata_name_value.items():
             dataset._metadata[name] = value
-        if '__extension__' in JSONified_dict:
-            dataset.extension = JSONified_dict['__extension__']
-        if '__validated_state__' in JSONified_dict:
-            dataset.validated_state = JSONified_dict['__validated_state__']
-        if '__validated_state_message__' in JSONified_dict:
-            dataset.validated_state_message = JSONified_dict['__validated_state_message__']
-        flag_modified(dataset, '_metadata')
+        if "__extension__" in JSONified_dict:
+            dataset.extension = JSONified_dict["__extension__"]
+        if "__validated_state__" in JSONified_dict:
+            dataset.validated_state = JSONified_dict["__validated_state__"]
+        if "__validated_state_message__" in JSONified_dict:
+            dataset.validated_state_message = JSONified_dict["__validated_state_message__"]
+        flag_modified(dataset, "_metadata")
 
     def to_JSON_dict(self, filename=None):
         meta_dict = {}
         dataset_meta_dict = self.parent._metadata
         for name, spec in self.spec.items():
             if name in dataset_meta_dict:
                 meta_dict[name] = spec.param.to_external_value(dataset_meta_dict[name])
-        if '__extension__' in dataset_meta_dict:
-            meta_dict['__extension__'] = dataset_meta_dict['__extension__']
-        if '__validated_state__' in dataset_meta_dict:
-            meta_dict['__validated_state__'] = dataset_meta_dict['__validated_state__']
-        if '__validated_state_message__' in dataset_meta_dict:
-            meta_dict['__validated_state_message__'] = dataset_meta_dict['__validated_state_message__']
+        if "__extension__" in dataset_meta_dict:
+            meta_dict["__extension__"] = dataset_meta_dict["__extension__"]
+        if "__validated_state__" in dataset_meta_dict:
+            meta_dict["__validated_state__"] = dataset_meta_dict["__validated_state__"]
+        if "__validated_state_message__" in dataset_meta_dict:
+            meta_dict["__validated_state_message__"] = dataset_meta_dict["__validated_state_message__"]
         try:
             encoded_meta_dict = galaxy.model.custom_types.json_encoder.encode(meta_dict)
         except Exception as e:
             raise Exception(f"Failed encoding metadata dictionary: {meta_dict}") from e
         if filename is None:
             return encoded_meta_dict
-        with open(filename, 'wt+') as fh:
+        with open(filename, "wt+") as fh:
             fh.write(encoded_meta_dict)
 
     def __getstate__(self):
         # cannot pickle a weakref item (self._parent), when
         # data._metadata_collection is None, it will be recreated on demand
         return None
 
@@ -286,15 +299,15 @@
     def __getattr__(self, name):
         if name not in self:
             raise AttributeError
         return self.get(name)
 
     def __repr__(self):
         # force elements to draw with __str__ for sphinx-apidoc
-        return ', '.join(item.__str__() for item in self.values())
+        return ", ".join(item.__str__() for item in self.values())
 
 
 class MetadataParameter:
     def __init__(self, spec):
         self.spec = spec
 
     def get_field(self, value=None, context=None, other_values=None, **kwd):
@@ -353,17 +366,26 @@
 
 class MetadataElementSpec:
     """
     Defines a metadata element and adds it to the metadata_spec (which
     is a MetadataSpecCollection) of datatype.
     """
 
-    def __init__(self, datatype, name=None, desc=None,
-                 param=MetadataParameter, default=None, no_value=None,
-                 visible=True, set_in_upload=False, **kwargs):
+    def __init__(
+        self,
+        datatype,
+        name=None,
+        desc=None,
+        param=MetadataParameter,
+        default=None,
+        no_value=None,
+        visible=True,
+        set_in_upload=False,
+        **kwargs,
+    ):
         self.name = name
         self.desc = desc or name
         self.default = default
         self.no_value = no_value
         self.visible = visible
         self.set_in_upload = set_in_upload
         # Catch-all, allows for extra attributes to be set
@@ -372,15 +394,15 @@
         self.param = param(self)
         # add spec element to the spec
         datatype.metadata_spec.append(self)
         # Should we validate that non-optional elements have been set ?
         # (The answer is yes, but not all datatypes control optionality appropriately at this point.)
         # This allows us to check that inherited MetadataElement instances from datatypes that set
         # check_required_metadata have been reviewed and considered really required.
-        self.check_required_metadata = datatype.__dict__.get('check_required_metadata', False)
+        self.check_required_metadata = datatype.__dict__.get("check_required_metadata", False)
 
     def get(self, name, default=None):
         return self.__dict__.get(name, default)
 
     def wrap(self, value, session):
         """
         Turns a stored value into its usable form.
@@ -393,15 +415,15 @@
         """
         return self.param.unwrap(value)
 
     def __str__(self):
         # TODO??: assuming param is the class of this MetadataElementSpec - add the plain class name for that
         spec_dict = dict(param_class=self.param.__class__.__name__)
         spec_dict.update(self.__dict__)
-        return ("{name} ({param_class}): {desc}, defaults to '{default}'".format(**spec_dict))
+        return "{name} ({param_class}): {desc}, defaults to '{default}'".format(**spec_dict)
 
 
 # create a statement class that, when called,
 #   will add a new MetadataElementSpec to a class's metadata_spec
 MetadataElement = Statement(MetadataElementSpec)
 
 
@@ -465,85 +487,86 @@
             return []
         if not isinstance(value, list):
             return [value]
         return value
 
 
 class DBKeyParameter(SelectParameter):
-
     def get_field(self, value=None, context=None, other_values=None, values=None, **kwd):
         context = context or {}
         other_values = other_values or {}
         try:
-            values = kwd['trans'].app.genome_builds.get_genome_build_names(kwd['trans'])
+            values = kwd["trans"].app.genome_builds.get_genome_build_names(kwd["trans"])
         except KeyError:
             pass
         return super().get_field(value, context, other_values, values, **kwd)
 
+    def make_copy(self, value, target_context: MetadataCollection, source_context):
+        value = listify(value)
+        return super().make_copy(value, target_context, source_context)
 
-class RangeParameter(SelectParameter):
 
+class RangeParameter(SelectParameter):
     def __init__(self, spec):
         SelectParameter.__init__(self, spec)
         # The spec must be set with min and max values
         self.min = spec.get("min") or 1
         self.max = spec.get("max") or 1
         self.step = self.spec.get("step") or 1
 
     def get_field(self, value=None, context=None, other_values=None, values=None, **kwd):
         context = context or {}
         other_values = other_values or {}
 
         if values is None:
             values = list(zip(range(self.min, self.max, self.step), range(self.min, self.max, self.step)))
-        return SelectParameter.get_field(self, value=value, context=context, other_values=other_values, values=values, **kwd)
+        return SelectParameter.get_field(
+            self, value=value, context=context, other_values=other_values, values=values, **kwd
+        )
 
     @classmethod
     def marshal(cls, value):
         value = SelectParameter.marshal(value)
         values = [int(x) for x in value]
         return values
 
 
 class ColumnParameter(RangeParameter):
-
     def get_field(self, value=None, context=None, other_values=None, values=None, **kwd):
         context = context or {}
         other_values = other_values or {}
 
         if values is None and context:
             column_range = range(1, (context.columns or 0) + 1, 1)
             values = list(zip(column_range, column_range))
-        return RangeParameter.get_field(self, value=value, context=context, other_values=other_values, values=values, **kwd)
+        return RangeParameter.get_field(
+            self, value=value, context=context, other_values=other_values, values=values, **kwd
+        )
 
 
 class ColumnTypesParameter(MetadataParameter):
-
     def to_string(self, value):
         return ",".join(map(str, value))
 
 
 class ListParameter(MetadataParameter):
-
     def to_string(self, value):
         return ",".join(str(x) for x in value)
 
 
 class DictParameter(MetadataParameter):
-
     def to_string(self, value):
         return json.dumps(value)
 
     def to_safe_string(self, value):
         # We do not sanitize json dicts
         return safe_dumps(value)
 
 
 class PythonObjectParameter(MetadataParameter):
-
     def to_string(self, value):
         if not value:
             return self.spec._to_string(self.spec.no_value)
         return self.spec._to_string(value)
 
     def get_field(self, value=None, context=None, other_values=None, **kwd):
         context = context or {}
@@ -552,15 +575,14 @@
 
     @classmethod
     def marshal(cls, value):
         return value
 
 
 class FileParameter(MetadataParameter):
-
     def to_string(self, value):
         if not value:
             return str(self.spec.no_value)
         return value.file_name
 
     def to_safe_string(self, value):
         # We do not sanitize file names
@@ -581,25 +603,29 @@
         else:
             return session.query(galaxy.model.MetadataFile).filter_by(uuid=value).one()
 
     def make_copy(self, value, target_context: MetadataCollection, source_context):
         session = target_context._object_session(target_context.parent)
         value = self.wrap(value, session=session)
         target_dataset = target_context.parent.dataset
+        if value and not value.id:
+            # This is a new MetadataFile object, we're not copying to another dataset.
+            # Just use it.
+            return self.unwrap(value)
         if value and target_dataset.object_store.exists(target_dataset):
             # Only copy MetadataFile if the target dataset has been created in an object store.
             # All current datatypes re-generate MetadataFile objects when setting metadata,
             # so this would ultimately get overwritten anyway.
             new_value = galaxy.model.MetadataFile(dataset=target_context.parent, name=self.spec.name)
             session.add(new_value)
             try:
-                shutil.copy(value.file_name, new_value.file_name)
+                new_value.update_from_file(value.file_name)
             except AssertionError:
                 session(target_context.parent).flush()
-                shutil.copy(value.file_name, new_value.file_name)
+                new_value.update_from_file(value.file_name)
             return self.unwrap(new_value)
         return None
 
     @classmethod
     def marshal(cls, value):
         if isinstance(value, galaxy.model.MetadataFile):
             # We want to push value.id to the database, but need to skip this when no session is available,
@@ -622,95 +648,91 @@
                 mf = self.new_file(dataset=parent, **value.kwds)
             # Ensure the metadata file gets updated with content
             file_name = value.file_name
             if path_rewriter:
                 # Job may have run with a different (non-local) tmp/working
                 # directory. Correct.
                 file_name = path_rewriter(file_name)
-            parent.dataset.object_store.update_from_file(mf,
-                                                         file_name=file_name,
-                                                         extra_dir='_metadata_files',
-                                                         extra_dir_at_root=True,
-                                                         alt_name=os.path.basename(mf.file_name))
+            mf.update_from_file(file_name)
             os.unlink(file_name)
             value = mf.id
         return value
 
     def to_external_value(self, value):
         """
         Turns a value read from a metadata into its value to be pushed directly into the external dict.
         """
         if isinstance(value, galaxy.model.MetadataFile):
             value = value.id
         elif isinstance(value, MetadataTempFile):
             value = MetadataTempFile.to_JSON(value)
         return value
 
-    def new_file(self, dataset=None, **kwds):
+    def new_file(self, dataset=None, metadata_tmp_files_dir=None, **kwds):
         # If there is a place to store the file (i.e. an object_store has been bound to
         # Dataset) then use a MetadataFile and assume it is accessible. Otherwise use
         # a MetadataTempFile.
-        if getattr(dataset.dataset, "object_store", False):
+        if getattr(dataset.dataset, "object_store", False) and not metadata_tmp_files_dir:
             mf = galaxy.model.MetadataFile(name=self.spec.name, dataset=dataset, **kwds)
             sa_session = object_session(dataset)
             if sa_session:
                 sa_session.add(mf)
                 sa_session.flush()  # flush to assign id
             return mf
         else:
             # we need to make a tmp file that is accessable to the head node,
             # we will be copying its contents into the MetadataFile objects filename after restoring from JSON
             # we do not include 'dataset' in the kwds passed, as from_JSON_value() will handle this for us
-            return MetadataTempFile(**kwds)
+            return MetadataTempFile(metadata_tmp_files_dir=metadata_tmp_files_dir, **kwds)
 
 
 # This class is used when a database file connection is not available
 class MetadataTempFile:
-    tmp_dir = 'database/tmp'  # this should be overwritten as necessary in calling scripts
+    tmp_dir = "database/tmp"  # this should be overwritten as necessary in calling scripts
 
-    def __init__(self, **kwds):
+    def __init__(self, metadata_tmp_files_dir=None, **kwds):
         self.kwds = kwds
+        if metadata_tmp_files_dir:
+            self.tmp_dir = metadata_tmp_files_dir
         self._filename = None
 
     @property
     def file_name(self):
         if self._filename is None:
             # we need to create a tmp file, accessable across all nodes/heads, save the name, and return it
             self._filename = abspath(tempfile.NamedTemporaryFile(dir=self.tmp_dir, prefix="metadata_temp_file_").name)
-            open(self._filename, 'wb+')  # create an empty file, so it can't be reused using tempfile
+            open(self._filename, "wb+")  # create an empty file, so it can't be reused using tempfile
         return self._filename
 
     def to_JSON(self):
-        return {'__class__': self.__class__.__name__,
-                'filename': self.file_name,
-                'kwds': self.kwds}
+        return {"__class__": self.__class__.__name__, "filename": self.file_name, "kwds": self.kwds}
 
     @classmethod
     def from_JSON(cls, json_dict):
         # need to ensure our keywords are not unicode
-        rval = cls(**stringify_dictionary_keys(json_dict['kwds']))
-        rval._filename = json_dict['filename']
+        rval = cls(**stringify_dictionary_keys(json_dict["kwds"]))
+        rval._filename = json_dict["filename"]
         return rval
 
     @classmethod
     def is_JSONified_value(cls, value):
-        return (isinstance(value, dict) and value.get('__class__', None) == cls.__name__)
+        return isinstance(value, dict) and value.get("__class__", None) == cls.__name__
 
     @classmethod
     def cleanup_from_JSON_dict_filename(cls, filename):
         try:
             with open(filename) as fh:
                 for value in json.load(fh).values():
                     if cls.is_JSONified_value(value):
                         value = cls.from_JSON(value)
                     if isinstance(value, cls) and os.path.exists(value.file_name):
-                        log.debug('Cleaning up abandoned MetadataTempFile file: %s', value.file_name)
+                        log.debug("Cleaning up abandoned MetadataTempFile file: %s", value.file_name)
                         os.unlink(value.file_name)
         except Exception as e:
-            log.debug('Failed to cleanup MetadataTempFile temp files from %s: %s', filename, unicodify(e))
+            log.debug("Failed to cleanup MetadataTempFile temp files from %s: %s", filename, unicodify(e))
 
 
 __all__ = (
     "Statement",
     "MetadataElement",
     "MetadataCollection",
     "MetadataSpecCollection",
```

### Comparing `galaxy-data-22.1.1/galaxy/model/migrate/triggers/history_update_time_field.py` & `galaxy-data-23.0.1/galaxy/model/triggers/history_update_time_field.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """
 Database trigger installation and removal
 """
 
-from galaxy.model.migrate.versions.util import execute_statements
+from galaxy.model.triggers.utils import execute_statements
 
 
 def install_timestamp_triggers(engine):
     """
     Install update_time propagation triggers for history table
     """
     statements = get_timestamp_install_sql(engine.name)
@@ -24,69 +24,64 @@
 def get_timestamp_install_sql(variant):
     """
     Generate a list of SQL statements for installation of timestamp triggers
     """
 
     sql = get_timestamp_drop_sql(variant)
 
-    if 'postgres' in variant:
+    if "postgres" in variant:
         # PostgreSQL has a separate function definition and a trigger
         # assignment. The first two statements the functions, and
         # the later assign those functions to triggers on tables
 
-        fn_name = 'update_history_update_time'
-        sql.append(build_pg_timestamp_fn(fn_name, 'history', source_key='history_id'))
-        sql.append(build_pg_trigger('history_dataset_association', fn_name))
-        sql.append(build_pg_trigger('history_dataset_collection_association', fn_name))
+        fn_name = "update_history_update_time"
+        sql.append(build_pg_timestamp_fn(fn_name, "history", source_key="history_id"))
+        sql.append(build_pg_trigger("history_dataset_association", fn_name))
+        sql.append(build_pg_trigger("history_dataset_collection_association", fn_name))
 
     else:
         # Other database variants are more granular. Requiring separate
         # statements for INSERT/UPDATE/DELETE, and the body of the trigger
         # is not necessarily reusable with a function
 
-        for operation in ['INSERT', 'UPDATE', 'DELETE']:
-
+        for operation in ["INSERT", "UPDATE", "DELETE"]:
             # change hda -> update history
-            sql.append(build_timestamp_trigger(
-                operation,
-                'history_dataset_association',
-                'history',
-                source_key='history_id'
-            ))
+            sql.append(
+                build_timestamp_trigger(operation, "history_dataset_association", "history", source_key="history_id")
+            )
 
             # change hdca -> update history
-            sql.append(build_timestamp_trigger(
-                operation,
-                'history_dataset_collection_association',
-                'history',
-                source_key='history_id'
-            ))
+            sql.append(
+                build_timestamp_trigger(
+                    operation, "history_dataset_collection_association", "history", source_key="history_id"
+                )
+            )
 
     return sql
 
 
 def get_timestamp_drop_sql(variant):
     """
     Generate a list of statements to drop the timestamp update triggers
     """
 
     sql = []
 
-    if 'postgres' in variant:
+    if "postgres" in variant:
         sql.append("DROP FUNCTION IF EXISTS update_history_update_time() CASCADE;")
     else:
-        for operation in ['INSERT', 'UPDATE', 'DELETE']:
-            for when in ['BEFORE', 'AFTER']:
-                sql.append(build_drop_trigger(operation, 'history_dataset_association', when))
-                sql.append(build_drop_trigger(operation, 'history_dataset_collection_association', when))
+        for operation in ["INSERT", "UPDATE", "DELETE"]:
+            for when in ["BEFORE", "AFTER"]:
+                sql.append(build_drop_trigger(operation, "history_dataset_association", when))
+                sql.append(build_drop_trigger(operation, "history_dataset_collection_association", when))
 
     return sql
 
 
-def build_pg_timestamp_fn(fn_name, target_table, source_key, target_key='id'):
+def build_pg_timestamp_fn(fn_name, target_table, source_key, target_key="id"):
     """Generates a PostgreSQL history update timestamp function"""
 
     return f"""
         CREATE OR REPLACE FUNCTION {fn_name}()
             RETURNS trigger
             LANGUAGE 'plpgsql'
         AS $BODY$
@@ -108,28 +103,28 @@
                     RETURN NEW;
                 END IF;
             END;
         $BODY$;
     """
 
 
-def build_pg_trigger(source_table, fn_name, when='AFTER'):
+def build_pg_trigger(source_table, fn_name, when="AFTER"):
     """Assigns a PostgreSQL trigger to indicated table, calling user-defined function"""
     when_initial = when.lower()[0]
     trigger_name = f"trigger_{source_table}_{when_initial}iudr"
     return f"""
         CREATE TRIGGER {trigger_name}
             {when} INSERT OR DELETE OR UPDATE
             ON {source_table}
             FOR EACH ROW
             EXECUTE PROCEDURE {fn_name}();
     """
 
 
-def build_timestamp_trigger(operation, source_table, target_table, source_key, target_key='id', when='AFTER'):
+def build_timestamp_trigger(operation, source_table, target_table, source_key, target_key="id", when="AFTER"):
     """Creates a non-PostgreSQL update_time trigger"""
 
     trigger_name = get_trigger_name(operation, source_table, when)
 
     # three different update clauses depending on update/insert/delete
     clause = ""
     if operation == "DELETE":
@@ -148,15 +143,15 @@
                 UPDATE {target_table}
                 SET update_time = CURRENT_TIMESTAMP
                 WHERE {clause};
             END;
     """
 
 
-def build_drop_trigger(operation, source_table, when='AFTER'):
+def build_drop_trigger(operation, source_table, when="AFTER"):
     """Drops a non-PostgreSQL trigger by name"""
     trigger_name = get_trigger_name(operation, source_table, when)
     return f"DROP TRIGGER IF EXISTS {trigger_name}"
 
 
 def get_trigger_name(operation, source_table, when):
     """Non-PostgreSQL trigger name"""
```

### Comparing `galaxy-data-22.1.1/galaxy/model/migrate/triggers/update_audit_table.py` & `galaxy-data-23.0.1/galaxy/model/triggers/update_audit_table.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,30 +1,29 @@
-from galaxy.model.migrate.versions.util import execute_statements
-
+from galaxy.model.triggers.utils import execute_statements
 
 # function name prefix
 fn_prefix = "fn_audit_history_by"
 
 # map between source table and associated incoming id field
 trigger_config = {
-    'history_dataset_association': "history_id",
-    'history_dataset_collection_association': "history_id",
-    'history': "id",
+    "history_dataset_association": "history_id",
+    "history_dataset_collection_association": "history_id",
+    "history": "id",
 }
 
 
 def install(engine):
     """Install history audit table triggers"""
-    sql = _postgres_install(engine) if 'postgres' in engine.name else _sqlite_install()
+    sql = _postgres_install(engine) if "postgres" in engine.name else _sqlite_install()
     execute_statements(engine, sql)
 
 
 def remove(engine):
     """Uninstall history audit table triggers"""
-    sql = _postgres_remove() if 'postgres' in engine.name else _sqlite_remove()
+    sql = _postgres_remove() if "postgres" in engine.name else _sqlite_remove()
     execute_statements(engine, sql)
 
 
 # Postgres trigger installation
 
 
 def _postgres_remove():
@@ -143,15 +142,14 @@
 
 
 def _sqlite_install():
     # delete old stuff first
     sql = _sqlite_remove()
 
     def trigger_def(source_table, id_field, operation, when="AFTER"):
-
         # only one trigger per operation/table in simple databases, so
         # trigger name is less descriptive
         trigger_name = get_trigger_name(source_table, operation, when)
 
         return f"""
             CREATE TRIGGER {trigger_name}
                 {when} {operation}
```

### Comparing `galaxy-data-22.1.1/galaxy/model/migrate/versions/0097_add_ctx_rev_column.py` & `galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/b182f655505f_add_workflow_source_metadata_column.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,35 +1,34 @@
-"""
-Migration script to add the ctx_rev column to the tool_shed_repository table.
-"""
+"""add workflow.source_metadata column
 
-import logging
+Revision ID: b182f655505f
+Revises: e7b6dcb09efd
+Create Date: 2022-03-14 12:56:57.067748
 
-from sqlalchemy import (
-    Column,
-    MetaData
-)
+"""
+from alembic import op
+from sqlalchemy import Column
 
-from galaxy.model.custom_types import TrimmedString
-from galaxy.model.migrate.versions.util import (
-    add_column,
-    drop_column
+from galaxy.model.custom_types import JSONType
+from galaxy.model.migrations.util import (
+    column_exists,
+    drop_column,
 )
 
-log = logging.getLogger(__name__)
-metadata = MetaData()
-
+# revision identifiers, used by Alembic.
+revision = "b182f655505f"
+down_revision = "e7b6dcb09efd"
+branch_labels = None
+depends_on = None
 
-def upgrade(migrate_engine):
-    print(__doc__)
-    metadata.bind = migrate_engine
-    metadata.reflect()
+# database object names used in this revision
+table_name = "workflow"
+column_name = "source_metadata"
 
-    col = Column("ctx_rev", TrimmedString(10))
-    add_column(col, 'tool_shed_repository', metadata)
 
+def upgrade():
+    if not column_exists(table_name, column_name):
+        op.add_column(table_name, Column(column_name, JSONType))
 
-def downgrade(migrate_engine):
-    metadata.bind = migrate_engine
-    metadata.reflect()
 
-    drop_column('ctx_rev', 'tool_shed_repository', metadata)
+def downgrade():
+    drop_column(table_name, column_name)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/migrate/versions/0153_add_custos_authnz_token_table.py` & `galaxy-data-23.0.1/galaxy/model/migrations/alembic/versions_gxy/59e024ceaca1_add_export_association_table.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,55 +1,44 @@
-"""
-Migration for adding custos_authnz_token table.
-"""
+"""add export association table
 
-import logging
+Revision ID: 59e024ceaca1
+Revises: e0e3bb173ee6
+Create Date: 2022-10-12 18:02:34.659770
 
+"""
+from alembic import op
 from sqlalchemy import (
     Column,
     DateTime,
-    ForeignKey,
     Integer,
-    MetaData,
-    String,
-    Table,
-    Text,
-    UniqueConstraint
 )
 
-from galaxy.model.migrate.versions.util import (
-    create_table,
-    drop_table
+from galaxy.model.custom_types import (
+    JSONType,
+    TrimmedString,
+    UUIDType,
 )
 
-log = logging.getLogger(__name__)
-metadata = MetaData()
-
-CustosAuthnzToken_table = Table(
-    "custos_authnz_token", metadata,
-    Column('id', Integer, primary_key=True),
-    Column('user_id', Integer, ForeignKey("galaxy_user.id")),
-    Column('external_user_id', String(64)),
-    Column('provider', String(255)),
-    Column('access_token', Text),
-    Column('id_token', Text),
-    Column('refresh_token', Text),
-    Column("expiration_time", DateTime),
-    Column("refresh_expiration_time", DateTime),
-    UniqueConstraint("user_id", "external_user_id", "provider"),
-    UniqueConstraint("external_user_id", "provider"),
-)
-
-
-def upgrade(migrate_engine):
-    print(__doc__)
-    metadata.bind = migrate_engine
-    metadata.reflect()
-
-    create_table(CustosAuthnzToken_table)
-
+# revision identifiers, used by Alembic.
+revision = "59e024ceaca1"
+down_revision = "e0e3bb173ee6"
+branch_labels = None
+depends_on = None
+
+
+table_name = "store_export_association"
+
+
+def upgrade():
+    op.create_table(
+        table_name,
+        Column("id", Integer, primary_key=True),
+        Column("task_uuid", UUIDType(), index=True, unique=True),
+        Column("create_time", DateTime),
+        Column("object_type", TrimmedString(32)),
+        Column("object_id", Integer),
+        Column("export_metadata", JSONType),
+    )
 
-def downgrade(migrate_engine):
-    metadata.bind = migrate_engine
-    metadata.reflect()
 
-    drop_table(CustosAuthnzToken_table)
+def downgrade():
+    op.drop_table(table_name)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/none_like.py` & `galaxy-data-23.0.1/galaxy/model/none_like.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,19 +15,20 @@
     def __getattr__(self, name):
         value = RecursiveNone()
         setattr(self, name, value)
         return value
 
     def __bool__(self):
         return False
+
     __nonzero__ = __bool__
 
 
 class NoneDataset(RecursiveNone):
-    def __init__(self, datatypes_registry=None, ext='data', dbkey='?'):
+    def __init__(self, datatypes_registry=None, ext="data", dbkey="?"):
         self.ext = self.extension = ext
         self.dbkey = dbkey
         assert datatypes_registry is not None
         self.datatype = datatypes_registry.get_datatype_by_extension(ext)
         self._metadata = None
         self.metadata = MetadataCollection(self)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/orm/engine_factory.py` & `galaxy-data-23.0.1/galaxy/model/orm/engine_factory.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import inspect
 import logging
 import os
 import threading
 import time
 from multiprocessing.util import register_after_fork
+from typing import Dict
 
 from sqlalchemy import (
     create_engine,
     event,
     exc,
 )
 from sqlalchemy.engine import Engine
@@ -22,82 +23,90 @@
     QUERY_COUNT_LOCAL.times = []
 
 
 def log_request_query_counts(req_id):
     try:
         times = QUERY_COUNT_LOCAL.times
         if times:
-            log.info(f"Executed [{len(times)}] SQL requests in for web request [{req_id}] ({sum(times) * 1000.0:0.3f} ms)")
+            log.info(
+                f"Executed [{len(times)}] SQL requests in for web request [{req_id}] ({sum(times) * 1000.0:0.3f} ms)"
+            )
     except AttributeError:
         # Didn't record anything so don't worry.
         pass
 
 
 def stripwd(s):
     if s.startswith(WORKING_DIRECTORY):
-        return s[len(WORKING_DIRECTORY):]
+        return s[len(WORKING_DIRECTORY) :]
     return s
 
 
 def pretty_stack():
     rval = []
     for _, fname, line, funcname, _, _ in inspect.stack()[2:]:
         rval.append("%s:%s@%d" % (stripwd(fname), funcname, line))
     return rval
 
 
-def build_engine(url, engine_options, database_query_profiling_proxy=False, trace_logger=None, slow_query_log_threshold=0, thread_local_log=None, log_query_counts=False):
+def build_engine(
+    url: str,
+    engine_options=None,
+    database_query_profiling_proxy=False,
+    trace_logger=None,
+    slow_query_log_threshold=0,
+    thread_local_log=None,
+    log_query_counts=False,
+):
     if database_query_profiling_proxy or slow_query_log_threshold or thread_local_log or log_query_counts:
 
         @event.listens_for(Engine, "before_cursor_execute")
         def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
-            conn.info.setdefault('query_start_time', []).append(time.time())
+            conn.info.setdefault("query_start_time", []).append(time.time())
 
     if slow_query_log_threshold or thread_local_log or log_query_counts:
 
         @event.listens_for(Engine, "after_cursor_execute")
         def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
-            total = time.time() - conn.info['query_start_time'].pop(-1)
-            fragment = 'Slow query: '
+            total = time.time() - conn.info["query_start_time"].pop(-1)
+            fragment = "Slow query: "
             if total > slow_query_log_threshold:
                 log.debug(f"{fragment}{total:f}(s)\n{statement}\nParameters: {parameters}")
             if database_query_profiling_proxy:
                 if trace_logger:
                     trace_logger.log(
                         "sqlalchemy_query",
                         message="Query executed",
                         statement=statement,
                         parameters=parameters,
                         executemany=executemany,
-                        duration=total
+                        duration=total,
                     )
                 else:
                     thread_ident = threading.get_ident()
                     stack = " > ".join(pretty_stack())
-                    log.debug(f"statement: {statement} parameters: {parameters} executemany: {executemany} duration: {total} stack: {stack} thread: {thread_ident}")
+                    log.debug(
+                        f"statement: {statement} parameters: {parameters} executemany: {executemany} duration: {total} stack: {stack} thread: {thread_ident}"
+                    )
             if log_query_counts:
                 try:
                     QUERY_COUNT_LOCAL.times.append(total)
                 except AttributeError:
                     # Not a web thread.
                     pass
             if thread_local_log is not None:
                 try:
                     if thread_local_log.log:
                         log.debug(f"Request query: {total:f}(s)\n{statement}\nParameters: {parameters}")
                 except AttributeError:
                     pass
 
-    # Set check_same_thread to False for sqlite, handled by request-specific session
-    # See https://fastapi.tiangolo.com/tutorial/sql-databases/#note
-    connect_args = {}
-    if 'sqlite://' in url:
-        connect_args['check_same_thread'] = False
-    # Create the database engine
-    engine = create_engine(url, connect_args=connect_args, **engine_options)
+    engine_options = engine_options or {}
+    engine_options = set_sqlite_connect_args(engine_options, url)
+    engine = create_engine(url, **engine_options)
 
     # Prevent sharing connection across fork: https://docs.sqlalchemy.org/en/14/core/pooling.html#using-connection-pools-with-multiprocessing-or-os-fork
     register_after_fork(engine, lambda e: e.dispose())
 
     @event.listens_for(engine, "connect")
     def connect(dbapi_connection, connection_record):
         connection_record.info["pid"] = os.getpid()
@@ -109,7 +118,19 @@
             connection_record.dbapi_connection = connection_proxy.dbapi_connection = None
             raise exc.DisconnectionError(
                 "Connection record belongs to pid %s, "
                 "attempting to check out in pid %s" % (connection_record.info["pid"], pid)
             )
 
     return engine
+
+
+def set_sqlite_connect_args(engine_options: Dict, url: str):
+    """
+    Add or update `connect_args` in `engine_options` if db is sqlite.
+    Set check_same_thread to False for sqlite, handled by request-specific session.
+    See https://fastapi.tiangolo.com/tutorial/sql-databases/#note
+    """
+    if url.startswith("sqlite://"):
+        connect_args = engine_options.setdefault("connect_args", {})
+        connect_args["check_same_thread"] = False
+    return engine_options
```

### Comparing `galaxy-data-22.1.1/galaxy/model/orm/scripts.py` & `galaxy-data-23.0.1/galaxy/model/orm/scripts.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,67 +1,68 @@
 """
-Code to support database helper scripts (create_db.py, manage_db.py, etc...).
+Code to support database helper scripts (create_toolshed_db.py, migrate_toolshed_db.py, etc...).
 """
 import argparse
 import logging
 import os
 import sys
 
-from migrate.versioning.shell import main as migrate_main
+import alembic.config
 
+from galaxy.model.migrations import (
+    GXY,
+    TSI,
+)
+from galaxy.model.migrations.scripts import get_configuration
 from galaxy.util.path import get_ext
-from galaxy.util.properties import find_config_file, get_data_dir, load_app_properties
+from galaxy.util.properties import (
+    find_config_file,
+    get_data_dir,
+    load_app_properties,
+)
 from galaxy.util.script import populate_config_args
 
-
 log = logging.getLogger(__name__)
 
-DEFAULT_CONFIG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, 'config', 'sample'))
-DEFAULT_CONFIG_NAMES = ['galaxy', 'universe_wsgi']
-DEFAULT_CONFIG_PREFIX = ''
-DEFAULT_DATABASE = 'galaxy'
+DEFAULT_CONFIG_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, "config", "sample"))
+DEFAULT_CONFIG_NAMES = ["galaxy", "universe_wsgi"]
+DEFAULT_CONFIG_PREFIX = ""
+DEFAULT_DATABASE = "galaxy"
 
 DATABASE = {
-    "galaxy":
-        {
-            'repo': 'galaxy/model/migrate',
-            'default_sqlite_file': 'universe.sqlite',
-            'config_override': 'GALAXY_CONFIG_',
-        },
-    "tools":
-        {
-            'repo': 'galaxy/model/tool_shed_install/migrate',
-            'default_sqlite_file': 'universe.sqlite',
-            'config_override': 'GALAXY_CONFIG_',
-        },
-    "tool_shed":
-        {
-            'repo': 'tool_shed/webapp/model/migrate',
-            'config_names': ['tool_shed', 'tool_shed_wsgi'],
-            'default_sqlite_file': 'community.sqlite',
-            'config_override': 'TOOL_SHED_CONFIG_',
-            'config_section': 'tool_shed',
-        },
-    "install":
-        {
-            'repo': 'galaxy/model/tool_shed_install/migrate',
-            'config_prefix': 'install_',
-            'default_sqlite_file': 'install.sqlite',
-            'config_override': 'GALAXY_INSTALL_CONFIG_',
-        },
+    "galaxy": {
+        "default_sqlite_file": "universe.sqlite",
+        "config_override": "GALAXY_CONFIG_",
+    },
+    "tool_shed": {
+        "repo": "tool_shed/webapp/model/migrate",
+        "config_names": ["tool_shed", "tool_shed_wsgi"],
+        "default_sqlite_file": "community.sqlite",
+        "config_override": "TOOL_SHED_CONFIG_",
+        "config_section": "tool_shed",
+    },
+    "install": {
+        "config_prefix": "install_",
+        "default_sqlite_file": "install.sqlite",
+        "config_override": "GALAXY_INSTALL_CONFIG_",
+    },
 }
 
 
 def _read_model_arguments(argv, use_argparse=False):
     if use_argparse:
         parser = argparse.ArgumentParser()
-        parser.add_argument('database', metavar='DATABASE', type=str,
-                            default="galaxy",
-                            nargs='?',
-                            help='database to target (galaxy, tool_shed, install)')
+        parser.add_argument(
+            "database",
+            metavar="DATABASE",
+            type=str,
+            default="galaxy",
+            nargs="?",
+            help="database to target (galaxy, tool_shed, install)",
+        )
         populate_config_args(parser)
         args = parser.parse_args(argv[1:] if argv else [])
         return args.config_file, args.config_section, args.database
     else:
         config_file = None
         for arg in ["-c", "--config", "--config-file"]:
             if arg in argv:
@@ -72,15 +73,15 @@
         if "--config-section" in argv:
             pos = argv.index("--config-section")
             argv.pop(pos)
             config_section = argv.pop(pos)
         if argv and (argv[-1] in DATABASE):
             database = argv.pop()  # database name tool_shed, galaxy, or install.
         else:
-            database = 'galaxy'
+            database = "galaxy"
         return config_file, config_section, database
 
 
 def get_config(argv, use_argparse=True, cwd=None):
     """
     Read sys.argv and parse out repository of migrations and database url.
 
@@ -102,46 +103,70 @@
     >>> config['db_url']
     'sqlite:///pg/testdb1'
     >>> write_ini('galaxy.ini', 'data_dir', '/moo')
     >>> config = get_config(['manage_db.py'], cwd=config_dir)
     >>> uri_with_env = os.getenv("GALAXY_TEST_DBURI", "sqlite:////moo/universe.sqlite?isolation_level=IMMEDIATE")
     >>> config['db_url'] == uri_with_env
     True
-    >>> config['repo'].endswith('galaxy/model/migrate')
-    True
     >>> rmtree(config_dir)
     """
     config_file, config_section, database = _read_model_arguments(argv, use_argparse=use_argparse)
     database_defaults = DATABASE[database]
     if config_file is None:
-        config_names = database_defaults.get('config_names', DEFAULT_CONFIG_NAMES)
+        config_names = database_defaults.get("config_names", DEFAULT_CONFIG_NAMES)
         if cwd:
-            cwd = [cwd, os.path.join(cwd, 'config')]
+            cwd = [cwd, os.path.join(cwd, "config")]
         else:
             cwd = [DEFAULT_CONFIG_DIR]
         config_file = find_config_file(config_names, dirs=cwd)
 
-    repo = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, os.pardir, database_defaults['repo'])
-    config_prefix = database_defaults.get('config_prefix', DEFAULT_CONFIG_PREFIX)
-    config_override = database_defaults.get('config_override', 'GALAXY_CONFIG_')
-    default_sqlite_file = database_defaults['default_sqlite_file']
+    repo = database_defaults.get("repo")
+    if repo:
+        repo = os.path.join(os.path.dirname(__file__), os.pardir, os.pardir, os.pardir, repo)
+
+    config_prefix = database_defaults.get("config_prefix", DEFAULT_CONFIG_PREFIX)
+    config_override = database_defaults.get("config_override", "GALAXY_CONFIG_")
+    default_sqlite_file = database_defaults["default_sqlite_file"]
     if config_section is None:
-        if not config_file or get_ext(config_file, ignore='sample') == 'yaml':
-            config_section = database_defaults.get('config_section', None)
+        if not config_file or get_ext(config_file, ignore="sample") == "yaml":
+            config_section = database_defaults.get("config_section", None)
         else:
             # Just use the default found by load_app_properties.
             config_section = None
-    properties = load_app_properties(config_file=config_file, config_prefix=config_override, config_section=config_section)
+    properties = load_app_properties(
+        config_file=config_file, config_prefix=config_override, config_section=config_section
+    )
 
     if (f"{config_prefix}database_connection") in properties:
         db_url = properties[f"{config_prefix}database_connection"]
     else:
         db_url = f"sqlite:///{os.path.join(get_data_dir(properties), default_sqlite_file)}?isolation_level=IMMEDIATE"
-    install_database_connection = properties.get('install_database_connection')
+    install_database_connection = properties.get("install_database_connection")
 
-    return dict(db_url=db_url, repo=repo, config_file=config_file, database=database, install_database_connection=install_database_connection)
+    return dict(
+        db_url=db_url,
+        repo=repo,
+        config_file=config_file,
+        database=database,
+        install_database_connection=install_database_connection,
+    )
 
 
 def manage_db():
-    # Migrate has its own args, so cannot use argparse
-    config = get_config(sys.argv, use_argparse=False, cwd=os.getcwd())
-    migrate_main(repository=config['repo'], url=config['db_url'])
+    # This is a duplicate implementation of scripts/migrate_db.py.
+    # See run_alembic.sh for usage.
+    def _insert_x_argument(key, value):
+        sys.argv.insert(1, f"{key}={value}")
+        sys.argv.insert(1, "-x")
+
+    gxy_config, tsi_config, _ = get_configuration(sys.argv, os.getcwd())
+    _insert_x_argument("tsi_url", tsi_config.url)
+    _insert_x_argument("gxy_url", gxy_config.url)
+
+    if "heads" in sys.argv and "upgrade" in sys.argv:
+        i = sys.argv.index("heads")
+        sys.argv[i] = f"{GXY}@head"
+        alembic.config.main()
+        sys.argv[i] = f"{TSI}@head"
+        alembic.config.main()
+    else:
+        alembic.config.main()
```

### Comparing `galaxy-data-22.1.1/galaxy/model/scoped_session.py` & `galaxy-data-23.0.1/galaxy/model/scoped_session.py`

 * *Files identical despite different names*

### Comparing `galaxy-data-22.1.1/galaxy/model/search.py` & `galaxy-data-23.0.1/galaxy/model/search.py`

 * *Files 8% similar despite different names*

```diff
@@ -49,15 +49,15 @@
     Library,
     LibraryDataset,
     LibraryDatasetDatasetAssociation,
     LibraryFolder,
     Page,
     PageRevision,
     StoredWorkflow,
-    StoredWorkflowTagAssociation
+    StoredWorkflowTagAssociation,
 )
 from galaxy.model.tool_shed_install import ToolVersion
 
 log = logging.getLogger(__name__)
 
 
 class ViewField:
@@ -96,30 +96,30 @@
     def __init__(self):
         self.query = None
         self.do_query = False
         self.state = {}
         self.post_filter = []
 
     def decode_query_ids(self, trans, conditional):
-        if conditional.operator == 'and':
+        if conditional.operator == "and":
             self.decode_query_ids(trans, conditional.left)
             self.decode_query_ids(trans, conditional.right)
         else:
-            left_base = conditional.left.split('.')[0]
+            left_base = conditional.left.split(".")[0]
             if left_base in self.FIELDS:
                 field = self.FIELDS[left_base]
                 if field.id_decode:
                     conditional.right = trans.security.decode_id(conditional.right)
 
     def filter(self, left, operator, right):
-        if operator == 'and':
+        if operator == "and":
             self.filter(left.left, left.operator, left.right)
             self.filter(right.left, right.operator, right.right)
         else:
-            left_base = left.split('.')[0]
+            left_base = left.split(".")[0]
             if left_base in self.FIELDS:
                 self.do_query = True
                 field = self.FIELDS[left_base]
                 if field.sqlalchemy_field is not None:
                     clazz, attribute = field.sqlalchemy_field
                     sqlalchemy_field_value = getattr(clazz, attribute)
                     if operator == "=":
@@ -157,132 +157,131 @@
 ##################
 # Library Dataset Searching
 ##################
 
 
 def library_extended_metadata_filter(view, left, operator, right):
     view.do_query = True
-    if 'extended_metadata_joined' not in view.state:
+    if "extended_metadata_joined" not in view.state:
         view.query = view.query.join(ExtendedMetadata)
-        view.state['extended_metadata_joined'] = True
+        view.state["extended_metadata_joined"] = True
     alias = aliased(ExtendedMetadataIndex)
     field = f"/{'/'.join(left.split('.')[1:])}"
     view.query = view.query.filter(
-        and_(
-            ExtendedMetadata.id == alias.extended_metadata_id,
-            alias.path == field,
-            alias.value == str(right)
-        )
+        and_(ExtendedMetadata.id == alias.extended_metadata_id, alias.path == field, alias.value == str(right))
     )
 
 
 def ldda_parent_library_filter(item, left, operator, right):
-    if operator == '=':
+    if operator == "=":
         return right == item.library_dataset.folder.parent_library.id
-    elif operator == '!=':
+    elif operator == "!=":
         return right != item.library_dataset.folder.parent_library.id
     raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 class LibraryDatasetDatasetView(ViewQueryBaseClass):
     VIEW_NAME = "library_dataset_dataset"
     FIELDS = {
-        'extended_metadata': ViewField('extended_metadata', handler=library_extended_metadata_filter),
-        'name': ViewField('name', sqlalchemy_field=(LibraryDatasetDatasetAssociation, "name")),
-        'id': ViewField('id', sqlalchemy_field=(LibraryDatasetDatasetAssociation, 'id'), id_decode=True),
-        'deleted': ViewField('deleted', sqlalchemy_field=(LibraryDatasetDatasetAssociation, "deleted")),
-        'parent_library_id': ViewField('parent_library_id', id_decode=True, post_filter=ldda_parent_library_filter),
-        'data_type': ViewField('data_type', sqlalchemy_field=(LibraryDatasetDatasetAssociation, "extension")),
+        "extended_metadata": ViewField("extended_metadata", handler=library_extended_metadata_filter),
+        "name": ViewField("name", sqlalchemy_field=(LibraryDatasetDatasetAssociation, "name")),
+        "id": ViewField("id", sqlalchemy_field=(LibraryDatasetDatasetAssociation, "id"), id_decode=True),
+        "deleted": ViewField("deleted", sqlalchemy_field=(LibraryDatasetDatasetAssociation, "deleted")),
+        "parent_library_id": ViewField("parent_library_id", id_decode=True, post_filter=ldda_parent_library_filter),
+        "data_type": ViewField("data_type", sqlalchemy_field=(LibraryDatasetDatasetAssociation, "extension")),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(LibraryDatasetDatasetAssociation)
 
 
 ##################
 # Library Searching
 ##################
 
+
 class LibraryView(ViewQueryBaseClass):
     VIEW_NAME = "library"
     FIELDS = {
-        'name': ViewField('name', sqlalchemy_field=(Library, "name")),
-        'id': ViewField('id', sqlalchemy_field=(Library, 'id'), id_decode=True),
-        'deleted': ViewField('deleted', sqlalchemy_field=(Library, "deleted")),
+        "name": ViewField("name", sqlalchemy_field=(Library, "name")),
+        "id": ViewField("id", sqlalchemy_field=(Library, "id"), id_decode=True),
+        "deleted": ViewField("deleted", sqlalchemy_field=(Library, "deleted")),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(Library)
 
 
 ##################
 # Library Folder Searching
 ##################
 def library_folder_parent_library_id_filter(item, left, operator, right):
-    if operator == '=':
+    if operator == "=":
         return item.parent_library.id == right
-    if operator == '!=':
+    if operator == "!=":
         return item.parent_library.id != right
     raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 def library_path_filter(item, left, operator, right):
     lpath = f"/{'/'.join(item.library_path)}"
-    if operator == '=':
+    if operator == "=":
         return lpath == right
-    if operator == '!=':
+    if operator == "!=":
         return lpath != right
     raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 class LibraryFolderView(ViewQueryBaseClass):
     VIEW_NAME = "library_folder"
     FIELDS = {
-        'name': ViewField('name', sqlalchemy_field=(LibraryFolder, "name")),
-        'id': ViewField('id', sqlalchemy_field=(LibraryFolder, "id"), id_decode=True),
-        'parent_id': ViewField('parent_id', sqlalchemy_field=(LibraryFolder, "parent_id"), id_decode=True),
-        'parent_library_id': ViewField('parent_library_id', post_filter=library_folder_parent_library_id_filter, id_decode=True),
-        'library_path': ViewField('library_path', post_filter=library_path_filter)
+        "name": ViewField("name", sqlalchemy_field=(LibraryFolder, "name")),
+        "id": ViewField("id", sqlalchemy_field=(LibraryFolder, "id"), id_decode=True),
+        "parent_id": ViewField("parent_id", sqlalchemy_field=(LibraryFolder, "parent_id"), id_decode=True),
+        "parent_library_id": ViewField(
+            "parent_library_id", post_filter=library_folder_parent_library_id_filter, id_decode=True
+        ),
+        "library_path": ViewField("library_path", post_filter=library_path_filter),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(LibraryFolder)
 
 
 ##################
 # Library Dataset Searching
 ##################
 def library_dataset_name_filter(item, left, operator, right):
-    if operator == '=':
+    if operator == "=":
         return item.name == right
-    if operator == '!=':
+    if operator == "!=":
         return item.name != right
     raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 class LibraryDatasetView(ViewQueryBaseClass):
     VIEW_NAME = "library_dataset"
     FIELDS = {
-        'name': ViewField('name', post_filter=library_dataset_name_filter),
-        'id': ViewField('id', sqlalchemy_field=(LibraryDataset, "id"), id_decode=True),
-        'folder_id': ViewField('folder_id', sqlalchemy_field=(LibraryDataset, "folder_id"), id_decode=True)
+        "name": ViewField("name", post_filter=library_dataset_name_filter),
+        "id": ViewField("id", sqlalchemy_field=(LibraryDataset, "id"), id_decode=True),
+        "folder_id": ViewField("folder_id", sqlalchemy_field=(LibraryDataset, "folder_id"), id_decode=True),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(LibraryDataset)
 
 
 ##################
 # Tool Searching
 ##################
 class ToolView(ViewQueryBaseClass):
     VIEW_NAME = "tool"
     FIELDS = {
-        'tool_id': ViewField('name', sqlalchemy_field=(ToolVersion, "tool_id")),
-        'id': ViewField('id', sqlalchemy_field=(ToolVersion, "id")),
+        "tool_id": ViewField("name", sqlalchemy_field=(ToolVersion, "tool_id")),
+        "id": ViewField("id", sqlalchemy_field=(ToolVersion, "id")),
     }
 
     def search(self, trans):
         self.query = trans.install_model.context.query(ToolVersion)
 
 
 ##################
@@ -290,56 +289,56 @@
 ##################
 def history_dataset_handle_tag(view, left, operator, right):
     if operator == "=":
         view.do_query = True
         # aliasing the tag association table, so multiple links to different tags can be formed during a single query
         tag_table = aliased(HistoryDatasetAssociationTagAssociation)
 
-        view.query = view.query.filter(
-            HistoryDatasetAssociation.id == tag_table.history_dataset_association_id
-        )
+        view.query = view.query.filter(HistoryDatasetAssociation.id == tag_table.history_dataset_association_id)
         tmp = right.split(":")
         view.query = view.query.filter(tag_table.user_tname == tmp[0])
         if len(tmp) > 1:
             view.query = view.query.filter(tag_table.user_value == tmp[1])
     else:
         raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 def history_dataset_extended_metadata_filter(view, left, operator, right):
     view.do_query = True
-    if 'extended_metadata_joined' not in view.state:
+    if "extended_metadata_joined" not in view.state:
         view.query = view.query.join(ExtendedMetadata)
-        view.state['extended_metadata_joined'] = True
+        view.state["extended_metadata_joined"] = True
     alias = aliased(ExtendedMetadataIndex)
     field = f"/{'/'.join(left.split('.')[1:])}"
     view.query = view.query.filter(
-        and_(
-            ExtendedMetadata.id == alias.extended_metadata_id,
-            alias.path == field,
-            alias.value == str(right)
-        )
+        and_(ExtendedMetadata.id == alias.extended_metadata_id, alias.path == field, alias.value == str(right))
     )
 
 
 class HistoryDatasetView(ViewQueryBaseClass):
     DOMAIN = "history_dataset"
     FIELDS = {
-        'name': ViewField('name', sqlalchemy_field=(HistoryDatasetAssociation, "name")),
-        'id': ViewField('id', sqlalchemy_field=(HistoryDatasetAssociation, "id"), id_decode=True),
-        'history_id': ViewField('history_id', sqlalchemy_field=(HistoryDatasetAssociation, "history_id"), id_decode=True),
-        'tag': ViewField("tag", handler=history_dataset_handle_tag),
-        'copied_from_ldda_id': ViewField("copied_from_ldda_id",
-                                         sqlalchemy_field=(HistoryDatasetAssociation, "copied_from_library_dataset_dataset_association_id"),
-                                         id_decode=True),
-        'copied_from_hda_id': ViewField("copied_from_hda_id",
-                                        sqlalchemy_field=(HistoryDatasetAssociation, "copied_from_history_dataset_association_id"),
-                                        id_decode=True),
-        'deleted': ViewField('deleted', sqlalchemy_field=(HistoryDatasetAssociation, "deleted")),
-        'extended_metadata': ViewField('extended_metadata', handler=history_dataset_extended_metadata_filter)
+        "name": ViewField("name", sqlalchemy_field=(HistoryDatasetAssociation, "name")),
+        "id": ViewField("id", sqlalchemy_field=(HistoryDatasetAssociation, "id"), id_decode=True),
+        "history_id": ViewField(
+            "history_id", sqlalchemy_field=(HistoryDatasetAssociation, "history_id"), id_decode=True
+        ),
+        "tag": ViewField("tag", handler=history_dataset_handle_tag),
+        "copied_from_ldda_id": ViewField(
+            "copied_from_ldda_id",
+            sqlalchemy_field=(HistoryDatasetAssociation, "copied_from_library_dataset_dataset_association_id"),
+            id_decode=True,
+        ),
+        "copied_from_hda_id": ViewField(
+            "copied_from_hda_id",
+            sqlalchemy_field=(HistoryDatasetAssociation, "copied_from_history_dataset_association_id"),
+            id_decode=True,
+        ),
+        "deleted": ViewField("deleted", sqlalchemy_field=(HistoryDatasetAssociation, "deleted")),
+        "extended_metadata": ViewField("extended_metadata", handler=history_dataset_extended_metadata_filter),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(HistoryDatasetAssociation)
 
 
 ##################
@@ -347,213 +346,188 @@
 ##################
 
 
 def history_handle_tag(view, left, operator, right):
     if operator == "=":
         view.do_query = True
         tag_table = aliased(HistoryTagAssociation)
-        view.query = view.query.filter(
-            History.id == tag_table.history_id
-        )
+        view.query = view.query.filter(History.id == tag_table.history_id)
         tmp = right.split(":")
         view.query = view.query.filter(tag_table.user_tname == tmp[0])
         if len(tmp) > 1:
             view.query = view.query.filter(tag_table.user_value == tmp[1])
     else:
         raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 def history_handle_annotation(view, left, operator, right):
     if operator == "=":
         view.do_query = True
-        view.query = view.query.filter(and_(
-            HistoryAnnotationAssociation.history_id == History.id,
-            HistoryAnnotationAssociation.annotation == right
-        ))
+        view.query = view.query.filter(
+            and_(
+                HistoryAnnotationAssociation.history_id == History.id, HistoryAnnotationAssociation.annotation == right
+            )
+        )
     elif operator == "like":
         view.do_query = True
-        view.query = view.query.filter(and_(
-            HistoryAnnotationAssociation.history_id == History.id,
-            HistoryAnnotationAssociation.annotation.like(right)
-        ))
+        view.query = view.query.filter(
+            and_(
+                HistoryAnnotationAssociation.history_id == History.id,
+                HistoryAnnotationAssociation.annotation.like(right),
+            )
+        )
     else:
         raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 class HistoryView(ViewQueryBaseClass):
     DOMAIN = "history"
     FIELDS = {
-        'name': ViewField('name', sqlalchemy_field=(History, "name")),
-        'id': ViewField('id', sqlalchemy_field=(History, "id"), id_decode=True),
-        'tag': ViewField("tag", handler=history_handle_tag),
-        'annotation': ViewField("annotation", handler=history_handle_annotation),
-        'deleted': ViewField('deleted', sqlalchemy_field=(History, "deleted"))
+        "name": ViewField("name", sqlalchemy_field=(History, "name")),
+        "id": ViewField("id", sqlalchemy_field=(History, "id"), id_decode=True),
+        "tag": ViewField("tag", handler=history_handle_tag),
+        "annotation": ViewField("annotation", handler=history_handle_annotation),
+        "deleted": ViewField("deleted", sqlalchemy_field=(History, "deleted")),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(History)
 
 
 ##################
 # Workflow Searching
 ##################
 
 
 def workflow_tag_handler(view, left, operator, right):
     if operator == "=":
         view.do_query = True
-        view.query = view.query.filter(
-            StoredWorkflow.id == StoredWorkflowTagAssociation.stored_workflow_id
-        )
+        view.query = view.query.filter(StoredWorkflow.id == StoredWorkflowTagAssociation.stored_workflow_id)
         tmp = right.split(":")
         view.query = view.query.filter(StoredWorkflowTagAssociation.user_tname == tmp[0])
         if len(tmp) > 1:
             view.query = view.query.filter(StoredWorkflowTagAssociation.user_value == tmp[1])
     else:
         raise GalaxyParseError(f"Invalid comparison operator: {operator}")
 
 
 class WorkflowView(ViewQueryBaseClass):
     DOMAIN = "workflow"
     FIELDS = {
-        'name': ViewField('name', sqlalchemy_field=(StoredWorkflow, "name")),
-        'id': ViewField('id', sqlalchemy_field=(StoredWorkflow, "id"), id_decode=True),
-        'tag': ViewField('tag', handler=workflow_tag_handler),
-        'deleted': ViewField('deleted', sqlalchemy_field=(StoredWorkflow, "deleted")),
+        "name": ViewField("name", sqlalchemy_field=(StoredWorkflow, "name")),
+        "id": ViewField("id", sqlalchemy_field=(StoredWorkflow, "id"), id_decode=True),
+        "tag": ViewField("tag", handler=workflow_tag_handler),
+        "deleted": ViewField("deleted", sqlalchemy_field=(StoredWorkflow, "deleted")),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(StoredWorkflow)
 
 
 ##################
 # Job Searching
 ##################
 
 
 def job_param_filter(view, left, operator, right):
     view.do_query = True
     alias = aliased(JobParameter)
-    param_name = re.sub(r'^param.', '', left)
-    view.query = view.query.filter(
-        and_(
-            Job.id == alias.job_id,
-            alias.name == param_name,
-            alias.value == dumps(right)
-        )
-    )
+    param_name = re.sub(r"^param.", "", left)
+    view.query = view.query.filter(and_(Job.id == alias.job_id, alias.name == param_name, alias.value == dumps(right)))
 
 
 def job_input_hda_filter(view, left, operator, right):
     view.do_query = True
     alias = aliased(JobToInputDatasetAssociation)
-    param_name = re.sub(r'^input_hda.', '', left)
-    view.query = view.query.filter(
-        and_(
-            Job.id == alias.job_id,
-            alias.name == param_name,
-            alias.dataset_id == right
-        )
-    )
+    param_name = re.sub(r"^input_hda.", "", left)
+    view.query = view.query.filter(and_(Job.id == alias.job_id, alias.name == param_name, alias.dataset_id == right))
 
 
 def job_input_ldda_filter(view, left, operator, right):
     view.do_query = True
     alias = aliased(JobToInputLibraryDatasetAssociation)
-    param_name = re.sub(r'^input_ldda.', '', left)
-    view.query = view.query.filter(
-        and_(
-            Job.id == alias.job_id,
-            alias.name == param_name,
-            alias.ldda_id == right
-        )
-    )
+    param_name = re.sub(r"^input_ldda.", "", left)
+    view.query = view.query.filter(and_(Job.id == alias.job_id, alias.name == param_name, alias.ldda_id == right))
 
 
 def job_output_hda_filter(view, left, operator, right):
     view.do_query = True
     alias = aliased(JobToOutputDatasetAssociation)
-    param_name = re.sub(r'^output_hda.', '', left)
-    view.query = view.query.filter(
-        and_(
-            Job.id == alias.job_id,
-            alias.name == param_name,
-            alias.dataset_id == right
-        )
-    )
+    param_name = re.sub(r"^output_hda.", "", left)
+    view.query = view.query.filter(and_(Job.id == alias.job_id, alias.name == param_name, alias.dataset_id == right))
 
 
 class JobView(ViewQueryBaseClass):
     DOMAIN = "job"
     FIELDS = {
-        'tool_name': ViewField('tool_name', sqlalchemy_field=(Job, "tool_id")),
-        'state': ViewField('state', sqlalchemy_field=(Job, "state")),
-        'param': ViewField('param', handler=job_param_filter),
-        'input_ldda': ViewField('input_ldda', handler=job_input_ldda_filter, id_decode=True),
-        'input_hda': ViewField('input_hda', handler=job_input_hda_filter, id_decode=True),
-        'output_hda': ViewField('output_hda', handler=job_output_hda_filter, id_decode=True)
+        "tool_name": ViewField("tool_name", sqlalchemy_field=(Job, "tool_id")),
+        "state": ViewField("state", sqlalchemy_field=(Job, "state")),
+        "param": ViewField("param", handler=job_param_filter),
+        "input_ldda": ViewField("input_ldda", handler=job_input_ldda_filter, id_decode=True),
+        "input_hda": ViewField("input_hda", handler=job_input_hda_filter, id_decode=True),
+        "output_hda": ViewField("output_hda", handler=job_output_hda_filter, id_decode=True),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(Job)
 
 
 ##################
 # Page Searching
 ##################
 
 
 class PageView(ViewQueryBaseClass):
     DOMAIN = "page"
     FIELDS = {
-        'id': ViewField('id', sqlalchemy_field=(Page, "id"), id_decode=True),
-        'slug': ViewField('slug', sqlalchemy_field=(Page, "slug")),
-        'title': ViewField('title', sqlalchemy_field=(Page, "title")),
-        'deleted': ViewField('deleted', sqlalchemy_field=(Page, "deleted"))
+        "id": ViewField("id", sqlalchemy_field=(Page, "id"), id_decode=True),
+        "slug": ViewField("slug", sqlalchemy_field=(Page, "slug")),
+        "title": ViewField("title", sqlalchemy_field=(Page, "title")),
+        "deleted": ViewField("deleted", sqlalchemy_field=(Page, "deleted")),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(Page)
 
 
 ##################
 # Page Revision Searching
 ##################
 
 
 class PageRevisionView(ViewQueryBaseClass):
     DOMAIN = "page_revision"
     FIELDS = {
-        'id': ViewField('id', sqlalchemy_field=(PageRevision, "id"), id_decode=True),
-        'title': ViewField('title', sqlalchemy_field=(PageRevision, "title")),
-        'page_id': ViewField('page_id', sqlalchemy_field=(PageRevision, "page_id"), id_decode=True),
+        "id": ViewField("id", sqlalchemy_field=(PageRevision, "id"), id_decode=True),
+        "title": ViewField("title", sqlalchemy_field=(PageRevision, "title")),
+        "page_id": ViewField("page_id", sqlalchemy_field=(PageRevision, "page_id"), id_decode=True),
     }
 
     def search(self, trans):
         self.query = trans.sa_session.query(PageRevision)
 
 
 # The view mapping takes a user's name for a table and maps it to a View class
 # that will handle queries.
 
 view_mapping = {
-    'library': LibraryView,
-    'library_folder': LibraryFolderView,
-    'library_dataset_dataset': LibraryDatasetDatasetView,
-    'library_dataset': LibraryDatasetView,
-    'lda': LibraryDatasetView,
-    'ldda': LibraryDatasetDatasetView,
-    'history_dataset': HistoryDatasetView,
-    'hda': HistoryDatasetView,
-    'history': HistoryView,
-    'workflow': WorkflowView,
-    'tool': ToolView,
-    'job': JobView,
-    'page': PageView,
-    'page_revision': PageRevisionView,
+    "library": LibraryView,
+    "library_folder": LibraryFolderView,
+    "library_dataset_dataset": LibraryDatasetDatasetView,
+    "library_dataset": LibraryDatasetView,
+    "lda": LibraryDatasetView,
+    "ldda": LibraryDatasetDatasetView,
+    "history_dataset": HistoryDatasetView,
+    "hda": HistoryDatasetView,
+    "history": HistoryView,
+    "workflow": WorkflowView,
+    "tool": ToolView,
+    "job": JobView,
+    "page": PageView,
+    "page_revision": PageRevisionView,
 }
 
 # The GQL gramar is defined in Parsley syntax ( https://parsley.readthedocs.io/ )
 
 gqlGrammar = r"""
 expr = 'select' bs field_desc:f bs 'from' bs word:t (
     bs 'where' bs conditional:c ws -> GalaxyQuery(f,t,c)
@@ -625,15 +599,15 @@
     """
     This class represents the data structure of the comparison arguments of a
     compiled GQL query (ie where name='Untitled History')
     """
 
     def __init__(self, left, right):
         self.left = left
-        self.operator = 'and'
+        self.operator = "and"
         self.right = right
 
 
 class GalaxyParseError(Exception):
     pass
 
 
@@ -645,23 +619,19 @@
     def decode_query_ids(self, trans):
         if self.query.conditional is not None:
             self.view.decode_query_ids(trans, self.query.conditional)
 
     def process(self, trans):
         self.view.search(trans)
         if self.query.conditional is not None:
-            self.view.filter(
-                self.query.conditional.left,
-                self.query.conditional.operator,
-                self.query.conditional.right
-            )
+            self.view.filter(self.query.conditional.left, self.query.conditional.operator, self.query.conditional.right)
         return self.view.get_results(True)
 
     def item_to_api_value(self, item):
-        r = item.to_dict(view='element')
+        r = item.to_dict(view="element")
         if self.query.field_list.count("*"):
             return r
         o = {}
         for a in r:
             if a in self.query.field_list:
                 o[a] = r[a]
         return o
@@ -669,20 +639,23 @@
 
 class GalaxySearchEngine:
     """
     Primary class for searching. Parses GQL (Galaxy Query Language) queries and returns a 'SearchQuery' class
     """
 
     def __init__(self):
-        self.parser = parsley.makeGrammar(gqlGrammar, {
-            're': re,
-            'GalaxyQuery': GalaxyQuery,
-            'GalaxyQueryComparison': GalaxyQueryComparison,
-            'GalaxyQueryAnd': GalaxyQueryAnd
-        })
+        self.parser = parsley.makeGrammar(
+            gqlGrammar,
+            {
+                "re": re,
+                "GalaxyQuery": GalaxyQuery,
+                "GalaxyQueryComparison": GalaxyQueryComparison,
+                "GalaxyQueryAnd": GalaxyQueryAnd,
+            },
+        )
 
     def query(self, query_text):
         q = self.parser(query_text).expr()
 
         if q.table_name in view_mapping:
             view = view_mapping[q.table_name]()
             return SearchQuery(view, q)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/security.py` & `galaxy-data-23.0.1/galaxy/model/security.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,47 @@
 import logging
 import socket
-from datetime import datetime, timedelta
-
-from sqlalchemy import and_, false, not_, or_
+from datetime import (
+    datetime,
+    timedelta,
+)
+from typing import List
+
+from sqlalchemy import (
+    and_,
+    false,
+    not_,
+    or_,
+)
 from sqlalchemy.orm import joinedload
 
 import galaxy.model
-from galaxy.security import Action, get_permitted_actions, RBACAgent
+from galaxy.security import (
+    Action,
+    get_permitted_actions,
+    RBACAgent,
+)
 from galaxy.util import listify
 from galaxy.util.bunch import Bunch
 
-
 log = logging.getLogger(__name__)
 
 
 class GalaxyRBACAgent(RBACAgent):
     def __init__(self, model, permitted_actions=None):
         self.model = model
         if permitted_actions:
             self.permitted_actions = permitted_actions
         # List of "library_item" objects and their associated permissions and info template objects
         self.library_item_assocs = (
             (self.model.Library, self.model.LibraryPermissions),
             (self.model.LibraryFolder, self.model.LibraryFolderPermissions),
             (self.model.LibraryDataset, self.model.LibraryDatasetPermissions),
-            (self.model.LibraryDatasetDatasetAssociation, self.model.LibraryDatasetDatasetAssociationPermissions))
+            (self.model.LibraryDatasetDatasetAssociation, self.model.LibraryDatasetDatasetAssociationPermissions),
+        )
 
     @property
     def sa_session(self):
         """Returns a SQLAlchemy session"""
         return self.model.context
 
     def sort_by_attr(self, seq, attr):
@@ -47,41 +60,49 @@
         intermed.sort()
         return [_[-1] for _ in intermed]
 
     def _get_npns_roles(self, trans):
         """
         non-private, non-sharing roles
         """
-        return trans.sa_session.query(trans.app.model.Role) \
-                    .filter(and_(self.model.Role.deleted == false(),
-                        self.model.Role.type != self.model.Role.types.PRIVATE,
-                        self.model.Role.type != self.model.Role.types.SHARING)) \
-                    .order_by(self.model.Role.name)
+        return (
+            trans.sa_session.query(trans.app.model.Role)
+            .filter(
+                and_(
+                    self.model.Role.deleted == false(),
+                    self.model.Role.type != self.model.Role.types.PRIVATE,
+                    self.model.Role.type != self.model.Role.types.SHARING,
+                )
+            )
+            .order_by(self.model.Role.name)
+        )
 
     def get_all_roles(self, trans, cntrller):
-        admin_controller = cntrller in ['library_admin']
+        admin_controller = cntrller in ["library_admin"]
         roles = set()
         if not trans.user:
             return self._get_npns_roles(trans)
         if admin_controller:
             # The library is public and the user is an admin, so all roles are legitimate
-            for role in trans.sa_session.query(trans.app.model.Role) \
-                                        .filter(self.model.Role.deleted == false()) \
-                                        .order_by(self.model.Role.name):
+            for role in (
+                trans.sa_session.query(trans.app.model.Role)
+                .filter(self.model.Role.deleted == false())
+                .order_by(self.model.Role.name)
+            ):
                 roles.add(role)
         else:
             # Add the current user's private role
             roles.add(self.get_private_user_role(trans.user))
             # Add the current user's sharing roles
             for role in self.get_sharing_roles(trans.user):
                 roles.add(role)
             # Add all remaining non-private, non-sharing roles
             for role in self._get_npns_roles(trans):
                 roles.add(role)
-        return self.sort_by_attr([role for role in roles], 'name')
+        return self.sort_by_attr([role for role in roles], "name")
 
     def get_roles_for_action(self, item, action):
         """
         Return a list containing the roles associated with given action on given item
         where item is one of Library, LibraryFolder, LibraryDatasetDatasetAssociation,
         LibraryDataset, Dataset.
         """
@@ -98,16 +119,16 @@
         in the item permissions form. Admins can select any role so the
         results are paginated in order to save the bandwidth and to speed
         things up.
         Standard users can select their own private role, any of their
         sharing roles and any public role (not private and not sharing).
         """
         roles = []
-        if query not in [None, '']:
-            query = query.strip().replace('_', '/_').replace('%', '/%').replace('/', '//')
+        if query not in [None, ""]:
+            query = query.strip().replace("_", "/_").replace("%", "/%").replace("/", "//")
             search_query = f"{query}%"
         else:
             search_query = None
         # Limit the query only to get the page needed
         if page is not None and page_limit is not None:
             limit = page * page_limit
         else:
@@ -123,19 +144,19 @@
             is_public_item = False
         # Admins can always choose from all non-deleted roles
         if trans.user_is_admin or trans.app.config.expose_user_email:
             if trans.user_is_admin:
                 db_query = trans.sa_session.query(trans.app.model.Role).filter(self.model.Role.deleted == false())
             else:
                 # User is not an admin but the configuration exposes all private roles to all users.
-                db_query = trans.sa_session.query(trans.app.model.Role) \
-                    .filter(and_(self.model.Role.deleted == false(),
-                                 self.model.Role.type == self.model.Role.types.PRIVATE))
+                db_query = trans.sa_session.query(trans.app.model.Role).filter(
+                    and_(self.model.Role.deleted == false(), self.model.Role.type == self.model.Role.types.PRIVATE)
+                )
             if search_query:
-                db_query = db_query.filter(self.model.Role.name.like(search_query, escape='/'))
+                db_query = db_query.filter(self.model.Role.name.like(search_query, escape="/"))
             total_count = db_query.count()
             if limit is not None:
                 # Takes the least number of results from beginning that includes the requested page
                 roles = db_query.order_by(self.model.Role.name).limit(limit).all()
                 page_start = (page * page_limit) - page_limit
                 page_end = page_start + page_limit
                 if total_count < page_start + 1:
@@ -178,15 +199,15 @@
                                 if self.ok_to_display(trans.user, ura.role):
                                     roles.append(ura.role)
 
         # Omit duplicated roles by converting to set
         return_roles = set(roles)
         if total_count is None:
             total_count = len(return_roles)
-        return self.sort_by_attr([role for role in return_roles], 'name'), total_count
+        return self.sort_by_attr([role for role in return_roles], "name"), total_count
 
     def get_legitimate_roles(self, trans, item, cntrller):
         """
         Return a sorted list of legitimate roles that can be associated with a permission on
         item where item is a Library or a Dataset.  The cntrller param is the controller from
         which the request is sent.  We cannot use trans.user_is_admin because the controller is
         what is important since admin users do not necessarily have permission to do things
@@ -203,18 +224,19 @@
 
             - if item is public, all non-private roles, except for the current user's private role,
               are legitimate.
             - if item is restricted, legitimate roles are derived from the users and groups associated
               with each role that is associated with the access permission on item.  Private roles, except
               for the current user's private role, will be excluded.
         """
-        admin_controller = cntrller in ['library_admin']
+        admin_controller = cntrller in ["library_admin"]
         roles = set()
-        if (isinstance(item, self.model.Library) and self.library_is_public(item)) or \
-                (isinstance(item, self.model.Dataset) and self.dataset_is_public(item)):
+        if (isinstance(item, self.model.Library) and self.library_is_public(item)) or (
+            isinstance(item, self.model.Dataset) and self.dataset_is_public(item)
+        ):
             return self.get_all_roles(trans, cntrller)
         # If item has roles associated with the access permission, we need to start with them.
         access_roles = item.get_access_roles(self)
         for role in access_roles:
             if admin_controller or self.ok_to_display(trans.user, role):
                 roles.add(role)
                 # Each role potentially has users.  We need to find all roles that each of those users have.
@@ -228,15 +250,15 @@
                 for gra in role.groups:
                     group = gra.group
                     for uga in group.users:
                         user = uga.user
                         for ura in user.roles:
                             if admin_controller or self.ok_to_display(trans.user, ura.role):
                                 roles.add(ura.role)
-        return self.sort_by_attr([role for role in roles], 'name')
+        return self.sort_by_attr([role for role in roles], "name")
 
     def ok_to_display(self, user, role):
         """
         Method for checking if:
         - a role is private and is the current user's private role
         - a role is a sharing role and belongs to the current user
         """
@@ -256,15 +278,15 @@
         specific action on an item, which must be one of:
         Dataset, Library, LibraryFolder, LibraryDataset, LibraryDatasetDatasetAssociation
         """
         # SM: Note that calling get_item_actions will emit a query.
         item_actions = self.get_item_actions(action, item)
 
         if not item_actions:
-            return action.model == 'restrict'
+            return action.model == "restrict"
         ret_val = False
         # For DATASET_ACCESS only, user must have ALL associated roles
         if action == self.permitted_actions.DATASET_ACCESS:
             for item_action in item_actions:
                 if item_action.role not in roles:
                     break
             else:
@@ -293,39 +315,51 @@
         # return the permissions associated with that given action.
         # We initialize the permissions list to be empty; we will return an
         # empty list by default.
         #
         # If the dataset id has no corresponding action in its permissions,
         # then the returned permissions will not carry an entry for the dataset.
         ret_permissions = {}
-        if (len(permission_items) > 0):
+        if len(permission_items) > 0:
             # SM: NB: LibraryDatasets became Datasets for some odd reason.
-            if (isinstance(permission_items[0], trans.model.LibraryDataset)):
+            if isinstance(permission_items[0], trans.model.LibraryDataset):
                 ids = [item.library_dataset_id for item in permission_items]
-                permissions = trans.sa_session.query(trans.model.LibraryDatasetPermissions) \
-                                   .filter(and_(trans.model.LibraryDatasetPermissions.library_dataset_id.in_(ids),
-                                                trans.model.LibraryDatasetPermissions.action == action.action)) \
-                                   .all()
+                permissions = (
+                    trans.sa_session.query(trans.model.LibraryDatasetPermissions)
+                    .filter(
+                        and_(
+                            trans.model.LibraryDatasetPermissions.library_dataset_id.in_(ids),
+                            trans.model.LibraryDatasetPermissions.action == action.action,
+                        )
+                    )
+                    .all()
+                )
 
                 # Massage the return data. We will return a list of permissions
                 # for each library dataset. So we initialize the return list to
                 # have an empty list for each dataset. Then each permission is
                 # appended to the right lib dataset.
                 # TODO: Consider eliminating the initialization and just return
                 # empty values for each library dataset id.
                 for item in permission_items:
                     ret_permissions[item.library_dataset_id] = []
                 for permission in permissions:
                     ret_permissions[permission.library_dataset_id].append(permission)
-            elif (isinstance(permission_items[0], trans.model.Dataset)):
+            elif isinstance(permission_items[0], trans.model.Dataset):
                 ids = [item.id for item in permission_items]
-                permissions = trans.sa_session.query(trans.model.DatasetPermissions) \
-                                   .filter(and_(trans.model.DatasetPermissions.dataset_id.in_(ids),
-                                                trans.model.DatasetPermissions.action == action.action)) \
-                                   .all()
+                permissions = (
+                    trans.sa_session.query(trans.model.DatasetPermissions)
+                    .filter(
+                        and_(
+                            trans.model.DatasetPermissions.dataset_id.in_(ids),
+                            trans.model.DatasetPermissions.action == action.action,
+                        )
+                    )
+                    .all()
+                )
 
                 # Massage the return data. We will return a list of permissions
                 # for each library dataset. So we initialize the return list to
                 # have an empty list for each dataset. Then each permission is
                 # appended to the right lib dataset.
                 # TODO: Consider eliminating the initialization and just return
                 # empty values for each library dataset id.
@@ -343,25 +377,26 @@
                     base_result = self.get_item_actions(action, item)
                     new_result = ret_permissions[item.library_dataset_id]
                     # For now, just test against LibraryDatasetIds; other classes
                     # are not tested yet.
                     if len(base_result) == len(new_result):
                         common_result = set(base_result).intersection(new_result)
                         if len(common_result) == len(base_result):
-                            log.debug("Match on permissions for id %d" %
-                                      item.library_dataset_id)
+                            log.debug("Match on permissions for id %d" % item.library_dataset_id)
                         # TODO: Fix this failure message:
                         else:
-                            log.debug("Error: dataset %d; originally: %s; now: %s"
-                                      % (item.library_dataset_id,
-                                         base_result, new_result))
+                            log.debug(
+                                "Error: dataset %d; originally: %s; now: %s"
+                                % (item.library_dataset_id, base_result, new_result)
+                            )
                     else:
-                        log.debug("Error: dataset %d: had %d entries, now %d entries"
-                                  % (item.library_dataset_id, len(base_result),
-                                     len(new_result)))
+                        log.debug(
+                            "Error: dataset %d: had %d entries, now %d entries"
+                            % (item.library_dataset_id, len(base_result), len(new_result))
+                        )
                 log.debug("get_actions_for_items: Test end")
             except Exception as e:
                 log.debug(f"Exception in test code: {e}")
 
         return ret_permissions
 
     def allow_action_on_libitems(self, trans, user_roles, action, items):
@@ -393,58 +428,59 @@
                     ret_allow_action[item.id] = False
                     for item_action in item_actions:
                         if item_action.role in user_roles:
                             ret_allow_action[item.id] = True
                             break
 
             else:
-                if 'restrict' == action.model:
+                if "restrict" == action.model:
                     ret_allow_action[item.id] = True
                 else:
                     ret_allow_action[item.id] = False
 
         # Test it: the result for each dataset should match the result for
         # allow_action:
         test_code = False
         if test_code:
             log.debug("allow_action_for_items: test start")
             for item in items:
                 orig_value = self.allow_action(user_roles, action, item)
                 if orig_value == ret_allow_action[item.id]:
                     log.debug("Item %d: success" % item.id)
                 else:
-                    log.debug("Item %d: fail: original: %s; new: %s"
-                              % (item.id, orig_value, ret_allow_action[item.id]))
+                    log.debug("Item %d: fail: original: %s; new: %s" % (item.id, orig_value, ret_allow_action[item.id]))
             log.debug("allow_action_for_items: test end")
         return ret_allow_action
 
     # DELETEME: SM: DO NOT TOUCH! This actually works.
     def dataset_access_mapping(self, trans, user_roles, datasets):
-        '''
+        """
         For the given list of datasets, return a mapping of the datasets' ids
         to whether they can be accessed by the user or not. The datasets input
         is expected to be a simple list of Dataset objects.
-        '''
+        """
         datasets_public_map = self.datasets_are_public(trans, datasets)
-        datasets_allow_action_map = self.allow_action_on_libitems(trans, user_roles, self.permitted_actions.DATASET_ACCESS, datasets)
+        datasets_allow_action_map = self.allow_action_on_libitems(
+            trans, user_roles, self.permitted_actions.DATASET_ACCESS, datasets
+        )
         can_access = {}
         for dataset in datasets:
             can_access[dataset.id] = datasets_public_map[dataset.id] or datasets_allow_action_map[dataset.id]
         return can_access
 
     def dataset_permission_map_for_access(self, trans, user_roles, libitems):
-        '''
+        """
         For a given list of library items (e.g., Datasets), return a map of the
         datasets' ids to whether they can have permission to use that action
         (e.g., "access" or "modify") on the dataset. The libitems input is
         expected to be a simple list of library items, such as Datasets or
         LibraryDatasets.
         NB: This is currently only usable for Datasets; it was intended to
         be used for any library item.
-        '''
+        """
         # Map the library items to whether they are publicly accessible or not.
         # Then determine what actions are allowed on the item (in case it's not
         # public). Finally, the item is accessible if it's publicly available
         # or the right permissions are enabled.
         # TODO: This only works for Datasets; other code is using X_is_public,
         # so this will have to be rewritten to support other items.
         libitems_public_map = self.datasets_are_public(trans, libitems)
@@ -453,87 +489,115 @@
         )
         can_access = {}
         for libitem in libitems:
             can_access[libitem.id] = libitems_public_map[libitem.id] or libitems_allow_action_map[libitem.id]
         return can_access
 
     def item_permission_map_for_modify(self, trans, user_roles, libitems):
-        return self.allow_action_on_libitems(
-            trans, user_roles, self.permitted_actions.LIBRARY_MODIFY, libitems
-        )
+        return self.allow_action_on_libitems(trans, user_roles, self.permitted_actions.LIBRARY_MODIFY, libitems)
 
     def item_permission_map_for_manage(self, trans, user_roles, libitems):
-        return self.allow_action_on_libitems(
-            trans, user_roles, self.permitted_actions.LIBRARY_MANAGE, libitems
-        )
+        return self.allow_action_on_libitems(trans, user_roles, self.permitted_actions.LIBRARY_MANAGE, libitems)
 
     def item_permission_map_for_add(self, trans, user_roles, libitems):
-        return self.allow_action_on_libitems(
-            trans, user_roles, self.permitted_actions.LIBRARY_ADD, libitems
-        )
+        return self.allow_action_on_libitems(trans, user_roles, self.permitted_actions.LIBRARY_ADD, libitems)
 
-    def can_access_dataset(self, user_roles, dataset):
+    def can_access_dataset(self, user_roles, dataset: galaxy.model.Dataset):
         # SM: dataset_is_public will access dataset.actions, which is a
         # backref that causes a query to be made to DatasetPermissions
-        retval = self.dataset_is_public(dataset) or self.allow_action(user_roles, self.permitted_actions.DATASET_ACCESS, dataset)
+        retval = self.dataset_is_public(dataset) or self.allow_action(
+            user_roles, self.permitted_actions.DATASET_ACCESS, dataset
+        )
         return retval
 
     def can_access_datasets(self, user_roles, action_tuples):
         user_role_ids = [galaxy.model.cached_id(r) for r in user_roles]
 
         # For DATASET_ACCESS, user must have ALL associated roles
         for action, user_role_id in action_tuples:
             if action == self.permitted_actions.DATASET_ACCESS.action:
                 if user_role_id not in user_role_ids:
                     return False
 
         return True
 
+    def can_access_collection(self, user_roles: List[galaxy.model.Role], collection: galaxy.model.DatasetCollection):
+        action_tuples = collection.dataset_action_tuples
+        if not self.can_access_datasets(user_roles, action_tuples):
+            return False
+        return True
+
     def can_manage_dataset(self, roles, dataset):
         return self.allow_action(roles, self.permitted_actions.DATASET_MANAGE_PERMISSIONS, dataset)
 
     def can_access_library(self, roles, library):
-        return self.library_is_public(library) or self.allow_action(roles, self.permitted_actions.LIBRARY_ACCESS, library)
+        return self.library_is_public(library) or self.allow_action(
+            roles, self.permitted_actions.LIBRARY_ACCESS, library
+        )
 
     def get_accessible_libraries(self, trans, user):
         """Return all data libraries that the received user can access"""
         accessible_libraries = []
         current_user_role_ids = [role.id for role in user.all_roles()]
         library_access_action = self.permitted_actions.LIBRARY_ACCESS.action
-        restricted_library_ids = [lp.library_id for lp in trans.sa_session.query(trans.model.LibraryPermissions)
-                                  .filter(trans.model.LibraryPermissions.table.c.action == library_access_action).distinct()]
-        accessible_restricted_library_ids = [lp.library_id for lp in trans.sa_session.query(trans.model.LibraryPermissions)
-            .filter(and_(
-                trans.model.LibraryPermissions.table.c.action == library_access_action,
-                trans.model.LibraryPermissions.table.c.role_id.in_(current_user_role_ids)))]
+        restricted_library_ids = [
+            lp.library_id
+            for lp in trans.sa_session.query(trans.model.LibraryPermissions)
+            .filter(trans.model.LibraryPermissions.table.c.action == library_access_action)
+            .distinct()
+        ]
+        accessible_restricted_library_ids = [
+            lp.library_id
+            for lp in trans.sa_session.query(trans.model.LibraryPermissions).filter(
+                and_(
+                    trans.model.LibraryPermissions.table.c.action == library_access_action,
+                    trans.model.LibraryPermissions.table.c.role_id.in_(current_user_role_ids),
+                )
+            )
+        ]
         # Filter to get libraries accessible by the current user.  Get both
         # public libraries and restricted libraries accessible by the current user.
-        for library in trans.sa_session.query(trans.model.Library) \
-                                       .filter(and_(trans.model.Library.table.c.deleted == false(),
-                                                    (or_(not_(trans.model.Library.table.c.id.in_(restricted_library_ids)),
-                                                         trans.model.Library.table.c.id.in_(accessible_restricted_library_ids))))) \
-                                       .order_by(trans.app.model.Library.name):
+        for library in (
+            trans.sa_session.query(trans.model.Library)
+            .filter(
+                and_(
+                    trans.model.Library.table.c.deleted == false(),
+                    (
+                        or_(
+                            not_(trans.model.Library.table.c.id.in_(restricted_library_ids)),
+                            trans.model.Library.table.c.id.in_(accessible_restricted_library_ids),
+                        )
+                    ),
+                )
+            )
+            .order_by(trans.app.model.Library.name)
+        ):
             accessible_libraries.append(library)
         return accessible_libraries
 
     def has_accessible_folders(self, trans, folder, user, roles, search_downward=True):
-        if self.has_accessible_library_datasets(trans, folder, user, roles, search_downward=search_downward) or \
-                self.can_add_library_item(roles, folder) or \
-                self.can_modify_library_item(roles, folder) or \
-                self.can_manage_library_item(roles, folder):
+        if (
+            self.has_accessible_library_datasets(trans, folder, user, roles, search_downward=search_downward)
+            or self.can_add_library_item(roles, folder)
+            or self.can_modify_library_item(roles, folder)
+            or self.can_manage_library_item(roles, folder)
+        ):
             return True
         if search_downward:
-            for folder in folder.active_folders:
-                return self.has_accessible_folders(trans, folder, user, roles, search_downward=search_downward)
+            for active_folder in folder.active_folders:
+                return self.has_accessible_folders(trans, active_folder, user, roles, search_downward=search_downward)
         return False
 
     def has_accessible_library_datasets(self, trans, folder, user, roles, search_downward=True):
-        for library_dataset in trans.sa_session.query(trans.model.LibraryDataset) \
-                .filter(and_(trans.model.LibraryDataset.table.c.deleted == false(),
-                             trans.app.model.LibraryDataset.table.c.folder_id == folder.id)):
+        for library_dataset in trans.sa_session.query(trans.model.LibraryDataset).filter(
+            and_(
+                trans.model.LibraryDataset.table.c.deleted == false(),
+                trans.app.model.LibraryDataset.table.c.folder_id == folder.id,
+            )
+        ):
             if self.can_access_library_item(roles, library_dataset, user):
                 return True
         if search_downward:
             return self.__active_folders_have_accessible_library_datasets(trans, folder, user, roles)
         return False
 
     def __active_folders_have_accessible_library_datasets(self, trans, folder, user, roles):
@@ -542,23 +606,29 @@
                 return True
         return False
 
     def can_access_library_item(self, roles, item, user):
         if type(item) == self.model.Library:
             return self.can_access_library(roles, item)
         elif type(item) == self.model.LibraryFolder:
-            return self.can_access_library(roles, item.parent_library) and self.check_folder_contents(user, roles, item)[0]
+            return (
+                self.can_access_library(roles, item.parent_library) and self.check_folder_contents(user, roles, item)[0]
+            )
         elif type(item) == self.model.LibraryDataset:
-            return self.can_access_library(roles, item.folder.parent_library) and self.can_access_dataset(roles, item.library_dataset_dataset_association.dataset)
+            return self.can_access_library(roles, item.folder.parent_library) and self.can_access_dataset(
+                roles, item.library_dataset_dataset_association.dataset
+            )
         elif type(item) == self.model.LibraryDatasetDatasetAssociation:
-            return self.can_access_library(roles, item.library_dataset.folder.parent_library) and self.can_access_dataset(roles, item.dataset)
+            return self.can_access_library(
+                roles, item.library_dataset.folder.parent_library
+            ) and self.can_access_dataset(roles, item.dataset)
         elif type(item) == self.model.LibraryDatasetCollectionAssociation:
             return self.can_access_library(roles, item.folder.parent_library)
         else:
-            log.warning(f'Unknown library item type: {type(item)}')
+            log.warning(f"Unknown library item type: {type(item)}")
             return False
 
     def can_add_library_item(self, roles, item):
         return self.allow_action(roles, self.permitted_actions.LIBRARY_ADD, item)
 
     def can_modify_library_item(self, roles, item):
         return self.allow_action(roles, self.permitted_actions.LIBRARY_MODIFY, item)
@@ -587,18 +657,18 @@
             # collect this dataset's perms
             these_perms = self.get_permissions(dataset)
             # join or intersect this dataset's permissions with others
             for action, roles in these_perms.items():
                 if action not in perms.keys():
                     perms[action] = roles
                 else:
-                    if action.model == 'grant':
+                    if action.model == "grant":
                         # intersect existing roles with new roles
                         perms[action] = [_ for _ in roles if _ in perms[action]]
-                    elif action.model == 'restrict':
+                    elif action.model == "restrict":
                         # join existing roles with new roles
                         perms[action].extend([_ for _ in roles if _ not in perms[action]])
         return perms
 
     def guess_derived_permissions(self, all_input_permissions):
         """Returns a dict of { action : [ role_id, role_id, ... ] } for the output dataset based upon input dataset permissions.
 
@@ -608,35 +678,35 @@
         for action_name, role_ids in all_input_permissions.items():
             if not role_ids:
                 continue
             action = self.get_action(action_name)
             if action not in perms.keys():
                 perms[action] = list(role_ids)
             else:
-                if action.model == 'grant':
+                if action.model == "grant":
                     # intersect existing roles with new roles
                     perms[action] = [_ for _ in role_ids if _ in perms[action]]
-                elif action.model == 'restrict':
+                elif action.model == "restrict":
                     # join existing roles with new roles
                     perms[action].extend([_ for _ in role_ids if _ not in perms[action]])
         return perms
 
     def associate_components(self, **kwd):
-        if 'user' in kwd:
-            if 'group' in kwd:
-                return self.associate_user_group(kwd['user'], kwd['group'])
-            elif 'role' in kwd:
-                return self.associate_user_role(kwd['user'], kwd['role'])
-        elif 'role' in kwd:
-            if 'group' in kwd:
-                return self.associate_group_role(kwd['group'], kwd['role'])
-        if 'action' in kwd:
-            if 'dataset' in kwd and 'role' in kwd:
-                return self.associate_action_dataset_role(kwd['action'], kwd['dataset'], kwd['role'])
-        raise Exception(f'No valid method of associating provided components: {kwd}')
+        if "user" in kwd:
+            if "group" in kwd:
+                return self.associate_user_group(kwd["user"], kwd["group"])
+            elif "role" in kwd:
+                return self.associate_user_role(kwd["user"], kwd["role"])
+        elif "role" in kwd:
+            if "group" in kwd:
+                return self.associate_group_role(kwd["group"], kwd["role"])
+        if "action" in kwd:
+            if "dataset" in kwd and "role" in kwd:
+                return self.associate_action_dataset_role(kwd["action"], kwd["dataset"], kwd["role"])
+        raise Exception(f"No valid method of associating provided components: {kwd}")
 
     def associate_user_group(self, user, group):
         assoc = self.model.UserGroupAssociation(user, group)
         self.sa_session.add(assoc)
         self.sa_session.flush()
         return assoc
 
@@ -670,33 +740,40 @@
                 self.user_set_default_permissions(user, history=True, dataset=True)
 
     def create_private_user_role(self, user):
         user.attempt_create_private_role()
         return self.get_private_user_role(user)
 
     def get_private_user_role(self, user, auto_create=False):
-        role = self.sa_session.query(self.model.Role) \
-                              .filter(and_(self.model.UserRoleAssociation.table.c.user_id == user.id,
-                                           self.model.Role.id == self.model.UserRoleAssociation.table.c.role_id,
-                                           self.model.Role.type == self.model.Role.types.PRIVATE)) \
-                              .one_or_none()
+        role = (
+            self.sa_session.query(self.model.Role)
+            .filter(
+                and_(
+                    self.model.UserRoleAssociation.table.c.user_id == user.id,
+                    self.model.Role.id == self.model.UserRoleAssociation.table.c.role_id,
+                    self.model.Role.type == self.model.Role.types.PRIVATE,
+                )
+            )
+            .one_or_none()
+        )
         if not role:
             if auto_create:
                 return self.create_private_user_role(user)
             else:
                 return None
         return role
 
     def get_role(self, name, type=None):
         type = type or self.model.Role.types.SYSTEM
         # will raise exception if not found
-        return self.sa_session.query(self.model.Role) \
-            .filter(and_(self.model.Role.name == name,
-                     self.model.Role.type == type)) \
+        return (
+            self.sa_session.query(self.model.Role)
+            .filter(and_(self.model.Role.name == name, self.model.Role.type == type))
             .one()
+        )
 
     def create_role(self, name, description, in_users, in_groups, create_group_for_role=False, type=None):
         type = type or self.model.Role.types.SYSTEM
         role = self.model.Role(name=name, description=description, type=type)
         self.sa_session.add(role)
         # Create the UserRoleAssociations
         for user in [self.sa_session.query(self.model.User).get(x) for x in in_users]:
@@ -713,27 +790,40 @@
             num_in_groups = len(in_groups) + 1
         else:
             num_in_groups = len(in_groups)
         self.sa_session.flush()
         return role, num_in_groups
 
     def get_sharing_roles(self, user):
-        return self.sa_session.query(self.model.Role) \
-                              .filter(and_((self.model.Role.name).like(f"Sharing role for: %{user.email}%"),
-                                           self.model.Role.type == self.model.Role.types.SHARING))
+        return self.sa_session.query(self.model.Role).filter(
+            and_(
+                (self.model.Role.name).like(f"Sharing role for: %{user.email}%"),
+                self.model.Role.type == self.model.Role.types.SHARING,
+            )
+        )
 
-    def user_set_default_permissions(self, user, permissions=None, history=False, dataset=False, bypass_manage_permission=False, default_access_private=False):
+    def user_set_default_permissions(
+        self,
+        user,
+        permissions=None,
+        history=False,
+        dataset=False,
+        bypass_manage_permission=False,
+        default_access_private=False,
+    ):
         # bypass_manage_permission is used to change permissions of datasets in a userless history when logging in
         flush_needed = False
         permissions = permissions or {}
         if user is None:
             return None
         if not permissions:
             # default permissions
-            permissions = {self.permitted_actions.DATASET_MANAGE_PERMISSIONS: [self.get_private_user_role(user, auto_create=True)]}
+            permissions = {
+                self.permitted_actions.DATASET_MANAGE_PERMISSIONS: [self.get_private_user_role(user, auto_create=True)]
+            }
             # new_user_dataset_access_role_default_private is set as True in config file
             if default_access_private:
                 permissions[self.permitted_actions.DATASET_ACCESS] = next(iter(permissions.values()))
         # Delete all of the current default permissions for the user
         for dup in user.default_permissions:
             self.sa_session.delete(dup)
             flush_needed = True
@@ -744,15 +834,17 @@
             for dup in [self.model.DefaultUserPermissions(user, action, role) for role in roles]:
                 self.sa_session.add(dup)
                 flush_needed = True
         if flush_needed:
             self.sa_session.flush()
         if history:
             for history in user.active_histories:
-                self.history_set_default_permissions(history, permissions=permissions, dataset=dataset, bypass_manage_permission=bypass_manage_permission)
+                self.history_set_default_permissions(
+                    history, permissions=permissions, dataset=dataset, bypass_manage_permission=bypass_manage_permission
+                )
 
     def user_get_default_permissions(self, user):
         permissions = {}
         for dup in user.default_permissions:
             action = self.get_action(dup.action)
             if action in permissions:
                 permissions[action].append(dup.role)
@@ -907,16 +999,17 @@
         if intersect:
             for role in intersect:
                 if not [_ for _ in [ura.user for ura in role.users] if _ not in users]:
                     # only use a role if it contains ONLY the users we're sharing with
                     sharing_role = role
                     break
         if sharing_role is None:
-            sharing_role = self.model.Role(name=f"Sharing role for: {', '.join(u.email for u in users)}",
-                                           type=self.model.Role.types.SHARING)
+            sharing_role = self.model.Role(
+                name=f"Sharing role for: {', '.join(u.email for u in users)}", type=self.model.Role.types.SHARING
+            )
             self.sa_session.add(sharing_role)
             self.sa_session.flush()
             for user in users:
                 self.associate_components(user=user, role=sharing_role)
         self.set_dataset_permission(dataset, {self.permitted_actions.DATASET_ACCESS: [sharing_role]})
 
     def set_all_library_permissions(self, trans, library_item, permissions=None):
@@ -939,15 +1032,17 @@
                         # Permission setting related to DATASET_MANAGE_PERMISSIONS was broken for a period of time,
                         # so it is possible that some Datasets have no roles associated with the DATASET_MANAGE_PERMISSIONS
                         # permission.  In this case, we'll reset this permission to the library_item user's private role.
                         if not library_item.dataset.has_manage_permissions_roles(self):
                             # Well this looks like a bug, this should be looked at.
                             # Default permissions above is single hash that keeps getting reeditted here
                             # because permission is being defined instead of permissions. -John
-                            permissions[self.permitted_actions.DATASET_MANAGE_PERMISSIONS] = [trans.app.security_agent.get_private_user_role(library_item.user)]
+                            permissions[self.permitted_actions.DATASET_MANAGE_PERMISSIONS] = [
+                                trans.app.security_agent.get_private_user_role(library_item.user)
+                            ]
                             self.set_dataset_permission(library_item.dataset, permissions)
                         if action == self.permitted_actions.LIBRARY_MANAGE.action and roles:
                             # Handle the special case when we are setting the LIBRARY_MANAGE_PERMISSION on a
                             # library_dataset_dataset_association since the roles need to be applied to the
                             # DATASET_MANAGE_PERMISSIONS permission on the associated dataset.
                             permissions = {}
                             permissions[self.permitted_actions.DATASET_MANAGE_PERMISSIONS] = roles
@@ -968,15 +1063,17 @@
             # Delete the current specific permission on the library item if one exists
             for item_permission in library_item.actions:
                 if item_permission.action == action:
                     self.sa_session.delete(item_permission)
                     flush_needed = True
             # Add the new specific permission on the library item
             if isinstance(library_item, self.model.LibraryDataset):
-                for item_permission in [self.model.LibraryDatasetPermissions(action, library_item, role) for role in roles]:
+                for item_permission in [
+                    self.model.LibraryDatasetPermissions(action, library_item, role) for role in roles
+                ]:
                     self.sa_session.add(item_permission)
                     flush_needed = True
             elif isinstance(library_item, self.model.LibraryPermissions):
                 for item_permission in [self.model.LibraryPermissions(action, library_item, role) for role in roles]:
                     self.sa_session.add(item_permission)
                     flush_needed = True
         if flush_needed:
@@ -1028,15 +1125,15 @@
             if not sub_folder.purged:
                 self.make_folder_public(sub_folder)
         for library_dataset in folder.datasets:
             dataset = library_dataset.library_dataset_dataset_association.dataset
             if not dataset.purged and not self.dataset_is_public(dataset):
                 self.make_dataset_public(dataset)
 
-    def dataset_is_public(self, dataset):
+    def dataset_is_public(self, dataset: galaxy.model.Dataset):
         """
         A dataset is considered public if there are no "access" actions
         associated with it.  Any other actions ( 'manage permissions',
         'edit metadata' ) are irrelevant. Accessing dataset.actions
         will cause a query to be emitted.
         """
         return self.permitted_actions.DATASET_ACCESS.action not in [a.action for a in dataset.actions]
@@ -1052,41 +1149,47 @@
         """
         If the Dataset object has exactly one access role and that is
         the current user's private role then we consider the dataset private.
         """
         private_role = self.get_private_user_role(trans.user)
         access_roles = dataset.get_access_roles(self)
 
-        if len(access_roles) != 1:
+        if len(access_roles) != 1 or private_role is None:
             return False
         else:
             if access_roles[0].id == private_role.id:
                 return True
             else:
                 return False
 
     def datasets_are_public(self, trans, datasets):
-        '''
+        """
         Given a transaction object and a list of Datasets, return
         a mapping from Dataset ids to whether the Dataset is public
         or not. All Dataset ids should be returned in the mapping's keys.
-        '''
+        """
         # We go the other way around from dataset_is_public: we start with
         # all datasets being marked as public. If there is an access action
         # associated with the dataset, then we mark it as nonpublic:
         datasets_public = {}
         dataset_ids = [dataset.id for dataset in datasets]
         for dataset_id in dataset_ids:
             datasets_public[dataset_id] = True
 
         # Now get all datasets which have DATASET_ACCESS actions:
-        access_data_perms = trans.sa_session.query(trans.app.model.DatasetPermissions) \
-                                 .filter(and_(trans.app.model.DatasetPermissions.dataset_id.in_(dataset_ids),
-                                              trans.app.model.DatasetPermissions.action == self.permitted_actions.DATASET_ACCESS.action)) \
-                                 .all()
+        access_data_perms = (
+            trans.sa_session.query(trans.app.model.DatasetPermissions)
+            .filter(
+                and_(
+                    trans.app.model.DatasetPermissions.dataset_id.in_(dataset_ids),
+                    trans.app.model.DatasetPermissions.action == self.permitted_actions.DATASET_ACCESS.action,
+                )
+            )
+            .all()
+        )
         # Every dataset returned has "access" privileges associated with it,
         # so it's not public.
         for permission in access_data_perms:
             datasets_public[permission.dataset_id] = False
         return datasets_public
 
     def make_dataset_public(self, dataset):
@@ -1100,26 +1203,26 @@
         if flush_needed:
             self.sa_session.flush()
 
     def derive_roles_from_access(self, trans, item_id, cntrller, library=False, **kwd):
         # Check the access permission on a dataset.  If library is true, item_id refers to a library.  If library
         # is False, item_id refers to a dataset ( item_id must currently be decoded before being sent ).  The
         # cntrller param is the calling controller, which needs to be passed to get_legitimate_roles().
-        msg = ''
+        msg = ""
         permissions = {}
         # accessible will be True only if at least 1 user has every role in DATASET_ACCESS_in
         accessible = False
         # legitimate will be True only if all roles in DATASET_ACCESS_in are in the set of roles returned from
         # get_legitimate_roles()
         # legitimate = False # TODO: not used
         # private_role_found will be true only if more than 1 role is being associated with the DATASET_ACCESS
         # permission on item, and at least 1 of the roles is private.
         private_role_found = False
         error = False
-        for k, v in get_permitted_actions(filter='DATASET').items():
+        for k, v in get_permitted_actions(filter="DATASET").items():
             # Change for removing the prefix '_in' from the roles select box
             in_roles = [self.sa_session.query(self.model.Role).get(x) for x in listify(kwd[k])]
             if not in_roles:
                 in_roles = [self.sa_session.query(self.model.Role).get(x) for x in listify(kwd.get(f"{k}_in", []))]
             if v == self.permitted_actions.DATASET_ACCESS and in_roles:
                 if library:
                     item = self.sa_session.query(self.model.Library).get(item_id)
@@ -1212,16 +1315,18 @@
         self.set_all_library_permissions(trans, target_library_item, permissions)
         if user:
             for item_class, permission_class in self.library_item_assocs:
                 if isinstance(target_library_item, item_class):
                     found_permission_class = permission_class
                     break
             else:
-                raise Exception('Invalid class (%s) specified for target_library_item (%s)' %
-                                (target_library_item.__class__, target_library_item.__class__.__name__))
+                raise Exception(
+                    "Invalid class (%s) specified for target_library_item (%s)"
+                    % (target_library_item.__class__, target_library_item.__class__.__name__)
+                )
             # Make sure user's private role is included
             private_role = self.model.security_agent.get_private_user_role(user)
             for action in self.permitted_actions.values():
                 if not found_permission_class.filter_by(role_id=private_role.id, action=action.action).first():
                     lp = found_permission_class(action.action, target_library_item, private_role)
                     self.sa_session.add(lp)
                     self.sa_session.flush()
@@ -1234,17 +1339,19 @@
         method below, and it returns libraries for which the received user has permission to
         perform the received actions.  Here is an example call to this method to return all
         libraries for which the received user has LIBRARY_ADD permission::
 
             libraries = trans.app.security_agent.get_permitted_libraries( trans, user,
                 [ trans.app.security_agent.permitted_actions.LIBRARY_ADD ] )
         """
-        all_libraries = trans.sa_session.query(trans.app.model.Library) \
-                                        .filter(trans.app.model.Library.table.c.deleted == false()) \
-                                        .order_by(trans.app.model.Library.name)
+        all_libraries = (
+            trans.sa_session.query(trans.app.model.Library)
+            .filter(trans.app.model.Library.table.c.deleted == false())
+            .order_by(trans.app.model.Library.name)
+        )
         roles = user.all_roles()
         actions_to_check = actions
         # The libraries dictionary looks like: { library : '1,2' }, library : '3' }
         # Its keys are the libraries that should be displayed for the current user and whose values are a
         # string of comma-separated folder ids, of the associated folders the should NOT be displayed.
         # The folders that should not be displayed may not be a complete list, but it is ultimately passed
         # to the calling method to keep from re-checking the same folders when the library / folder
@@ -1252,50 +1359,56 @@
         libraries = {}
         for library in all_libraries:
             can_show, hidden_folder_ids = self.show_library_item(self, roles, library, actions_to_check)
             if can_show:
                 libraries[library] = hidden_folder_ids
         return libraries
 
-    def show_library_item(self, user, roles, library_item, actions_to_check, hidden_folder_ids=''):
+    def show_library_item(self, user, roles, library_item, actions_to_check, hidden_folder_ids=""):
         """
         This method must be sent an instance of Library() or LibraryFolder().  Recursive execution produces a
         comma-separated string of folder ids whose folders do NOT meet the criteria for showing. Along with
         the string, True is returned if the current user has permission to perform any 1 of actions_to_check
         on library_item. Otherwise, cycle through all sub-folders in library_item until one is found that meets
         this criteria, if it exists.  This method does not necessarily scan the entire library as it returns
         when it finds the first library_item that allows user to perform any one action in actions_to_check.
         """
         for action in actions_to_check:
             if self.allow_action(roles, action, library_item):
                 return True, hidden_folder_ids
         if isinstance(library_item, self.model.Library):
-            return self.show_library_item(user, roles, library_item.root_folder, actions_to_check, hidden_folder_ids='')
+            return self.show_library_item(user, roles, library_item.root_folder, actions_to_check, hidden_folder_ids="")
         if isinstance(library_item, self.model.LibraryFolder):
             for folder in library_item.active_folders:
-                can_show, hidden_folder_ids = self.show_library_item(user, roles, folder, actions_to_check, hidden_folder_ids=hidden_folder_ids)
+                can_show, hidden_folder_ids = self.show_library_item(
+                    user, roles, folder, actions_to_check, hidden_folder_ids=hidden_folder_ids
+                )
                 if can_show:
                     return True, hidden_folder_ids
                 if hidden_folder_ids:
-                    hidden_folder_ids = '%s,%d' % (hidden_folder_ids, folder.id)
+                    hidden_folder_ids = "%s,%d" % (hidden_folder_ids, folder.id)
                 else:
-                    hidden_folder_ids = '%d' % folder.id
+                    hidden_folder_ids = "%d" % folder.id
         return False, hidden_folder_ids
 
-    def get_showable_folders(self, user, roles, library_item, actions_to_check, hidden_folder_ids=None, showable_folders=None):
+    def get_showable_folders(
+        self, user, roles, library_item, actions_to_check, hidden_folder_ids=None, showable_folders=None
+    ):
         """
         This method must be sent an instance of Library(), all the folders of which are scanned to determine if
         user is allowed to perform any action in actions_to_check. The param hidden_folder_ids, if passed, should
         contain a list of folder IDs which was generated when the library was previously scanned
         using the same actions_to_check. A list of showable folders is generated. This method scans the entire library.
         """
         hidden_folder_ids = hidden_folder_ids or []
         showable_folders = showable_folders or []
         if isinstance(library_item, self.model.Library):
-            return self.get_showable_folders(user, roles, library_item.root_folder, actions_to_check, showable_folders=[])
+            return self.get_showable_folders(
+                user, roles, library_item.root_folder, actions_to_check, showable_folders=[]
+            )
         if isinstance(library_item, self.model.LibraryFolder):
             if library_item.id not in hidden_folder_ids:
                 for action in actions_to_check:
                     if self.allow_action(roles, action, library_item):
                         showable_folders.append(library_item)
                         break
             for folder in library_item.active_folders:
@@ -1353,103 +1466,141 @@
                     self.sa_session.flush()
             for user in users:
                 self.associate_components(user=user, role=role)
             for group in groups:
                 self.associate_components(group=group, role=role)
 
     def get_component_associations(self, **kwd):
-        assert len(kwd) == 2, 'You must specify exactly 2 Galaxy security components to check for associations.'
-        if 'dataset' in kwd:
-            if 'action' in kwd:
-                return self.sa_session.query(self.model.DatasetPermissions).filter_by(action=kwd['action'].action, dataset_id=kwd['dataset'].id).first()
-        elif 'user' in kwd:
-            if 'group' in kwd:
-                return self.sa_session.query(self.model.UserGroupAssociation).filter_by(group_id=kwd['group'].id, user_id=kwd['user'].id).first()
-            elif 'role' in kwd:
-                return self.sa_session.query(self.model.UserRoleAssociation).filter_by(role_id=kwd['role'].id, user_id=kwd['user'].id).first()
-        elif 'group' in kwd:
-            if 'role' in kwd:
-                return self.sa_session.query(self.model.GroupRoleAssociation).filter_by(role_id=kwd['role'].id, group_id=kwd['group'].id).first()
-        raise Exception(f'No valid method of associating provided components: {kwd}')
+        assert len(kwd) == 2, "You must specify exactly 2 Galaxy security components to check for associations."
+        if "dataset" in kwd:
+            if "action" in kwd:
+                return (
+                    self.sa_session.query(self.model.DatasetPermissions)
+                    .filter_by(action=kwd["action"].action, dataset_id=kwd["dataset"].id)
+                    .first()
+                )
+        elif "user" in kwd:
+            if "group" in kwd:
+                return (
+                    self.sa_session.query(self.model.UserGroupAssociation)
+                    .filter_by(group_id=kwd["group"].id, user_id=kwd["user"].id)
+                    .first()
+                )
+            elif "role" in kwd:
+                return (
+                    self.sa_session.query(self.model.UserRoleAssociation)
+                    .filter_by(role_id=kwd["role"].id, user_id=kwd["user"].id)
+                    .first()
+                )
+        elif "group" in kwd:
+            if "role" in kwd:
+                return (
+                    self.sa_session.query(self.model.GroupRoleAssociation)
+                    .filter_by(role_id=kwd["role"].id, group_id=kwd["group"].id)
+                    .first()
+                )
+        raise Exception(f"No valid method of associating provided components: {kwd}")
 
-    def check_folder_contents(self, user, roles, folder, hidden_folder_ids=''):
+    def check_folder_contents(self, user, roles, folder, hidden_folder_ids=""):
         """
         This method must always be sent an instance of LibraryFolder().  Recursive execution produces a
         comma-separated string of folder ids whose folders do NOT meet the criteria for showing.  Along
         with the string, True is returned if the current user has permission to access folder. Otherwise,
         cycle through all sub-folders in folder until one is found that meets this criteria, if it exists.
         This method does not necessarily scan the entire library as it returns when it finds the first
         folder that is accessible to user.
         """
         # If a folder is writeable, it's accessable and we need not go further
         if self.can_add_library_item(roles, folder):
-            return True, ''
+            return True, ""
         action = self.permitted_actions.DATASET_ACCESS
 
-        lddas = self.sa_session.query(self.model.LibraryDatasetDatasetAssociation) \
-                               .join("library_dataset") \
-                               .filter(self.model.LibraryDataset.folder == folder) \
-                               .join("dataset") \
-                               .options(joinedload("dataset").joinedload("actions")) \
-                               .all()
+        lddas = (
+            self.sa_session.query(self.model.LibraryDatasetDatasetAssociation)
+            .join("library_dataset")
+            .filter(self.model.LibraryDataset.folder == folder)
+            .join("dataset")
+            .options(
+                joinedload(self.model.LibraryDatasetDatasetAssociation.dataset).joinedload(self.model.Dataset.actions)
+            )
+            .all()
+        )
 
         for ldda in lddas:
             ldda_access_permissions = self.get_item_actions(action, ldda.dataset)
             if not ldda_access_permissions:
                 # Dataset is public
                 return True, hidden_folder_ids
             for ldda_access_permission in ldda_access_permissions:
                 if ldda_access_permission.role in roles:
                     # The current user has access permission on the dataset
                     return True, hidden_folder_ids
         for sub_folder in folder.active_folders:
-            can_access, hidden_folder_ids = self.check_folder_contents(user, roles, sub_folder, hidden_folder_ids=hidden_folder_ids)
+            can_access, hidden_folder_ids = self.check_folder_contents(
+                user, roles, sub_folder, hidden_folder_ids=hidden_folder_ids
+            )
             if can_access:
                 return True, hidden_folder_ids
             if hidden_folder_ids:
-                hidden_folder_ids = '%s,%d' % (hidden_folder_ids, sub_folder.id)
+                hidden_folder_ids = "%s,%d" % (hidden_folder_ids, sub_folder.id)
             else:
-                hidden_folder_ids = '%d' % sub_folder.id
+                hidden_folder_ids = "%d" % sub_folder.id
         return False, hidden_folder_ids
 
 
 class HostAgent(RBACAgent):
     """
     A simple security agent which allows access to datasets based on host.
     This exists so that externals sites such as UCSC can gain access to
     datasets which have permissions which would normally prevent such access.
     """
+
     # TODO: Make sites user configurable
     sites = Bunch(
-        ucsc_main=('hgw1.cse.ucsc.edu', 'hgw2.cse.ucsc.edu', 'hgw3.cse.ucsc.edu', 'hgw4.cse.ucsc.edu',
-                   'hgw5.cse.ucsc.edu', 'hgw6.cse.ucsc.edu', 'hgw7.cse.ucsc.edu', 'hgw8.cse.ucsc.edu'),
-        ucsc_test=('hgwdev.cse.ucsc.edu', ),
-        ucsc_archaea=('lowepub.cse.ucsc.edu', )
+        ucsc_main=(
+            "hgw1.cse.ucsc.edu",
+            "hgw2.cse.ucsc.edu",
+            "hgw3.cse.ucsc.edu",
+            "hgw4.cse.ucsc.edu",
+            "hgw5.cse.ucsc.edu",
+            "hgw6.cse.ucsc.edu",
+            "hgw7.cse.ucsc.edu",
+            "hgw8.cse.ucsc.edu",
+        ),
+        ucsc_test=("hgwdev.cse.ucsc.edu",),
+        ucsc_archaea=("lowepub.cse.ucsc.edu",),
     )
 
     def __init__(self, model, permitted_actions=None):
         self.model = model
         if permitted_actions:
             self.permitted_actions = permitted_actions
 
     @property
     def sa_session(self):
         """Returns a SQLAlchemy session"""
         return self.model.context
 
     def allow_action(self, addr, action, **kwd):
-        if 'dataset' in kwd and action == self.permitted_actions.DATASET_ACCESS:
-            hda = kwd['dataset']
-            if action == self.permitted_actions.DATASET_ACCESS and action.action not in [dp.action for dp in hda.dataset.actions]:
-                log.debug('Allowing access to public dataset with hda: %i.' % hda.id)
+        if "dataset" in kwd and action == self.permitted_actions.DATASET_ACCESS:
+            hda = kwd["dataset"]
+            if action == self.permitted_actions.DATASET_ACCESS and action.action not in [
+                dp.action for dp in hda.dataset.actions
+            ]:
+                log.debug("Allowing access to public dataset with hda: %i." % hda.id)
                 return True  # dataset has no roles associated with the access permission, thus is already public
-            hdadaa = self.sa_session.query(self.model.HistoryDatasetAssociationDisplayAtAuthorization) \
-                                    .filter_by(history_dataset_association_id=hda.id).first()
+            hdadaa = (
+                self.sa_session.query(self.model.HistoryDatasetAssociationDisplayAtAuthorization)
+                .filter_by(history_dataset_association_id=hda.id)
+                .first()
+            )
             if not hdadaa:
-                log.debug('Denying access to private dataset with hda: %i.  No hdadaa record for this dataset.' % hda.id)
+                log.debug(
+                    "Denying access to private dataset with hda: %i.  No hdadaa record for this dataset." % hda.id
+                )
                 return False  # no auth
             # We could just look up the reverse of addr, but then we'd also
             # have to verify it with the forward address and special case any
             # IPs (instead of hosts) in the server list.
             #
             # This would be improved by caching, but that's what the OS's name
             # service cache daemon is for (you ARE running nscd, right?).
@@ -1458,26 +1609,35 @@
                 # balancing their connections (as UCSC does), this is okay.
                 try:
                     if socket.gethostbyname(server) == addr:
                         break  # remote host is in the server list
                 except (OSError, socket.gaierror):
                     pass  # can't resolve, try next
             else:
-                log.debug('Denying access to private dataset with hda: %i.  Remote addr is not a valid server for site: %s.' % (hda.id, hdadaa.site))
+                log.debug(
+                    "Denying access to private dataset with hda: %i.  Remote addr is not a valid server for site: %s."
+                    % (hda.id, hdadaa.site)
+                )
                 return False  # remote addr is not in the server list
             if (datetime.utcnow() - hdadaa.update_time) > timedelta(seconds=60):
-                log.debug('Denying access to private dataset with hda: %i.  Authorization was granted, but has expired.' % hda.id)
+                log.debug(
+                    "Denying access to private dataset with hda: %i.  Authorization was granted, but has expired."
+                    % hda.id
+                )
                 return False  # not authz'd in the last 60 seconds
-            log.debug('Allowing access to private dataset with hda: %i.  Remote server is: %s.' % (hda.id, server))
+            log.debug("Allowing access to private dataset with hda: %i.  Remote server is: %s." % (hda.id, server))
             return True
         else:
-            raise Exception('The dataset access permission is the only valid permission in the host security agent.')
+            raise Exception("The dataset access permission is the only valid permission in the host security agent.")
 
     def set_dataset_permissions(self, hda, user, site):
-        hdadaa = self.sa_session.query(self.model.HistoryDatasetAssociationDisplayAtAuthorization) \
-                                .filter_by(history_dataset_association_id=hda.id).first()
+        hdadaa = (
+            self.sa_session.query(self.model.HistoryDatasetAssociationDisplayAtAuthorization)
+            .filter_by(history_dataset_association_id=hda.id)
+            .first()
+        )
         if hdadaa:
             hdadaa.update_time = datetime.utcnow()
         else:
             hdadaa = self.model.HistoryDatasetAssociationDisplayAtAuthorization(hda=hda, user=user, site=site)
         self.sa_session.add(hdadaa)
         self.sa_session.flush()
```

### Comparing `galaxy-data-22.1.1/galaxy/model/store/build_objects.py` & `galaxy-data-23.0.1/galaxy/model/store/build_objects.py`

 * *Files 3% similar despite different names*

```diff
@@ -85,14 +85,15 @@
         umask=os.umask(0o77),
         gid=os.getgid(),
     )
     object_store = build_object_store_from_config(object_store_config)
     galaxy.model.Dataset.object_store = object_store
     galaxy.model.set_datatypes_registry(example_datatype_registry_for_sample())
     from galaxy.model import mapping
+
     mapping.init("/tmp", "sqlite:///:memory:", create_tables=True, object_store=object_store)
 
     with open(args.objects) as f:
         targets = yaml.safe_load(f)
         if not isinstance(targets, list):
             targets = [targets]
 
@@ -116,16 +117,16 @@
     with store_class(export_path, **export_kwds) as export_store:
         for target in targets:
             persist_target_to_export_store(target, export_store, object_store, ".")
 
 
 def _arg_parser():
     parser = argparse.ArgumentParser(description=DESCRIPTION, formatter_class=argparse.RawDescriptionHelpFormatter)
-    parser.add_argument('objects', metavar='OBJECT_CONFIG', help='config file describing files to build objects for')
-    parser.add_argument('--object-store-config', help="object store configuration file")
-    parser.add_argument('-e', '--export', default="export", help='export path')
-    parser.add_argument('--export-type', default=None, help='export type (if needed)')
+    parser.add_argument("objects", metavar="OBJECT_CONFIG", help="config file describing files to build objects for")
+    parser.add_argument("--object-store-config", help="object store configuration file")
+    parser.add_argument("-e", "--export", default="export", help="export path")
+    parser.add_argument("--export-type", default=None, help="export type (if needed)")
     return parser
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `galaxy-data-22.1.1/galaxy/model/store/discover.py` & `galaxy-data-23.0.1/galaxy/model/store/discover.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,17 +4,14 @@
 imports, etc... High-level utilities in this file can be used during
 job output discovery or for persisting Galaxy model objects
 corresponding to files in other contexts.
 """
 import abc
 import logging
 import os
-from collections import (
-    namedtuple,
-)
 from typing import (
     Any,
     Callable,
     Dict,
     List,
     NamedTuple,
     Optional,
@@ -22,48 +19,49 @@
     Union,
 )
 
 from sqlalchemy.orm.scoping import ScopedSession
 
 import galaxy.model
 from galaxy import util
-from galaxy.exceptions import (
-    RequestParameterInvalidException
-)
+from galaxy.exceptions import RequestParameterInvalidException
 from galaxy.model.dataset_collections import builder
 from galaxy.model.tags import GalaxySessionlessTagHandler
 from galaxy.objectstore import ObjectStore
 from galaxy.util import (
     chunk_iterable,
-    ExecutionTimer
+    ExecutionTimer,
 )
 from galaxy.util.hash_util import HASH_NAME_MAP
 
 if TYPE_CHECKING:
-    from galaxy.job_execution.output_collect import JobContext, SessionlessJobContext
-
+    from galaxy.model.store import ModelExportStore
 
 log = logging.getLogger(__name__)
 
 UNSET = object()
 DEFAULT_CHUNK_SIZE = 1000
 
 
 class MaxDiscoveredFilesExceededError(ValueError):
     pass
 
 
+CollectorT = Any  # TODO: setup an interface for these file collectors data classes.
+
+
 class ModelPersistenceContext(metaclass=abc.ABCMeta):
     """Class for creating datasets while finding files.
 
     This class implement the create_dataset method that takes care of populating metadata
     required for datasets and other potential model objects.
     """
 
-    max_discovered_files = float('inf')
+    job_working_directory: str  # TODO: rename
+    max_discovered_files = float("inf")
     discovered_file_count: int
 
     def create_dataset(
         self,
         ext,
         designation,
         visible,
@@ -78,15 +76,15 @@
         primary_data=None,
         init_from=None,
         dataset_attributes=None,
         tag_list=None,
         sources=None,
         hashes=None,
         created_from_basename=None,
-        final_job_state='ok',
+        final_job_state="ok",
         creating_job_id=None,
         storage_callbacks=None,
     ):
         tag_list = tag_list or []
         sources = sources or []
         hashes = hashes or []
         dataset_attributes = dataset_attributes or {}
@@ -102,43 +100,46 @@
         if metadata_source_name:
             assert init_from is None
         if init_from:
             assert metadata_source_name is None
 
         if primary_data is not None:
             primary_data.extension = ext
-            primary_data.visible = visible
             primary_data.dbkey = dbkey
         else:
             if not library_folder:
-                primary_data = galaxy.model.HistoryDatasetAssociation(extension=ext,
-                                                                      designation=designation,
-                                                                      visible=visible,
-                                                                      dbkey=dbkey,
-                                                                      create_dataset=True,
-                                                                      flush=False,
-                                                                      sa_session=sa_session,
-                                                                      creating_job_id=creating_job_id)
+                primary_data = galaxy.model.HistoryDatasetAssociation(
+                    extension=ext,
+                    designation=designation,
+                    visible=visible,
+                    dbkey=dbkey,
+                    create_dataset=True,
+                    flush=False,
+                    sa_session=sa_session,
+                    creating_job_id=creating_job_id,
+                )
                 self.persist_object(primary_data)
 
                 if init_from:
                     self.permission_provider.copy_dataset_permissions(init_from, primary_data)
                     primary_data.state = init_from.state
                 else:
                     self.permission_provider.set_default_hda_permissions(primary_data)
             else:
                 ld = galaxy.model.LibraryDataset(folder=library_folder, name=name)
-                ldda = galaxy.model.LibraryDatasetDatasetAssociation(name=name,
-                                                                     extension=ext,
-                                                                     dbkey=dbkey,
-                                                                     # library_dataset=ld,
-                                                                     user=self.user,
-                                                                     create_dataset=True,
-                                                                     flush=False,
-                                                                     sa_session=sa_session)
+                ldda = galaxy.model.LibraryDatasetDatasetAssociation(
+                    name=name,
+                    extension=ext,
+                    dbkey=dbkey,
+                    # library_dataset=ld,
+                    user=self.user,
+                    create_dataset=True,
+                    flush=False,
+                    sa_session=sa_session,
+                )
                 ld.library_dataset_dataset_association = ldda
 
                 self.add_library_dataset_to_folder(library_folder, ld)
                 primary_data = ldda
         primary_data.raw_set_dataset_state(final_job_state)
         if final_job_state == galaxy.model.Job.states.ERROR and not self.get_implicit_collection_jobs_association_id():
             primary_data.visible = True
@@ -179,17 +180,31 @@
         else:
             primary_data.init_meta()
 
         if info is not None:
             primary_data.info = info
         if filename:
             if storage_callbacks is None:
-                self.finalize_storage(primary_data=primary_data, dataset_attributes=dataset_attributes, extra_files=extra_files, filename=filename, link_data=link_data)
+                self.finalize_storage(
+                    primary_data=primary_data,
+                    dataset_attributes=dataset_attributes,
+                    extra_files=extra_files,
+                    filename=filename,
+                    link_data=link_data,
+                )
             else:
-                storage_callbacks.append(lambda: self.finalize_storage(primary_data=primary_data, dataset_attributes=dataset_attributes, extra_files=extra_files, filename=filename, link_data=link_data))
+                storage_callbacks.append(
+                    lambda: self.finalize_storage(
+                        primary_data=primary_data,
+                        dataset_attributes=dataset_attributes,
+                        extra_files=extra_files,
+                        filename=filename,
+                        link_data=link_data,
+                    )
+                )
         return primary_data
 
     def finalize_storage(self, primary_data, dataset_attributes, extra_files, filename, link_data):
         # Move data from temp location to dataset location
         if not link_data:
             self.object_store.update_from_file(primary_data.dataset, file_name=filename, create=True)
         else:
@@ -208,21 +223,25 @@
         datasets_attributes = datasets_attributes or [{} for _ in datasets]
         for primary_data, dataset_attributes in zip(datasets, datasets_attributes):
             # add tool/metadata provided information
             if dataset_attributes:
                 # TODO: discover_files should produce a match that encorporates this -
                 # would simplify ToolProvidedMetadata interface and eliminate this
                 # crap path.
-                dataset_att_by_name = dict(ext='extension')
-                for att_set in ['name', 'info', 'ext', 'dbkey']:
+                dataset_att_by_name = dict(ext="extension")
+                for att_set in ["name", "info", "ext", "dbkey"]:
                     dataset_att_name = dataset_att_by_name.get(att_set, att_set)
-                    setattr(primary_data, dataset_att_name, dataset_attributes.get(att_set, getattr(primary_data, dataset_att_name)))
+                    setattr(
+                        primary_data,
+                        dataset_att_name,
+                        dataset_attributes.get(att_set, getattr(primary_data, dataset_att_name)),
+                    )
 
             try:
-                metadata_dict = dataset_attributes.get('metadata', None)
+                metadata_dict = dataset_attributes.get("metadata", None)
                 if metadata_dict:
                     if "dbkey" in dataset_attributes:
                         metadata_dict["dbkey"] = dataset_attributes["dbkey"]
                     # branch tested with tool_provided_metadata_3 / tool_provided_metadata_10
                     primary_data.metadata.from_JSON_dict(json_dict=metadata_dict)
                 else:
                     primary_data.set_meta()
@@ -232,36 +251,63 @@
                 log.exception("Exception occured while setting metdata")
 
             try:
                 primary_data.set_peek()
             except Exception:
                 log.exception("Exception occured while setting dataset peek")
 
-    def populate_collection_elements(self, collection, root_collection_builder, filenames, name=None, metadata_source_name=None, final_job_state='ok'):
+    def populate_collection_elements(
+        self,
+        collection,
+        root_collection_builder,
+        discovered_files,
+        name=None,
+        metadata_source_name=None,
+        final_job_state="ok",
+    ):
         # TODO: allow configurable sorting.
         #    <sort by="lexical" /> <!-- default -->
         #    <sort by="reverse_lexical" />
         #    <sort regex="example.(\d+).fastq" by="1:numerical" />
         #    <sort regex="part_(\d+)_sample_([^_]+).fastq" by="2:lexical,1:numerical" />
         if name is None:
             name = "unnamed output"
         if self.flush_per_n_datasets and self.flush_per_n_datasets > 0:
-            for chunk in chunk_iterable(filenames.items(), size=self.flush_per_n_datasets):
-                self._populate_elements(chunk=chunk, name=name, root_collection_builder=root_collection_builder, metadata_source_name=metadata_source_name, final_job_state=final_job_state)
+            for chunk in chunk_iterable(discovered_files, size=self.flush_per_n_datasets):
+                self._populate_elements(
+                    chunk=chunk,
+                    name=name,
+                    root_collection_builder=root_collection_builder,
+                    metadata_source_name=metadata_source_name,
+                    final_job_state=final_job_state,
+                )
                 if len(chunk) == self.flush_per_n_datasets:
                     # In most cases we don't need to flush, that happens in the caller.
                     # Only flush here for saving memory.
                     root_collection_builder.populate_partial()
                     self.flush()
         else:
-            self._populate_elements(chunk=filenames.items(), name=name, root_collection_builder=root_collection_builder, metadata_source_name=metadata_source_name, final_job_state=final_job_state)
+            self._populate_elements(
+                chunk=discovered_files,
+                name=name,
+                root_collection_builder=root_collection_builder,
+                metadata_source_name=metadata_source_name,
+                final_job_state=final_job_state,
+            )
 
     def _populate_elements(self, chunk, name, root_collection_builder, metadata_source_name, final_job_state):
-        element_datasets: Dict[str, List[Any]] = {'element_identifiers': [], 'datasets': [], 'tag_lists': [], 'paths': [], 'extra_files': []}
-        for filename, discovered_file in chunk:
+        element_datasets: Dict[str, List[Any]] = {
+            "element_identifiers": [],
+            "datasets": [],
+            "tag_lists": [],
+            "paths": [],
+            "extra_files": [],
+        }
+        for discovered_file in chunk:
+            filename = discovered_file.path
             create_dataset_timer = ExecutionTimer()
             fields_match = discovered_file.match
             if not fields_match:
                 raise Exception(f"Problem parsing metadata fields for file {filename}")
             element_identifiers = fields_match.element_identifiers
             designation = fields_match.designation
             visible = fields_match.visible
@@ -276,15 +322,17 @@
             dataset_name = fields_match.name or designation
 
             link_data = discovered_file.match.link_data
 
             sources = discovered_file.match.sources
             hashes = discovered_file.match.hashes
             created_from_basename = discovered_file.match.created_from_basename
-
+            effective_state = fields_match.effective_state
+            if final_job_state == "ok" and effective_state != "ok":
+                final_job_state = effective_state
             dataset = self.create_dataset(
                 ext=ext,
                 designation=designation,
                 visible=visible,
                 dbkey=dbkey,
                 name=dataset_name,
                 metadata_source_name=metadata_source_name,
@@ -298,42 +346,46 @@
                 "(%s) Created dynamic collection dataset for path [%s] with element identifier [%s] for output [%s] %s",
                 self.job_id(),
                 filename,
                 designation,
                 name,
                 create_dataset_timer,
             )
-            element_datasets['element_identifiers'].append(element_identifiers)
-            element_datasets['extra_files'].append(extra_files)
-            element_datasets['datasets'].append(dataset)
-            element_datasets['tag_lists'].append(discovered_file.match.tag_list)
-            element_datasets['paths'].append(filename)
+            element_datasets["element_identifiers"].append(element_identifiers)
+            element_datasets["extra_files"].append(extra_files)
+            element_datasets["datasets"].append(dataset)
+            element_datasets["tag_lists"].append(discovered_file.match.tag_list)
+            element_datasets["paths"].append(filename)
 
-        self.add_tags_to_datasets(datasets=element_datasets['datasets'], tag_lists=element_datasets['tag_lists'])
-        for (element_identifiers, dataset) in zip(element_datasets['element_identifiers'], element_datasets['datasets']):
+        self.add_tags_to_datasets(datasets=element_datasets["datasets"], tag_lists=element_datasets["tag_lists"])
+        for element_identifiers, dataset in zip(element_datasets["element_identifiers"], element_datasets["datasets"]):
             current_builder = root_collection_builder
             for element_identifier in element_identifiers[:-1]:
                 current_builder = current_builder.get_level(element_identifier)
             current_builder.add_dataset(element_identifiers[-1], dataset)
 
             # Associate new dataset with job
             element_identifier_str = ":".join(element_identifiers)
-            association_name = f'__new_primary_file_{name}|{element_identifier_str}__'
+            association_name = f"__new_primary_file_{name}|{element_identifier_str}__"
             self.add_output_dataset_association(association_name, dataset)
 
         add_datasets_timer = ExecutionTimer()
-        self.add_datasets_to_history(element_datasets['datasets'])
-        self.update_object_store_with_datasets(datasets=element_datasets['datasets'], paths=element_datasets['paths'], extra_files=element_datasets['extra_files'])
+        self.add_datasets_to_history(element_datasets["datasets"])
+        self.update_object_store_with_datasets(
+            datasets=element_datasets["datasets"],
+            paths=element_datasets["paths"],
+            extra_files=element_datasets["extra_files"],
+        )
         log.debug(
             "(%s) Add dynamic collection datasets to history for output [%s] %s",
             self.job_id(),
             name,
             add_datasets_timer,
         )
-        self.set_datasets_metadata(datasets=element_datasets['datasets'])
+        self.set_datasets_metadata(datasets=element_datasets["datasets"])
 
     def add_tags_to_datasets(self, datasets, tag_lists):
         if any(tag_lists):
             for dataset, tags in zip(datasets, tag_lists):
                 self.tag_handler.add_tags_from_list(self.user, dataset, tags, flush=False)
 
     def update_object_store_with_datasets(self, datasets, paths, extra_files):
@@ -341,94 +393,107 @@
             self.object_store.update_from_file(dataset.dataset, file_name=path, create=True)
             if extra_file:
                 persist_extra_files(self.object_store, extra_files, dataset)
                 dataset.set_size()
             else:
                 dataset.set_size(no_extra_files=True)
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def tag_handler(self):
         """Return a galaxy.model.tags.TagHandler-like object for persisting tags."""
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def user(self):
         """If bound to a database, return the user the datasets should be created for.
 
         Return None otherwise.
         """
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def sa_session(self) -> Optional[ScopedSession]:
         """If bound to a database, return the SQL Alchemy session.
 
         Return None otherwise.
         """
 
-    @abc.abstractproperty
-    def permission_provider(self) -> 'PermissionProvider':
+    @property
+    @abc.abstractmethod
+    def permission_provider(self) -> "PermissionProvider":
         """If bound to a database, return the SQL Alchemy session.
 
         Return None otherwise.
         """
 
     def get_implicit_collection_jobs_association_id(self) -> Optional[str]:
         """No-op, no job context."""
         return None
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def job(self) -> Optional[galaxy.model.Job]:
         """Return associated job object if bound to a job finish context connected to a database."""
 
-    @abc.abstractproperty
-    def metadata_source_provider(self) -> 'MetadataSourceProvider':
+    @property
+    @abc.abstractmethod
+    def metadata_source_provider(self) -> "MetadataSourceProvider":
         """Return associated MetadataSourceProvider object."""
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def object_store(self) -> ObjectStore:
         """Return object store to use for populating discovered dataset contents."""
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def flush_per_n_datasets(self) -> Optional[int]:
         pass
 
     @property
     def input_dbkey(self) -> str:
-        return '?'
+        return "?"
 
     @abc.abstractmethod
     def add_library_dataset_to_folder(self, library_folder, ld):
         """Add library dataset to persisted library folder."""
 
     @abc.abstractmethod
     def create_library_folder(self, parent_folder, name, description):
         """Create a library folder ready from supplied attributes for supplied parent."""
 
     @abc.abstractmethod
     def add_output_dataset_association(self, name, dataset):
         """If discovering outputs for a job, persist output dataset association."""
 
+    @abc.abstractmethod
     def add_datasets_to_history(self, datasets, for_output_dataset=None):
         """Add datasets to the history this context points at."""
 
     def job_id(self):
-        return ''
+        return ""
 
+    @abc.abstractmethod
     def persist_object(self, obj):
         """Add the target to the persistence layer."""
 
-    def persist_library_folder(self, library_folder):
+    def persist_library_folder(self, library_folder: galaxy.model.LibraryFolder) -> None:  # noqa: B027
         """Add library folder to sessionless export. Noop for session export."""
 
+    @abc.abstractmethod
     def flush(self):
         """If database bound, flush the persisted objects to ensure IDs."""
 
     def increment_discovered_file_count(self):
         self.discovered_file_count += 1
         if self.discovered_file_count > self.max_discovered_files:
-            raise MaxDiscoveredFilesExceededError(f"Job generated more than maximum number ({self.max_discovered_files}) of output datasets")
+            raise MaxDiscoveredFilesExceededError(
+                f"Job generated more than maximum number ({self.max_discovered_files}) of output datasets"
+            )
 
 
 class PermissionProvider(metaclass=abc.ABCMeta):
     """Interface for working with permissions while importing datasets with ModelPersistenceContext."""
 
     @property
     def permissions(self):
@@ -439,15 +504,14 @@
 
     @abc.abstractmethod
     def copy_dataset_permissions(self, init_from, primary_data):
         """Copy dataset permissions from supplied input dataset."""
 
 
 class UnusedPermissionProvider(PermissionProvider):
-
     def copy_dataset_permissions(self, init_from, primary_data):
         """Throws NotImplementedError.
 
         This should only be called as part of job output collection where
         there should be a session available to initialize this from.
         """
         # TODO: what should this do in the sessionless context?
@@ -459,36 +523,35 @@
 
     @abc.abstractmethod
     def get_metadata_source(self, input_name):
         """Get metadata for supplied input_name."""
 
 
 class UnusedMetadataSourceProvider(MetadataSourceProvider):
-
     def get_metadata_source(self, input_name):
         """Throws NotImplementedError.
 
         This should only be called as part of job output collection where
         one can actually collect metadata from inputs, this is unused in the
         context of SessionlessModelPersistenceContext.
         """
         raise NotImplementedError()
 
 
 class SessionlessModelPersistenceContext(ModelPersistenceContext):
     """A variant of ModelPersistenceContext that persists to an export store instead of database directly."""
 
-    def __init__(self, object_store, export_store, working_directory):
+    def __init__(self, object_store, export_store: "ModelExportStore", working_directory: str) -> None:
         self._permission_provider = UnusedPermissionProvider()
         self._metadata_source_provider = UnusedMetadataSourceProvider()
         self._object_store = object_store
         self.export_store = export_store
         self._flush_per_n_datasets = None
         self.discovered_file_count = 0
-        self.max_discovered_files = float('inf')
+        self.max_discovered_files = float("inf")
         self.job_working_directory = working_directory  # TODO: rename...
 
     @property
     def tag_handler(self):
         return GalaxySessionlessTagHandler(self.sa_session)
 
     @property
@@ -535,25 +598,29 @@
         folder.id = destination.get("library_folder_id")
         return folder
 
     def get_hdca(self, object_id):
         raise NotImplementedError()
 
     def create_hdca(self, name, structure):
-        collection = galaxy.model.DatasetCollection(collection_type=structure.collection_type_description.collection_type, populated=False)
+        collection = galaxy.model.DatasetCollection(
+            collection_type=structure.collection_type_description.collection_type, populated=False
+        )
         return galaxy.model.HistoryDatasetCollectionAssociation(name=name, collection=collection)
 
     def create_library_folder(self, parent_folder, name, description):
-        nested_folder = galaxy.model.LibraryFolder(name=name, description=description, order_id=parent_folder.item_count)
+        nested_folder = galaxy.model.LibraryFolder(
+            name=name, description=description, order_id=parent_folder.item_count
+        )
         parent_folder.item_count += 1
         parent_folder.folders.append(nested_folder)
         return nested_folder
 
-    def persist_library_folder(self, library_folder):
-        self.export_store.export_library(library_folder)
+    def persist_library_folder(self, library_folder: galaxy.model.LibraryFolder) -> None:
+        self.export_store.export_library_folder(library_folder)
 
     def add_datasets_to_history(self, datasets, for_output_dataset=None):
         # Consider copying these datasets to for_output_dataset copied histories
         # somehow. Not sure it is worth the effort/complexity?
         for dataset in datasets:
             self.export_store.add_dataset(dataset)
 
@@ -575,24 +642,26 @@
 
 
 def persist_extra_files(object_store, src_extra_files_path, primary_data):
     if src_extra_files_path and os.path.exists(src_extra_files_path):
         primary_data.dataset.create_extra_files_path()
         target_extra_files_path = primary_data.extra_files_path
         for root, _dirs, files in os.walk(src_extra_files_path):
-            extra_dir = os.path.join(target_extra_files_path, root.replace(src_extra_files_path, '', 1).lstrip(os.path.sep))
+            extra_dir = os.path.join(
+                target_extra_files_path, root.replace(src_extra_files_path, "", 1).lstrip(os.path.sep)
+            )
             extra_dir = os.path.normpath(extra_dir)
             for f in files:
                 object_store.update_from_file(
                     primary_data.dataset,
                     extra_dir=extra_dir,
                     alt_name=f,
                     file_name=os.path.join(root, f),
                     create=True,
-                    preserve_symlinks=True
+                    preserve_symlinks=True,
                 )
 
 
 def persist_target_to_export_store(target_dict, export_store, object_store, work_directory):
     replace_request_syntax_sugar(target_dict)
     model_persistence_context = SessionlessModelPersistenceContext(object_store, export_store, work_directory)
 
@@ -605,15 +674,15 @@
     destination_type = destination["type"]
 
     assert destination_type in ["library", "hdas", "hdca"]
     if destination_type == "library":
         name = get_required_item(destination, "name", "Must specify a library name")
         description = destination.get("description", "")
         synopsis = destination.get("synopsis", "")
-        root_folder = galaxy.model.LibraryFolder(name=name, description='')
+        root_folder = galaxy.model.LibraryFolder(name=name, description="")
         library = galaxy.model.Library(
             name=name,
             description=description,
             synopsis=synopsis,
             root_folder=root_folder,
         )
         persist_elements_to_folder(model_persistence_context, elements, root_folder)
@@ -630,34 +699,41 @@
             name=name,
             collection=collection,
         )
         persist_elements_to_hdca(model_persistence_context, elements, hdca)
         export_store.add_dataset_collection(hdca)
 
 
-def persist_elements_to_hdca(model_persistence_context: Union['JobContext', 'SessionlessJobContext', SessionlessModelPersistenceContext], elements, hdca, collector=None):
-    filenames = {}
+def persist_elements_to_hdca(
+    model_persistence_context: ModelPersistenceContext,
+    elements,
+    hdca,
+    collector=None,
+):
+    discovered_files = []
 
     def add_to_discovered_files(elements, parent_identifiers=None):
         parent_identifiers = parent_identifiers or []
         for element in elements:
             if "elements" in element:
                 add_to_discovered_files(element["elements"], parent_identifiers + [element["name"]])
             else:
-                discovered_file = discovered_file_for_element(element, model_persistence_context, parent_identifiers, collector=collector)
-                filenames[discovered_file.path] = discovered_file
+                discovered_file = discovered_file_for_element(
+                    element, model_persistence_context, parent_identifiers, collector=collector
+                )
+                discovered_files.append(discovered_file)
 
     add_to_discovered_files(elements)
 
     collection = hdca.collection
     collection_builder = builder.BoundCollectionBuilder(collection)
     model_persistence_context.populate_collection_elements(
         collection,
         collection_builder,
-        filenames,
+        discovered_files,
     )
     collection_builder.populate()
 
 
 def persist_elements_to_folder(model_persistence_context, elements, library_folder):
     for element in elements:
         if "elements" in element:
@@ -669,27 +745,24 @@
         else:
             discovered_file = discovered_file_for_element(element, model_persistence_context)
             fields_match = discovered_file.match
             designation = fields_match.designation
             visible = fields_match.visible
             ext = fields_match.ext
             dbkey = fields_match.dbkey
-            info = element.get("info", None)
             link_data = discovered_file.match.link_data
 
             # Create new primary dataset
             name = fields_match.name or designation
 
             sources = fields_match.sources
             hashes = fields_match.hashes
             created_from_basename = fields_match.created_from_basename
-            state = 'ok'
-            if hasattr(discovered_file, "error_message"):
-                info = discovered_file.error_message
-                state = "error"
+            effective_state = fields_match.effective_state
+            info, state = discovered_file.discovered_state(element, final_job_state=effective_state)
             model_persistence_context.create_dataset(
                 ext=ext,
                 designation=designation,
                 visible=visible,
                 dbkey=dbkey,
                 name=name,
                 filename=discovered_file.path,
@@ -699,68 +772,70 @@
                 sources=sources,
                 hashes=hashes,
                 created_from_basename=created_from_basename,
                 final_job_state=state,
             )
 
 
-def persist_hdas(elements, model_persistence_context, final_job_state='ok'):
+def persist_hdas(elements, model_persistence_context, final_job_state="ok"):
     # discover files as individual datasets for the target history
     datasets = []
     storage_callbacks: List[Callable] = []
 
     def collect_elements_for_history(elements):
         for element in elements:
             if "elements" in element:
                 collect_elements_for_history(element["elements"])
             else:
                 discovered_file = discovered_file_for_element(element, model_persistence_context)
                 fields_match = discovered_file.match
                 designation = fields_match.designation
                 ext = fields_match.ext
                 dbkey = fields_match.dbkey
-                info = element.get("info", None)
                 tag_list = element.get("tags")
                 link_data = discovered_file.match.link_data
 
                 # Create new primary dataset
                 name = fields_match.name or designation
 
                 hda_id = discovered_file.match.object_id
+
                 primary_dataset = None
                 if hda_id:
-                    sa_session = model_persistence_context.sa_session or model_persistence_context.import_store.sa_session
+                    sa_session = (
+                        model_persistence_context.sa_session or model_persistence_context.import_store.sa_session
+                    )
                     primary_dataset = sa_session.query(galaxy.model.HistoryDatasetAssociation).get(hda_id)
 
                 sources = fields_match.sources
                 hashes = fields_match.hashes
                 created_from_basename = fields_match.created_from_basename
                 extra_files = fields_match.extra_files
-                state = final_job_state
-                if hasattr(discovered_file, "error_message"):
-                    state = "error"
-                    info = discovered_file.error_message
+                visible = fields_match.visible
+
+                info, state = discovered_file.discovered_state(element, final_job_state)
                 dataset = model_persistence_context.create_dataset(
                     ext=ext,
                     designation=designation,
-                    visible=True,
+                    visible=visible,
                     dbkey=dbkey,
                     name=name,
                     filename=discovered_file.path,
                     extra_files=extra_files,
                     info=info,
                     tag_list=tag_list,
                     link_data=link_data,
                     primary_data=primary_dataset,
                     sources=sources,
                     hashes=hashes,
                     created_from_basename=created_from_basename,
                     final_job_state=state,
                     storage_callbacks=storage_callbacks,
                 )
+                dataset.discovered = True
                 if not hda_id:
                     datasets.append(dataset)
 
     collect_elements_for_history(elements)
     model_persistence_context.add_datasets_to_history(datasets)
     for callback in storage_callbacks:
         callback()
@@ -817,50 +892,102 @@
                     del obj[key.lower()]
 
             if "hashes" not in obj:
                 obj["hashes"] = []
             obj["hashes"].extend(new_hashes)
 
 
-DiscoveredFile = namedtuple('DiscoveredFile', ['path', 'collector', 'match'])
+class DiscoveredFile(NamedTuple):
+    path: str
+    collector: Optional[CollectorT]
+    match: "JsonCollectedDatasetMatch"
+
+    def discovered_state(self, element: Dict[str, Any], final_job_state="ok") -> "DiscoveredResultState":
+        info = element.get("info", None)
+        return DiscoveredResultState(info, final_job_state)
+
+
+class DiscoveredResultState(NamedTuple):
+    info: Optional[str]
+    state: str
+
+
+class DiscoveredDeferredFile(NamedTuple):
+    collector: Optional[CollectorT]
+    match: "JsonCollectedDatasetMatch"
 
+    def discovered_state(self, element: Dict[str, Any], final_job_state="ok") -> DiscoveredResultState:
+        info = element.get("info", None)
+        state = "deferred" if final_job_state == "ok" else final_job_state
+        return DiscoveredResultState(info, state)
 
-def discovered_file_for_element(dataset, model_persistence_context: Union['JobContext', 'SessionlessJobContext', SessionlessModelPersistenceContext], parent_identifiers=None, collector=None):
+    @property
+    def path(self):
+        return None
+
+
+DiscoveredResult = Union[DiscoveredFile, DiscoveredDeferredFile, "DiscoveredFileError"]
+
+
+def discovered_file_for_element(
+    dataset,
+    model_persistence_context: ModelPersistenceContext,
+    parent_identifiers=None,
+    collector=None,
+) -> DiscoveredResult:
     model_persistence_context.increment_discovered_file_count()
     parent_identifiers = parent_identifiers or []
-    target_directory = discover_target_directory(getattr(collector, "directory", None), model_persistence_context.job_working_directory)
+    target_directory = discover_target_directory(
+        getattr(collector, "directory", None), model_persistence_context.job_working_directory
+    )
     filename = dataset.get("filename")
     error_message = dataset.get("error_message")
     if error_message is None:
+        if dataset.get("state") == "deferred":
+            return DiscoveredDeferredFile(
+                collector, JsonCollectedDatasetMatch(dataset, collector, None, parent_identifiers=parent_identifiers)
+            )
+
         # handle link_data_only here, verify filename is in directory if not linking...
         if not dataset.get("link_data_only"):
             path = os.path.join(target_directory, filename)
             if not util.in_directory(path, target_directory):
-                raise Exception("Problem with tool configuration, attempting to pull in datasets from outside working directory.")
+                raise Exception(
+                    "Problem with tool configuration, attempting to pull in datasets from outside working directory."
+                )
         else:
             path = filename
-        return DiscoveredFile(path, collector, JsonCollectedDatasetMatch(dataset, collector, filename, path=path, parent_identifiers=parent_identifiers))
+        return DiscoveredFile(
+            path,
+            collector,
+            JsonCollectedDatasetMatch(dataset, collector, filename, path=path, parent_identifiers=parent_identifiers),
+        )
     else:
         assert "error_message" in dataset
-        return DiscoveredFileError(dataset['error_message'], collector, JsonCollectedDatasetMatch(dataset, collector, None, parent_identifiers=parent_identifiers))
+        return DiscoveredFileError(
+            dataset["error_message"],
+            collector,
+            JsonCollectedDatasetMatch(dataset, collector, None, parent_identifiers=parent_identifiers),
+        )
 
 
 def discover_target_directory(dir_name, job_working_directory):
     if dir_name:
         directory = os.path.join(job_working_directory, dir_name)
         if not util.in_directory(directory, job_working_directory):
-            raise Exception("Problem with tool configuration, attempting to pull in datasets from outside working directory.")
+            raise Exception(
+                "Problem with tool configuration, attempting to pull in datasets from outside working directory."
+            )
         return directory
     else:
         return job_working_directory
 
 
 class JsonCollectedDatasetMatch:
-
-    def __init__(self, as_dict, collector, filename, path=None, parent_identifiers=None):
+    def __init__(self, as_dict, collector: Optional[CollectorT], filename, path=None, parent_identifiers=None):
         parent_identifiers = parent_identifiers or []
         self.as_dict = as_dict
         self.collector = collector
         self.filename = filename
         self.path = path
         self._parent_identifiers = parent_identifiers
 
@@ -894,16 +1021,15 @@
                 break
             i += 1
 
         return identifiers
 
     @property
     def name(self):
-        """ Return name or None if not defined by the discovery pattern.
-        """
+        """Return name or None if not defined by the discovery pattern."""
         return self.as_dict.get("name")
 
     @property
     def dbkey(self):
         return self.as_dict.get("dbkey", getattr(self.collector, "default_dbkey", "?"))
 
     @property
@@ -941,21 +1067,26 @@
     def created_from_basename(self):
         return self.as_dict.get("created_from_basename")
 
     @property
     def extra_files(self):
         return self.as_dict.get("extra_files")
 
+    @property
+    def effective_state(self):
+        return self.as_dict.get("state") or "ok"
 
-class RegexCollectedDatasetMatch(JsonCollectedDatasetMatch):
 
-    def __init__(self, re_match, collector, filename, path=None):
-        super().__init__(
-            re_match.groupdict(), collector, filename, path=path
-        )
+class RegexCollectedDatasetMatch(JsonCollectedDatasetMatch):
+    def __init__(self, re_match, collector: Optional[CollectorT], filename, path=None):
+        super().__init__(re_match.groupdict(), collector, filename, path=path)
 
 
 class DiscoveredFileError(NamedTuple):
     error_message: str
-    collector: Any   # TODO: setup interface for this
+    collector: Optional[CollectorT]
     match: JsonCollectedDatasetMatch
     path: Optional[str] = None
+
+    def discovered_state(self, element: Dict[str, Any], final_job_state="ok") -> DiscoveredResultState:
+        info = self.error_message
+        return DiscoveredResultState(info, "error")
```

### Comparing `galaxy-data-22.1.1/galaxy/model/tags.py` & `galaxy-data-23.0.1/galaxy/model/tags.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,15 @@
 import logging
 import re
-from typing import Dict, List, Optional, Tuple
+from typing import (
+    Dict,
+    List,
+    Optional,
+    Tuple,
+)
 
 from sqlalchemy.exc import IntegrityError
 from sqlalchemy.orm import sessionmaker
 from sqlalchemy.sql import select
 from sqlalchemy.sql.expression import func
 
 import galaxy.model
@@ -33,45 +38,45 @@
     def __init__(self, sa_session: galaxy_scoped_session) -> None:
         self.sa_session = sa_session
         # Minimum tag length.
         self.min_tag_len = 1
         # Maximum tag length.
         self.max_tag_len = 255
         # Tag separator.
-        self.tag_separators = ',;'
+        self.tag_separators = ",;"
         # Hierarchy separator.
-        self.hierarchy_separator = '.'
+        self.hierarchy_separator = "."
         # Key-value separator.
         self.key_value_separators = "=:"
         # Initialize with known classes - add to this in subclasses.
         self.item_tag_assoc_info: Dict[str, ItemTagAssocInfo] = {}
 
     def create_tag_handler_session(self):
         # Creates a transient tag handler that avoids repeated flushes
         return GalaxyTagHandlerSession(self.sa_session)
 
     def add_tags_from_list(self, user, item, new_tags_list, flush=True):
         new_tags_set = set(new_tags_list)
         if item.tags:
-            new_tags_set.update(self.get_tags_str(item.tags).split(','))
+            new_tags_set.update(self.get_tags_str(item.tags).split(","))
         return self.set_tags_from_list(user, item, new_tags_set, flush=flush)
 
     def remove_tags_from_list(self, user, item, tag_to_remove_list, flush=True):
         tag_to_remove_set = set(tag_to_remove_list)
-        tags_set = {_.strip() for _ in self.get_tags_str(item.tags).split(',')}
+        tags_set = {_.strip() for _ in self.get_tags_str(item.tags).split(",")}
         if item.tags:
             tags_set -= tag_to_remove_set
         return self.set_tags_from_list(user, item, tags_set, flush=flush)
 
     def set_tags_from_list(self, user, item, new_tags_list, flush=True):
         # precondition: item is already security checked against user
         # precondition: incoming tags is a list of sanitized/formatted strings
         self.delete_item_tags(user, item)
-        new_tags_str = ','.join(new_tags_list)
-        self.apply_item_tags(user, item, unicodify(new_tags_str, 'utf-8'), flush=flush)
+        new_tags_str = ",".join(new_tags_list)
+        self.apply_item_tags(user, item, unicodify(new_tags_str, "utf-8"), flush=flush)
         if flush:
             self.sa_session.flush()
         return item.tags
 
     def get_tag_assoc_class(self, item_class):
         """Returns tag association class for item class."""
         return self.item_tag_assoc_info[item_class.__name__].tag_assoc_class
@@ -84,37 +89,40 @@
         """Returns community tags for an item."""
         # Get item-tag association class.
         item_class = item.__class__
         item_tag_assoc_class = self.get_tag_assoc_class(item_class)
         if not item_tag_assoc_class:
             return []
         # Build select statement.
-        cols_to_select = [item_tag_assoc_class.table.c.tag_id, func.count('*')]
+        cols_to_select = [item_tag_assoc_class.table.c.tag_id, func.count("*")]
         from_obj = item_tag_assoc_class.table.join(item_class.table).join(galaxy.model.Tag.table)
-        where_clause = (self.get_id_col_in_item_tag_assoc_table(item_class) == item.id)
+        where_clause = self.get_id_col_in_item_tag_assoc_table(item_class) == item.id
         order_by = [func.count("*").desc()]
         group_by = item_tag_assoc_class.table.c.tag_id
         # Do query and get result set.
-        query = select(columns=cols_to_select,
-                       from_obj=from_obj,
-                       whereclause=where_clause,
-                       group_by=group_by,
-                       order_by=order_by,
-                       limit=limit)
+        query = select(
+            columns=cols_to_select,
+            from_obj=from_obj,
+            whereclause=where_clause,
+            group_by=group_by,
+            order_by=order_by,
+            limit=limit,
+        )
         result_set = self.sa_session.execute(query)
         # Return community tags.
         community_tags = []
         for row in result_set:
             tag_id = row[0]
             community_tags.append(self.get_tag_by_id(tag_id))
         return community_tags
 
     def get_tool_tags(self):
-        query = select(columns=[galaxy.model.ToolTagAssociation.table.c.tag_id],
-                       from_obj=galaxy.model.ToolTagAssociation.table).distinct()
+        query = select(
+            columns=[galaxy.model.ToolTagAssociation.table.c.tag_id], from_obj=galaxy.model.ToolTagAssociation.table
+        ).distinct()
         result_set = self.sa_session.execute(query)
 
         tags = []
         for row in result_set:
             tag_id = row[0]
             tags.append(self.get_tag_by_id(tag_id))
         return tags
@@ -325,25 +333,25 @@
 
     def _scrub_tag_value(self, value):
         """Scrub a tag value."""
         # Gracefully handle None:
         if not value:
             return None
         # Remove whitespace from value.
-        reg_exp = re.compile(r'\s')
+        reg_exp = re.compile(r"\s")
         scrubbed_value = re.sub(reg_exp, "", value)
         return scrubbed_value
 
     def _scrub_tag_name(self, name):
         """Scrub a tag name."""
         # Gracefully handle None:
         if not name:
             return None
         # Remove whitespace from name.
-        reg_exp = re.compile(r'\s')
+        reg_exp = re.compile(r"\s")
         scrubbed_name = re.sub(reg_exp, "", name)
         # Ignore starting ':' char.
         if scrubbed_name.startswith(self.hierarchy_separator):
             scrubbed_name = scrubbed_name[1:]
         # If name is too short or too long, return None.
         if len(scrubbed_name) < self.min_tag_len or len(scrubbed_name) > self.max_tag_len:
             return None
@@ -355,52 +363,58 @@
         for tag in tag_name_list:
             scrubbed_tag_list.append(self._scrub_tag_name(tag))
         return scrubbed_tag_list
 
     def _get_name_value_pair(self, tag_str) -> List[Optional[str]]:
         """Get name, value pair from a tag string."""
         # Use regular expression to parse name, value.
-        if tag_str.startswith('#'):
+        if tag_str.startswith("#"):
             tag_str = f"name:{tag_str[1:]}"
         reg_exp = re.compile(f"[{self.key_value_separators}]")
         name_value_pair: List[Optional[str]] = list(reg_exp.split(tag_str, 1))
         # Add empty slot if tag does not have value.
         if len(name_value_pair) < 2:
             name_value_pair.append(None)
         return name_value_pair
 
 
 class GalaxyTagHandler(TagHandler):
     def __init__(self, sa_session: galaxy_scoped_session):
         from galaxy import model
+
         TagHandler.__init__(self, sa_session)
-        self.item_tag_assoc_info["History"] = ItemTagAssocInfo(model.History,
-                                                               model.HistoryTagAssociation,
-                                                               model.HistoryTagAssociation.history_id)
-        self.item_tag_assoc_info["HistoryDatasetAssociation"] = \
-            ItemTagAssocInfo(model.HistoryDatasetAssociation,
-                             model.HistoryDatasetAssociationTagAssociation,
-                             model.HistoryDatasetAssociationTagAssociation.history_dataset_association_id)
-        self.item_tag_assoc_info["HistoryDatasetCollectionAssociation"] = \
-            ItemTagAssocInfo(model.HistoryDatasetCollectionAssociation,
-                             model.HistoryDatasetCollectionTagAssociation,
-                             model.HistoryDatasetCollectionTagAssociation.history_dataset_collection_id)
-        self.item_tag_assoc_info["LibraryDatasetDatasetAssociation"] = \
-            ItemTagAssocInfo(model.LibraryDatasetDatasetAssociation,
-                             model.LibraryDatasetDatasetAssociationTagAssociation,
-                             model.LibraryDatasetDatasetAssociationTagAssociation.library_dataset_dataset_association_id)
-        self.item_tag_assoc_info["Page"] = ItemTagAssocInfo(model.Page,
-                                                            model.PageTagAssociation,
-                                                            model.PageTagAssociation.page_id)
-        self.item_tag_assoc_info["StoredWorkflow"] = ItemTagAssocInfo(model.StoredWorkflow,
-                                                                      model.StoredWorkflowTagAssociation,
-                                                                      model.StoredWorkflowTagAssociation.stored_workflow_id)
-        self.item_tag_assoc_info["Visualization"] = ItemTagAssocInfo(model.Visualization,
-                                                                     model.VisualizationTagAssociation,
-                                                                     model.VisualizationTagAssociation.visualization_id)
+        self.item_tag_assoc_info["History"] = ItemTagAssocInfo(
+            model.History, model.HistoryTagAssociation, model.HistoryTagAssociation.history_id
+        )
+        self.item_tag_assoc_info["HistoryDatasetAssociation"] = ItemTagAssocInfo(
+            model.HistoryDatasetAssociation,
+            model.HistoryDatasetAssociationTagAssociation,
+            model.HistoryDatasetAssociationTagAssociation.history_dataset_association_id,
+        )
+        self.item_tag_assoc_info["HistoryDatasetCollectionAssociation"] = ItemTagAssocInfo(
+            model.HistoryDatasetCollectionAssociation,
+            model.HistoryDatasetCollectionTagAssociation,
+            model.HistoryDatasetCollectionTagAssociation.history_dataset_collection_id,
+        )
+        self.item_tag_assoc_info["LibraryDatasetDatasetAssociation"] = ItemTagAssocInfo(
+            model.LibraryDatasetDatasetAssociation,
+            model.LibraryDatasetDatasetAssociationTagAssociation,
+            model.LibraryDatasetDatasetAssociationTagAssociation.library_dataset_dataset_association_id,
+        )
+        self.item_tag_assoc_info["Page"] = ItemTagAssocInfo(
+            model.Page, model.PageTagAssociation, model.PageTagAssociation.page_id
+        )
+        self.item_tag_assoc_info["StoredWorkflow"] = ItemTagAssocInfo(
+            model.StoredWorkflow,
+            model.StoredWorkflowTagAssociation,
+            model.StoredWorkflowTagAssociation.stored_workflow_id,
+        )
+        self.item_tag_assoc_info["Visualization"] = ItemTagAssocInfo(
+            model.Visualization, model.VisualizationTagAssociation, model.VisualizationTagAssociation.visualization_id
+        )
 
 
 class GalaxyTagHandlerSession(GalaxyTagHandler):
     """Like GalaxyTagHandler, but avoids one flush per created tag."""
 
     def __init__(self, sa_session):
         super().__init__(sa_session)
@@ -415,15 +429,14 @@
         """Create tag and and store in cache."""
         tag = super()._create_tag_instance(tag_name)
         self.created_tags[tag_name] = tag
         return tag
 
 
 class GalaxySessionlessTagHandler(GalaxyTagHandlerSession):
-
     def _get_tag(self, tag_name):
         """Get tag from cache or database."""
         # Short-circuit session access
         return self.created_tags.get(tag_name)
 
     def get_tag_by_name(self, tag_name):
         return self.created_tags.get(tag_name)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/tool_shed_install/__init__.py` & `galaxy-data-23.0.1/galaxy/model/tool_shed_install/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,134 +1,187 @@
 import logging
 import os
 from enum import Enum
-from typing import TYPE_CHECKING
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Optional,
+    TYPE_CHECKING,
+)
 
 from sqlalchemy import (
     Boolean,
     Column,
     DateTime,
     ForeignKey,
     Integer,
     String,
-    Table,
     TEXT,
 )
 from sqlalchemy.orm import (
     registry,
     relationship,
 )
-from sqlalchemy.orm.decl_api import DeclarativeMeta
 
 from galaxy.model.custom_types import (
     MutableJSONType,
-    TrimmedString
+    TrimmedString,
 )
 from galaxy.model.orm.now import now
 from galaxy.util import asbool
 from galaxy.util.bunch import Bunch
 from galaxy.util.dictifiable import Dictifiable
 from galaxy.util.tool_shed import common_util
 
 log = logging.getLogger(__name__)
 
 mapper_registry = registry()
 
 if TYPE_CHECKING:
-    class _HasTable:
-        table: Table
+    # Workaround for https://github.com/python/mypy/issues/14182
+    from sqlalchemy.orm.decl_api import DeclarativeMeta as _DeclarativeMeta
+
+    class DeclarativeMeta(_DeclarativeMeta, type):
+        pass
+
 else:
-    _HasTable = object
+    from sqlalchemy.orm.decl_api import DeclarativeMeta
 
 
 class Base(metaclass=DeclarativeMeta):
     __abstract__ = True
     registry = mapper_registry
     metadata = mapper_registry.metadata
     __init__ = mapper_registry.constructor
 
     @classmethod
     def __declare_last__(cls):
         cls.table = cls.__table__
 
 
-class ToolShedRepository(Base, _HasTable):
-    __tablename__ = 'tool_shed_repository'
+class ToolShedRepository(Base):
+    __tablename__ = "tool_shed_repository"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     tool_shed = Column(TrimmedString(255), index=True)
     name = Column(TrimmedString(255), index=True)
     description = Column(TEXT)
     owner = Column(TrimmedString(255), index=True)
     installed_changeset_revision = Column(TrimmedString(255))
     changeset_revision = Column(TrimmedString(255), index=True)
     ctx_rev = Column(TrimmedString(10))
-    metadata_ = Column('metadata', MutableJSONType, nullable=True)
+    metadata_ = Column("metadata", MutableJSONType, nullable=True)
     includes_datatypes = Column(Boolean, index=True, default=False)
     tool_shed_status = Column(MutableJSONType, nullable=True)
     deleted = Column(Boolean, index=True, default=False)
     uninstalled = Column(Boolean, default=False)
     dist_to_shed = Column(Boolean, default=False)
     status = Column(TrimmedString(255))
     error_message = Column(TEXT)
-    tool_versions = relationship('ToolVersion', back_populates='tool_shed_repository')
-    tool_dependencies = relationship('ToolDependency', order_by='ToolDependency.name',
-        back_populates='tool_shed_repository')
-    required_repositories = relationship('RepositoryRepositoryDependencyAssociation',
-        back_populates='repository')
-
-    dict_collection_visible_keys = ['id', 'tool_shed', 'name', 'owner', 'installed_changeset_revision', 'changeset_revision', 'ctx_rev', 'includes_datatypes',
-                                    'tool_shed_status', 'deleted', 'uninstalled', 'dist_to_shed', 'status', 'error_message', 'description']
-    dict_element_visible_keys = ['id', 'tool_shed', 'name', 'owner', 'installed_changeset_revision', 'changeset_revision', 'ctx_rev', 'includes_datatypes',
-                                 'tool_shed_status', 'deleted', 'uninstalled', 'dist_to_shed', 'status', 'error_message', 'description']
+    tool_versions = relationship("ToolVersion", back_populates="tool_shed_repository")
+    tool_dependencies = relationship(
+        "ToolDependency", order_by="ToolDependency.name", back_populates="tool_shed_repository"
+    )
+    required_repositories = relationship("RepositoryRepositoryDependencyAssociation", back_populates="repository")
+
+    dict_collection_visible_keys = [
+        "id",
+        "tool_shed",
+        "name",
+        "owner",
+        "installed_changeset_revision",
+        "changeset_revision",
+        "ctx_rev",
+        "includes_datatypes",
+        "tool_shed_status",
+        "deleted",
+        "uninstalled",
+        "dist_to_shed",
+        "status",
+        "error_message",
+        "description",
+    ]
+    dict_element_visible_keys = [
+        "id",
+        "tool_shed",
+        "name",
+        "owner",
+        "installed_changeset_revision",
+        "changeset_revision",
+        "ctx_rev",
+        "includes_datatypes",
+        "tool_shed_status",
+        "deleted",
+        "uninstalled",
+        "dist_to_shed",
+        "status",
+        "error_message",
+        "description",
+    ]
 
     class installation_status(str, Enum):
-        NEW = 'New'
-        CLONING = 'Cloning'
-        SETTING_TOOL_VERSIONS = 'Setting tool versions'
-        INSTALLING_REPOSITORY_DEPENDENCIES = 'Installing repository dependencies'
-        INSTALLING_TOOL_DEPENDENCIES = 'Installing tool dependencies'
-        LOADING_PROPRIETARY_DATATYPES = 'Loading proprietary datatypes'
-        INSTALLED = 'Installed'
-        DEACTIVATED = 'Deactivated'
-        ERROR = 'Error'
-        UNINSTALLED = 'Uninstalled'
+        NEW = "New"
+        CLONING = "Cloning"
+        SETTING_TOOL_VERSIONS = "Setting tool versions"
+        INSTALLING_REPOSITORY_DEPENDENCIES = "Installing repository dependencies"
+        INSTALLING_TOOL_DEPENDENCIES = "Installing tool dependencies"
+        LOADING_PROPRIETARY_DATATYPES = "Loading proprietary datatypes"
+        INSTALLED = "Installed"
+        DEACTIVATED = "Deactivated"
+        ERROR = "Error"
+        UNINSTALLED = "Uninstalled"
 
     class states(str, Enum):
-        INSTALLING = 'running'
-        OK = 'ok'
-        WARNING = 'queued'
-        ERROR = 'error'
-        UNINSTALLED = 'deleted_new'
-
-    def __init__(self, id=None, create_time=None, tool_shed=None, name=None, description=None, owner=None, installed_changeset_revision=None,
-                 changeset_revision=None, ctx_rev=None, metadata_=None, includes_datatypes=False, tool_shed_status=None, deleted=False,
-                 uninstalled=False, dist_to_shed=False, status=None, error_message=None):
+        INSTALLING = "running"
+        OK = "ok"
+        WARNING = "queued"
+        ERROR = "error"
+        UNINSTALLED = "deleted_new"
+
+    def __init__(
+        self,
+        id=None,
+        create_time=None,
+        tool_shed=None,
+        name=None,
+        description=None,
+        owner=None,
+        installed_changeset_revision=None,
+        changeset_revision=None,
+        ctx_rev=None,
+        metadata_=None,
+        tool_shed_status=None,
+        deleted=False,
+        uninstalled=False,
+        dist_to_shed=False,
+        status=None,
+        error_message=None,
+    ):
         self.id = id
         self.create_time = create_time
         self.tool_shed = tool_shed
         self.name = name
         self.description = description
         self.owner = owner
         self.installed_changeset_revision = installed_changeset_revision
         self.changeset_revision = changeset_revision
         self.ctx_rev = ctx_rev
         self.metadata_ = metadata_ or {}
-        self.includes_datatypes = includes_datatypes
         self.tool_shed_status = tool_shed_status
         self.deleted = deleted
         self.uninstalled = uninstalled
         self.dist_to_shed = dist_to_shed
         self.status = status
         self.error_message = error_message
 
-    def as_dict(self, value_mapper=None):
-        return self.to_dict(view='element', value_mapper=value_mapper)
+    def as_dict(self, value_mapper: Optional[Dict[str, Callable]] = None) -> Dict[str, Any]:
+        return self.to_dict(view="element", value_mapper=value_mapper)
 
     @property
     def can_install(self):
         return self.status == self.installation_status.NEW
 
     @property
     def can_reset_metadata(self):
@@ -136,32 +189,34 @@
 
     @property
     def can_uninstall(self):
         return self.status != self.installation_status.UNINSTALLED
 
     @property
     def can_deactivate(self):
-        return self.status not in [self.installation_status.DEACTIVATED,
-                                   self.installation_status.ERROR,
-                                   self.installation_status.UNINSTALLED]
+        return self.status not in [
+            self.installation_status.DEACTIVATED,
+            self.installation_status.ERROR,
+            self.installation_status.UNINSTALLED,
+        ]
 
     @property
     def can_reinstall_or_activate(self):
         return self.deleted
 
     def get_sharable_url(self, app):
         return common_util.get_tool_shed_repository_url(app, self.tool_shed, self.owner, self.name)
 
     @property
     def shed_config_filename(self):
-        return self.metadata_.get('shed_config_filename')
+        return self.metadata_.get("shed_config_filename")
 
     @shed_config_filename.setter
     def shed_config_filename(self, value):
-        self.metadata_['shed_config_filename'] = os.path.abspath(value)
+        self.metadata_["shed_config_filename"] = os.path.abspath(value)
 
     def get_shed_config_dict(self, app):
         """
         Return the in-memory version of the shed_tool_conf file, which is stored in the config_elems entry
         in the shed_tool_conf_dict.
         """
         if self.shed_config_filename:
@@ -172,107 +227,117 @@
 
     def get_tool_relative_path(self, app):
         # This is a somewhat public function, used by data_manager_manual for instance
         shed_conf_dict = self.get_shed_config_dict(app)
         tool_path = None
         relative_path = None
         if shed_conf_dict:
-            tool_path = shed_conf_dict['tool_path']
-            relative_path = os.path.join(self.tool_shed_path_name, 'repos', self.owner, self.name, self.installed_changeset_revision)
+            tool_path = shed_conf_dict["tool_path"]
+            relative_path = os.path.join(
+                self.tool_shed_path_name, "repos", self.owner, self.name, self.installed_changeset_revision
+            )
         return tool_path, relative_path
 
     def guess_shed_config(self, app):
         tool_ids = []
-        for tool in self.metadata_.get('tools', []):
-            tool_ids.append(tool.get('guid'))
+        for tool in self.metadata_.get("tools", []):
+            tool_ids.append(tool.get("guid"))
         for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
-            name = shed_tool_conf_dict['config_filename']
-            for elem in shed_tool_conf_dict['config_elems']:
-                if elem.tag == 'tool':
-                    for sub_elem in elem.findall('id'):
+            name = shed_tool_conf_dict["config_filename"]
+            for elem in shed_tool_conf_dict["config_elems"]:
+                if elem.tag == "tool":
+                    for sub_elem in elem.findall("id"):
                         tool_id = sub_elem.text.strip()
                         if tool_id in tool_ids:
                             self.shed_config_filename = name
                             return shed_tool_conf_dict
                 elif elem.tag == "section":
-                    for tool_elem in elem.findall('tool'):
-                        for sub_elem in tool_elem.findall('id'):
+                    for tool_elem in elem.findall("tool"):
+                        for sub_elem in tool_elem.findall("id"):
                             tool_id = sub_elem.text.strip()
                             if tool_id in tool_ids:
                                 self.shed_config_filename = name
                                 return shed_tool_conf_dict
         # We need to search by file paths here, which is less desirable.
         tool_shed = common_util.remove_protocol_and_port_from_tool_shed_url(self.tool_shed)
         for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
-            tool_path = shed_tool_conf_dict['tool_path']
-            relative_path = os.path.join(tool_path, tool_shed, 'repos', self.owner, self.name)
+            tool_path = shed_tool_conf_dict["tool_path"]
+            relative_path = os.path.join(tool_path, tool_shed, "repos", self.owner, self.name)
             if os.path.exists(relative_path):
-                self.shed_config_filename = shed_tool_conf_dict['config_filename']
+                self.shed_config_filename = shed_tool_conf_dict["config_filename"]
                 return shed_tool_conf_dict
         # Very last resort, get default shed_tool_config file for this instance
         shed_tool_conf_dict = app.toolbox.default_shed_tool_conf_dict()
-        self.shed_config_filename = shed_tool_conf_dict['config_filename']
+        self.shed_config_filename = shed_tool_conf_dict["config_filename"]
         return shed_tool_conf_dict
 
     @property
     def has_readme_files(self):
-        return 'readme_files' in self.metadata_
+        return "readme_files" in self.metadata_
 
     @property
     def has_repository_dependencies(self):
-        repository_dependencies_dict = self.metadata_.get('repository_dependencies', {})
-        repository_dependencies = repository_dependencies_dict.get('repository_dependencies', [])
+        repository_dependencies_dict = self.metadata_.get("repository_dependencies", {})
+        repository_dependencies = repository_dependencies_dict.get("repository_dependencies", [])
         # [["http://localhost:9009", "package_libgtextutils_0_6", "test", "e2003cbf18cd", "True", "True"]]
         for rd_tup in repository_dependencies:
-            tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                common_util.parse_repository_dependency_tuple(rd_tup)
+            (
+                tool_shed,
+                name,
+                owner,
+                changeset_revision,
+                prior_installation_required,
+                only_if_compiling_contained_td,
+            ) = common_util.parse_repository_dependency_tuple(rd_tup)
             if not asbool(only_if_compiling_contained_td):
                 return True
         return False
 
     @property
     def has_repository_dependencies_only_if_compiling_contained_td(self):
-        repository_dependencies_dict = self.metadata_.get('repository_dependencies', {})
-        repository_dependencies = repository_dependencies_dict.get('repository_dependencies', [])
+        repository_dependencies_dict = self.metadata_.get("repository_dependencies", {})
+        repository_dependencies = repository_dependencies_dict.get("repository_dependencies", [])
         # [["http://localhost:9009", "package_libgtextutils_0_6", "test", "e2003cbf18cd", "True", "True"]]
         for rd_tup in repository_dependencies:
-            tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                common_util.parse_repository_dependency_tuple(rd_tup)
+            (
+                tool_shed,
+                name,
+                owner,
+                changeset_revision,
+                prior_installation_required,
+                only_if_compiling_contained_td,
+            ) = common_util.parse_repository_dependency_tuple(rd_tup)
             if not asbool(only_if_compiling_contained_td):
                 return False
         return True
 
     @property
     def in_error_state(self):
         return self.status == self.installation_status.ERROR
 
     @property
     def includes_data_managers(self):
-        return bool(len(self.metadata_.get('data_manager', {}).get('data_managers', {})))
+        return bool(len(self.metadata_.get("data_manager", {}).get("data_managers", {})))
 
     @property
     def includes_tools(self):
-        return 'tools' in self.metadata_
+        return "tools" in self.metadata_
 
     @property
     def includes_tools_for_display_in_tool_panel(self):
         if self.includes_tools:
-            tool_dicts = self.metadata_['tools']
+            tool_dicts = self.metadata_["tools"]
             for tool_dict in tool_dicts:
-                if tool_dict.get('add_to_tool_panel', True):
+                if tool_dict.get("add_to_tool_panel", True):
                     return True
         return False
 
     @property
     def includes_tool_dependencies(self):
-        return 'tool_dependencies' in self.metadata_
-
-    @property
-    def includes_workflows(self):
-        return 'workflows' in self.metadata_
+        return "tool_dependencies" in self.metadata_
 
     @property
     def installed_repository_dependencies(self):
         """Return the repository's repository dependencies that are currently installed."""
         installed_required_repositories = []
         for required_repository in self.repository_dependencies:
             if required_repository.status == self.installation_status.INSTALLED:
@@ -287,30 +352,29 @@
             if tool_dependency.status in [ToolDependency.installation_status.INSTALLED]:
                 installed_dependencies.append(tool_dependency)
         return installed_dependencies
 
     @property
     def is_deprecated_in_tool_shed(self):
         if self.tool_shed_status:
-            return asbool(self.tool_shed_status.get('repository_deprecated', False))
+            return asbool(self.tool_shed_status.get("repository_deprecated", False))
         return False
 
     @property
     def is_deactivated_or_installed(self):
-        return self.status in [self.installation_status.DEACTIVATED,
-                               self.installation_status.INSTALLED]
+        return self.status in [self.installation_status.DEACTIVATED, self.installation_status.INSTALLED]
 
     @property
     def is_installed(self):
         return self.status == self.installation_status.INSTALLED
 
     @property
     def is_latest_installable_revision(self):
         if self.tool_shed_status:
-            return asbool(self.tool_shed_status.get('latest_installable_revision', False))
+            return asbool(self.tool_shed_status.get("latest_installable_revision", False))
         return False
 
     @property
     def is_new(self):
         return self.status == self.installation_status.NEW
 
     @property
@@ -336,16 +400,18 @@
         if repo_path:
             return os.path.join(repo_path, self.name)
         return None
 
     def repo_path(self, app):
         tool_shed = common_util.remove_protocol_and_port_from_tool_shed_url(self.tool_shed)
         for shed_tool_conf_dict in app.toolbox.dynamic_confs(include_migrated_tool_conf=True):
-            tool_path = shed_tool_conf_dict['tool_path']
-            relative_path = os.path.join(tool_path, tool_shed, 'repos', self.owner, self.name, self.installed_changeset_revision)
+            tool_path = shed_tool_conf_dict["tool_path"]
+            relative_path = os.path.join(
+                tool_path, tool_shed, "repos", self.owner, self.name, self.installed_changeset_revision
+            )
             if os.path.exists(relative_path):
                 return relative_path
         return None
 
     @property
     def repository_dependencies(self):
         """
@@ -361,31 +427,35 @@
         return required_repositories
 
     @property
     def repository_dependencies_being_installed(self):
         """Return the repository's repository dependencies that are currently being installed."""
         required_repositories_being_installed = []
         for required_repository in self.repository_dependencies:
-            if required_repository.status in [self.installation_status.CLONING,
-                                              self.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES,
-                                              self.installation_status.INSTALLING_TOOL_DEPENDENCIES,
-                                              self.installation_status.LOADING_PROPRIETARY_DATATYPES,
-                                              self.installation_status.SETTING_TOOL_VERSIONS]:
+            if required_repository.status in [
+                self.installation_status.CLONING,
+                self.installation_status.INSTALLING_REPOSITORY_DEPENDENCIES,
+                self.installation_status.INSTALLING_TOOL_DEPENDENCIES,
+                self.installation_status.LOADING_PROPRIETARY_DATATYPES,
+                self.installation_status.SETTING_TOOL_VERSIONS,
+            ]:
                 required_repositories_being_installed.append(required_repository)
         return required_repositories_being_installed
 
     @property
     def repository_dependencies_missing_or_being_installed(self):
         """Return the repository's repository dependencies that are either missing or currently being installed."""
         required_repositories_missing_or_being_installed = []
         for required_repository in self.repository_dependencies:
-            if required_repository.status in [self.installation_status.ERROR,
-                                              self.installation_status.INSTALLING,
-                                              self.installation_status.NEVER_INSTALLED,
-                                              self.installation_status.UNINSTALLED]:
+            if required_repository.status in [
+                self.installation_status.ERROR,
+                self.installation_status.INSTALLING,
+                self.installation_status.NEVER_INSTALLED,
+                self.installation_status.UNINSTALLED,
+            ]:
                 required_repositories_missing_or_being_installed.append(required_repository)
         return required_repositories_missing_or_being_installed
 
     @property
     def repository_dependencies_with_installation_errors(self):
         """Return the repository's repository dependencies that have installation errors."""
         required_repositories_with_installation_errors = []
@@ -403,55 +473,71 @@
         repository with prior_installation_required set to True place them in a special category in that the required repositories must be
         installed before this repository is installed.  Among other things, this enables these "special" repository dependencies to include
         information that enables the successful installation of this repository.  This method is not used during the initial installation of
         this repository, but only after it has been installed (metadata must be set for this repository in order for this method to be useful).
         """
         required_rd_tups_that_must_be_installed = []
         if self.has_repository_dependencies:
-            rd_tups = self.metadata_['repository_dependencies']['repository_dependencies']
+            rd_tups = self.metadata_["repository_dependencies"]["repository_dependencies"]
             for rd_tup in rd_tups:
                 if len(rd_tup) == 5:
-                    tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                        common_util.parse_repository_dependency_tuple(rd_tup, contains_error=False)
+                    (
+                        tool_shed,
+                        name,
+                        owner,
+                        changeset_revision,
+                        prior_installation_required,
+                        only_if_compiling_contained_td,
+                    ) = common_util.parse_repository_dependency_tuple(rd_tup, contains_error=False)
                     if asbool(prior_installation_required):
-                        required_rd_tups_that_must_be_installed.append((tool_shed, name, owner, changeset_revision, 'True', 'False'))
+                        required_rd_tups_that_must_be_installed.append(
+                            (tool_shed, name, owner, changeset_revision, "True", "False")
+                        )
                 elif len(rd_tup) == 6:
-                    tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = \
-                        common_util.parse_repository_dependency_tuple(rd_tup, contains_error=False)
+                    (
+                        tool_shed,
+                        name,
+                        owner,
+                        changeset_revision,
+                        prior_installation_required,
+                        only_if_compiling_contained_td,
+                    ) = common_util.parse_repository_dependency_tuple(rd_tup, contains_error=False)
                     # The repository dependency will only be required to be previously installed if it does not fall into the category of
                     # a repository that must be installed only so that its contained tool dependency can be used for compiling the tool
                     # dependency of the dependent repository.
                     if not asbool(only_if_compiling_contained_td):
                         if asbool(prior_installation_required):
-                            required_rd_tups_that_must_be_installed.append((tool_shed, name, owner, changeset_revision, 'True', 'False'))
+                            required_rd_tups_that_must_be_installed.append(
+                                (tool_shed, name, owner, changeset_revision, "True", "False")
+                            )
         return required_rd_tups_that_must_be_installed
 
     @property
     def revision_update_available(self):
         # This method should be named update_available, but since it is no longer possible to drop a table column using migration scripts
         # with the sqlite database (see ~/galaxy/model/migrate/versions/0016_drop_update_available_col_add_tool_shed_status_col.py), we
         # have to name it in such a way that it will not conflict with the eliminated tool_shed_repository.update_available column (which
         # cannot be eliminated if using the sqlite database).
         if self.tool_shed_status:
-            return asbool(self.tool_shed_status.get('revision_update', False))
+            return asbool(self.tool_shed_status.get("revision_update", False))
         return False
 
-    def to_dict(self, view='collection', value_mapper=None):
+    def to_dict(self, view="collection", value_mapper: Optional[Dict[str, Callable]] = None) -> Dict[str, Any]:
         if value_mapper is None:
             value_mapper = {}
         rval = {}
         try:
             visible_keys = self.__getattribute__(f"dict_{view}_visible_keys")
         except AttributeError:
-            raise Exception(f'Unknown API view: {view}')
+            raise Exception(f"Unknown API view: {view}")
         for key in visible_keys:
             try:
                 rval[key] = self.__getattribute__(key)
                 if key in value_mapper:
-                    rval[key] = value_mapper.get(key, rval[key])
+                    rval[key] = value_mapper[key](rval[key])
             except AttributeError:
                 rval[key] = None
         return rval
 
     @property
     def tool_dependencies_being_installed(self):
         dependencies_being_installed = []
@@ -461,60 +547,73 @@
         return dependencies_being_installed
 
     @property
     def tool_dependencies_installed_or_in_error(self):
         """Return the repository's tool dependencies that are currently installed, but possibly in an error state."""
         installed_dependencies = []
         for tool_dependency in self.tool_dependencies:
-            if tool_dependency.status in [ToolDependency.installation_status.INSTALLED,
-                                          ToolDependency.installation_status.ERROR]:
+            if tool_dependency.status in [
+                ToolDependency.installation_status.INSTALLED,
+                ToolDependency.installation_status.ERROR,
+            ]:
                 installed_dependencies.append(tool_dependency)
         return installed_dependencies
 
     @property
     def tool_dependencies_missing_or_being_installed(self):
         dependencies_missing_or_being_installed = []
         for tool_dependency in self.tool_dependencies:
-            if tool_dependency.status in [ToolDependency.installation_status.ERROR,
-                                          ToolDependency.installation_status.INSTALLING,
-                                          ToolDependency.installation_status.NEVER_INSTALLED,
-                                          ToolDependency.installation_status.UNINSTALLED]:
+            if tool_dependency.status in [
+                ToolDependency.installation_status.ERROR,
+                ToolDependency.installation_status.INSTALLING,
+                ToolDependency.installation_status.NEVER_INSTALLED,
+                ToolDependency.installation_status.UNINSTALLED,
+            ]:
                 dependencies_missing_or_being_installed.append(tool_dependency)
         return dependencies_missing_or_being_installed
 
     @property
     def tool_dependencies_with_installation_errors(self):
         dependencies_with_installation_errors = []
         for tool_dependency in self.tool_dependencies:
             if tool_dependency.status == ToolDependency.installation_status.ERROR:
                 dependencies_with_installation_errors.append(tool_dependency)
         return dependencies_with_installation_errors
 
     @property
     def tool_shed_path_name(self):
         tool_shed_url = self.tool_shed
-        if tool_shed_url.find(':') > 0:
+        if tool_shed_url.find(":") > 0:
             # Eliminate the port, if any, since it will result in an invalid directory name.
-            tool_shed_url = tool_shed_url.split(':')[0]
-        return tool_shed_url.rstrip('/')
+            tool_shed_url = tool_shed_url.split(":")[0]
+        return tool_shed_url.rstrip("/")
 
     @property
     def tuples_of_repository_dependencies_needed_for_compiling_td(self):
         """
         Return tuples defining this repository's repository dependencies that are necessary only for compiling this repository's tool
         dependencies.
         """
         rd_tups_of_repositories_needed_for_compiling_td = []
-        repository_dependencies = self.metadata_.get('repository_dependencies', {})
-        rd_tups = repository_dependencies.get('repository_dependencies', [])
+        repository_dependencies = self.metadata_.get("repository_dependencies", {})
+        rd_tups = repository_dependencies.get("repository_dependencies", [])
         for rd_tup in rd_tups:
             if len(rd_tup) == 6:
-                tool_shed, name, owner, changeset_revision, prior_installation_required, only_if_compiling_contained_td = rd_tup
+                (
+                    tool_shed,
+                    name,
+                    owner,
+                    changeset_revision,
+                    prior_installation_required,
+                    only_if_compiling_contained_td,
+                ) = rd_tup
                 if asbool(only_if_compiling_contained_td):
-                    rd_tups_of_repositories_needed_for_compiling_td.append((tool_shed, name, owner, changeset_revision, 'False', 'True'))
+                    rd_tups_of_repositories_needed_for_compiling_td.append(
+                        (tool_shed, name, owner, changeset_revision, "False", "True")
+                    )
         return rd_tups_of_repositories_needed_for_compiling_td
 
     @property
     def uninstalled_repository_dependencies(self):
         """Return the repository's repository dependencies that have been uninstalled."""
         uninstalled_required_repositories = []
         for required_repository in self.repository_dependencies:
@@ -533,77 +632,81 @@
 
     @property
     def upgrade_available(self):
         if self.tool_shed_status:
             if self.is_deprecated_in_tool_shed:
                 # Only allow revision upgrades if the repository is not deprecated in the tool shed.
                 return False
-            return asbool(self.tool_shed_status.get('revision_upgrade', False))
+            return asbool(self.tool_shed_status.get("revision_upgrade", False))
         return False
 
 
-class RepositoryRepositoryDependencyAssociation(Base, _HasTable):
-    __tablename__ = 'repository_repository_dependency_association'
+class RepositoryRepositoryDependencyAssociation(Base):
+    __tablename__ = "repository_repository_dependency_association"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    tool_shed_repository_id = Column(ForeignKey('tool_shed_repository.id'), index=True)
-    repository_dependency_id = Column(ForeignKey('repository_dependency.id'), index=True)
-    repository = relationship('ToolShedRepository', back_populates='required_repositories')
-    repository_dependency = relationship('RepositoryDependency')
+    tool_shed_repository_id = Column(ForeignKey("tool_shed_repository.id"), index=True)
+    repository_dependency_id = Column(ForeignKey("repository_dependency.id"), index=True)
+    repository = relationship("ToolShedRepository", back_populates="required_repositories")
+    repository_dependency = relationship("RepositoryDependency")
 
     def __init__(self, tool_shed_repository_id=None, repository_dependency_id=None):
         self.tool_shed_repository_id = tool_shed_repository_id
         self.repository_dependency_id = repository_dependency_id
 
 
-class RepositoryDependency(Base, _HasTable):
-    __tablename__ = 'repository_dependency'
+class RepositoryDependency(Base):
+    __tablename__ = "repository_dependency"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    tool_shed_repository_id = Column(ForeignKey('tool_shed_repository.id'), index=True, nullable=False)
-    repository = relationship('ToolShedRepository')
+    tool_shed_repository_id = Column(ForeignKey("tool_shed_repository.id"), index=True, nullable=False)
+    repository = relationship("ToolShedRepository")
 
     def __init__(self, tool_shed_repository_id=None):
         self.tool_shed_repository_id = tool_shed_repository_id
 
 
-class ToolDependency(Base, _HasTable):
-    __tablename__ = 'tool_dependency'
+class ToolDependency(Base):
+    __tablename__ = "tool_dependency"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
-    tool_shed_repository_id = Column(ForeignKey('tool_shed_repository.id'), index=True, nullable=False)
+    tool_shed_repository_id = Column(ForeignKey("tool_shed_repository.id"), index=True, nullable=False)
     name = Column(TrimmedString(255))
     version = Column(TEXT)
     type = Column(TrimmedString(40))
     status = Column(TrimmedString(255), nullable=False)
     error_message = Column(TEXT)
-    tool_shed_repository = relationship('ToolShedRepository', back_populates='tool_dependencies')
+    tool_shed_repository = relationship("ToolShedRepository", back_populates="tool_dependencies")
 
     # converting this one to Enum breaks the tool shed tests,
     # don't know why though -John
-    installation_status = Bunch(NEVER_INSTALLED='Never installed',
-                                INSTALLING='Installing',
-                                INSTALLED='Installed',
-                                ERROR='Error',
-                                UNINSTALLED='Uninstalled')
+    installation_status = Bunch(
+        NEVER_INSTALLED="Never installed",
+        INSTALLING="Installing",
+        INSTALLED="Installed",
+        ERROR="Error",
+        UNINSTALLED="Uninstalled",
+    )
 
     class states(str, Enum):
-        INSTALLING = 'running'
-        OK = 'ok'
-        WARNING = 'queued'
-        ERROR = 'error'
-        UNINSTALLED = 'deleted_new'
-
-    def __init__(self, tool_shed_repository_id=None, name=None, version=None, type=None, status=None, error_message=None):
+        INSTALLING = "running"
+        OK = "ok"
+        WARNING = "queued"
+        ERROR = "error"
+        UNINSTALLED = "deleted_new"
+
+    def __init__(
+        self, tool_shed_repository_id=None, name=None, version=None, type=None, status=None, error_message=None
+    ):
         self.tool_shed_repository_id = tool_shed_repository_id
         self.name = name
         self.version = version
         self.type = type
         self.status = status
         self.error_message = error_message
 
@@ -613,76 +716,78 @@
 
     @property
     def can_uninstall(self):
         return self.status in [self.installation_status.ERROR, self.installation_status.INSTALLED]
 
     @property
     def can_update(self):
-        return self.status in [self.installation_status.NEVER_INSTALLED,
-                               self.installation_status.INSTALLED,
-                               self.installation_status.ERROR,
-                               self.installation_status.UNINSTALLED]
-
-    def get_env_shell_file_path(self, app):
-        installation_directory = self.installation_directory(app)
-        file_path = os.path.join(installation_directory, 'env.sh')
-        if os.path.exists(file_path):
-            return file_path
-        return None
+        return self.status in [
+            self.installation_status.NEVER_INSTALLED,
+            self.installation_status.INSTALLED,
+            self.installation_status.ERROR,
+            self.installation_status.UNINSTALLED,
+        ]
 
     @property
     def in_error_state(self):
         return self.status == self.installation_status.ERROR
 
     def installation_directory(self, app):
-        if self.type == 'package':
-            return os.path.join(app.tool_dependency_dir,
-                                self.name,
-                                self.version,
-                                self.tool_shed_repository.owner,
-                                self.tool_shed_repository.name,
-                                self.tool_shed_repository.installed_changeset_revision)
-        if self.type == 'set_environment':
-            return os.path.join(app.tool_dependency_dir,
-                                'environment_settings',
-                                self.name,
-                                self.tool_shed_repository.owner,
-                                self.tool_shed_repository.name,
-                                self.tool_shed_repository.installed_changeset_revision)
+        if self.type == "package":
+            return os.path.join(
+                app.tool_dependency_dir,
+                self.name,
+                self.version,
+                self.tool_shed_repository.owner,
+                self.tool_shed_repository.name,
+                self.tool_shed_repository.installed_changeset_revision,
+            )
+        if self.type == "set_environment":
+            return os.path.join(
+                app.tool_dependency_dir,
+                "environment_settings",
+                self.name,
+                self.tool_shed_repository.owner,
+                self.tool_shed_repository.name,
+                self.tool_shed_repository.installed_changeset_revision,
+            )
+        return None
 
     @property
     def is_installed(self):
         return self.status == self.installation_status.INSTALLED
 
 
-class ToolVersion(Base, Dictifiable, _HasTable):
-    __tablename__ = 'tool_version'
+class ToolVersion(Base, Dictifiable):
+    __tablename__ = "tool_version"
 
     id = Column(Integer, primary_key=True)
     create_time = Column(DateTime, default=now)
     update_time = Column(DateTime, default=now, onupdate=now)
     tool_id = Column(String(255))
-    tool_shed_repository_id = Column(ForeignKey('tool_shed_repository.id'), index=True, nullable=True)
-    parent_tool_association = relationship('ToolVersionAssociation',
-        primaryjoin=(lambda: ToolVersion.id == ToolVersionAssociation.tool_id))
-    child_tool_association = relationship('ToolVersionAssociation',
-        primaryjoin=(lambda: ToolVersion.id == ToolVersionAssociation.parent_id))
-    tool_shed_repository = relationship('ToolShedRepository', back_populates='tool_versions')
+    tool_shed_repository_id = Column(ForeignKey("tool_shed_repository.id"), index=True, nullable=True)
+    parent_tool_association = relationship(
+        "ToolVersionAssociation", primaryjoin=(lambda: ToolVersion.id == ToolVersionAssociation.tool_id)
+    )
+    child_tool_association = relationship(
+        "ToolVersionAssociation", primaryjoin=(lambda: ToolVersion.id == ToolVersionAssociation.parent_id)
+    )
+    tool_shed_repository = relationship("ToolShedRepository", back_populates="tool_versions")
 
-    dict_element_visible_keys = ['id', 'tool_shed_repository']
+    dict_element_visible_keys = ["id", "tool_shed_repository"]
 
-    def to_dict(self, view='element'):
+    def to_dict(self, view="element"):
         rval = super().to_dict(view=view)
-        rval['tool_name'] = self.tool_id
+        rval["tool_name"] = self.tool_id
         for a in self.parent_tool_association:
-            rval['parent_tool_id'] = a.parent_id
+            rval["parent_tool_id"] = a.parent_id
         for a in self.child_tool_association:
-            rval['child_tool_id'] = a.tool_id
+            rval["child_tool_id"] = a.tool_id
         return rval
 
 
-class ToolVersionAssociation(Base, _HasTable):
-    __tablename__ = 'tool_version_association'
+class ToolVersionAssociation(Base):
+    __tablename__ = "tool_version_association"
 
     id = Column(Integer, primary_key=True)
-    tool_id = Column(ForeignKey('tool_version.id'), index=True, nullable=False)
-    parent_id = Column(ForeignKey('tool_version.id'), index=True, nullable=False)
+    tool_id = Column(ForeignKey("tool_version.id"), index=True, nullable=False)
+    parent_id = Column(ForeignKey("tool_version.id"), index=True, nullable=False)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/unittest_utils/data_app.py` & `galaxy-data-23.0.1/galaxy/model/unittest_utils/data_app.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,76 +4,89 @@
 and it has dependencies from across the app. This mock application and config is
 more appropriate for testing galaxy-data functionality and will be included with
 galaxy-data.
 """
 import os
 import shutil
 import tempfile
+from typing import Optional
 
-from galaxy import model, objectstore
+from galaxy import (
+    model,
+    objectstore,
+)
 from galaxy.datatypes import registry
-from galaxy.model.mapping import GalaxyModelMapping, init
+from galaxy.files import (
+    ConfiguredFileSources,
+    NullConfiguredFileSources,
+)
+from galaxy.model.mapping import (
+    GalaxyModelMapping,
+    init,
+)
 from galaxy.model.security import GalaxyRBACAgent
 from galaxy.model.tags import GalaxyTagHandler
 from galaxy.security.idencoding import IdEncodingHelper
 from galaxy.util.bunch import Bunch
 
-
-GALAXY_TEST_UNITTEST_SECRET = '6e46ed6483a833c100e68cc3f1d0dd76'
+GALAXY_TEST_UNITTEST_SECRET = "6e46ed6483a833c100e68cc3f1d0dd76"
 GALAXY_TEST_IN_MEMORY_DB_CONNECTION = "sqlite:///:memory:"
 
 
 class GalaxyDataTestConfig(Bunch):
     """Minimal Galaxy mock config object that exposes and uses only what is needed for the galaxy-data package."""
+
     security: IdEncodingHelper
     database_connection: str
     root: str
     data_dir: str
     _remove_root: bool
 
     def __init__(self, root=None, **kwd):
         Bunch.__init__(self, **kwd)
         if not root:
             root = tempfile.mkdtemp()
             self._remove_root = True
         else:
             self._remove_root = False
         self.root = root
-        self.data_dir = os.path.join(root, 'database')
+        self.data_dir = os.path.join(root, "database")
 
         self.security = IdEncodingHelper(id_secret=GALAXY_TEST_UNITTEST_SECRET)
-        self.database_connection = kwd.get('database_connection', GALAXY_TEST_IN_MEMORY_DB_CONNECTION)
+        self.database_connection = kwd.get("database_connection", GALAXY_TEST_IN_MEMORY_DB_CONNECTION)
 
         # objectstore config values...
-        self.object_store_config_file = ''
-        self.object_store = 'disk'
+        self.object_store_config_file = ""
+        self.object_store = "disk"
         self.object_store_check_old_style = False
-        self.object_store_cache_path = '/tmp/cache'
+        self.object_store_cache_path = "/tmp/cache"
         self.object_store_store_by = "uuid"
 
         self.umask = os.umask(0o77)
         self.gid = os.getgid()
         # objectstore config directories...
-        self.jobs_directory = os.path.join(self.data_dir, 'jobs_directory')
-        self.new_file_path = os.path.join(self.data_dir, 'tmp')
-        self.file_path = os.path.join(self.data_dir, 'files')
+        self.jobs_directory = os.path.join(self.data_dir, "jobs_directory")
+        self.new_file_path = os.path.join(self.data_dir, "tmp")
+        self.file_path = os.path.join(self.data_dir, "files")
         self.server_name = "main"
 
     def __del__(self):
         if self._remove_root:
             shutil.rmtree(self.root)
 
 
-class GalaxyDataTestApp():
+class GalaxyDataTestApp:
     """Minimal Galaxy mock app object that exposes and uses only what is needed for the galaxy-data package."""
+
     security: IdEncodingHelper
     model: GalaxyModelMapping
     security_agent: GalaxyRBACAgent
+    file_sources: ConfiguredFileSources = NullConfiguredFileSources()
 
-    def __init__(self, config: GalaxyDataTestConfig = None, **kwd):
+    def __init__(self, config: Optional[GalaxyDataTestConfig] = None, **kwd):
         config = config or GalaxyDataTestConfig(**kwd)
         self.config = config
         self.security = config.security
         self.object_store = objectstore.build_object_store_from_config(self.config)
         self.model = init("/tmp", self.config.database_connection, create_tables=True, object_store=self.object_store)
         self.security_agent = self.model.security_agent
         self.tag_handler = GalaxyTagHandler(self.model.context)
@@ -84,10 +97,9 @@
         datatypes_registry.load_datatypes()
         model.set_datatypes_registry(datatypes_registry)
         datatypes_registry.set_external_metadata_tool = MockSetExternalTool()
         self.datatypes_registry = datatypes_registry
 
 
 class MockSetExternalTool:
-
     def regenerate_imported_metadata_if_needed(self, *args, **kwds):
         pass
```

### Comparing `galaxy-data-22.1.1/galaxy/model/view/__init__.py` & `galaxy-data-23.0.1/galaxy/model/view/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 """
 Galaxy sql view models
 """
 from sqlalchemy import Integer
 from sqlalchemy.orm import registry
-from sqlalchemy.sql import column, text
+from sqlalchemy.sql import (
+    column,
+    text,
+)
 
 from galaxy.model.view.utils import View
 
 
 class HistoryDatasetCollectionJobStateSummary(View):
-    name = 'collection_job_state_summary_view'
+    name = "collection_job_state_summary_view"
 
     aggregate_state_query = """
 SELECT
     hdca_id,
     SUM(CASE WHEN state = 'new' THEN 1 ELSE 0 END) AS new,
     SUM(CASE WHEN state = 'resubmitted' THEN 1 ELSE 0 END) AS resubmitted,
     SUM(CASE WHEN state = 'waiting' THEN 1 ELSE 0 END) AS waiting,
     SUM(CASE WHEN state = 'queued' THEN 1 ELSE 0 END) AS queued,
     SUM(CASE WHEN state = 'running' THEN 1 ELSE 0 END) AS running,
     SUM(CASE WHEN state = 'ok' THEN 1 ELSE 0 END) AS ok,
     SUM(CASE WHEN state = 'error' THEN 1 ELSE 0 END) AS error,
     SUM(CASE WHEN state = 'failed' THEN 1 ELSE 0 END) AS failed,
     SUM(CASE WHEN state = 'paused' THEN 1 ELSE 0 END) AS paused,
+    SUM(CASE WHEN state = 'skipped' THEN 1 ELSE 0 END) AS skipped,
     SUM(CASE WHEN state = 'deleted' THEN 1 ELSE 0 END) AS deleted,
     SUM(CASE WHEN state = 'deleted_new' THEN 1 ELSE 0 END) AS deleted_new,
     SUM(CASE WHEN state = 'upload' THEN 1 ELSE 0 END) AS upload,
     SUM(CASE WHEN job_id IS NOT NULL THEN 1 ELSE 0 END) AS all_jobs
 FROM (
     SELECT hdca.id AS hdca_id, job.id AS job_id, job.state as state
     FROM history_dataset_collection_association hdca
@@ -44,29 +48,31 @@
     LEFT JOIN job
         ON hdca.job_id = job.id
 ) jobstates
 GROUP BY jobstates.hdca_id
 """
 
     __view__ = text(aggregate_state_query).columns(
-        column('hdca_id', Integer),
-        column('new', Integer),
-        column('resubmitted', Integer),
-        column('waiting', Integer),
-        column('queued', Integer),
-        column('running', Integer),
-        column('ok', Integer),
-        column('error', Integer),
-        column('failed', Integer),
-        column('paused', Integer),
-        column('deleted', Integer),
-        column('deleted_new', Integer),
-        column('upload', Integer),
-        column('all_jobs', Integer)
+        column("hdca_id", Integer),
+        column("new", Integer),
+        column("resubmitted", Integer),
+        column("waiting", Integer),
+        column("queued", Integer),
+        column("running", Integer),
+        column("ok", Integer),
+        column("error", Integer),
+        column("failed", Integer),
+        column("paused", Integer),
+        column("skipped", Integer),
+        column("deleted", Integer),
+        column("deleted_new", Integer),
+        column("upload", Integer),
+        column("all_jobs", Integer),
     )
-    pkeys = {'hdca_id'}
+    pkeys = {"hdca_id"}
     __table__ = View._make_table(name, __view__, pkeys)
 
 
 mapper_registry = registry()
 mapper_registry.map_imperatively(
-    HistoryDatasetCollectionJobStateSummary, HistoryDatasetCollectionJobStateSummary.__table__)
+    HistoryDatasetCollectionJobStateSummary, HistoryDatasetCollectionJobStateSummary.__table__
+)
```

### Comparing `galaxy-data-22.1.1/galaxy/model/view/utils.py` & `galaxy-data-23.0.1/galaxy/model/view/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,28 +13,21 @@
 
 
 class View:
     """Base class for Views."""
 
     @staticmethod
     def _make_table(name, selectable, pkeys):
-        """ Create a view.
+        """Create a view.
 
         :param name: The name of the view.
         :param selectable: SQLAlchemy selectable.
         :param pkeys: set of primary keys for the selectable.
         """
-        columns = [
-            Column(
-                c.name,
-                c.type,
-                primary_key=(c.name in pkeys)
-            )
-            for c in selectable.subquery().columns
-        ]
+        columns = [Column(c.name, c.type, primary_key=(c.name in pkeys)) for c in selectable.subquery().columns]
         # We do not use the metadata object from model.mapping.py that contains all the Table objects
         # because that would create a circular import (create_view is called from View objects
         # in model.view; but those View objects are imported into model.mapping.py where the
         # metadata object we need is defined). Thus, we do not use the after_create/before_drop
         # hooks to automate creating/dropping views.  Instead, this is taken care of in install_views().
 
         # The metadata object passed to Table() should be empty: this table is internal to a View
@@ -52,28 +45,29 @@
     def __init__(self, name):
         self.name = name
 
 
 @compiler.compiles(CreateView)
 def compile_create_view(element, compiler, **kw):
     compiled_selectable = compiler.sql_compiler.process(element.selectable, literal_binds=True)
-    return f'CREATE VIEW {element.name} AS {compiled_selectable}'
+    return f"CREATE VIEW {element.name} AS {compiled_selectable}"
 
 
 @compiler.compiles(DropView)
 def compile_drop_view(element, compiler, **kw):
-    return f'DROP VIEW IF EXISTS {element.name}'
+    return f"DROP VIEW IF EXISTS {element.name}"
 
 
 def is_view_model(o):
-    return hasattr(o, '__view__') and issubclass(o, View)
+    return hasattr(o, "__view__") and issubclass(o, View)
 
 
 def install_views(engine):
     import galaxy.model.view
+
     views = getmembers(galaxy.model.view, is_view_model)
     for _, view in views:
         # adding DropView here because our unit-testing calls this function when
         # it mocks the app and CreateView will attempt to rebuild an existing
         # view in a database that is already made, the right answer is probably
         # to change the sql that gest emitted when CreateView is rendered.
         with engine.begin() as conn:
```

### Comparing `galaxy-data-22.1.1/galaxy/quota/__init__.py` & `galaxy-data-23.0.1/galaxy/quota/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import logging
 
 import galaxy.util
 
 log = logging.getLogger(__name__)
 
 
-class QuotaAgent():  # metaclass=abc.ABCMeta
+class QuotaAgent:  # metaclass=abc.ABCMeta
     """Abstraction around querying Galaxy for quota available and used.
 
     Certain parts of the app that deal directly with modifying the quota assume more than
     this interface - they assume the availability of the methods on DatabaseQuotaAgent that
     implements this interface. But for read-only quota operations - such as checking available
     quota or reporting it to users - methods defined on this interface should be sufficient
     and the NoQuotaAgent should be a valid choice.
@@ -26,15 +26,15 @@
 
     def get_quota_nice_size(self, user):
         """Return quota as a human-readable string or 'unlimited' if no quota is set."""
         quota_bytes = self.get_quota(user)
         if quota_bytes is not None:
             quota_str = galaxy.util.nice_size(quota_bytes)
         else:
-            quota_str = 'unlimited'
+            quota_str = "unlimited"
         return quota_str
 
     # TODO: make abstractmethod after they work better with mypy
     def get_percent(self, trans=None, user=False, history=False, usage=False, quota=False):
         """Return the percentage of any storage quota applicable to the user/transaction."""
 
     def get_usage(self, trans=None, user=False, history=False):
@@ -110,24 +110,24 @@
         use_default = True
         max = 0
         adjustment = 0
         rval = 0
         for quota in quotas:
             if quota.deleted:
                 continue
-            if quota.operation == '=' and quota.bytes == -1:
+            if quota.operation == "=" and quota.bytes == -1:
                 rval = None
                 break
-            elif quota.operation == '=':
+            elif quota.operation == "=":
                 use_default = False
                 if quota.bytes > max:
                     max = quota.bytes
-            elif quota.operation == '+':
+            elif quota.operation == "+":
                 adjustment += quota.bytes
-            elif quota.operation == '-':
+            elif quota.operation == "-":
                 adjustment -= quota.bytes
         if use_default:
             max = self.default_registered_quota
             if max is None:
                 rval = None
         if rval is not None:
             rval = max + adjustment
@@ -140,15 +140,19 @@
         return self._default_quota(self.model.DefaultQuotaAssociation.types.UNREGISTERED)
 
     @property
     def default_registered_quota(self):
         return self._default_quota(self.model.DefaultQuotaAssociation.types.REGISTERED)
 
     def _default_quota(self, default_type):
-        dqa = self.sa_session.query(self.model.DefaultQuotaAssociation).filter(self.model.DefaultQuotaAssociation.type == default_type).first()
+        dqa = (
+            self.sa_session.query(self.model.DefaultQuotaAssociation)
+            .filter(self.model.DefaultQuotaAssociation.type == default_type)
+            .first()
+        )
         if not dqa:
             return None
         if dqa.quota.bytes < 0:
             return None
         return dqa.quota.bytes
 
     def set_default_quota(self, default_type, quota):
@@ -157,15 +161,19 @@
             self.sa_session.delete(dqa)
         # Unset the current users/groups associated with this quota
         for uqa in quota.users:
             self.sa_session.delete(uqa)
         for gqa in quota.groups:
             self.sa_session.delete(gqa)
         # Find the old default, assign the new quota if it exists
-        dqa = self.sa_session.query(self.model.DefaultQuotaAssociation).filter(self.model.DefaultQuotaAssociation.type == default_type).first()
+        dqa = (
+            self.sa_session.query(self.model.DefaultQuotaAssociation)
+            .filter(self.model.DefaultQuotaAssociation.type == default_type)
+            .first()
+        )
         if dqa:
             dqa.quota = quota
         # Or create if necessary
         else:
             dqa = self.model.DefaultQuotaAssociation(default_type, quota)
         self.sa_session.add(dqa)
         self.sa_session.flush()
@@ -232,8 +240,8 @@
     if config.enable_quotas:
         quota_agent = galaxy.quota.DatabaseQuotaAgent(model)
     else:
         quota_agent = galaxy.quota.NoQuotaAgent()
     return quota_agent
 
 
-__all__ = ('get_quota_agent', 'NoQuotaAgent')
+__all__ = ("get_quota_agent", "NoQuotaAgent")
```

### Comparing `galaxy-data-22.1.1/galaxy/quota/_schema.py` & `galaxy-data-23.0.1/galaxy/quota/_schema.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,47 +1,48 @@
 from enum import Enum
 from typing import (
     List,
     Optional,
 )
 
-from pydantic import (
-    BaseModel,
-    Field,
-)
+from pydantic import Field
+from typing_extensions import Literal
 
 from galaxy.schema.fields import (
-    EncodedDatabaseIdField,
+    DecodedDatabaseIdField,
     ModelClassField,
 )
 from galaxy.schema.schema import (
     GroupModel,
+    Model,
     UserModel,
 )
 
-QUOTA_MODEL_CLASS_NAME = "Quota"
-USER_QUOTA_ASSOCIATION_MODEL_CLASS_NAME = "UserQuotaAssociation"
-GROUP_QUOTA_ASSOCIATION_MODEL_CLASS_NAME = "GroupQuotaAssociation"
-DEFAULT_QUOTA_ASSOCIATION_MODEL_CLASS_NAME = "DefaultQuotaAssociation"
+QUOTA = Literal["Quota"]
+USER_QUOTA_ASSOCIATION = Literal["UserQuotaAssociation"]
+GROUP_QUOTA_ASSOCIATION = Literal["GroupQuotaAssociation"]
+DEFAULT_QUOTA_ASSOCIATION = Literal["DefaultQuotaAssociation"]
 
 
 class QuotaOperation(str, Enum):
     EXACT = "="
     ADD = "+"
     SUBTRACT = "-"
 
 
-class DefaultQuotaTypes(str, Enum):  # TODO: should this replace lib.galaxy.model.DefaultQuotaAssociation.types at some point?
-    UNREGISTERED = 'unregistered'
-    REGISTERED = 'registered'
+class DefaultQuotaTypes(
+    str, Enum
+):  # TODO: should this replace lib.galaxy.model.DefaultQuotaAssociation.types at some point?
+    UNREGISTERED = "unregistered"
+    REGISTERED = "registered"
 
 
 class DefaultQuotaValues(str, Enum):
-    UNREGISTERED = 'unregistered'
-    REGISTERED = 'registered'
+    UNREGISTERED = "unregistered"
+    REGISTERED = "registered"
     NO = "no"
 
 
 QuotaNameField = Field(
     ...,
     title="Name",
     description="The name of the quota. This must be unique within a Galaxy instance.",
@@ -57,74 +58,76 @@
     QuotaOperation.EXACT,
     title="Operation",
     description=(
         "Quotas can have one of three `operations`:"
         "- `=` : The quota is exactly the amount specified"
         "- `+` : The amount specified will be added to the amounts of the user's other associated quota definitions"
         "- `-` : The amount specified will be subtracted from the amounts of the user's other associated quota definitions"
-    )
+    ),
 )
 
 
-class DefaultQuota(BaseModel):  # TODO: should this replace lib.galaxy.model.DefaultQuotaAssociation at some point?
-    model_class: str = ModelClassField(DEFAULT_QUOTA_ASSOCIATION_MODEL_CLASS_NAME)
+class DefaultQuota(Model):  # TODO: should this replace lib.galaxy.model.DefaultQuotaAssociation at some point?
+    model_class: DEFAULT_QUOTA_ASSOCIATION = ModelClassField(DEFAULT_QUOTA_ASSOCIATION)
     type: DefaultQuotaTypes = Field(
         ...,
         title="Type",
         description=(
             "The type of the default quota. Either one of:\n"
             " - `registered`: the associated quota will affect registered users.\n"
             " - `unregistered`: the associated quota will affect unregistered users.\n"
-        )
+        ),
     )
 
 
-class UserQuota(BaseModel):
-    model_class: str = ModelClassField(USER_QUOTA_ASSOCIATION_MODEL_CLASS_NAME)
+class UserQuota(Model):
+    model_class: USER_QUOTA_ASSOCIATION = ModelClassField(USER_QUOTA_ASSOCIATION)
     user: UserModel = Field(
         ...,
         title="User",
         description="Information about a user associated with a quota.",
     )
 
 
-class GroupQuota(BaseModel):
-    model_class: str = ModelClassField(GROUP_QUOTA_ASSOCIATION_MODEL_CLASS_NAME)
+class GroupQuota(Model):
+    model_class: GROUP_QUOTA_ASSOCIATION = ModelClassField(GROUP_QUOTA_ASSOCIATION)
     group: GroupModel = Field(
         ...,
         title="Group",
         description="Information about a user group associated with a quota.",
     )
 
 
-class QuotaBase(BaseModel):
+class QuotaBase(Model):
     """Base model containing common fields for Quotas."""
-    model_class: str = ModelClassField(QUOTA_MODEL_CLASS_NAME)
-    id: EncodedDatabaseIdField = Field(
+
+    model_class: QUOTA = ModelClassField(QUOTA)
+    id: DecodedDatabaseIdField = Field(
         ...,
         title="ID",
         description="The `encoded identifier` of the quota.",
     )
     name: str = QuotaNameField
 
 
 class QuotaSummary(QuotaBase):
     """Contains basic information about a Quota"""
+
     url: str = Field(
         ...,
         title="URL",
         description="The relative URL to get this particular Quota details from the rest API.",
         deprecated=True,
     )
 
 
-class QuotaSummaryList(BaseModel):
+class QuotaSummaryList(Model):
     __root__: List[QuotaSummary] = Field(
         default=[],
-        title='List with summary information of Quotas.',
+        title="List with summary information of Quotas.",
     )
 
 
 class QuotaDetails(QuotaBase):
     description: str = QuotaDescriptionField
     bytes: int = Field(
         ...,
@@ -158,15 +161,15 @@
     message: str = Field(
         ...,
         title="Message",
         description="Text message describing the result of the operation.",
     )
 
 
-class CreateQuotaParams(BaseModel):
+class CreateQuotaParams(Model):
     name: str = QuotaNameField
     description: str = QuotaDescriptionField
     amount: str = Field(
         ...,
         title="Amount",
         description="Quota size (E.g. ``10000MB``, ``99 gb``, ``0.2T``, ``unlimited``)",
     )
@@ -188,15 +191,15 @@
     in_groups: Optional[List[str]] = Field(
         default=[],
         title="Groups",
         description="A list of group IDs or names to associate with this quota.",
     )
 
 
-class UpdateQuotaParams(BaseModel):
+class UpdateQuotaParams(Model):
     name: Optional[str] = Field(
         default=None,
         title="Name",
         description="The new name of the quota. This must be unique within a Galaxy instance.",
     )
     description: Optional[str] = Field(
         None,
@@ -210,15 +213,15 @@
     )
     operation: QuotaOperation = Field(
         QuotaOperation.EXACT,
         title="Operation",
         description=(
             "One of (``+``, ``-``, ``=``). If you wish to change this value,"
             " you must also provide the ``amount``, otherwise it will not take effect."
-        )
+        ),
     )
     default: Optional[DefaultQuotaValues] = Field(
         default=None,
         title="Default",
         description=(
             "Whether or not this is a default quota. Valid values"
             " are ``no``, ``unregistered``, ``registered``."
@@ -235,13 +238,13 @@
     in_groups: Optional[List[str]] = Field(
         default=None,
         title="Groups",
         description="A list of group IDs or names to associate with this quota.",
     )
 
 
-class DeleteQuotaPayload(BaseModel):
+class DeleteQuotaPayload(Model):
     purge: bool = Field(
         False,
         title="Purge",
         description="Whether to also purge the Quota after deleting it.",
     )
```

### Comparing `galaxy-data-22.1.1/galaxy/schema/__init__.py` & `galaxy-data-23.0.1/galaxy/schema/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,90 +1,115 @@
+from datetime import datetime
 from enum import Enum
 from typing import (
     Dict,
     List,
     Optional,
     Union,
 )
 
 from pydantic import (
     BaseModel,
     Field,
+    Required,
 )
 
 
 class BootstrapAdminUser(BaseModel):
     id = 0
     email: Optional[str] = None
     preferences: Dict[str, str] = {}
     bootstrap_admin_user = True
 
     def all_roles(*args) -> list:
         return []
 
 
-class FilterQueryParams(BaseModel):
+class ValueFilterQueryParams(BaseModel):
+    """Allows filtering/querying elements by value like `q=<property>-<operator>&qv=<value>`
+
+    Multiple `q/qv` queries can be concatenated.
+    """
+
     q: Optional[Union[List[str], str]] = Field(
         default=None,
         title="Filter Query",
         description="Generally a property name to filter by followed by an (often optional) hyphen and operator string.",
         example="create_time-gt",
     )
     qv: Optional[Union[List[str], str]] = Field(
         default=None,
         title="Filter Value",
         description="The value to filter by.",
         example="2015-01-29",
     )
+
+
+class PaginationQueryParams(BaseModel):
+    """Used to paginate a the request results by limiting and offsetting."""
+
     offset: Optional[int] = Field(
         default=0,
         ge=0,
         title="Offset",
         description="Starts at the beginning skip the first ( offset - 1 ) items and begin returning at the Nth item",
     )
     limit: Optional[int] = Field(
         default=None,
         ge=1,
         title="Limit",
         description="The maximum number of items to return.",
     )
+
+
+class FilterQueryParams(ValueFilterQueryParams, PaginationQueryParams):
+    """Contains full filtering options to query elements, paginate and order them."""
+
     order: Optional[str] = Field(
         default=None,
         title="Order",
         description=(
             "String containing one of the valid ordering attributes followed (optionally) "
             "by '-asc' or '-dsc' for ascending and descending order respectively. "
             "Orders can be stacked as a comma-separated list of values."
         ),
         example="name-dsc,create_time",
     )
 
 
 class SerializationParams(BaseModel):
     """Contains common parameters for customizing model serialization."""
+
     view: Optional[str] = Field(
         default=None,
-        title='View',
+        title="View",
         description=(
-            'The name of the view used to serialize this item. '
-            'This will return a predefined set of attributes of the item.'
+            "The name of the view used to serialize this item. "
+            "This will return a predefined set of attributes of the item."
         ),
-        example="summary"
+        example="summary",
     )
     keys: Optional[List[str]] = Field(
         default=None,
-        title='Keys',
+        title="Keys",
         description=(
-            'List of keys (name of the attributes) that will be returned in addition '
-            'to the ones included in the `view`.'
+            "List of keys (name of the attributes) that will be returned in addition "
+            "to the ones included in the `view`."
         ),
     )
     default_view: Optional[str] = Field(
         default=None,
-        title='Default View',
-        description='The item view that will be used in case none was specified.',
+        title="Default View",
+        description="The item view that will be used in case none was specified.",
     )
 
 
 class PdfDocumentType(str, Enum):
     invocation_report = "invocation_report"
     page = "page"
+
+
+class APIKeyModel(BaseModel):
+    key: str = Field(Required, title="Key", description="API key to interact with the Galaxy API")
+    create_time: datetime = Field(
+        Required, title="Create Time", description="The time and date this API key was created."
+    )
```

### Comparing `galaxy-data-22.1.1/galaxy/schema/fields.py` & `galaxy-data-23.0.1/galaxy/schema/fields.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import re
-from typing import Optional
 
 from pydantic import Field
+from typing_extensions import get_args
 
 from galaxy.security.idencoding import IdEncodingHelper
 
-ENCODED_DATABASE_ID_PATTERN = re.compile('f?[0-9a-f]+')
+ENCODED_DATABASE_ID_PATTERN = re.compile("f?[0-9a-f]+")
 ENCODED_ID_LENGTH_MULTIPLE = 16
 
 
 class BaseDatabaseIdField:
     """
     Database ID validation.
     """
@@ -24,85 +24,96 @@
         yield cls.validate
 
     @classmethod
     def validate(cls, v):
         return v
 
     @classmethod
+    def ensure_valid(cls, v: str):
+        len_v = len(v)
+        if len_v % ENCODED_ID_LENGTH_MULTIPLE:
+            raise ValueError("Invalid id length, must be multiple of 16")
+        m = ENCODED_DATABASE_ID_PATTERN.fullmatch(v.lower())
+        if not m:
+            raise ValueError("Invalid characters in encoded ID")
+
+    @classmethod
     def __modify_schema__(cls, field_schema):
         # __modify_schema__ should mutate the dict it receives in place,
         # the returned value will be ignored
         field_schema.update(
-            min_length=16,
-            pattern='[0-9a-fA-F]+',
-            examples=['0123456789ABCDEF'],
+            minLength=16,
+            pattern="[0-9a-fA-F]+",
+            example="0123456789ABCDEF",
             type="string",
         )
 
     def __repr__(self):
-        return f'DatabaseID ({super().__repr__()})'
+        return f"DatabaseID ({super().__repr__()})"
 
 
 class DecodedDatabaseIdField(int, BaseDatabaseIdField):
+    @classmethod
+    def validate(cls, v):
+        if not isinstance(v, str):
+            raise TypeError("String required")
+        cls.ensure_valid(v)
+        return cls(cls.security.decode_id(v))
+
+    @classmethod
+    def encode(cls, v) -> str:
+        return cls.security.encode_id(v)
 
+
+class LibraryFolderDatabaseIdField(int, BaseDatabaseIdField):
     @classmethod
     def validate(cls, v):
         if not isinstance(v, str):
-            raise TypeError('String required')
-        if v.startswith("F"):
-            # Library Folder ids start with an additional "F"
-            v = v[1:]
-        len_v = len(v)
-        if len_v % ENCODED_ID_LENGTH_MULTIPLE:
-            raise ValueError('Invalid id length, must be multiple of 16')
-        m = ENCODED_DATABASE_ID_PATTERN.fullmatch(v.lower())
-        if not m:
-            raise ValueError('Invalid characters in encoded ID')
+            raise TypeError("String required")
+        if not v.startswith("F"):
+            raise TypeError("Invalid library folder ID. Folder IDs must start with an 'F'")
+        v = v[1:]
+        cls.ensure_valid(v)
         return cls(cls.security.decode_id(v))
 
+    @classmethod
+    def encode(cls, v) -> str:
+        return f"F{cls.security.encode_id(v)}"
+
 
 class EncodedDatabaseIdField(str, BaseDatabaseIdField):
-
     @classmethod
     def validate(cls, v):
         if isinstance(v, int):
             return cls(cls.security.encode_id(v))
         if not isinstance(v, str):
-            raise TypeError('String required')
-        if v.startswith("F"):
-            # Library Folder ids start with an additional "F"
-            len_v = len(v) - 1
-        else:
-            len_v = len(v)
-        if len_v % ENCODED_ID_LENGTH_MULTIPLE:
-            raise ValueError('Invalid id length, must be multiple of 16')
-        m = ENCODED_DATABASE_ID_PATTERN.fullmatch(v.lower())
-        if not m:
-            raise ValueError('Invalid characters in encoded ID')
+            raise TypeError("String required")
+        cls.ensure_valid(v)
         return cls(v)
 
+    @classmethod
+    def decode(cls, v) -> int:
+        return cls.security.decode_id(v)
 
-def ModelClassField(class_name: str) -> str:
+
+def literal_to_value(arg):
+    val = get_args(arg)
+    if not val:
+        return arg
+    if len(val) > 1:
+        raise Exception("Can't extract default argument for unions")
+    return val[0]
+
+
+def ModelClassField(default_value=...):
     """Represents a database model class name annotated as a constant
     pydantic Field.
     :param class_name: The name of the database class.
     :return: A constant pydantic Field with default annotations for model classes.
     """
     return Field(
-        class_name,
+        literal_to_value(default_value),
         title="Model class",
         description="The name of the database model class.",
-        const=True,  # Make this field constant
-    )
-
-
-def OrderParamField(default_order: str) -> Optional[str]:
-    return Field(
-        default=default_order,
-        title="Order",
-        description=(
-            "String containing one of the valid ordering attributes followed (optionally) "
-            "by '-asc' or '-dsc' for ascending and descending order respectively. "
-            "Orders can be stacked as a comma-separated list of values."
-        ),
-        example="name-dsc,create_time",
+        const=True,
+        mark_required_in_schema=True,
     )
```

### Comparing `galaxy-data-22.1.1/galaxy/schema/remote_files.py` & `galaxy-data-23.0.1/galaxy/schema/remote_files.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 from enum import Enum
 from typing import (
     List,
     Optional,
 )
 
 from pydantic import (
-    BaseModel,
     Extra,
     Field,
 )
 
+from galaxy.schema.schema import Model
+
 
 class RemoteFilesTarget(str, Enum):
     ftpdir = "ftpdir"
     userdir = "userdir"
     importdir = "importdir"
 
 
@@ -24,15 +25,15 @@
 
 
 class RemoteFilesDisableMode(str, Enum):
     folders = "folders"
     files = "files"
 
 
-class FilesSourcePlugin(BaseModel):
+class FilesSourcePlugin(Model):
     id: str = Field(
         ...,  # This field is required
         title="ID",
         description="The `FilesSource` plugin identifier",
         example="_import",
     )
     type: str = Field(
@@ -79,20 +80,22 @@
     class Config:
         # This allows additional fields (that are not validated)
         # to be serialized/deserealized. This allows to have
         # different fields depending on the plugin type
         extra = Extra.allow
 
 
-class FilesSourcePluginList(BaseModel):
+class FilesSourcePluginList(Model):
     __root__: List[FilesSourcePlugin] = Field(
         default=[],
-        title='List of files source plugins',
-        example=[{
-            "id": "_import",
-            "type": "gximport",
-            "uri_root": "gximport://",
-            "label": "Library Import Directory",
-            "doc": "Galaxy's library import directory",
-            "writable": False
-        }]
+        title="List of files source plugins",
+        example=[
+            {
+                "id": "_import",
+                "type": "gximport",
+                "uri_root": "gximport://",
+                "label": "Library Import Directory",
+                "doc": "Galaxy's library import directory",
+                "writable": False,
+            }
+        ],
     )
```

### Comparing `galaxy-data-22.1.1/galaxy/schema/schema.py` & `galaxy-data-23.0.1/galaxy/schema/schema.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,56 +1,70 @@
 """This module contains general pydantic models and common schema field annotations for them."""
 
-import json
 import re
-from datetime import datetime
+from datetime import (
+    date,
+    datetime,
+)
 from enum import Enum
 from typing import (
     Any,
     Dict,
     List,
     Optional,
     Set,
     Union,
 )
+from uuid import UUID
 
 from pydantic import (
     AnyHttpUrl,
     AnyUrl,
     BaseModel,
     ConstrainedStr,
     Extra,
     Field,
     Json,
     UUID4,
 )
+from typing_extensions import (
+    Annotated,
+    Literal,
+)
 
 from galaxy.model import (
     Dataset,
     DatasetCollection,
     DatasetInstance,
     Job,
 )
+from galaxy.schema.bco import XrefItem
 from galaxy.schema.fields import (
     DecodedDatabaseIdField,
     EncodedDatabaseIdField,
+    LibraryFolderDatabaseIdField,
     ModelClassField,
 )
-from galaxy.schema.types import RelativeUrl
-
-USER_MODEL_CLASS_NAME = "User"
-GROUP_MODEL_CLASS_NAME = "Group"
-HDA_MODEL_CLASS_NAME = "HistoryDatasetAssociation"
-DC_MODEL_CLASS_NAME = "DatasetCollection"
-DCE_MODEL_CLASS_NAME = "DatasetCollectionElement"
-HDCA_MODEL_CLASS_NAME = "HistoryDatasetCollectionAssociation"
-HISTORY_MODEL_CLASS_NAME = "History"
-JOB_MODEL_CLASS_NAME = "Job"
-STORED_WORKFLOW_MODEL_CLASS_NAME = "StoredWorkflow"
+from galaxy.schema.types import (
+    OffsetNaiveDatetime,
+    RelativeUrl,
+)
+
+USER_MODEL_CLASS = Literal["User"]
+GROUP_MODEL_CLASS = Literal["Group"]
+HDA_MODEL_CLASS = Literal["HistoryDatasetAssociation"]
+DC_MODEL_CLASS = Literal["DatasetCollection"]
+DCE_MODEL_CLASS = Literal["DatasetCollectionElement"]
+HDCA_MODEL_CLASS = Literal["HistoryDatasetCollectionAssociation"]
+HISTORY_MODEL_CLASS = Literal["History"]
+JOB_MODEL_CLASS = Literal["Job"]
+STORED_WORKFLOW_MODEL_CLASS = Literal["StoredWorkflow"]
+PAGE_MODEL_CLASS = Literal["Page"]
 
+OptionalNumberT = Optional[Union[int, float]]
 
 # Generic and common Field annotations that can be reused across models
 
 RelativeUrlField: RelativeUrl = Field(
     ...,
     title="URL",
     description="The relative URL to access this item.",
@@ -71,15 +85,15 @@
 
 AccessibleField: bool = Field(
     ...,
     title="Accessible",
     description="Whether this item is accessible to the current user due to permissions.",
 )
 
-EncodedEntityIdField: EncodedDatabaseIdField = Field(
+EntityIdField = Field(
     ...,
     title="ID",
     description="The encoded ID of this entity.",
 )
 
 DatasetStateField: Dataset.states = Field(
     ...,
@@ -141,15 +155,15 @@
 
 ElementsField = Field(
     [],
     title="Elements",
     description="The summary information of each of the elements inside the dataset collection.",
 )
 
-HistoryIdField: EncodedDatabaseIdField = Field(
+HistoryIdField: DecodedDatabaseIdField = Field(
     ...,
     title="History ID",
     description="The encoded ID of the history associated with this item.",
 )
 
 UuidField: UUID4 = Field(
     ...,
@@ -167,66 +181,103 @@
     title="Contents URL",
     description="The relative URL to access the contents of this History.",
 )
 
 
 class Model(BaseModel):
     """Base model definition with common configuration used by all derived models."""
+
     class Config:
         use_enum_values = True  # when using .dict()
         allow_population_by_field_name = True
+        json_encoders = {
+            # This will ensure all IDs are encoded when serialized to JSON
+            DecodedDatabaseIdField: lambda v: DecodedDatabaseIdField.encode(v),
+            LibraryFolderDatabaseIdField: lambda v: LibraryFolderDatabaseIdField.encode(v),
+        }
+
+        @staticmethod
+        def schema_extra(schema: Dict[str, Any], model) -> None:
+            # pydantic doesn't currently allow creating a constant that isn't optional,
+            # which makes sense for validation, but an openapi schema that describes
+            # a response should be able to declare that a field is always present,
+            # even if it is generated from a default value.
+            # Pass `mark_required_in_schema=True` when constructing a pydantic Field instance
+            # to indicate that the field is always present.
+            remove_prop_keys = set()  # hidden items shouldn't be added to schema
+            properties = schema.get("properties", {})
+            for prop_key, prop in properties.items():
+                required_in_schema = prop.pop("mark_required_in_schema", None)
+                hidden = prop.get("hidden")
+                if hidden:
+                    remove_prop_keys.add(prop_key)
+                if required_in_schema:
+                    # const is not valid in response?
+                    prop.pop("const", None)
+                    if "required" in schema:
+                        schema["required"].append(prop_key)
+                    else:
+                        schema["required"] = [prop_key]
+            for prop_key_to_remove in remove_prop_keys:
+                del properties[prop_key_to_remove]
 
 
 class UserModel(Model):
     """User in a transaction context."""
-    id: EncodedDatabaseIdField = Field(title='ID', description='User ID')
-    username: str = Field(title='Username', description='User username')
-    email: str = Field(title='Email', description='User email')
-    active: bool = Field(title='Active', description='User is active')
-    deleted: bool = Field(title='Deleted', description='User is deleted')
-    last_password_change: datetime = Field(title='Last password change', description='')
-    model_class: str = ModelClassField(USER_MODEL_CLASS_NAME)
+
+    id: DecodedDatabaseIdField = Field(title="ID", description="User ID")
+    username: str = Field(title="Username", description="User username")
+    email: str = Field(title="Email", description="User email")
+    active: bool = Field(title="Active", description="User is active")
+    deleted: bool = Field(title="Deleted", description="User is deleted")
+    last_password_change: Optional[datetime] = Field(title="Last password change", description="")
+    model_class: USER_MODEL_CLASS = ModelClassField(USER_MODEL_CLASS)
 
 
-class GroupModel(BaseModel):
+class GroupModel(Model):
     """User group model"""
-    model_class: str = ModelClassField(GROUP_MODEL_CLASS_NAME)
-    id: EncodedDatabaseIdField = Field(
+
+    model_class: GROUP_MODEL_CLASS = ModelClassField(GROUP_MODEL_CLASS)
+    id: DecodedDatabaseIdField = Field(
         ...,  # Required
-        title='ID',
-        description='Encoded group ID',
+        title="ID",
+        description="Encoded group ID",
     )
     name: str = Field(
         ...,  # Required
         title="Name",
         description="The name of the group.",
     )
 
 
 class JobSourceType(str, Enum):
     """Available types of job sources (model classes) that produce dataset collections."""
+
     Job = "Job"
     ImplicitCollectionJobs = "ImplicitCollectionJobs"
     WorkflowInvocation = "WorkflowInvocation"
 
 
 class HistoryContentType(str, Enum):
     """Available types of History contents."""
+
     dataset = "dataset"
     dataset_collection = "dataset_collection"
 
 
 class HistoryImportArchiveSourceType(str, Enum):
     """Available types of History archive sources."""
+
     url = "url"
     file = "file"
 
 
 class DCEType(str, Enum):  # TODO: suspiciously similar to HistoryContentType
     """Available types of dataset collection elements."""
+
     hda = "hda"
     dataset_collection = "dataset_collection"
 
 
 class DatasetSourceType(str, Enum):
     hda = "hda"
     ldda = "ldda"
@@ -243,63 +294,62 @@
     hda = "hda"
     hdca = "hdca"
     library = "library"
     library_folder = "library_folder"
     new_collection = "new_collection"
 
 
-class DatasetCollectionInstanceType(str, Enum):
-    history = "history"
-    library = "library"
+DatasetCollectionInstanceType = Literal["history", "library"]
 
 
 class TagItem(ConstrainedStr):
     regex = re.compile(r"^([^\s.:])+(.[^\s.:]+)*(:[^\s.:]+)?$")
 
 
 class TagCollection(Model):
     """Represents the collection of tags associated with an item."""
+
     __root__: List[TagItem] = Field(
         default=...,
         title="Tags",
         description="The collection of tags associated with an item.",
     )
 
 
 class MetadataFile(Model):
     """Metadata file associated with a dataset."""
+
     file_type: str = Field(
         ...,
         title="File Type",
         description="TODO",
     )
     download_url: RelativeUrl = DownloadUrlField
 
 
 class DatasetPermissions(Model):
     """Role-based permissions for accessing and managing a dataset."""
-    manage: List[EncodedDatabaseIdField] = Field(
+
+    manage: List[DecodedDatabaseIdField] = Field(
         [],
         title="Management",
         description="The set of roles (encoded IDs) that can manage this dataset.",
     )
-    access: List[EncodedDatabaseIdField] = Field(
+    access: List[DecodedDatabaseIdField] = Field(
         [],
         title="Access",
         description="The set of roles (encoded IDs) that can access this dataset.",
     )
 
 
 class Hyperlink(Model):
     """Represents some text with an Hyperlink."""
+
     target: str = Field(
-        ...,
-        title="Target",
-        description="Specifies where to open the linked document.",
-        example="_blank"
+        ..., title="Target", description="Specifies where to open the linked document.", example="_blank"
     )
     href: AnyUrl = Field(
         ...,
         title="HRef",
         description="Specifies the linked document, resource, or location.",
     )
     text: str = Field(
@@ -307,14 +357,15 @@
         title="Text",
         description="The text placeholder for the link.",
     )
 
 
 class DisplayApp(Model):
     """Basic linked information about an application that can display certain datatypes."""
+
     label: str = Field(
         ...,
         title="Label",
         description="The label or title of the Display Application.",
     )
     links: List[Hyperlink] = Field(
         ...,
@@ -326,20 +377,21 @@
 class Visualization(Model):  # TODO annotate this model
     class Config:
         extra = Extra.allow  # Allow any fields temporarily until the model is annotated
 
 
 class HistoryItemBase(Model):
     """Basic information provided by items contained in a History."""
-    id: EncodedDatabaseIdField = EncodedEntityIdField
+
+    id: DecodedDatabaseIdField = EntityIdField
     name: Optional[str] = Field(
         title="Name",
         description="The name of the item.",
     )
-    history_id: EncodedDatabaseIdField = HistoryIdField
+    history_id: DecodedDatabaseIdField = HistoryIdField
     hid: int = Field(
         ...,
         title="HID",
         description="The index position of this item in the History.",
     )
     history_content_type: HistoryContentType = Field(
         ...,
@@ -356,14 +408,15 @@
         title="Visible",
         description="Whether this item is visible or hidden to the user by default.",
     )
 
 
 class HistoryItemCommon(HistoryItemBase):
     """Common information provided by items contained in a History."""
+
     class Config:
         extra = Extra.allow
 
     type_id: Optional[str] = Field(
         default=None,
         title="Type - ID",
         description="The type and the encoded ID of this item. Used for caching.",
@@ -378,15 +431,16 @@
     update_time: Optional[datetime] = UpdateTimeField
     url: RelativeUrl = RelativeUrlField
     tags: TagCollection
 
 
 class HDASummary(HistoryItemCommon):
     """History Dataset Association summary information."""
-    dataset_id: EncodedDatabaseIdField = Field(
+
+    dataset_id: DecodedDatabaseIdField = Field(
         ...,
         title="Dataset ID",
         description="The encoded ID of the dataset associated with this item.",
     )
     state: Dataset.states = DatasetStateField
     extension: str = Field(
         ...,
@@ -399,30 +453,32 @@
         title="Purged",
         description="Whether this dataset has been removed from disk.",
     )
 
 
 class HDAInaccessible(HistoryItemBase):
     """History Dataset Association information when the user can not access it."""
+
     accessible: bool = AccessibleField
     state: Dataset.states = DatasetStateField
 
 
 HdaLddaField = Field(
     DatasetSourceType.hda,
     const=True,
     title="HDA or LDDA",
     description="Whether this dataset belongs to a history (HDA) or a library (LDDA).",
-    deprecated=False  # TODO Should this field be deprecated in favor of model_class?
+    deprecated=False,  # TODO Should this field be deprecated in favor of model_class?
 )
 
 
 class HDADetailed(HDASummary):
     """History Dataset Association detailed information."""
-    model_class: str = ModelClassField(HDA_MODEL_CLASS_NAME)
+
+    model_class: Annotated[HDA_MODEL_CLASS, ModelClassField()]
     hda_ldda: DatasetSourceType = HdaLddaField
     accessible: bool = AccessibleField
     genome_build: Optional[str] = GenomeBuildField
     misc_info: Optional[str] = Field(
         default=None,
         title="Miscellaneous Information",
         description="TODO",
@@ -467,15 +523,15 @@
         title="Metadata Files",
         description="Collection of metadata files associated with this dataset.",
     )
     data_type: str = Field(
         ...,
         title="Data Type",
         description="The fully qualified name of the class implementing the data type of this dataset.",
-        example="galaxy.datatypes.data.Text"
+        example="galaxy.datatypes.data.Text",
     )
     peek: Optional[str] = Field(
         default=None,
         title="Peek",
         description="A few lines of contents from the start of the file.",
     )
     creating_job: str = Field(
@@ -523,98 +579,100 @@
     validated_state_message: Optional[str] = Field(
         ...,
         title="Validated State Message",
         description="The message with details about the datatype validation result for this dataset.",
     )
     annotation: Optional[str] = AnnotationField
     download_url: RelativeUrl = DownloadUrlField
-    type: str = Field(
-        "file",
-        const=True,
-        title="Type",
-        description="This is always `file` for datasets.",
-    )
-    api_type: str = Field(
-        "file",
-        const=True,
-        title="API Type",
-        description="TODO",
-        deprecated=False,  # TODO: Should this field be deprecated as announced in release 16.04?
-    )
+    type: Annotated[
+        Literal["file"],
+        Field(
+            title="Type",
+            description="This is always `file` for datasets.",
+        ),
+    ] = "file"
+    api_type: Annotated[
+        Literal["file"],
+        Field(
+            title="API Type",
+            description="TODO",
+            deprecated=False,  # TODO: Should this field be deprecated as announced in release 16.04?
+        ),
+    ] = "file"
     created_from_basename: Optional[str] = Field(
         None,
         title="Created from basename",
         description="The basename of the output that produced this dataset.",  # TODO: is that correct?
     )
 
 
 class HDAExtended(HDADetailed):
     """History Dataset Association extended information."""
+
     tool_version: str = Field(
         ...,
         title="Tool Version",
         description="The version of the tool that produced this dataset.",
     )
-    parent_id: Optional[EncodedDatabaseIdField] = Field(
+    parent_id: Optional[DecodedDatabaseIdField] = Field(
         None,
         title="Parent ID",
         description="TODO",
     )
     designation: Optional[str] = Field(
         None,
         title="Designation",
         description="TODO",
     )
 
 
-class HDABeta(HDADetailed):  # TODO: change HDABeta name to a more appropriate one.
-    """History Dataset Association information used in the new Beta History."""
-    # Equivalent to `betawebclient` serialization view for HDAs
-    pass
-
-
 class DCSummary(Model):
     """Dataset Collection summary information."""
-    model_class: str = ModelClassField(DC_MODEL_CLASS_NAME)
-    id: EncodedDatabaseIdField = EncodedEntityIdField
+
+    model_class: DC_MODEL_CLASS = ModelClassField(DC_MODEL_CLASS)
+    id: DecodedDatabaseIdField = EntityIdField
     create_time: datetime = CreateTimeField
     update_time: datetime = UpdateTimeField
     collection_type: CollectionType = CollectionTypeField
     populated_state: DatasetCollection.populated_states = PopulatedStateField
     populated_state_message: Optional[str] = PopulatedStateMessageField
     element_count: Optional[int] = ElementCountField
 
 
 class HDAObject(Model):
     """History Dataset Association Object"""
-    id: EncodedDatabaseIdField = EncodedEntityIdField
-    model_class: str = ModelClassField(HDA_MODEL_CLASS_NAME)
+
+    id: DecodedDatabaseIdField = EntityIdField
+    model_class: HDA_MODEL_CLASS = ModelClassField(HDA_MODEL_CLASS)
     state: Dataset.states = DatasetStateField
     hda_ldda: DatasetSourceType = HdaLddaField
-    history_id: EncodedDatabaseIdField = HistoryIdField
+    history_id: DecodedDatabaseIdField = HistoryIdField
+    tags: List[str]
 
     class Config:
         extra = Extra.allow  # Can contain more fields like metadata_*
 
 
 class DCObject(Model):
     """Dataset Collection Object"""
-    id: EncodedDatabaseIdField = EncodedEntityIdField
-    model_class: str = ModelClassField(DC_MODEL_CLASS_NAME)
+
+    id: DecodedDatabaseIdField = EntityIdField
+    model_class: DC_MODEL_CLASS = ModelClassField(DC_MODEL_CLASS)
     collection_type: CollectionType = CollectionTypeField
     populated: Optional[bool] = PopulatedField
     element_count: Optional[int] = ElementCountField
     contents_url: Optional[RelativeUrl] = ContentsUrlField
-    elements: List['DCESummary'] = ElementsField
+    elements: List["DCESummary"] = ElementsField
 
 
 class DCESummary(Model):
     """Dataset Collection Element summary information."""
-    id: EncodedDatabaseIdField = EncodedEntityIdField
-    model_class: str = ModelClassField(DCE_MODEL_CLASS_NAME)
+
+    id: DecodedDatabaseIdField = EntityIdField
+    model_class: DCE_MODEL_CLASS = ModelClassField(DCE_MODEL_CLASS)
     element_index: int = Field(
         ...,
         title="Element Index",
         description="The position index of this element inside the collection.",
     )
     element_identifier: str = Field(
         ...,
@@ -634,105 +692,257 @@
 
 
 DCObject.update_forward_refs()
 
 
 class DCDetailed(DCSummary):
     """Dataset Collection detailed information."""
+
     populated: bool = PopulatedField
     elements: List[DCESummary] = ElementsField
 
 
+class HDCJobStateSummary(Model):
+    """Overview of the job states working inside a dataset collection."""
+
+    all_jobs: int = Field(
+        0,
+        title="All jobs",
+        description="Total number of jobs associated with a dataset collection.",
+    )
+    new: int = Field(
+        0,
+        title="New jobs",
+        description="Number of jobs in the `new` state.",
+    )
+    waiting: int = Field(
+        0,
+        title="Waiting jobs",
+        description="Number of jobs in the `waiting` state.",
+    )
+    running: int = Field(
+        0,
+        title="Running jobs",
+        description="Number of jobs in the `running` state.",
+    )
+    error: int = Field(
+        0,
+        title="Jobs with errors",
+        description="Number of jobs in the `error` state.",
+    )
+    paused: int = Field(
+        0,
+        title="Paused jobs",
+        description="Number of jobs in the `paused` state.",
+    )
+    skipped: int = Field(
+        0,
+        title="Skipped jobs",
+        description="Number of jobs that were skipped due to conditional workflow step execution.",
+    )
+    deleted_new: int = Field(
+        0,
+        title="Deleted new jobs",
+        description="Number of jobs in the `deleted_new` state.",
+    )
+    resubmitted: int = Field(
+        0,
+        title="Resubmitted jobs",
+        description="Number of jobs in the `resubmitted` state.",
+    )
+    queued: int = Field(
+        0,
+        title="Queued jobs",
+        description="Number of jobs in the `queued` state.",
+    )
+    ok: int = Field(
+        0,
+        title="OK jobs",
+        description="Number of jobs in the `ok` state.",
+    )
+    failed: int = Field(
+        0,
+        title="Failed jobs",
+        description="Number of jobs in the `failed` state.",
+    )
+    deleted: int = Field(
+        0,
+        title="Deleted jobs",
+        description="Number of jobs in the `deleted` state.",
+    )
+    upload: int = Field(
+        0,
+        title="Upload jobs",
+        description="Number of jobs in the `upload` state.",
+    )
+
+
 class HDCASummary(HistoryItemCommon):
     """History Dataset Collection Association summary information."""
-    model_class: str = ModelClassField(HDCA_MODEL_CLASS_NAME)  # TODO: inconsistency? HDASummary does not have model_class only the detailed view has it...
-    type: str = Field(
-        "collection",
-        const=True,
-        title="Type",
-        description="This is always `collection` for dataset collections.",
-    )
+
+    model_class: HDCA_MODEL_CLASS = ModelClassField(HDCA_MODEL_CLASS)
+    type: Annotated[
+        Literal["collection"],
+        Field(
+            title="Type",
+            description="This is always `collection` for dataset collections.",
+        ),
+    ] = "collection"
     collection_type: CollectionType = CollectionTypeField
     populated_state: DatasetCollection.populated_states = PopulatedStateField
     populated_state_message: Optional[str] = PopulatedStateMessageField
     element_count: Optional[int] = ElementCountField
-    job_source_id: Optional[EncodedDatabaseIdField] = Field(
+    job_source_id: Optional[DecodedDatabaseIdField] = Field(
         None,
         title="Job Source ID",
         description="The encoded ID of the Job that produced this dataset collection. Used to track the state of the job.",
     )
     job_source_type: Optional[JobSourceType] = Field(
         None,
         title="Job Source Type",
         description="The type of job (model class) that produced this dataset collection. Used to track the state of the job.",
     )
+    job_state_summary: Optional[HDCJobStateSummary] = Field(
+        None,
+        title="Job State Summary",
+        description="Overview of the job states working inside the dataset collection.",
+    )
     contents_url: RelativeUrl = ContentsUrlField
+    collection_id: DecodedDatabaseIdField = Field(
+        ...,
+        title="Collection ID",
+        description="The encoded ID of the dataset collection associated with this HDCA.",
+    )
 
 
 class HDCADetailed(HDCASummary):
     """History Dataset Collection Association detailed information."""
+
     populated: bool = PopulatedField
     elements: List[DCESummary] = ElementsField
+    elements_datatypes: Set[str] = Field(
+        ..., description="A set containing all the different element datatypes in the collection."
+    )
 
 
-class HistoryBase(BaseModel):
+class HistoryBase(Model):
     """Provides basic configuration for all the History models."""
+
     class Config:
-        use_enum_values = True  # When using .dict()
         extra = Extra.allow  # Allow any other extra fields
 
 
-class UpdateContentItem(HistoryBase):
-    """Used for updating a particular HDA. All fields are optional."""
+class HistoryContentItem(Model):
+    """Identifies a dataset or collection contained in a History."""
+
     history_content_type: HistoryContentType = Field(
         ...,
         title="Content Type",
         description="The type of this item.",
     )
-    id: EncodedDatabaseIdField = EncodedEntityIdField
+    id: DecodedDatabaseIdField = EntityIdField
+
+
+class UpdateContentItem(HistoryContentItem):
+    """Used for updating a particular history item. All fields are optional."""
+
+    class Config:
+        use_enum_values = True  # When using .dict()
+        extra = Extra.allow  # Allow any other extra fields
 
 
 class UpdateHistoryContentsBatchPayload(HistoryBase):
     """Contains property values that will be updated for all the history `items` provided."""
 
     items: List[UpdateContentItem] = Field(
         ...,
         title="Items",
         description="A list of content items to update with the changes.",
     )
 
     class Config:
         schema_extra = {
             "example": {
-                "items": [
-                    {
-                        "history_content_type": "dataset",
-                        "id": "string"
-                    }
-                ],
+                "items": [{"history_content_type": "dataset", "id": "string"}],
                 "visible": False,
             }
         }
 
 
+class HistoryContentItemOperation(str, Enum):
+    hide = "hide"
+    unhide = "unhide"
+    delete = "delete"
+    undelete = "undelete"
+    purge = "purge"
+    change_datatype = "change_datatype"
+    change_dbkey = "change_dbkey"
+    add_tags = "add_tags"
+    remove_tags = "remove_tags"
+
+
+class BulkOperationParams(Model):
+    type: str
+
+
+class ChangeDatatypeOperationParams(BulkOperationParams):
+    type: Literal["change_datatype"]
+    datatype: str
+
+
+class ChangeDbkeyOperationParams(BulkOperationParams):
+    type: Literal["change_dbkey"]
+    dbkey: str
+
+
+class TagOperationParams(BulkOperationParams):
+    type: Union[Literal["add_tags"], Literal["remove_tags"]]
+    tags: List[str]
+
+
+AnyBulkOperationParams = Union[
+    ChangeDatatypeOperationParams,
+    ChangeDbkeyOperationParams,
+    TagOperationParams,
+]
+
+
+class HistoryContentBulkOperationPayload(Model):
+    operation: HistoryContentItemOperation
+    items: Optional[List[HistoryContentItem]]
+    params: Optional[AnyBulkOperationParams]
+
+
+class BulkOperationItemError(Model):
+    item: HistoryContentItem
+    error: str
+
+
+class HistoryContentBulkOperationResult(Model):
+    success_count: int
+    errors: List[BulkOperationItemError]
+
+
 class UpdateHistoryContentsPayload(HistoryBase):
     """Contains arbitrary property values that will be updated for a particular history item."""
+
     class Config:
         schema_extra = {
             "example": {
                 "visible": False,
                 "annotation": "Test",
             }
         }
 
 
 class HistorySummary(HistoryBase):
     """History summary information."""
-    model_class: str = ModelClassField(HISTORY_MODEL_CLASS_NAME)
-    id: EncodedDatabaseIdField = EncodedEntityIdField
+
+    model_class: HISTORY_MODEL_CLASS = ModelClassField(HISTORY_MODEL_CLASS)
+    id: DecodedDatabaseIdField = EntityIdField
     name: str = Field(
         ...,
         title="Name",
         description="The name of the history.",
     )
     deleted: bool = Field(
         ...,
@@ -746,20 +956,26 @@
     )
     url: RelativeUrl = RelativeUrlField
     published: bool = Field(
         ...,
         title="Published",
         description="Whether this resource is currently publicly available to all users.",
     )
+    count: int = Field(
+        ...,
+        title="Count",
+        description="The number of items in the history.",
+    )
     annotation: Optional[str] = AnnotationField
     tags: TagCollection
 
 
 class HistoryActiveContentCounts(Model):
     """Contains the number of active, deleted or hidden items in a History."""
+
     active: int = Field(
         ...,
         title="Active",
         description="Number of active datasets.",
     )
     hidden: int = Field(
         ...,
@@ -770,26 +986,27 @@
         ...,
         title="Deleted",
         description="Number of deleted datasets.",
     )
 
 
 HistoryStateCounts = Dict[Dataset.states, int]
-HistoryStateIds = Dict[Dataset.states, List[EncodedDatabaseIdField]]
+HistoryStateIds = Dict[Dataset.states, List[DecodedDatabaseIdField]]
 
 
 class HistoryDetailed(HistorySummary):  # Equivalent to 'dev-detailed' view, which seems the default
     """History detailed information."""
+
     contents_url: RelativeUrl = ContentsUrlField
     size: int = Field(
         ...,
         title="Size",
         description="The total size of the contents of this history in bytes.",
     )
-    user_id: EncodedDatabaseIdField = Field(
+    user_id: DecodedDatabaseIdField = Field(
         ...,
         title="User ID",
         description="The encoded ID of the user that owns this History.",
     )
     create_time: datetime = CreateTimeField
     update_time: datetime = UpdateTimeField
     importable: bool = Field(
@@ -827,48 +1044,17 @@
         description=(
             "A dictionary keyed to possible dataset states and valued with the number "
             "of datasets in this history that have those states."
         ),
     )
 
 
-class HistoryBeta(HistoryDetailed):
-    """History detailed information used in the new Beta History."""
-    annotation: Optional[str] = AnnotationField
-    empty: bool = Field(
-        ...,
-        title="Empty",
-        description="Whether this History has any content.",
-    )
-    nice_size: str = Field(
-        ...,
-        title="Nice Size",
-        description="Human-readable size of the contents of this history.",
-        example="95.4 MB"
-    )
-    purged: bool = Field(
-        ...,
-        title="Purged",
-        description="Whether this History has been permanently removed.",
-    )
-    tags: TagCollection
-    hid_counter: int = Field(
-        ...,
-        title="HID Counter",
-        description="TODO",
-    )
-    contents_active: HistoryActiveContentCounts = Field(
-        ...,
-        title="Active Contents",
-        description="Contains the number of active, deleted or hidden items in the History.",
-    )
-
-
 AnyHistoryView = Union[
-    HistoryBeta, HistoryDetailed, HistorySummary,
+    HistorySummary,
+    HistoryDetailed,
     # Any will cover those cases in which only specific `keys` are requested
     # otherwise the validation will fail because the required fields are not returned
     Any,
 ]
 
 
 class ExportHistoryArchivePayload(Model):
@@ -904,79 +1090,128 @@
         default=None,
         title="Force Rebuild",
         description="Whether to force a rebuild of the history archive.",
         hidden=True,  # Avoids displaying this field in the documentation
     )
 
 
-class SortByEnum(str, Enum):
+WorkflowSortByEnum = Literal["create_time", "update_time", "name"]
+
+
+class WorkflowIndexQueryPayload(Model):
+    show_deleted: bool = False
+    show_hidden: bool = False
+    show_published: Optional[bool] = None
+    show_shared: Optional[bool] = None
+    sort_by: Optional[WorkflowSortByEnum] = Field(title="Sort By", description="Sort workflows by this attribute")
+    sort_desc: Optional[bool] = Field(
+        title="Sort descending", description="Explicitly sort by descending if sort_by is specified."
+    )
+    limit: Optional[int] = Field(
+        default=None,
+        lt=1000,
+    )
+    offset: Optional[int] = Field(default=0, description="Number of workflows to skip")
+    search: Optional[str] = Field(default=None, title="Filter text", description="Freetext to search.")
+    skip_step_counts: bool = False
+
+
+class JobIndexSortByEnum(str, Enum):
+    create_time = "create_time"
+    update_time = "update_time"
+
+
+class JobIndexQueryPayload(Model):
+    states: Optional[List[str]] = None
+    user_details: bool = False
+    user_id: Optional[DecodedDatabaseIdField] = None
+    tool_ids: Optional[List[str]] = None
+    tool_ids_like: Optional[List[str]] = None
+    date_range_min: Optional[Union[OffsetNaiveDatetime, date]] = None
+    date_range_max: Optional[Union[OffsetNaiveDatetime, date]] = None
+    history_id: Optional[DecodedDatabaseIdField] = None
+    workflow_id: Optional[DecodedDatabaseIdField] = None
+    invocation_id: Optional[DecodedDatabaseIdField] = None
+    order_by: JobIndexSortByEnum = JobIndexSortByEnum.update_time
+    search: Optional[str] = None
+    limit: int = 500
+    offset: int = 0
+
+
+class InvocationSortByEnum(str, Enum):
     create_time = "create_time"
     update_time = "update_time"
     none = None
 
 
-class InvocationIndexPayload(Model):
-    workflow_id: Optional[DecodedDatabaseIdField] = Field(title="Workflow ID", description="Return only invocations for this Workflow ID")
-    history_id: Optional[DecodedDatabaseIdField] = Field(title="History ID", description="Return only invocations for this History ID")
-    job_id: Optional[DecodedDatabaseIdField] = Field(title="Job ID", description="Return only invocations for this Job ID")
-    user_id: Optional[DecodedDatabaseIdField] = Field(title="User ID", description="Return invocations for this User ID")
-    sort_by: Optional[SortByEnum] = Field(
-        title="Sort By",
-        description="Sort Workflow Invocations by this attribute"
+class InvocationIndexQueryPayload(Model):
+    workflow_id: Optional[DecodedDatabaseIdField] = Field(
+        title="Workflow ID", description="Return only invocations for this Workflow ID"
     )
-    sort_desc: bool = Field(
-        default=False,
-        descritpion="Sort in descending order?"
+    history_id: Optional[DecodedDatabaseIdField] = Field(
+        title="History ID", description="Return only invocations for this History ID"
     )
-    include_terminal: bool = Field(
-        default=True,
-        description="Set to false to only include terminal Invocations."
+    job_id: Optional[DecodedDatabaseIdField] = Field(
+        title="Job ID", description="Return only invocations for this Job ID"
+    )
+    user_id: Optional[DecodedDatabaseIdField] = Field(
+        title="User ID", description="Return invocations for this User ID"
+    )
+    sort_by: Optional[InvocationSortByEnum] = Field(
+        title="Sort By", description="Sort Workflow Invocations by this attribute"
     )
+    sort_desc: bool = Field(default=False, descritpion="Sort in descending order?")
+    include_terminal: bool = Field(default=True, description="Set to false to only include terminal Invocations.")
     limit: Optional[int] = Field(
         default=100,
         lt=1000,
     )
-    offset: Optional[int] = Field(
-        default=0,
-        description="Number of invocations to skip"
-    )
-    instance: bool = Field(
-        default=False,
-        description="Is provided workflow id for Workflow instead of StoredWorkflow?"
-    )
+    offset: Optional[int] = Field(default=0, description="Number of invocations to skip")
+
+
+class PageSortByEnum(str, Enum):
+    create_time = "create_time"
+    update_time = "update_time"
+
+
+class PageIndexQueryPayload(Model):
+    deleted: bool = False
+    user_id: Optional[DecodedDatabaseIdField] = None
+    sort_by: PageSortByEnum = PageSortByEnum.update_time
+    sort_desc: bool = Field(default=True, descritpion="Sort in descending order?")
+    show_published: bool = True
+    show_shared: bool = False
+    limit: int = 500
+    offset: int = 0
 
 
 class CreateHistoryPayload(Model):
     name: Optional[str] = Field(
         default=None,
         title="Name",
         description="The new history name.",
     )
-    history_id: Optional[EncodedDatabaseIdField] = Field(
+    history_id: Optional[DecodedDatabaseIdField] = Field(
         default=None,
         title="History ID",
         description=(
-            "The encoded ID of the history to copy. "
-            "Provide this value only if you want to copy an existing history."
+            "The encoded ID of the history to copy. " "Provide this value only if you want to copy an existing history."
         ),
     )
     all_datasets: Optional[bool] = Field(
         default=True,
         title="All Datasets",
         description=(
-            "Whether to copy also deleted HDAs/HDCAs. Only applies when "
-            "providing a `history_id` to copy from."
+            "Whether to copy also deleted HDAs/HDCAs. Only applies when " "providing a `history_id` to copy from."
         ),
     )
     archive_source: Optional[str] = Field(
         default=None,
         title="Archive Source",
-        description=(
-            "The URL that will generate the archive to import when `archive_type='url'`. "
-        ),
+        description=("The URL that will generate the archive to import when `archive_type='url'`. "),
     )
     archive_type: Optional[HistoryImportArchiveSourceType] = Field(
         default=HistoryImportArchiveSourceType.url,
         title="Archive Type",
         description="The type of source from where the new history will be imported.",
     )
     archive_file: Optional[Any] = Field(
@@ -993,21 +1228,21 @@
         description="The name of the element.",
     )
     src: ColletionSourceType = Field(
         ...,
         title="Source",
         description="The source of the element.",
     )
-    id: Optional[EncodedDatabaseIdField] = Field(
+    id: Optional[DecodedDatabaseIdField] = Field(
         default=None,
         title="ID",
         description="The encoded ID of the element.",
     )
     collection_type: Optional[CollectionType] = CollectionTypeField
-    element_identifiers: Optional[List['CollectionElementIdentifier']] = Field(
+    element_identifiers: Optional[List["CollectionElementIdentifier"]] = Field(
         default=None,
         title="Element Identifiers",
         description="List of elements that should be in the new sub-collection.",
     )
     tags: Optional[List[str]] = Field(
         default=None,
         title="Tags",
@@ -1039,53 +1274,143 @@
     )
     copy_elements: Optional[bool] = Field(
         default=False,
         title="Copy Elements",
         description="Whether to create a copy of the source HDAs for the new collection.",
     )
     instance_type: Optional[DatasetCollectionInstanceType] = Field(
-        default=DatasetCollectionInstanceType.history,
+        default="history",
         title="Instance Type",
         description="The type of the instance, either `history` (default) or `library`.",
     )
-    history_id: Optional[EncodedDatabaseIdField] = Field(
+    history_id: Optional[DecodedDatabaseIdField] = Field(
         default=None,
         description="The ID of the history that will contain the collection. Required if `instance_type=history`.",
     )
-    folder_id: Optional[EncodedDatabaseIdField] = Field(
+    folder_id: Optional[LibraryFolderDatabaseIdField] = Field(
         default=None,
-        description="The ID of the history that will contain the collection. Required if `instance_type=library`.",
+        description="The ID of the library folder that will contain the collection. Required if `instance_type=library`.",
     )
 
 
-class JobExportHistoryArchiveModel(Model):
-    id: EncodedDatabaseIdField = Field(
+class ModelStoreFormat(str, Enum):
+    """Available types of model stores for export."""
+
+    TGZ = "tgz"
+    TAR = "tar"
+    TAR_DOT_GZ = "tar.gz"
+    BAG_DOT_ZIP = "bag.zip"
+    BAG_DOT_TAR = "bag.tar"
+    BAG_DOT_TGZ = "bag.tgz"
+    ROCRATE_ZIP = "rocrate.zip"
+    BCO_JSON = "bco.json"
+
+    @classmethod
+    def is_compressed(cls, value: "ModelStoreFormat"):
+        return value in [cls.TAR_DOT_GZ, cls.TGZ, cls.TAR, cls.ROCRATE_ZIP]
+
+    @classmethod
+    def is_bag(cls, value: "ModelStoreFormat"):
+        return value in [cls.BAG_DOT_TAR, cls.BAG_DOT_TGZ, cls.BAG_DOT_ZIP]
+
+
+class StoreContentSource(Model):
+    store_content_uri: Optional[str]
+    store_dict: Optional[Dict[str, Any]]
+    model_store_format: Optional["ModelStoreFormat"] = None
+
+
+class CreateHistoryFromStore(StoreContentSource):
+    pass
+
+
+class StoreExportPayload(Model):
+    model_store_format: ModelStoreFormat = Field(
+        default=ModelStoreFormat.TAR_DOT_GZ,
+        description="format of model store to export",
+    )
+    include_files: bool = Field(
+        default=True,
+        description="include materialized files in export when available",
+    )
+    include_deleted: bool = Field(
+        default=False,
+        title="Include deleted",
+        description="Include file contents for deleted datasets (if include_files is True).",
+    )
+    include_hidden: bool = Field(
+        default=False,
+        title="Include hidden",
+        description="Include file contents for hidden datasets (if include_files is True).",
+    )
+
+
+class ShortTermStoreExportPayload(StoreExportPayload):
+    short_term_storage_request_id: UUID
+    duration: OptionalNumberT
+
+
+class BcoGenerationParametersMixin(BaseModel):
+    bco_merge_history_metadata: bool = Field(
+        default=False, description="When reading tags/annotations to generate BCO object include history metadata."
+    )
+    bco_override_environment_variables: Optional[Dict[str, str]] = Field(
+        default=None,
+        description="Override environment variables for 'execution_domain' when generating BioCompute object.",
+    )
+    bco_override_empirical_error: Optional[Dict[str, str]] = Field(
+        default=None,
+        description="Override empirical error for 'error domain' when generating BioCompute object.",
+    )
+    bco_override_algorithmic_error: Optional[Dict[str, str]] = Field(
+        default=None,
+        description="Override algorithmic error for 'error domain' when generating BioCompute object.",
+    )
+    bco_override_xref: Optional[List[XrefItem]] = Field(
+        default=None,
+        description="Override xref for 'description domain' when generating BioCompute object.",
+    )
+
+
+class WriteStoreToPayload(StoreExportPayload):
+    target_uri: str = Field(
         ...,
-        title="ID",
-        description="The encoded database ID of the job that is currently processing a particular request.",
+        title="Target URI",
+        description="Galaxy Files URI to write mode store content to.",
     )
-    job_id: EncodedDatabaseIdField = Field(
+
+
+class ObjectExportResponseBase(Model):
+    id: EncodedDatabaseIdField = Field(
         ...,
-        title="Job ID",
-        description="The encoded database ID of the job that generated this history export archive.",
+        title="ID",
+        description="The encoded database ID of the export request.",
     )
     ready: bool = Field(
         ...,
         title="Ready",
-        description="Whether the export history job has completed successfully and the archive is ready to download",
+        description="Whether the export has completed successfully and the archive is ready",
     )
     preparing: bool = Field(
         ...,
         title="Preparing",
-        description="Whether the history archive is currently being built or in preparation.",
+        description="Whether the archive is currently being built or in preparation.",
     )
     up_to_date: bool = Field(
         ...,
         title="Up to Date",
-        description="False, if a new export archive should be generated for the corresponding history.",
+        description="False, if a new export archive should be generated.",
+    )
+
+
+class JobExportHistoryArchiveModel(ObjectExportResponseBase):
+    job_id: EncodedDatabaseIdField = Field(
+        ...,
+        title="Job ID",
+        description="The encoded database ID of the job that generated this history export archive.",
     )
     download_url: RelativeUrl = Field(
         ...,
         title="Download URL",
         description="Relative API URL to download the exported history archive.",
     )
     external_download_latest_url: AnyUrl = Field(
@@ -1096,33 +1421,73 @@
     external_download_permanent_url: AnyUrl = Field(
         ...,
         title="External Download Permanent URL",
         description="Fully qualified URL to download this particular version of the exported history archive.",
     )
 
 
-class JobExportHistoryArchiveCollection(Model):
+class ExportObjectType(str, Enum):
+    """Types of objects that can be exported."""
+
+    HISTORY = "history"
+    INVOCATION = "invocation"
+
+
+class ExportObjectRequestMetadata(Model):
+    object_id: EncodedDatabaseIdField
+    object_type: ExportObjectType
+    user_id: Optional[EncodedDatabaseIdField]
+    payload: Union[WriteStoreToPayload, ShortTermStoreExportPayload]
+
+
+class ExportObjectResultMetadata(Model):
+    success: bool
+    error: Optional[str]
+
+
+class ExportObjectMetadata(Model):
+    request_data: ExportObjectRequestMetadata
+    result_data: Optional[ExportObjectResultMetadata]
+
+
+class ObjectExportTaskResponse(ObjectExportResponseBase):
+    task_uuid: UUID4 = Field(
+        ...,
+        title="Task ID",
+        description="The identifier of the task processing the export.",
+    )
+    create_time: datetime = CreateTimeField
+    export_metadata: Optional[ExportObjectMetadata]
+
+
+class JobExportHistoryArchiveListResponse(Model):
     __root__: List[JobExportHistoryArchiveModel]
 
 
-class LabelValuePair(BaseModel):
+class ExportTaskListResponse(Model):
+    __root__: List[ObjectExportTaskResponse]
+    __accept_type__ = "application/vnd.galaxy.task.export+json"
+
+
+class LabelValuePair(Model):
     """Generic Label/Value pair model."""
+
     label: str = Field(
         ...,
         title="Label",
         description="The label of the item.",
     )
     value: str = Field(
         ...,
         title="Value",
         description="The value of the item.",
     )
 
 
-class CustomBuildsMetadataResponse(BaseModel):
+class CustomBuildsMetadataResponse(Model):
     installed_builds: List[LabelValuePair] = Field(
         ...,
         title="Installed Builds",
         description="TODO",
     )
     fasta_hdas: List[LabelValuePair] = Field(
         ...,
@@ -1131,122 +1496,33 @@
             "A list of label/value pairs with all the datasets of type `FASTA` contained in the History.\n"
             " - `label` is item position followed by the name of the dataset.\n"
             " - `value` is the encoded database ID of the dataset.\n"
         ),
     )
 
 
-class JobIdResponse(BaseModel):
+class JobIdResponse(Model):
     """Contains the ID of the job associated with a particular request."""
+
     job_id: EncodedDatabaseIdField = Field(
         ...,
         title="Job ID",
         description="The encoded database ID of the job that is currently processing a particular request.",
     )
 
 
-class HDCJobStateSummary(Model):
-    """Overview of the job states working inside a dataset collection."""
-    all_jobs: int = Field(
-        0,
-        title="All jobs",
-        description="Total number of jobs associated with a dataset collection.",
-    )
-    new: int = Field(
-        0,
-        title="New jobs",
-        description="Number of jobs in the `new` state.",
-    )
-    waiting: int = Field(
-        0,
-        title="Waiting jobs",
-        description="Number of jobs in the `waiting` state.",
-    )
-    running: int = Field(
-        0,
-        title="Running jobs",
-        description="Number of jobs in the `running` state.",
-    )
-    error: int = Field(
-        0,
-        title="Jobs with errors",
-        description="Number of jobs in the `error` state.",
-    )
-    paused: int = Field(
-        0,
-        title="Paused jobs",
-        description="Number of jobs in the `paused` state.",
-    )
-    deleted_new: int = Field(
-        0,
-        title="Deleted new jobs",
-        description="Number of jobs in the `deleted_new` state.",
-    )
-    resubmitted: int = Field(
-        0,
-        title="Resubmitted jobs",
-        description="Number of jobs in the `resubmitted` state.",
-    )
-    queued: int = Field(
-        0,
-        title="Queued jobs",
-        description="Number of jobs in the `queued` state.",
-    )
-    ok: int = Field(
-        0,
-        title="OK jobs",
-        description="Number of jobs in the `ok` state.",
-    )
-    failed: int = Field(
-        0,
-        title="Failed jobs",
-        description="Number of jobs in the `failed` state.",
-    )
-    deleted: int = Field(
-        0,
-        title="Deleted jobs",
-        description="Number of jobs in the `deleted` state.",
-    )
-    upload: int = Field(
-        0,
-        title="Upload jobs",
-        description="Number of jobs in the `upload` state.",
-    )
-
-
-class HDCABeta(HDCADetailed):  # TODO: change HDCABeta name to a more appropriate one.
-    """History Dataset Collection Association information used in the new Beta History."""
-    # Equivalent to `betawebclient` serialization view for HDCAs
-    collection_id: EncodedDatabaseIdField = Field(
-        # TODO: inconsistency? the equivalent counterpart for HDAs, `dataset_id`, is declared in `HDASummary` scope
-        # while in HDCAs it is only serialized in the new `betawebclient` view?
-        ...,
-        title="Collection ID",
-        description="The encoded ID of the dataset collection associated with this HDCA.",
-    )
-    job_state_summary: Optional[HDCJobStateSummary] = Field(
-        None,
-        title="Job State Summary",
-        description="Overview of the job states working inside the dataset collection.",
-    )
-    elements_datatypes: Set[str] = Field(
-        ...,
-        description="A set containing all the different element datatypes in the collection."
-    )
-
-
 class JobBaseModel(Model):
-    id: EncodedDatabaseIdField = EncodedEntityIdField
-    model_class: str = ModelClassField(JOB_MODEL_CLASS_NAME)
+    id: DecodedDatabaseIdField = EntityIdField
+    model_class: JOB_MODEL_CLASS = ModelClassField(JOB_MODEL_CLASS)
     tool_id: str = Field(
         ...,
         title="Tool ID",
         description="Identifier of the tool that generated this job.",
     )
-    history_id: Optional[EncodedDatabaseIdField] = Field(
+    history_id: Optional[DecodedDatabaseIdField] = Field(
         None,
         title="History ID",
         description="The encoded ID of the history associated with this item.",
     )
     state: Job.states = Field(
         ...,
         title="State",
@@ -1259,56 +1535,55 @@
     )
     create_time: datetime = CreateTimeField
     update_time: datetime = UpdateTimeField
     galaxy_version: str = Field(
         ...,
         title="Galaxy Version",
         description="The (major) version of Galaxy used to create this job.",
-        example="21.05"
+        example="21.05",
     )
 
 
 class JobImportHistoryResponse(JobBaseModel):
     message: str = Field(
         ...,
         title="Message",
         description="Text message containing information about the history import.",
     )
 
 
-class JobStateSummary(Model):
-    id: EncodedDatabaseIdField = EncodedEntityIdField
-    model: str = ModelClassField("Job")
+class ItemStateSummary(Model):
+    id: DecodedDatabaseIdField = EntityIdField
     populated_state: DatasetCollection.populated_states = PopulatedStateField
     states: Dict[Job.states, int] = Field(
-        {},
-        title="States",
-        description=(
-            "A dictionary of job states and the number of jobs in that state."
-        )
+        {}, title="States", description=("A dictionary of job states and the number of jobs in that state.")
     )
 
 
-class ImplicitCollectionJobsStateSummary(JobStateSummary):
-    model: str = ModelClassField("ImplicitCollectionJobs")
+class JobStateSummary(ItemStateSummary):
+    model: Literal["Job"] = ModelClassField("Job")
+
+
+class ImplicitCollectionJobsStateSummary(ItemStateSummary):
+    model: Literal["ImplicitCollectionJobs"] = ModelClassField("ImplicitCollectionJobs")
 
 
-class WorkflowInvocationStateSummary(JobStateSummary):
-    model: str = ModelClassField("WorkflowInvocation")
+class WorkflowInvocationStateSummary(ItemStateSummary):
+    model: Literal["WorkflowInvocation"] = ModelClassField("WorkflowInvocation")
 
 
 class JobSummary(JobBaseModel):
     """Basic information about a job."""
+
     external_id: Optional[str] = Field(
         None,
         title="External ID",
         description=(
-            "The job id used by the external job runner (Condor, Pulsar, etc.)"
-            "Only administrator can see this value."
-        )
+            "The job id used by the external job runner (Condor, Pulsar, etc.)" "Only administrator can see this value."
+        ),
     )
     command_line: Optional[str] = Field(
         None,
         title="Command Line",
         description=(
             "The command line produced by the job. "
             "Users can see this value if allowed in the configuration, administrator can always see this value."
@@ -1320,21 +1595,24 @@
         description=(
             "The email of the user that owns this job. "
             "Only the owner of the job and administrators can see this value."
         ),
     )
 
 
-class DatasetJobInfo(Model):
-    id: EncodedDatabaseIdField = EncodedEntityIdField
+class DatasetSourceId(Model):
+    id: DecodedDatabaseIdField = EntityIdField
     src: DatasetSourceType = Field(
         ...,
         title="Source",
         description="The source of this dataset, either `hda` or `ldda` depending of its origin.",
     )
+
+
+class DatasetJobInfo(DatasetSourceId):
     uuid: UUID4 = UuidField
 
 
 class JobDetails(JobSummary):
     command_version: str = Field(
         ...,
         title="Command Version",
@@ -1390,21 +1668,22 @@
     class Config:
         schema_extra = {
             "example": {
                 "title": "Job Start Time",
                 "value": "2021-02-25 14:55:40",
                 "plugin": "core",
                 "name": "start_epoch",
-                "raw_value": "1614261340.0000000"
+                "raw_value": "1614261340.0000000",
             }
         }
 
 
 class JobMetricCollection(Model):
     """Represents a collection of metrics associated with a Job."""
+
     __root__: List[JobMetric] = Field(
         [],
         title="Job Metrics",
         description="Collections of metrics provided by `JobInstrumenter` plugins on a particular job.",
     )
 
 
@@ -1451,16 +1730,16 @@
             "Collections of metrics provided by `JobInstrumenter` plugins on a particular job. "
             "Only administrators can see these metrics."
         ),
     )
 
 
 class StoredWorkflowSummary(Model):
-    id: EncodedDatabaseIdField = EncodedEntityIdField
-    model_class: str = ModelClassField(STORED_WORKFLOW_MODEL_CLASS_NAME)
+    id: DecodedDatabaseIdField = EntityIdField
+    model_class: STORED_WORKFLOW_MODEL_CLASS = ModelClassField(STORED_WORKFLOW_MODEL_CLASS)
     create_time: datetime = CreateTimeField
     update_time: datetime = UpdateTimeField
     name: str = Field(
         ...,
         title="Name",
         description="The name of the history.",
     )
@@ -1504,15 +1783,15 @@
     show_in_tool_panel: bool = Field(
         ...,
         title="Show in Tool Panel",
         description="Whether to display this workflow in the Tools Panel.",
     )
 
 
-class WorkflowInput(BaseModel):
+class WorkflowInput(Model):
     label: str = Field(
         ...,
         title="Label",
         description="Label of the input.",
     )
     value: str = Field(
         ...,
@@ -1522,15 +1801,15 @@
     uuid: UUID4 = Field(
         ...,
         title="UUID",
         description="Universal unique identifier of the input.",
     )
 
 
-class WorkflowOutput(BaseModel):
+class WorkflowOutput(Model):
     label: Optional[str] = Field(
         None,
         title="Label",
         description="Label of the output.",
     )
     output_name: str = Field(
         ...,
@@ -1540,139 +1819,105 @@
     uuid: Optional[UUID4] = Field(
         ...,
         title="UUID",
         description="Universal unique identifier of the output.",
     )
 
 
-class InputStep(BaseModel):
+class InputStep(Model):
     source_step: int = Field(
         ...,
         title="Source Step",
         description="The identifier of the workflow step connected to this particular input.",
     )
     step_output: str = Field(
         ...,
         title="Step Output",
         description="The name of the output generated by the source step.",
     )
 
 
 class WorkflowModuleType(str, Enum):
     """Available types of modules that represent a step in a Workflow."""
+
     data_input = "data_input"
     data_collection_input = "data_collection_input"
     parameter_input = "parameter_input"
     subworkflow = "subworkflow"
     tool = "tool"
     pause = "pause"  # Experimental
 
 
-class WorkflowStepBase(BaseModel):
+class WorkflowStepBase(Model):
     id: int = Field(
         ...,
         title="ID",
-        description="The identifier of the step. It matches the index order of the step inside the workflow."
-    )
-    type: WorkflowModuleType = Field(
-        ...,
-        title="Type",
-        description="The type of workflow module."
+        description="The identifier of the step. It matches the index order of the step inside the workflow.",
     )
+    type: WorkflowModuleType = Field(..., title="Type", description="The type of workflow module.")
     annotation: Optional[str] = AnnotationField
     input_steps: Dict[str, InputStep] = Field(
         ...,
         title="Input Steps",
-        description="A dictionary containing information about the inputs connected to this workflow step."
+        description="A dictionary containing information about the inputs connected to this workflow step.",
     )
 
 
 class ToolBasedWorkflowStep(WorkflowStepBase):
     tool_id: Optional[str] = Field(
-        None,
-        title="Tool ID",
-        description="The unique name of the tool associated with this step."
+        None, title="Tool ID", description="The unique name of the tool associated with this step."
     )
     tool_version: Optional[str] = Field(
-        None,
-        title="Tool Version",
-        description="The version of the tool associated with this step."
-    )
-    tool_inputs: Any = Field(
-        ...,
-        title="Tool Inputs",
-        description="TODO"
+        None, title="Tool Version", description="The version of the tool associated with this step."
     )
+    tool_inputs: Any = Field(..., title="Tool Inputs", description="TODO")
 
 
 class InputDataStep(ToolBasedWorkflowStep):
     type: WorkflowModuleType = Field(
-        WorkflowModuleType.data_input, const=True,
-        title="Type",
-        description="The type of workflow module."
+        WorkflowModuleType.data_input, const=True, title="Type", description="The type of workflow module."
     )
 
 
 class InputDataCollectionStep(ToolBasedWorkflowStep):
     type: WorkflowModuleType = Field(
-        WorkflowModuleType.data_collection_input, const=True,
-        title="Type",
-        description="The type of workflow module."
+        WorkflowModuleType.data_collection_input, const=True, title="Type", description="The type of workflow module."
     )
 
 
 class InputParameterStep(ToolBasedWorkflowStep):
     type: WorkflowModuleType = Field(
-        WorkflowModuleType.parameter_input, const=True,
-        title="Type",
-        description="The type of workflow module."
+        WorkflowModuleType.parameter_input, const=True, title="Type", description="The type of workflow module."
     )
 
 
 class PauseStep(WorkflowStepBase):
     type: WorkflowModuleType = Field(
-        WorkflowModuleType.pause, const=True,
-        title="Type",
-        description="The type of workflow module."
+        WorkflowModuleType.pause, const=True, title="Type", description="The type of workflow module."
     )
 
 
 class ToolStep(ToolBasedWorkflowStep):
     type: WorkflowModuleType = Field(
-        WorkflowModuleType.tool, const=True,
-        title="Type",
-        description="The type of workflow module."
+        WorkflowModuleType.tool, const=True, title="Type", description="The type of workflow module."
     )
 
 
 class SubworkflowStep(WorkflowStepBase):
     type: WorkflowModuleType = Field(
-        WorkflowModuleType.subworkflow, const=True,
-        title="Type",
-        description="The type of workflow module."
+        WorkflowModuleType.subworkflow, const=True, title="Type", description="The type of workflow module."
     )
-    workflow_id: EncodedDatabaseIdField = Field(
-        ...,
-        title="Workflow ID",
-        description="The encoded ID of the workflow that will be run on this step."
+    workflow_id: DecodedDatabaseIdField = Field(
+        ..., title="Workflow ID", description="The encoded ID of the workflow that will be run on this step."
     )
 
 
-class Creator(BaseModel):
-    class_: str = Field(
-        ...,
-        alias="class",
-        title="Class",
-        description="The class representing this creator."
-    )
-    name: str = Field(
-        ...,
-        title="Name",
-        description="The name of the creator."
-    )
+class Creator(Model):
+    class_: str = Field(..., alias="class", title="Class", description="The class representing this creator.")
+    name: str = Field(..., title="Name", description="The name of the creator.")
     address: Optional[str] = Field(
         None,
         title="Address",
     )
     alternate_name: Optional[str] = Field(
         None,
         alias="alternateName",
@@ -1683,19 +1928,15 @@
         title="Email",
     )
     fax_number: Optional[str] = Field(
         None,
         alias="faxNumber",
         title="Fax Number",
     )
-    identifier: Optional[str] = Field(
-        None,
-        title="Identifier",
-        description="Identifier (typically an orcid.org ID)"
-    )
+    identifier: Optional[str] = Field(None, title="Identifier", description="Identifier (typically an orcid.org ID)")
     image: Optional[AnyHttpUrl] = Field(
         None,
         title="Image URL",
     )
     telephone: Optional[str] = Field(
         None,
         title="Telephone",
@@ -1733,177 +1974,100 @@
     honorific_prefix: Optional[str] = Field(
         None,
         alias="honorificPrefix",
         title="Honorific Prefix",
         description="Honorific Prefix (e.g. Dr/Mrs/Mr)",
     )
     honorific_suffix: Optional[str] = Field(
-        None,
-        alias="honorificSuffix",
-        title="Honorific Suffix",
-        description="Honorific Suffix (e.g. M.D.)"
+        None, alias="honorificSuffix", title="Honorific Suffix", description="Honorific Suffix (e.g. M.D.)"
     )
     job_title: Optional[str] = Field(
         None,
         alias="jobTitle",
         title="Job Title",
     )
 
 
 class StoredWorkflowDetailed(StoredWorkflowSummary):
     annotation: Optional[str] = AnnotationField  # Inconsistency? See comment on StoredWorkflowSummary.annotations
     license: Optional[str] = Field(
-        None,
-        title="License",
-        description="SPDX Identifier of the license associated with this workflow."
+        None, title="License", description="SPDX Identifier of the license associated with this workflow."
     )
     version: int = Field(
-        ...,
-        title="Version",
-        description="The version of the workflow represented by an incremental number."
+        ..., title="Version", description="The version of the workflow represented by an incremental number."
     )
     inputs: Dict[int, WorkflowInput] = Field(
-        {},
-        title="Inputs",
-        description="A dictionary containing information about all the inputs of the workflow."
+        {}, title="Inputs", description="A dictionary containing information about all the inputs of the workflow."
     )
     creator: Optional[List[Union[Person, Organization]]] = Field(
         None,
         title="Creator",
-        description=(
-            "Additional information about the creator (or multiple creators) of this workflow."
-        )
+        description=("Additional information about the creator (or multiple creators) of this workflow."),
     )
-    steps: Dict[int,
+    steps: Dict[
+        int,
         Union[
             InputDataStep,
             InputDataCollectionStep,
             InputParameterStep,
             PauseStep,
             ToolStep,
             SubworkflowStep,
-        ]
-    ] = Field(
-        {},
-        title="Steps",
-        description="A dictionary with information about all the steps of the workflow."
-    )
+        ],
+    ] = Field({}, title="Steps", description="A dictionary with information about all the steps of the workflow.")
 
 
-class Input(BaseModel):
-    name: str = Field(
-        ...,
-        title="Name",
-        description="The name of the input."
-    )
-    description: str = Field(
-        ...,
-        title="Description",
-        description="The annotation or description of the input."
+class Input(Model):
+    name: str = Field(..., title="Name", description="The name of the input.")
+    description: str = Field(..., title="Description", description="The annotation or description of the input.")
 
-    )
 
+class Output(Model):
+    name: str = Field(..., title="Name", description="The name of the output.")
+    type: str = Field(..., title="Type", description="The extension or type of output.")
 
-class Output(BaseModel):
-    name: str = Field(
-        ...,
-        title="Name",
-        description="The name of the output."
-    )
-    type: str = Field(
-        ...,
-        title="Type",
-        description="The extension or type of output."
-
-    )
 
-
-class InputConnection(BaseModel):
-    id: int = Field(
-        ...,
-        title="ID",
-        description="The identifier of the input."
-    )
+class InputConnection(Model):
+    id: int = Field(..., title="ID", description="The identifier of the input.")
     output_name: str = Field(
         ...,
         title="Output Name",
         description="The name assigned to the output.",
     )
     input_subworkflow_step_id: Optional[int] = Field(
         None,
         title="Input Subworkflow Step ID",
         description="TODO",
     )
 
 
-class WorkflowStepLayoutPosition(BaseModel):
+class WorkflowStepLayoutPosition(Model):
     """Position and dimensions of the workflow step represented by a box on the graph."""
-    bottom: int = Field(
-        ...,
-        title="Bottom",
-        description="Position in pixels of the bottom of the box."
-    )
-    top: int = Field(
-        ...,
-        title="Top",
-        description="Position in pixels of the top of the box."
-    )
-    left: int = Field(
-        ...,
-        title="Left",
-        description="Left margin or left-most position of the box."
-    )
-    right: int = Field(
-        ...,
-        title="Right",
-        description="Right margin or right-most position of the box."
-    )
-    x: int = Field(
-        ...,
-        title="X",
-        description="Horizontal pixel coordinate of the top right corner of the box."
-    )
-    y: int = Field(
-        ...,
-        title="Y",
-        description="Vertical pixel coordinate of the top right corner of the box."
-    )
-    height: int = Field(
-        ...,
-        title="Height",
-        description="Height of the box in pixels."
-    )
-    width: int = Field(
-        ...,
-        title="Width",
-        description="Width of the box in pixels."
-    )
+
+    bottom: int = Field(..., title="Bottom", description="Position in pixels of the bottom of the box.")
+    top: int = Field(..., title="Top", description="Position in pixels of the top of the box.")
+    left: int = Field(..., title="Left", description="Left margin or left-most position of the box.")
+    right: int = Field(..., title="Right", description="Right margin or right-most position of the box.")
+    x: int = Field(..., title="X", description="Horizontal pixel coordinate of the top right corner of the box.")
+    y: int = Field(..., title="Y", description="Vertical pixel coordinate of the top right corner of the box.")
+    height: int = Field(..., title="Height", description="Height of the box in pixels.")
+    width: int = Field(..., title="Width", description="Width of the box in pixels.")
 
 
-class WorkflowStepToExportBase(BaseModel):
+class WorkflowStepToExportBase(Model):
     id: int = Field(
         ...,
         title="ID",
-        description="The identifier of the step. It matches the index order of the step inside the workflow."
-    )
-    type: str = Field(
-        ...,
-        title="Type",
-        description="The type of workflow module."
-    )
-    name: str = Field(
-        ...,
-        title="Name",
-        description="The descriptive name of the module or step."
+        description="The identifier of the step. It matches the index order of the step inside the workflow.",
     )
+    type: str = Field(..., title="Type", description="The type of workflow module.")
+    name: str = Field(..., title="Name", description="The descriptive name of the module or step.")
     annotation: Optional[str] = AnnotationField
     tool_id: Optional[str] = Field(  # Duplicate of `content_id` or viceversa?
-        None,
-        title="Tool ID",
-        description="The unique name of the tool associated with this step."
+        None, title="Tool ID", description="The unique name of the tool associated with this step."
     )
     uuid: UUID4 = Field(
         ...,
         title="UUID",
         description="Universal unique identifier of the workflow.",
     )
     label: Optional[str] = Field(
@@ -1927,44 +2091,38 @@
     )
     position: WorkflowStepLayoutPosition = Field(
         ...,
         title="Position",
         description="Layout position of this step in the graph",
     )
     workflow_outputs: List[WorkflowOutput] = Field(
-        [],
-        title="Workflow Outputs",
-        description="The version of the tool associated with this step."
+        [], title="Workflow Outputs", description="The version of the tool associated with this step."
     )
 
 
 class WorkflowStepToExport(WorkflowStepToExportBase):
     content_id: Optional[str] = Field(  # Duplicate of `tool_id` or viceversa?
-        None,
-        title="Content ID",
-        description="TODO"
+        None, title="Content ID", description="TODO"
     )
     tool_version: Optional[str] = Field(
-        None,
-        title="Tool Version",
-        description="The version of the tool associated with this step."
+        None, title="Tool Version", description="The version of the tool associated with this step."
     )
     tool_state: Json = Field(
         ...,
         title="Tool State",
         description="JSON string containing the serialized representation of the persistable state of the step.",
     )
     errors: Optional[str] = Field(
         None,
         title="Errors",
         description="An message indicating possible errors in the step.",
     )
 
 
-class ToolShedRepositorySummary(BaseModel):
+class ToolShedRepositorySummary(Model):
     name: str = Field(
         ...,
         title="Name",
         description="The name of the repository.",
     )
     owner: str = Field(
         ...,
@@ -1979,15 +2137,15 @@
     tool_shed: str = Field(
         ...,
         title="Tool Shed",
         description="The Tool Shed base URL.",
     )
 
 
-class PostJobAction(BaseModel):
+class PostJobAction(Model):
     action_type: str = Field(
         ...,
         title="Action Type",
         description="The type of action to run.",
     )
     output_name: str = Field(
         ...,
@@ -1999,139 +2157,228 @@
         title="Action Arguments",
         description="Any additional arguments needed by the action.",
     )
 
 
 class WorkflowToolStepToExport(WorkflowStepToExportBase):
     tool_shed_repository: ToolShedRepositorySummary = Field(
-        ...,
-        title="Tool Shed Repository",
-        description="Information about the origin repository of this tool."
+        ..., title="Tool Shed Repository", description="Information about the origin repository of this tool."
     )
     post_job_actions: Dict[str, PostJobAction] = Field(
-        ...,
-        title="Post-job Actions",
-        description="Set of actions that will be run when the job finish."
+        ..., title="Post-job Actions", description="Set of actions that will be run when the job finish."
     )
 
 
 class SubworkflowStepToExport(WorkflowStepToExportBase):
-    subworkflow: 'WorkflowToExport' = Field(
-        ...,
-        title="Subworkflow",
-        description="Full information about the subworkflow associated with this step."
+    subworkflow: "WorkflowToExport" = Field(
+        ..., title="Subworkflow", description="Full information about the subworkflow associated with this step."
     )
 
 
-class WorkflowToExport(BaseModel):
+class WorkflowToExport(Model):
     a_galaxy_workflow: str = Field(  # Is this meant to be a bool instead?
-        "true",
-        title="Galaxy Workflow",
-        description="Whether this workflow is a Galaxy Workflow."
+        "true", title="Galaxy Workflow", description="Whether this workflow is a Galaxy Workflow."
     )
     format_version: str = Field(
         "0.1",
         alias="format-version",  # why this field uses `-` instead of `_`?
         title="Galaxy Workflow",
-        description="Whether this workflow is a Galaxy Workflow."
-    )
-    name: str = Field(
-        ...,
-        title="Name",
-        description="The name of the workflow."
+        description="Whether this workflow is a Galaxy Workflow.",
     )
+    name: str = Field(..., title="Name", description="The name of the workflow.")
     annotation: Optional[str] = AnnotationField
     tags: TagCollection
     uuid: Optional[UUID4] = Field(
         None,
         title="UUID",
         description="Universal unique identifier of the workflow.",
     )
     creator: Optional[List[Union[Person, Organization]]] = Field(
         None,
         title="Creator",
-        description=(
-            "Additional information about the creator (or multiple creators) of this workflow."
-        )
+        description=("Additional information about the creator (or multiple creators) of this workflow."),
     )
     license: Optional[str] = Field(
-        None,
-        title="License",
-        description="SPDX Identifier of the license associated with this workflow."
+        None, title="License", description="SPDX Identifier of the license associated with this workflow."
     )
     version: int = Field(
-        ...,
-        title="Version",
-        description="The version of the workflow represented by an incremental number."
+        ..., title="Version", description="The version of the workflow represented by an incremental number."
     )
     steps: Dict[int, Union[SubworkflowStepToExport, WorkflowToolStepToExport, WorkflowStepToExport]] = Field(
-        {},
-        title="Steps",
-        description="A dictionary with information about all the steps of the workflow."
+        {}, title="Steps", description="A dictionary with information about all the steps of the workflow."
     )
 
 
 # Roles -----------------------------------------------------------------
 
 RoleIdField = Field(title="ID", description="Encoded ID of the role")
 RoleNameField = Field(title="Name", description="Name of the role")
 RoleDescriptionField = Field(title="Description", description="Description of the role")
 
 
-class BasicRoleModel(BaseModel):
-    id: EncodedDatabaseIdField = RoleIdField
+class BasicRoleModel(Model):
+    id: DecodedDatabaseIdField = RoleIdField
     name: str = RoleNameField
     type: str = Field(title="Type", description="Type or category of the role")
 
 
 class RoleModel(BasicRoleModel):
     description: Optional[str] = RoleDescriptionField
-    url: str = Field(title="URL", description="URL for the role")
-    model_class: str = ModelClassField("Role")
+    url: RelativeUrl = RelativeUrlField
+    model_class: Literal["Role"] = ModelClassField("Role")
 
 
-class RoleDefinitionModel(BaseModel):
+class RoleDefinitionModel(Model):
     name: str = RoleNameField
     description: str = RoleDescriptionField
-    user_ids: Optional[List[EncodedDatabaseIdField]] = Field(title="User IDs", default=[])
-    group_ids: Optional[List[EncodedDatabaseIdField]] = Field(title="Group IDs", default=[])
+    user_ids: Optional[List[DecodedDatabaseIdField]] = Field(title="User IDs", default=[])
+    group_ids: Optional[List[DecodedDatabaseIdField]] = Field(title="Group IDs", default=[])
 
 
-class RoleListModel(BaseModel):
+class RoleListModel(Model):
     __root__: List[RoleModel]
 
 
 # The tuple should probably be another proper model instead?
 # Keeping it as a Tuple for now for backward compatibility
 # TODO: Use Tuple again when https://github.com/tiangolo/fastapi/issues/3665 is fixed upstream
-RoleNameIdTuple = List[str]  # Tuple[str, EncodedDatabaseIdField]
+RoleNameIdTuple = List[str]  # Tuple[str, DecodedDatabaseIdField]
 
 # Group_Roles -----------------------------------------------------------------
 
 
-class GroupRoleModel(BaseModel):
-    id: EncodedDatabaseIdField = RoleIdField
+class GroupRoleModel(Model):
+    id: DecodedDatabaseIdField = RoleIdField
     name: str = RoleNameField
     url: RelativeUrl = RelativeUrlField
 
 
-class GroupRoleListModel(BaseModel):
+class GroupRoleListModel(Model):
     __root__: List[GroupRoleModel]
 
+
+# Users -----------------------------------------------------------------
+
+UserIdField = Field(title="ID", description="Encoded ID of the user")
+UserEmailField = Field(title="Email", description="Email of the user")
+UserDescriptionField = Field(title="Description", description="Description of the user")
+
+# Group_Users -----------------------------------------------------------------
+
+
+class GroupUserModel(Model):
+    id: DecodedDatabaseIdField = UserIdField
+    email: str = UserEmailField
+    url: RelativeUrl = RelativeUrlField
+
+
+class GroupUserListModel(Model):
+    __root__: List[GroupUserModel]
+
+
+class ImportToolDataBundleUriSource(Model):
+    src: Literal["uri"] = Field(title="src", description="Indicates that the tool data should be resolved by a URI.")
+    uri: str = Field(
+        title="uri",
+        description="URI to fetch tool data bundle from (file:// URIs are fine because this is an admin-only operation)",
+    )
+
+
+class ImportToolDataBundleDatasetSource(Model):
+    src: Literal["hda", "ldda"] = Field(
+        title="src", description="Indicates that the tool data should be resolved from a dataset."
+    )
+    id: DecodedDatabaseIdField = EntityIdField
+
+
+ImportToolDataBundleSource = Union[ImportToolDataBundleDatasetSource, ImportToolDataBundleUriSource]
+
+
+class ToolShedRepository(Model):
+    tool_shed_url: str = Field(
+        title="Tool Shed URL", default="https://toolshed.g2.bx.psu.edu/", description="Tool Shed target"
+    )
+    name: str = Field(title="Name", description="Name of repository")
+    owner: str = Field(title="Owner", description="Owner of repository")
+
+
+class ToolShedRepositoryChangeset(ToolShedRepository):
+    changeset_revision: str
+
+
+class InstalledRepositoryToolShedStatus(Model):
+    # See https://github.com/galaxyproject/galaxy/issues/10453 , bad booleans
+    # See https://github.com/galaxyproject/galaxy/issues/16135 , optional fields
+    latest_installable_revision: Optional[str] = Field(
+        title="Latest installed revision", description="Most recent version available on the tool shed"
+    )
+    revision_update: str
+    revision_upgrade: Optional[str]
+    repository_deprecated: Optional[str] = Field(
+        title="Repository deprecated", description="Repository has been depreciated on the tool shed"
+    )
+
+
+class InstalledToolShedRepository(Model):
+    model_class: Literal["ToolShedRepository"] = ModelClassField("ToolShedRepository")
+    id: EncodedDatabaseIdField = Field(
+        ...,
+        title="ID",
+        description="Encoded ID of the install tool shed repository.",
+    )
+    status: str
+    name: str = Field(title="Name", description="Name of repository")
+    owner: str = Field(title="Owner", description="Owner of repository")
+    deleted: bool
+    # This should be an int... but it would break backward compatiblity. Probably switch it at some point anyway?
+    ctx_rev: Optional[str] = Field(
+        title="Changeset revision number",
+        description="The linearized 0-based index of the changeset on the tool shed (0, 1, 2,...)",
+    )
+    error_message: str = Field("Installation error message, the empty string means no error was recorded")
+    installed_changeset_revision: str = Field(
+        title="Installed changeset revision",
+        description="Initially installed changeset revision. Used to construct path to repository within Galaxies filesystem. Does not change if a repository is updated.",
+    )
+    tool_shed: str = Field(title="Tool shed", description="Hostname of the tool shed this was installed from")
+    dist_to_shed: bool
+    uninstalled: bool
+    changeset_revision: str = Field(
+        title="Changeset revision", description="Changeset revision of the repository - a mercurial commit hash"
+    )
+    tool_shed_status: Optional[InstalledRepositoryToolShedStatus] = Field(
+        title="Latest updated status from the tool shed"
+    )
+
+
+class InstalledToolShedRepositories(Model):
+    __root__: List[InstalledToolShedRepository]
+
+
+CheckForUpdatesResponseStatusT = Literal["ok", "error"]
+
+
+class CheckForUpdatesResponse(Model):
+    status: CheckForUpdatesResponseStatusT = Field(title="Status", description="'ok' or 'error'")
+    message: str = Field(
+        title="Message", description="Unstructured description of tool shed updates discovered or failure"
+    )
+
+
 # Libraries -----------------------------------------------------------------
 
 
 class LibraryPermissionScope(str, Enum):
     current = "current"
     available = "available"
 
 
-class LibraryLegacySummary(BaseModel):
-    model_class: str = ModelClassField("Library")
-    id: EncodedDatabaseIdField = Field(
+class LibraryLegacySummary(Model):
+    model_class: Literal["Library"] = ModelClassField("Library")
+    id: DecodedDatabaseIdField = Field(
         ...,
         title="ID",
         description="Encoded ID of the Library.",
     )
     name: str = Field(
         ...,
         title="Name",
@@ -2143,15 +2390,15 @@
         description="A detailed description of the Library.",
     )
     synopsis: Optional[str] = Field(
         None,
         title="Description",
         description="A short text describing the contents of the Library.",
     )
-    root_folder_id: EncodedDatabaseIdField = Field(
+    root_folder_id: LibraryFolderDatabaseIdField = Field(
         ...,
         title="Root Folder ID",
         description="Encoded ID of the Library's base folder.",
     )
     create_time: datetime = Field(
         ...,
         title="Create Time",
@@ -2189,22 +2436,22 @@
     can_user_manage: bool = Field(
         ...,
         title="Can User Manage",
         description="Whether the current user can manage the Library and its contents.",
     )
 
 
-class LibrarySummaryList(BaseModel):
+class LibrarySummaryList(Model):
     __root__: List[LibrarySummary] = Field(
         default=[],
-        title='List with summary information of Libraries.',
+        title="List with summary information of Libraries.",
     )
 
 
-class CreateLibraryPayload(BaseModel):
+class CreateLibraryPayload(Model):
     name: str = Field(
         ...,
         title="Name",
         description="The name of the Library.",
     )
     description: Optional[str] = Field(
         "",
@@ -2214,15 +2461,19 @@
     synopsis: Optional[str] = Field(
         "",
         title="Synopsis",
         description="A short text describing the contents of the Library.",
     )
 
 
-class UpdateLibraryPayload(BaseModel):
+class CreateLibrariesFromStore(StoreContentSource):
+    pass
+
+
+class UpdateLibraryPayload(Model):
     name: Optional[str] = Field(
         None,
         title="Name",
         description="The new name of the Library. Leave unset to keep the existing.",
     )
     description: Optional[str] = Field(
         None,
@@ -2232,23 +2483,23 @@
     synopsis: Optional[str] = Field(
         None,
         title="Synopsis",
         description="A short text describing the contents of the Library. Leave unset to keep the existing.",
     )
 
 
-class DeleteLibraryPayload(BaseModel):
+class DeleteLibraryPayload(Model):
     undelete: bool = Field(
         ...,
         title="Undelete",
         description="Whether to restore this previously deleted library.",
     )
 
 
-class LibraryCurrentPermissions(BaseModel):
+class LibraryCurrentPermissions(Model):
     access_library_role_list: List[RoleNameIdTuple] = Field(
         ...,
         title="Access Role List",
         description="A list containing pairs of role names and corresponding encoded IDs which have access to the Library.",
     )
     modify_library_role_list: List[RoleNameIdTuple] = Field(
         ...,
@@ -2263,18 +2514,20 @@
     add_library_item_role_list: List[RoleNameIdTuple] = Field(
         ...,
         title="Add Role List",
         description="A list containing pairs of role names and corresponding encoded IDs which can add items to the Library.",
     )
 
 
-RoleIdList = Union[List[EncodedDatabaseIdField], EncodedDatabaseIdField]  # Should we support just List[EncodedDatabaseIdField] in the future?
+RoleIdList = Union[
+    List[DecodedDatabaseIdField], DecodedDatabaseIdField
+]  # Should we support just List[DecodedDatabaseIdField] in the future?
 
 
-class LegacyLibraryPermissionsPayload(BaseModel):
+class LegacyLibraryPermissionsPayload(Model):
     LIBRARY_ACCESS_in: Optional[RoleIdList] = Field(
         [],
         title="Access IDs",
         description="A list of role encoded IDs defining roles that should have access permission on the library.",
     )
     LIBRARY_MODIFY_in: Optional[RoleIdList] = Field(
         [],
@@ -2337,14 +2590,15 @@
         title="Access IDs",
         description="A list of role encoded IDs defining roles that should have access permission on the library.",
     )
 
 
 # Library Folders -----------------------------------------------------------------
 
+
 class LibraryFolderPermissionAction(str, Enum):
     set_permissions = "set_permissions"
 
 
 FolderNameField: str = Field(
     ...,
     title="Name",
@@ -2361,34 +2615,34 @@
     action: Optional[LibraryFolderPermissionAction] = Field(
         None,
         title="Action",
         description="Indicates what action should be performed on the library folder.",
     )
 
 
-class LibraryFolderDetails(BaseModel):
-    model_class: str = ModelClassField("LibraryFolder")
-    id: EncodedDatabaseIdField = Field(
+class LibraryFolderDetails(Model):
+    model_class: Literal["LibraryFolder"] = ModelClassField("LibraryFolder")
+    id: LibraryFolderDatabaseIdField = Field(
         ...,
         title="ID",
         description="Encoded ID of the library folder.",
     )
     name: str = FolderNameField
     description: Optional[str] = FolderDescriptionField
     item_count: int = Field(
         ...,
         title="Item Count",
         description="A detailed description of the library folder.",
     )
-    parent_library_id: EncodedDatabaseIdField = Field(
+    parent_library_id: DecodedDatabaseIdField = Field(
         ...,
         title="Parent Library ID",
         description="Encoded ID of the Library this folder belongs to.",
     )
-    parent_id: Optional[EncodedDatabaseIdField] = Field(
+    parent_id: Optional[LibraryFolderDatabaseIdField] = Field(
         None,
         title="Parent Folder ID",
         description="Encoded ID of the parent folder. Empty if it's the root folder.",
     )
     genome_build: Optional[str] = GenomeBuildField
     update_time: datetime = UpdateTimeField
     deleted: bool = Field(
@@ -2399,33 +2653,33 @@
     library_path: List[str] = Field(
         [],
         title="Path",
         description="The list of folder names composing the path to this folder.",
     )
 
 
-class CreateLibraryFolderPayload(BaseModel):
+class CreateLibraryFolderPayload(Model):
     name: str = FolderNameField
     description: Optional[str] = FolderDescriptionField
 
 
-class UpdateLibraryFolderPayload(BaseModel):
+class UpdateLibraryFolderPayload(Model):
     name: Optional[str] = Field(
         default=None,
         title="Name",
         description="The new name of the library folder.",
     )
     description: Optional[str] = Field(
         default=None,
         title="Description",
         description="The new description of the library folder.",
     )
 
 
-class LibraryAvailablePermissions(BaseModel):
+class LibraryAvailablePermissions(Model):
     roles: List[BasicRoleModel] = Field(
         ...,
         title="Roles",
         description="A list available roles that can be assigned to a particular permission.",
     )
     page: int = Field(
         ...,
@@ -2440,15 +2694,15 @@
     total: int = Field(
         ...,
         title="Total",
         description="Total number of items",
     )
 
 
-class LibraryFolderCurrentPermissions(BaseModel):
+class LibraryFolderCurrentPermissions(Model):
     modify_folder_role_list: List[RoleNameIdTuple] = Field(
         ...,
         title="Modify Role List",
         description="A list containing pairs of role names and corresponding encoded IDs which can modify the Library folder.",
     )
     manage_folder_role_list: List[RoleNameIdTuple] = Field(
         ...,
@@ -2458,14 +2712,95 @@
     add_library_item_role_list: List[RoleNameIdTuple] = Field(
         ...,
         title="Add Role List",
         description="A list containing pairs of role names and corresponding encoded IDs which can add items to the Library folder.",
     )
 
 
+LibraryFolderContentsIndexSortByEnum = Literal["name", "description", "type", "size", "update_time"]
+
+
+class LibraryFolderContentsIndexQueryPayload(Model):
+    limit: int = 10
+    offset: int = 0
+    search_text: Optional[str] = None
+    include_deleted: Optional[bool] = None
+    order_by: LibraryFolderContentsIndexSortByEnum = "name"
+    sort_desc: Optional[bool] = False
+
+
+class LibraryFolderItemBase(Model):
+    id: DecodedDatabaseIdField
+    name: str
+    type: str
+    create_time: datetime = CreateTimeField
+    update_time: datetime = UpdateTimeField
+    can_manage: bool
+    deleted: bool
+
+
+class FolderLibraryFolderItem(LibraryFolderItemBase):
+    type: Literal["folder"]
+    can_modify: bool
+    description: Optional[str] = FolderDescriptionField
+
+
+class FileLibraryFolderItem(LibraryFolderItemBase):
+    type: Literal["file"]
+    file_ext: str
+    date_uploaded: datetime
+    is_unrestricted: bool
+    is_private: bool
+    state: Dataset.states = DatasetStateField
+    file_size: str
+    raw_size: int
+    ldda_id: DecodedDatabaseIdField
+    tags: str
+    message: Optional[str]
+
+
+AnyLibraryFolderItem = Annotated[Union[FileLibraryFolderItem, FolderLibraryFolderItem], Field(discriminator="type")]
+
+
+class LibraryFolderMetadata(Model):
+    parent_library_id: DecodedDatabaseIdField
+    folder_name: str
+    folder_description: str
+    total_rows: int
+    can_modify_folder: bool
+    can_add_library_item: bool
+    full_path: List[List[str]]
+
+
+class LibraryFolderContentsIndexResult(Model):
+    metadata: LibraryFolderMetadata
+    folder_contents: List[AnyLibraryFolderItem]
+
+
+class CreateLibraryFilePayload(Model):
+    from_hda_id: Optional[DecodedDatabaseIdField] = Field(
+        default=None,
+        title="From HDA ID",
+        description="The ID of an accessible HDA to copy into the library.",
+    )
+    from_hdca_id: Optional[DecodedDatabaseIdField] = Field(
+        default=None,
+        title="From HDCA ID",
+        description=(
+            "The ID of an accessible HDCA to copy into the library. "
+            "Nested collections are not allowed, you must flatten the collection first."
+        ),
+    )
+    ldda_message: Optional[str] = Field(
+        default="",
+        title="LDDA Message",
+        description="The new message attribute of the LDDA created.",
+    )
+
+
 class DatasetAssociationRoles(Model):
     access_dataset_roles: List[RoleNameIdTuple] = Field(
         default=[],
         title="Access Roles",
         description=(
             "A list of roles that can access the dataset. "
             "The user has to **have all these roles** in order to access this dataset. "
@@ -2520,54 +2855,65 @@
 
 class CustomHistoryItem(Model):
     """Can contain any serializable property of the item.
 
     Allows arbitrary custom keys to be specified in the serialization
     parameters without a particular view (predefined set of keys).
     """
+
     class Config:
         extra = Extra.allow
 
 
-AnyHDA = Union[HDABeta, HDADetailed, HDASummary]
-AnyHDCA = Union[HDCABeta, HDCADetailed, HDCASummary]
+AnyHDA = Union[HDADetailed, HDASummary]
+AnyHDCA = Union[HDCADetailed, HDCASummary]
 AnyHistoryContentItem = Union[
     AnyHDA,
     AnyHDCA,
     CustomHistoryItem,
 ]
 
 
-AnyJobStateSummary = Union[
-    JobStateSummary,
-    ImplicitCollectionJobsStateSummary,
-    WorkflowInvocationStateSummary,
+AnyJobStateSummary = Annotated[
+    Union[
+        JobStateSummary,
+        ImplicitCollectionJobsStateSummary,
+        WorkflowInvocationStateSummary,
+    ],
+    Field(..., discriminator="model"),
 ]
 
+
 HistoryArchiveExportResult = Union[JobExportHistoryArchiveModel, JobIdResponse]
 
 
-class DeleteHistoryContentPayload(BaseModel):
+class DeleteHistoryContentPayload(Model):
     purge: bool = Field(
         default=False,
         title="Purge",
         description="Whether to remove the dataset from storage. Datasets will only be removed from storage once all HDAs or LDDAs that refer to this datasets are deleted.",
     )
     recursive: bool = Field(
         default=False,
         title="Recursive",
         description="When deleting a dataset collection, whether to also delete containing datasets.",
     )
+    stop_job: bool = Field(
+        default=False,
+        title="Stop Job",
+        description="Whether to stop the creating job if all the job's outputs are deleted.",
+    )
 
 
 class DeleteHistoryContentResult(CustomHistoryItem):
     """Contains minimum information about the deletion state of a history item.
 
     Can also contain any other properties of the item."""
-    id: EncodedDatabaseIdField = Field(
+
+    id: DecodedDatabaseIdField = Field(
         ...,
         title="ID",
         description="The encoded ID of the history item.",
     )
     deleted: bool = Field(
         ...,
         title="Deleted",
@@ -2576,94 +2922,93 @@
     purged: Optional[bool] = Field(
         default=None,
         title="Purged",
         description="True if the item was successfully removed from disk.",
     )
 
 
-class HistoryContentsArchiveDryRunResult(BaseModel):
+class HistoryContentsArchiveDryRunResult(Model):
     """
     Contains a collection of filepath/filename entries that represent
     the contents that would have been included in the archive.
     This is returned when the `dry_run` flag is active when
     creating an archive with the contents of the history.
 
     This is used for debugging purposes.
     """
+
     # TODO: Use Tuple again when https://github.com/tiangolo/fastapi/issues/3665 is fixed upstream
     __root__: List[List[str]]  # List[Tuple[str, str]]
 
 
-class ContentsNearStats(BaseModel):
-    """Stats used by the `contents_near` endpoint."""
-    matches: int
-    matches_up: int
-    matches_down: int
-    total_matches: int
-    total_matches_up: int
-    total_matches_down: int
-    max_hid: Optional[int] = None
-    min_hid: Optional[int] = None
-    history_size: int
-    history_empty: bool
-
-    def to_headers(self) -> Dict[str, str]:
-        """Converts all field values to json strings.
-
-        The headers values need to be json strings or updating the response
-        headers will raise encoding errors."""
-        headers = {}
-        for key, val in self:
-            headers[key] = json.dumps(val)
-        return headers
+class HistoryContentStats(Model):
+    total_matches: int = Field(
+        ...,
+        title="Total Matches",
+        description=("The total number of items that match the search query without any pagination"),
+    )
 
 
 class HistoryContentsResult(Model):
-    """Collection of history content items.
+    """List of history content items.
     Can contain different views and kinds of items.
     """
+
     __root__: List[AnyHistoryContentItem]
 
 
-class ContentsNearResult(BaseModel):
-    contents: HistoryContentsResult
-    stats: ContentsNearStats
+class HistoryContentsWithStatsResult(Model):
+    """Includes stats with items counting"""
 
+    stats: HistoryContentStats = Field(
+        ...,
+        title="Stats",
+        description=("Contains counting stats for the query."),
+    )
+    contents: List[AnyHistoryContentItem] = Field(
+        ...,
+        title="Contents",
+        description=(
+            "The items matching the search query. Only the items fitting in the current page limit will be returned."
+        ),
+    )
 
-# Sharing -----------------------------------------------------------------
+    # This field is ignored and contains the content type associated with this model
+    __accept_type__ = "application/vnd.galaxy.history.contents.stats+json"
 
 
+# Sharing -----------------------------------------------------------------
 class SharingOptions(str, Enum):
     """Options for sharing resources that may have restricted access to all or part of their contents."""
+
     make_public = "make_public"
     make_accessible_to_shared = "make_accessible_to_shared"
     no_changes = "no_changes"
 
 
-class ShareWithExtra(BaseModel):
+class ShareWithExtra(Model):
     can_share: bool = Field(
         False,
         title="Can Share",
         description="Indicates whether the resource can be directly shared or requires further actions.",
     )
 
     class Config:
         extra = Extra.allow
 
 
-UserIdentifier = Union[EncodedDatabaseIdField, str]
+UserIdentifier = Union[DecodedDatabaseIdField, str]
 
 
-class ShareWithPayload(BaseModel):
+class ShareWithPayload(Model):
     user_ids: List[UserIdentifier] = Field(
         ...,
         title="User Identifiers",
         description=(
-            "A collection of encoded IDs (or email addresses) of users "
-            "that this resource will be shared with."
+            "A collection of encoded IDs (or email addresses) of users " "that this resource will be shared with."
         ),
     )
     share_option: Optional[SharingOptions] = Field(
         None,
         title="Share Option",
         description=(
             "User choice for sharing resources which its contents may be restricted:\n"
@@ -2671,37 +3016,45 @@
             f" - {SharingOptions.make_public}: The contents of the resource will be made publicly accessible.\n"
             f" - {SharingOptions.make_accessible_to_shared}: This will automatically create a new `sharing role` allowing protected contents to be accessed only by the desired users.\n"
             f" - {SharingOptions.no_changes}: This won't change the current permissions for the contents. The user which this resource will be shared may not be able to access all its contents.\n"
         ),
     )
 
 
-class SetSlugPayload(BaseModel):
+class SetSlugPayload(Model):
     new_slug: str = Field(
         ...,
         title="New Slug",
         description="The slug that will be used to access this shared item.",
     )
 
 
-class UserEmail(BaseModel):
-    id: EncodedDatabaseIdField = Field(
+class UserEmail(Model):
+    id: DecodedDatabaseIdField = Field(
         ...,
         title="User ID",
         description="The encoded ID of the user.",
     )
     email: str = Field(
         ...,
         title="Email",
         description="The email of the user.",
     )
 
 
-class SharingStatus(BaseModel):
-    id: EncodedDatabaseIdField = Field(
+class UserBeaconSetting(Model):
+    enabled: bool = Field(
+        ...,
+        title="Enabled",
+        description="True if beacon sharing is enabled",
+    )
+
+
+class SharingStatus(Model):
+    id: DecodedDatabaseIdField = Field(
         ...,
         title="ID",
         description="The encoded ID of the resource to be shared.",
     )
     title: str = Field(
         ...,
         title="Title",
@@ -2718,14 +3071,24 @@
         description="Whether this resource is currently published.",
     )
     users_shared_with: List[UserEmail] = Field(
         [],
         title="Users shared with",
         description="The list of encoded ids for users the resource has been shared.",
     )
+    email_hash: Optional[str] = Field(
+        None,
+        title="Encoded Email",
+        description="Encoded owner email.",
+    )
+    username: Optional[str] = Field(
+        None,
+        title="Username",
+        description="The owner's username.",
+    )
     username_and_slug: Optional[str] = Field(
         None,
         title="Username and slug",
         description="The relative URL in the form of /u/{username}/{resource_single_char}/{slug}",
     )
 
 
@@ -2741,16 +3104,16 @@
         description=(
             "Optional extra information about this shareable resource that may be of interest. "
             "The contents of this field depend on the particular resource."
         ),
     )
 
 
-class HDABasicInfo(BaseModel):
-    id: EncodedDatabaseIdField
+class HDABasicInfo(Model):
+    id: DecodedDatabaseIdField
     name: str
 
 
 class ShareHistoryExtra(ShareWithExtra):
     can_change: List[HDABasicInfo] = Field(
         [],
         title="Can Change",
@@ -2766,19 +3129,18 @@
             "A collection of datasets that are not accessible by one or more of the target users "
             "and that cannot be made accessible for others by the user sharing the history."
         ),
     )
     accessible_count: int = Field(
         0,
         title="Accessible Count",
-        description=(
-            "The number of datasets in the history that are public or accessible by all the target users."
-        ),
+        description=("The number of datasets in the history that are public or accessible by all the target users."),
     )
 
+
 # Pages -------------------------------------------------------
 
 
 class PageContentFormat(str, Enum):
     markdown = "markdown"
     html = "html"
 
@@ -2792,64 +3154,111 @@
 ContentField: Optional[str] = Field(
     default="",
     title="Content",
     description="Raw text contents of the first page revision (type dependent on content_format).",
 )
 
 
-class PageSummaryBase(BaseModel):
+class PageSummaryBase(Model):
     title: str = Field(
         ...,  # Required
         title="Title",
         description="The name of the page",
     )
     slug: str = Field(
         ...,  # Required
         title="Identifier",
         description="The title slug for the page URL, must be unique.",
         regex=r"^[a-z0-9\-]+$",
     )
 
 
+class MaterializeDatasetInstanceAPIRequest(Model):
+    source: DatasetSourceType = Field(
+        None,
+        title="Source",
+        description="The source of the content. Can be other history element to be copied or library elements.",
+    )
+    content: DecodedDatabaseIdField = Field(
+        None,
+        title="Content",
+        description=(
+            "Depending on the `source` it can be:\n"
+            "- The encoded id of the source library dataset\n"
+            "- The encoded id of the the HDA\n"
+        ),
+    )
+
+
+class MaterializeDatasetInstanceRequest(MaterializeDatasetInstanceAPIRequest):
+    history_id: DecodedDatabaseIdField
+
+
 class CreatePagePayload(PageSummaryBase):
     content_format: PageContentFormat = ContentFormatField
     content: Optional[str] = ContentField
     annotation: Optional[str] = Field(
         default=None,
         title="Annotation",
         description="Annotation that will be attached to the page.",
     )
-    invocation_id: Optional[EncodedDatabaseIdField] = Field(
+    invocation_id: Optional[DecodedDatabaseIdField] = Field(
         None,
         title="Workflow invocation ID",
         description="Encoded ID used by workflow generated reports.",
     )
 
     class Config:
         use_enum_values = True  # When using .dict()
         extra = Extra.allow  # Allow any other extra fields
 
 
+class AsyncTaskResultSummary(Model):
+    id: str = Field(
+        ...,
+        title="ID",
+        description="Celery AsyncResult ID for this task",
+    )
+    ignored: bool = Field(
+        ...,
+        title="Ignored",
+        description="Indicated whether the Celery AsyncResult will be available for retrieval",
+    )
+    name: Optional[str] = Field(
+        None,
+        title="Name of task being done derived from Celery AsyncResult",
+    )
+    queue: Optional[str] = Field(
+        None,
+        title="Queue of task being done derived from Celery AsyncResult",
+    )
+
+
+class AsyncFile(Model):
+    storage_request_id: UUID
+    task: AsyncTaskResultSummary
+
+
 class PageSummary(PageSummaryBase):
-    id: EncodedDatabaseIdField = Field(
+    id: DecodedDatabaseIdField = Field(
         ...,  # Required
         title="ID",
         description="Encoded ID of the Page.",
     )
-    model_class: str = Field(
-        ...,  # Required
-        title="Model class",
-        description="The class of the model associated with the ID.",
-        example="Page",
-    )
+    model_class: PAGE_MODEL_CLASS = ModelClassField(PAGE_MODEL_CLASS)
     username: str = Field(
         ...,  # Required
         title="Username",
         description="The name of the user owning this Page.",
     )
+    email_hash: str = Field(
+        ...,  # Required
+        title="Encoded email",
+        description="The encoded email of the user",
+    )
     published: bool = Field(
         ...,  # Required
         title="Published",
         description="Whether this Page has been published.",
     )
     importable: bool = Field(
         ...,  # Required
@@ -2857,20 +3266,20 @@
         description="Whether this Page can be imported.",
     )
     deleted: bool = Field(
         ...,  # Required
         title="Deleted",
         description="Whether this Page has been deleted.",
     )
-    latest_revision_id: EncodedDatabaseIdField = Field(
+    latest_revision_id: DecodedDatabaseIdField = Field(
         ...,  # Required
         title="Latest revision ID",
         description="The encoded ID of the last revision of this Page.",
     )
-    revision_ids: List[EncodedDatabaseIdField] = Field(
+    revision_ids: List[DecodedDatabaseIdField] = Field(
         ...,  # Required
         title="List of revisions",
         description="The history with the encoded ID of each revision of the Page.",
     )
 
 
 class PageDetails(PageSummary):
@@ -2887,12 +3296,12 @@
         description="The date this page was generated.",
     )
 
     class Config:
         extra = Extra.allow  # Allow any other extra fields
 
 
-class PageSummaryList(BaseModel):
+class PageSummaryList(Model):
     __root__: List[PageSummary] = Field(
         default=[],
-        title='List with summary information of Pages.',
+        title="List with summary information of Pages.",
     )
```

### Comparing `galaxy-data-22.1.1/galaxy/security/__init__.py` & `galaxy-data-23.0.1/galaxy/security/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,45 +1,77 @@
 """
 Galaxy Security
 
 """
+from typing import (
+    List,
+    Optional,
+)
+
+from typing_extensions import Literal
+
 from galaxy.util.bunch import Bunch
 
+ActionModel = Literal["grant", "restrict"]
+
 
 class Action:
-    def __init__(self, action, description, model):
+    action: str
+    description: str
+    model: ActionModel
+
+    def __init__(self, action: str, description: str, model: ActionModel):
         self.action = action
         self.description = description
         self.model = model
 
 
 class RBACAgent:
     """Class that handles galaxy security"""
+
     permitted_actions = Bunch(
-        DATASET_MANAGE_PERMISSIONS=Action("manage permissions", "Users having associated role can manage the roles associated with permissions on this dataset.", "grant"),
-        DATASET_ACCESS=Action("access", "Users having associated role can import this dataset into their history for analysis.", "restrict"),
-        LIBRARY_ACCESS=Action("access library", "Restrict access to this library to only users having associated role", "restrict"),
-        LIBRARY_ADD=Action("add library item", "Users having associated role can add library items to this library item", "grant"),
-        LIBRARY_MODIFY=Action("modify library item", "Users having associated role can modify this library item", "grant"),
-        LIBRARY_MANAGE=Action("manage library permissions", "Users having associated role can manage roles associated with permissions on this library item", "grant")
+        DATASET_MANAGE_PERMISSIONS=Action(
+            "manage permissions",
+            "Users having associated role can manage the roles associated with permissions on this dataset.",
+            "grant",
+        ),
+        DATASET_ACCESS=Action(
+            "access",
+            "Users having associated role can import this dataset into their history for analysis.",
+            "restrict",
+        ),
+        LIBRARY_ACCESS=Action(
+            "access library", "Restrict access to this library to only users having associated role", "restrict"
+        ),
+        LIBRARY_ADD=Action(
+            "add library item", "Users having associated role can add library items to this library item", "grant"
+        ),
+        LIBRARY_MODIFY=Action(
+            "modify library item", "Users having associated role can modify this library item", "grant"
+        ),
+        LIBRARY_MANAGE=Action(
+            "manage library permissions",
+            "Users having associated role can manage roles associated with permissions on this library item",
+            "grant",
+        ),
     )
 
-    def get_action(self, name, default=None):
+    def get_action(self, name: str, default: Optional[Action] = None) -> Optional[Action]:
         """Get a permitted action by its dict key or action name"""
         for k, v in self.permitted_actions.items():
             if k == name or v.action == name:
                 return v
         return default
 
-    def get_actions(self):
+    def get_actions(self) -> List[Action]:
         """Get all permitted actions as a list of Action objects"""
         return list(self.permitted_actions.__dict__.values())
 
     def get_item_actions(self, action, item):
-        raise Exception(f'No valid method of retrieving action ({action}) for item {item}.')
+        raise Exception(f"No valid method of retrieving action ({action}) for item {item}.")
 
     def guess_derived_permissions_for_datasets(self, datasets=None):
         datasets = datasets or []
         raise Exception("Unimplemented Method")
 
     def can_access_dataset(self, roles, dataset):
         raise Exception("Unimplemented Method")
@@ -56,15 +88,15 @@
     def can_modify_library_item(self, roles, item):
         raise Exception("Unimplemented Method")
 
     def can_manage_library_item(self, roles, item):
         raise Exception("Unimplemented Method")
 
     def associate_components(self, **kwd):
-        raise Exception(f'No valid method of associating provided components: {kwd}')
+        raise Exception(f"No valid method of associating provided components: {kwd}")
 
     def create_private_user_role(self, user):
         raise Exception("Unimplemented Method")
 
     def get_private_user_role(self, user):
         raise Exception("Unimplemented Method")
 
@@ -130,17 +162,21 @@
         return bool(self.get_component_associations(**kwd))
 
     def convert_permitted_action_strings(self, permitted_action_strings):
         """
         When getting permitted actions from an untrusted source like a
         form, ensure that they match our actual permitted actions.
         """
-        return [_ for _ in [self.permitted_actions.get(action_string) for action_string in permitted_action_strings] if _ is not None]
+        return [
+            _
+            for _ in [self.permitted_actions.get(action_string) for action_string in permitted_action_strings]
+            if _ is not None
+        ]
 
 
 def get_permitted_actions(filter=None):
-    '''Utility method to return a subset of RBACAgent's permitted actions'''
+    """Utility method to return a subset of RBACAgent's permitted actions"""
     if filter is None:
         return RBACAgent.permitted_actions
     tmp_bunch = Bunch()
-    [tmp_bunch.__dict__.__setitem__(k, v) for k, v in RBACAgent.permitted_actions.items() if k.startswith(filter)]
+    [tmp_bunch.dict().__setitem__(k, v) for k, v in RBACAgent.permitted_actions.items() if k.startswith(filter)]
     return tmp_bunch
```

### Comparing `galaxy-data-22.1.1/galaxy/security/idencoding.py` & `galaxy-data-23.0.1/galaxy/security/idencoding.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,75 +1,79 @@
 import codecs
 import collections
 import logging
-from typing import Optional
+from typing import (
+    Optional,
+    Union,
+)
 
 from Crypto.Cipher import Blowfish
 from Crypto.Random import get_random_bytes
 
 import galaxy.exceptions
 from galaxy.util import (
     smart_str,
-    unicodify
+    unicodify,
 )
 
 log = logging.getLogger(__name__)
 
 MAXIMUM_ID_SECRET_BITS = 448
 MAXIMUM_ID_SECRET_LENGTH = int(MAXIMUM_ID_SECRET_BITS / 8)
-KIND_TOO_LONG_MESSAGE = "Galaxy coding error, keep encryption 'kinds' smaller to utilize more bites of randomness from id_secret values."
+KIND_TOO_LONG_MESSAGE = (
+    "Galaxy coding error, keep encryption 'kinds' smaller to utilize more bites of randomness from id_secret values."
+)
 
 
 class IdEncodingHelper:
-
     def __init__(self, **config):
-        id_secret = config['id_secret']
+        id_secret = config["id_secret"]
         self.id_secret = id_secret
         self.id_cipher = Blowfish.new(smart_str(self.id_secret), mode=Blowfish.MODE_ECB)
 
-        per_kind_id_secret_base = config.get('per_kind_id_secret_base', self.id_secret)
+        per_kind_id_secret_base = config.get("per_kind_id_secret_base", self.id_secret)
         self.id_ciphers_for_kind = _cipher_cache(per_kind_id_secret_base)
 
     def encode_id(self, obj_id, kind=None):
         if obj_id is None:
             raise galaxy.exceptions.MalformedId("Attempted to encode None id")
         id_cipher = self.__id_cipher(kind)
         # Convert to bytes
         s = smart_str(obj_id)
         # Pad to a multiple of 8 with leading "!"
         s = (b"!" * (8 - len(s) % 8)) + s
         # Encrypt
-        return unicodify(codecs.encode(id_cipher.encrypt(s), 'hex'))
+        return unicodify(codecs.encode(id_cipher.encrypt(s), "hex"))
 
     def encode_dict_ids(self, a_dict, kind=None, skip_startswith=None):
         """
         Encode all ids in dictionary. Ids are identified by (a) an 'id' key or
         (b) a key that ends with '_id'
         """
         for key, val in a_dict.items():
-            if key == 'id' or key.endswith('_id') and (skip_startswith is None or not key.startswith(skip_startswith)):
+            if key == "id" or key.endswith("_id") and (skip_startswith is None or not key.startswith(skip_startswith)):
                 a_dict[key] = self.encode_id(val, kind=kind)
 
         return a_dict
 
     def encode_all_ids(self, rval, recursive=False):
         """
         Encodes all integer values in the dict rval whose keys are 'id' or end
         with '_id' excluding `tool_id` which are consumed and produced as is
         via the API.
         """
         if not isinstance(rval, dict):
             return rval
         for k, v in rval.items():
-            if (k == 'id' or k.endswith('_id')) and v is not None and k not in ['tool_id', 'external_id']:
+            if (k == "id" or k.endswith("_id")) and v is not None and k not in ["tool_id", "external_id"]:
                 try:
                     rval[k] = self.encode_id(v)
                 except Exception:
                     pass  # probably already encoded
-            if (k.endswith("_ids") and isinstance(v, list)):
+            if k.endswith("_ids") and isinstance(v, list):
                 try:
                     o = []
                     for i in v:
                         o.append(self.encode_id(i))
                     rval[k] = o
                 except Exception:
                     pass
@@ -79,61 +83,66 @@
                 elif recursive and isinstance(v, list):
                     rval[k] = [self.encode_all_ids(el, True) for el in v]
         return rval
 
     def decode_id(self, obj_id, kind=None, object_name: Optional[str] = None):
         try:
             id_cipher = self.__id_cipher(kind)
-            return int(unicodify(id_cipher.decrypt(codecs.decode(obj_id, 'hex'))).lstrip("!"))
+            return int(unicodify(id_cipher.decrypt(codecs.decode(obj_id, "hex"))).lstrip("!"))
         except TypeError:
-            raise galaxy.exceptions.MalformedId(f"Malformed {object_name if object_name is not None else ''} id ( {obj_id} ) specified, unable to decode.")
+            raise galaxy.exceptions.MalformedId(
+                f"Malformed {object_name if object_name is not None else ''} id ( {obj_id} ) specified, unable to decode."
+            )
         except ValueError:
-            raise galaxy.exceptions.MalformedId(f"Wrong {object_name if object_name is not None else ''} id ( {obj_id} ) specified, unable to decode.")
+            raise galaxy.exceptions.MalformedId(
+                f"Wrong {object_name if object_name is not None else ''} id ( {obj_id} ) specified, unable to decode."
+            )
 
     def encode_guid(self, session_key):
         # Session keys are strings
         # Pad to a multiple of 8 with leading "!"
         session_key = smart_str(session_key)
         s = (b"!" * (8 - len(session_key) % 8)) + session_key
         # Encrypt
-        return codecs.encode(self.id_cipher.encrypt(s), 'hex')
+        return codecs.encode(self.id_cipher.encrypt(s), "hex")
 
-    def decode_guid(self, session_key):
+    def decode_guid(self, session_key: Union[bytes, str]) -> str:
         # Session keys are strings
         try:
             decoded_session_key = codecs.decode(session_key, "hex")
-            return unicodify(self.id_cipher.decrypt(decoded_session_key)).lstrip("!")
+            stripped_decoded_session_key = unicodify(self.id_cipher.decrypt(decoded_session_key)).lstrip("!")
+            # Ensure session key is hexadecimal value
+            int(stripped_decoded_session_key, 16)
+            return stripped_decoded_session_key
         except TypeError:
-            raise galaxy.exceptions.MalformedId(f"Malformed guid '{session_key}' specified, unable to decode.")
+            raise galaxy.exceptions.MalformedId(f"Malformed guid '{session_key!r}' specified, unable to decode.")
         except ValueError:
-            raise galaxy.exceptions.MalformedId(f"Wrong guid '{session_key}' specified, unable to decode.")
+            raise galaxy.exceptions.MalformedId(f"Wrong guid '{session_key!r}' specified, unable to decode.")
 
     def get_new_guid(self):
         # Generate a unique, high entropy 128 bit random number
-        return unicodify(codecs.encode(get_random_bytes(16), 'hex'))
+        return unicodify(codecs.encode(get_random_bytes(16), "hex"))
 
     def __id_cipher(self, kind):
         if not kind:
             id_cipher = self.id_cipher
         else:
             id_cipher = self.id_ciphers_for_kind[kind]
         return id_cipher
 
 
 class _cipher_cache(collections.defaultdict):
-
     def __init__(self, secret_base):
         self.secret_base = secret_base
 
     def __missing__(self, key):
         assert len(key) < 15, KIND_TOO_LONG_MESSAGE
         secret = f"{self.secret_base}__{key}"
         return Blowfish.new(_last_bits(secret), mode=Blowfish.MODE_ECB)
 
 
 def _last_bits(secret):
-    """We append the kind at the end, so just use the bits at the end.
-    """
+    """We append the kind at the end, so just use the bits at the end."""
     last_bits = smart_str(secret)
     if len(last_bits) > MAXIMUM_ID_SECRET_LENGTH:
         last_bits = last_bits[-MAXIMUM_ID_SECRET_LENGTH:]
     return last_bits
```

### Comparing `galaxy-data-22.1.1/galaxy/security/object_wrapper.py` & `galaxy-data-23.0.1/galaxy/security/object_wrapper.py`

 * *Files 2% similar despite different names*

```diff
@@ -41,62 +41,100 @@
 from galaxy.util import sanitize_lists_to_string as _sanitize_lists_to_string
 
 log = logging.getLogger(__name__)
 
 # Define different behaviors for different types, see also: https://docs.python.org/2/library/types.html
 
 # Known Callable types
-__CALLABLE_TYPES__ = (FunctionType, MethodType, GeneratorType, CodeType, BuiltinFunctionType, BuiltinMethodType, )
+__CALLABLE_TYPES__ = (
+    FunctionType,
+    MethodType,
+    GeneratorType,
+    CodeType,
+    BuiltinFunctionType,
+    BuiltinMethodType,
+)
 
 # Always wrap these types without attempting to subclass
-__WRAP_NO_SUBCLASS__ = (ModuleType, XRangeType, SliceType, BufferType, TracebackType, FrameType, DictProxyType,
-                        GetSetDescriptorType, MemberDescriptorType) + __CALLABLE_TYPES__
+__WRAP_NO_SUBCLASS__ = (
+    ModuleType,
+    XRangeType,
+    SliceType,
+    BufferType,
+    TracebackType,
+    FrameType,
+    DictProxyType,
+    GetSetDescriptorType,
+    MemberDescriptorType,
+) + __CALLABLE_TYPES__
 
 # Don't wrap or sanitize.
-__DONT_SANITIZE_TYPES__ = (Number, bool, NoneType, NotImplementedType, EllipsisType, bytearray, )
+__DONT_SANITIZE_TYPES__ = (
+    Number,
+    bool,
+    NoneType,
+    NotImplementedType,
+    EllipsisType,
+    bytearray,
+)
 
 # Wrap contents, but not the container
-__WRAP_SEQUENCES__ = (tuple, list, )
-__WRAP_SETS__ = (set, frozenset, )
-__WRAP_MAPPINGS__ = (dict, UserDict, )
+__WRAP_SEQUENCES__ = (
+    tuple,
+    list,
+)
+__WRAP_SETS__ = (
+    set,
+    frozenset,
+)
+__WRAP_MAPPINGS__ = (
+    dict,
+    UserDict,
+)
 
 
 # Define the set of characters that are not sanitized, and define a set of mappings for those that are.
 # characters that are valid
 VALID_CHARACTERS = set(f"{string.ascii_letters + string.digits} -=_.()/+*^,:?!@")
 
 # characters that are allowed but need to be escaped
-CHARACTER_MAP = {'>': '__gt__',
-                 '<': '__lt__',
-                 "'": '__sq__',
-                 '"': '__dq__',
-                 '[': '__ob__',
-                 ']': '__cb__',
-                 '{': '__oc__',
-                 '}': '__cc__',
-                 '\n': '__cn__',
-                 '\r': '__cr__',
-                 '\t': '__tc__',
-                 '#': '__pd__'}
+CHARACTER_MAP = {
+    ">": "__gt__",
+    "<": "__lt__",
+    "'": "__sq__",
+    '"': "__dq__",
+    "[": "__ob__",
+    "]": "__cb__",
+    "{": "__oc__",
+    "}": "__cc__",
+    "\n": "__cn__",
+    "\r": "__cr__",
+    "\t": "__tc__",
+    "#": "__pd__",
+}
 
 INVALID_CHARACTER = "X"
 
 
 def coerce(x, y):
     # __coerce__ doesn't do anything under Python anyway.
     return x
 
 
 def cmp(x, y):
     # Builtin in Python 2, but not Python 3.
     return (x > y) - (x < y)
 
 
-def sanitize_lists_to_string(values, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP, invalid_character=INVALID_CHARACTER):
-    return _sanitize_lists_to_string(values, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character)
+def sanitize_lists_to_string(
+    values, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP, invalid_character=INVALID_CHARACTER
+):
+    return _sanitize_lists_to_string(
+        values, valid_characters=valid_characters, character_map=character_map, invalid_character=invalid_character
+    )
 
 
 def wrap_with_safe_string(value, no_wrap_classes=None):
     """
     Recursively wrap values that should be wrapped.
     """
 
@@ -126,37 +164,53 @@
             wrapped_class = value
         except Exception:
             wrapped_class_name = value.__class__.__name__
             wrapped_class = value.__class__
         value_mod = inspect.getmodule(value)
         if value_mod:
             wrapped_class_name = f"{value_mod.__name__}.{wrapped_class_name}"
-        wrapped_class_name = f"SafeStringWrapper({wrapped_class_name}:{','.join(sorted(map(str, no_wrap_classes)))})"
+        wrapped_class_name = (
+            f"SafeStringWrapper__{wrapped_class_name}__{'__'.join(sorted(c.__name__ for c in no_wrap_classes))}"
+        )
         do_wrap_func_name = f"__do_wrap_{wrapped_class_name}"
         do_wrap_func = __do_wrap
         global_dict = globals()
         if wrapped_class_name in global_dict:
             # Check to see if we have created a wrapper for this class yet, if so, reuse
             wrapped_class = global_dict.get(wrapped_class_name)
             do_wrap_func = global_dict.get(do_wrap_func_name, __do_wrap)
         else:
             try:
-                wrapped_class = type(wrapped_class_name, (safe_class, wrapped_class, ), {})
+                wrapped_class = type(
+                    wrapped_class_name,
+                    (
+                        safe_class,
+                        wrapped_class,
+                    ),
+                    {},
+                )
             except TypeError as e:
                 # Fail-safe for when a class cannot be dynamically subclassed.
                 log.warning(f"Unable to create dynamic subclass {wrapped_class_name} for {type(value)}, {value}: {e}")
-                wrapped_class = type(wrapped_class_name, (safe_class, ), {})
+                wrapped_class = type(wrapped_class_name, (safe_class,), {})
             if wrapped_class not in (SafeStringWrapper, CallableSafeStringWrapper):
                 # Save this wrapper for reuse and pickling/copying
                 global_dict[wrapped_class_name] = wrapped_class
                 do_wrap_func.__name__ = do_wrap_func_name
                 global_dict[do_wrap_func_name] = do_wrap_func
 
                 def pickle_safe_object(safe_object):
-                    return (wrapped_class, (safe_object.unsanitized, do_wrap_func, ))
+                    return (
+                        wrapped_class,
+                        (
+                            safe_object.unsanitized,
+                            do_wrap_func,
+                        ),
+                    )
+
                 # Set pickle and copy properties
                 copyreg.pickle(wrapped_class, pickle_safe_object, do_wrap_func)
         return wrapped_class(value, safe_string_wrapper_function=do_wrap_func)
 
     # Determine classes not to wrap
     if no_wrap_classes:
         if not isinstance(no_wrap_classes, (tuple, list)):
@@ -182,34 +236,39 @@
     method should be used.
 
     This wrapping occurs in a recursive/parasitic fashion, as all called attributes of
     the originally wrapped object will also be wrapped and sanitized, unless the attribute
     is of a type found in __DONT_SANITIZE_TYPES__ + __DONT_WRAP_TYPES__, where e.g. ~(strings
     will still be sanitized, but not wrapped), and e.g. integers will have neither.
     """
-    __UNSANITIZED_ATTRIBUTE_NAME__ = 'unsanitized'
-    __NO_WRAP_NAMES__ = ['__safe_string_wrapper_function__', '__class__', __UNSANITIZED_ATTRIBUTE_NAME__]
+
+    __UNSANITIZED_ATTRIBUTE_NAME__ = "unsanitized"
+    __NO_WRAP_NAMES__ = ["__safe_string_wrapper_function__", "__class__", __UNSANITIZED_ATTRIBUTE_NAME__]
 
     def __new__(cls, *arg, **kwd):
         # We need to define a __new__ since, we are subclassing from e.g. immutable str, which internally sets data
         # that will be used when other + this (this + other is handled by __add__)
         try:
-            sanitized_value = sanitize_lists_to_string(arg[0], valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP)
+            sanitized_value = sanitize_lists_to_string(
+                arg[0], valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP
+            )
             return super().__new__(cls, sanitized_value)
         except TypeError:
             # Class to be wrapped takes no parameters.
             # This is pefectly normal for mutable types.
             return super().__new__(cls)
 
     def __init__(self, value, safe_string_wrapper_function=wrap_with_safe_string):
         self.unsanitized = value
         self.__safe_string_wrapper_function__ = safe_string_wrapper_function
 
     def __str__(self):
-        return sanitize_lists_to_string(self.unsanitized, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP)
+        return sanitize_lists_to_string(
+            self.unsanitized, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP
+        )
 
     def __repr__(self):
         return f"{sanitize_lists_to_string(self.__class__.__name__, valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP)} object at {id(self):x} on: {sanitize_lists_to_string(repr(self.unsanitized), valid_characters=VALID_CHARACTERS, character_map=CHARACTER_MAP)}"
 
     def __lt__(self, other):
         while isinstance(other, SafeStringWrapper):
             other = other.unsanitized
@@ -248,14 +307,15 @@
     # Do not implement __rcmp__, python 2.2 < 2.6
 
     def __hash__(self):
         return hash(self.unsanitized)
 
     def __bool__(self):
         return bool(self.unsanitized)
+
     __nonzero__ = __bool__
 
     # Do not implement __unicode__, we will rely on __str__
 
     def __getattr__(self, name):
         if name in SafeStringWrapper.__NO_WRAP_NAMES__:
             # FIXME: is this ever reached?
@@ -279,15 +339,15 @@
         if name in SafeStringWrapper.__NO_WRAP_NAMES__:
             return object.__delattr__(self, name)
         return delattr(self.unsanitized, name)
 
     def __getattribute__(self, name):
         if name in SafeStringWrapper.__NO_WRAP_NAMES__:
             return object.__getattribute__(self, name)
-        return self.__safe_string_wrapper_function__(getattr(object.__getattribute__(self, 'unsanitized'), name))
+        return self.__safe_string_wrapper_function__(getattr(object.__getattribute__(self, "unsanitized"), name))
 
     # Skip Descriptors
 
     # Skip __slots__
 
     # Don't need to define a metaclass, we'll use the helper function to handle with subclassing for e.g. isinstance()
 
@@ -454,22 +514,21 @@
         return self.unsanitized.__enter__()
 
     def __exit__(self, *args):
         return self.unsanitized.__exit__(*args)
 
 
 class CallableSafeStringWrapper(SafeStringWrapper):
-
     def __call__(self, *args, **kwds):
         return self.__safe_string_wrapper_function__(self.unsanitized(*args, **kwds))
 
 
 # Enable pickling/deepcopy
 def pickle_SafeStringWrapper(safe_object):
-    args = (safe_object.unsanitized, )
+    args = (safe_object.unsanitized,)
     cls = SafeStringWrapper
     if isinstance(safe_object, CallableSafeStringWrapper):
         cls = CallableSafeStringWrapper
     return (cls, args)
 
 
 copyreg.pickle(SafeStringWrapper, pickle_SafeStringWrapper, wrap_with_safe_string)
```

### Comparing `galaxy-data-22.1.1/galaxy/security/passwords.py` & `galaxy-data-23.0.1/galaxy/security/passwords.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 import hashlib
 from base64 import b64encode
 from os import urandom
 
 from galaxy.util import (
     safe_str_cmp,
     smart_str,
-    unicodify
+    unicodify,
 )
 
 SALT_LENGTH = 12
 KEY_LENGTH = 24
-HASH_FUNCTION = 'sha256'
+HASH_FUNCTION = "sha256"
 COST_FACTOR = 100000
 
 
 def hash_password(password):
     """
     Hash a password, currently will use the PBKDF2 scheme.
     """
@@ -42,20 +42,20 @@
 def hash_password_PBKDF2(password):
     # Generate a random salt
     salt = b64encode(urandom(SALT_LENGTH))
     # Apply the pbkdf2 encoding
     hashed_password = pbkdf2_bin(password, salt, COST_FACTOR, KEY_LENGTH, HASH_FUNCTION)
     encoded_password = unicodify(b64encode(hashed_password))
     # Format
-    return f'PBKDF2${HASH_FUNCTION}${COST_FACTOR}${unicodify(salt)}${encoded_password}'
+    return f"PBKDF2${HASH_FUNCTION}${COST_FACTOR}${unicodify(salt)}${encoded_password}"
 
 
 def check_password_PBKDF2(guess, hashed):
     # Split the database representation to extract cost_factor and salt
-    name, hash_function, cost_factor, salt, encoded_original = hashed.split('$', 5)
+    name, hash_function, cost_factor, salt, encoded_original = hashed.split("$", 5)
     # Hash the guess using the same parameters
     hashed_guess = pbkdf2_bin(guess, salt, int(cost_factor), KEY_LENGTH, hash_function)
     encoded_guess = unicodify(b64encode(hashed_guess))
     return safe_str_cmp(encoded_original, encoded_guess)
 
 
 def pbkdf2_bin(data, salt, iterations=COST_FACTOR, keylen=KEY_LENGTH, hashfunc=HASH_FUNCTION):
```

### Comparing `galaxy-data-22.1.1/galaxy/security/ssh_util.py` & `galaxy-data-23.0.1/galaxy/security/ssh_util.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
     public_key_file: str
 
 
 def generate_ssh_keys() -> SSHKeys:
     """Returns a named tuple with private and public key and their paths."""
     key = RSA.generate(2048)
     private_key = key.export_key()
-    public_key = key.publickey().export_key(format='OpenSSH')
+    public_key = key.publickey().export_key(format="OpenSSH")
     with tempfile.NamedTemporaryFile(delete=False) as f:
         f.write(private_key)
         private_key_file = f.name
     with tempfile.NamedTemporaryFile(delete=False) as f:
         f.write(public_key)
         public_key_file = f.name
     return SSHKeys(private_key, public_key, private_key_file, public_key_file)
```

### Comparing `galaxy-data-22.1.1/galaxy/security/validate_user_input.py` & `galaxy-data-23.0.1/galaxy/security/validate_user_input.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,119 +3,155 @@
 
 The validate_* methods in this file return simple messages that do not contain
 user inputs - so these methods do not need to be escaped.
 """
 import logging
 import re
 
+import dns.resolver
+from dns.exception import DNSException
 from sqlalchemy import func
+from typing_extensions import LiteralString
 
 log = logging.getLogger(__name__)
 
 # Email validity parameters
-VALID_EMAIL_RE = re.compile(r"[^@]+@[^@]+\.[^@]+")
+#
+# Many words (and regexes) have been written about validating email addresses and there is no perfect answer on how it
+# should be done. We choose to use the HTML5 spec (and corresponding regex) that engages in a "willful violation" of RFC
+# 5322 to provide a reasonably good validation. Additionally, we allow Unicode characters in both the user and domain
+# parts of the email by using re's '\w' character. Note that \w includes "word" characters but appears to exclude emoji
+# characters, which should in fact be valid.
+#
+# https://html.spec.whatwg.org/multipage/input.html#e-mail-state-(type%3Demail)
+VALID_EMAIL_RE = re.compile(r"^[\w.!#$%&'*+\/=?^_`{|}~-]+@[\w](?:[\w-]{0,61}[\w])?(?:\.[\w](?:[\w-]{0,61}[\w])?)*$")
 EMAIL_MAX_LEN = 255
 
 # Public name validity parameters
-PUBLICNAME_MIN_LEN = 3
 PUBLICNAME_MAX_LEN = 255
 VALID_PUBLICNAME_RE = re.compile(r"^[a-z0-9._\-]+$")
 VALID_PUBLICNAME_SUB = re.compile(r"[^a-z0-9._\-]")
-FILL_CHAR = '-'
+FILL_CHAR = "-"
 
 # Password validity parameters
 PASSWORD_MIN_LEN = 6
 
 
 def validate_email_str(email):
     """Validates a string containing an email address."""
     if not email:
         return "No email address was provided."
-    if not(VALID_EMAIL_RE.match(email)):
+    if not (VALID_EMAIL_RE.match(email)):
         return "The format of the email address is not correct."
     elif len(email) > EMAIL_MAX_LEN:
         return "Email address cannot be more than %d characters in length." % EMAIL_MAX_LEN
     return ""
 
 
 def validate_password_str(password):
     if not password or len(password) < PASSWORD_MIN_LEN:
         return "Use a password of at least %d characters." % PASSWORD_MIN_LEN
     return ""
 
 
 def validate_publicname_str(publicname):
     """Validates a string containing a public username."""
-    if len(publicname) < PUBLICNAME_MIN_LEN:
-        return "Public name must be at least %d characters in length." % (PUBLICNAME_MIN_LEN)
+    if not publicname:
+        return "Public name cannot be empty"
     if len(publicname) > PUBLICNAME_MAX_LEN:
         return "Public name cannot be more than %d characters in length." % (PUBLICNAME_MAX_LEN)
-    if not(VALID_PUBLICNAME_RE.match(publicname)):
+    if not (VALID_PUBLICNAME_RE.match(publicname)):
         return "Public name must contain only lower-case letters, numbers, '.', '_' and '-'."
-    return ''
+    return ""
 
 
-def validate_email(trans, email, user=None, check_dup=True, allow_empty=False):
+def validate_email(trans, email, user=None, check_dup=True, allow_empty=False, validate_domain=False):
     """
     Validates the email format, also checks whether the domain is blocklisted in the disposable domains configuration.
     """
     if (user and user.email == email) or (email == "" and allow_empty):
-        return ''
+        return ""
     message = validate_email_str(email)
-    if message:
-        pass
-    elif check_dup and trans.sa_session.query(trans.app.model.User).filter(func.lower(trans.app.model.User.table.c.email) == email.lower()).first():
-        message = f"User with email '{email}' already exists."
-    #  If the allowlist is not empty filter out any domain not in the list and ignore blocklist.
-    elif trans.app.config.email_domain_allowlist_content is not None:
+    if not message and validate_domain:
         domain = extract_domain(email)
-        if domain not in trans.app.config.email_domain_allowlist_content:
-            message = "Please enter an allowed domain email address for this server."
-    #  If the blocklist is not empty filter out the disposable domains.
-    elif trans.app.config.email_domain_blocklist_content is not None:
-        domain = extract_domain(email, base_only=True)
-        if domain in trans.app.config.email_domain_blocklist_content:
-            message = "Please enter your permanent email address."
+        message = validate_email_domain_name(domain)
+
+    if (
+        not message
+        and check_dup
+        and trans.sa_session.query(trans.app.model.User)
+        .filter(func.lower(trans.app.model.User.table.c.email) == email.lower())
+        .first()
+    ):
+        message = f"User with email '{email}' already exists."
+
+    if not message:
+        # If the allowlist is not empty filter out any domain not in the list and ignore blocklist.
+        if trans.app.config.email_domain_allowlist_content is not None:
+            domain = extract_domain(email)
+            if domain not in trans.app.config.email_domain_allowlist_content:
+                message = "Please enter an allowed domain email address for this server."
+        # If the blocklist is not empty filter out the disposable domains.
+        elif trans.app.config.email_domain_blocklist_content is not None:
+            domain = extract_domain(email, base_only=True)
+            if domain in trans.app.config.email_domain_blocklist_content:
+                message = "Please enter your permanent email address."
+
+    return message
+
+
+def validate_email_domain_name(domain: str) -> LiteralString:
+    message = ""
+    try:
+        dns.resolver.resolve(domain, "MX")
+    except DNSException:
+        try:
+            # Per RFC 5321, try to fall back to the A record (implicit MX) for
+            # the domain, see https://www.rfc-editor.org/rfc/rfc5321#section-5.1
+            dns.resolver.resolve(domain, "A")
+        except DNSException:
+            message = "The email domain cannot be resolved."
     return message
 
 
 def extract_domain(email, base_only=False):
-    domain = email.split('@')[1]
-    parts = domain.split('.')
+    domain = email.rsplit("@", 1)[-1]
+    parts = domain.split(".")
     if len(parts) > 2 and base_only:
-        return ('.').join(parts[-2:])
+        return (".").join(parts[-2:])
     return domain
 
 
 def validate_publicname(trans, publicname, user=None):
     """
     Check that publicname respects the minimum and maximum string length, the
     allowed characters, and that the username is not taken already.
     """
     if user and user.username == publicname:
-        return ''
+        return ""
     message = validate_publicname_str(publicname)
     if message:
         return message
     if trans.sa_session.query(trans.app.model.User).filter_by(username=publicname).first():
         return "Public name is taken; please choose another."
-    return ''
+    return ""
 
 
 def transform_publicname(publicname):
     """
     Transform publicname to respect the minimum and maximum string length, and
     the allowed characters.
     FILL_CHAR is used to extend or replace characters.
     """
     # TODO: Enhance to allow generation of semi-random publicnnames e.g., when valid but taken
-    if publicname not in ['None', None, '']:
-        publicname = publicname.lower()
-        publicname = re.sub(VALID_PUBLICNAME_SUB, FILL_CHAR, publicname)
-        publicname = publicname.ljust(PUBLICNAME_MIN_LEN + 1, FILL_CHAR)[:PUBLICNAME_MAX_LEN]
+    if not publicname:
+        raise ValueError("Public name cannot be empty")
+    publicname = publicname.lower()
+    publicname = re.sub(VALID_PUBLICNAME_SUB, FILL_CHAR, publicname)
+    publicname = publicname[:PUBLICNAME_MAX_LEN]
     return publicname
 
 
 def validate_password(trans, password, confirm):
     if password != confirm:
         return "Passwords do not match."
     return validate_password_str(password)
```

### Comparing `galaxy-data-22.1.1/galaxy/security/vault.py` & `galaxy-data-23.0.1/galaxy/security/vault.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,29 @@
 import abc
 import logging
 import os
 import re
-from typing import List, Optional
+from typing import (
+    List,
+    Optional,
+)
 
 import yaml
-from cryptography.fernet import Fernet, MultiFernet
+from cryptography.fernet import (
+    Fernet,
+    MultiFernet,
+)
 
 try:
     from custos.clients.resource_secret_management_client import ResourceSecretManagementClient
     from custos.clients.utils.exceptions.CustosExceptions import KeyDoesNotExist
     from custos.transport.settings import CustosServerClientSettings
+
+    logging.getLogger("custos.clients.resource_secret_management_client").setLevel(logging.CRITICAL)
+
     custos_sdk_available = True
 except ImportError:
     custos_sdk_available = False
 
 try:
     import hvac
 except ImportError:
@@ -44,81 +53,81 @@
     def read_secret(self, key: str) -> Optional[str]:
         """
         Reads a secret from the vault.
 
         :param key: The key to read. Typically a hierarchical path such as `/galaxy/user/1/preferences/editor`
         :return: The string value stored at the key, such as 'ace_editor'.
         """
-        pass
 
     @abc.abstractmethod
     def write_secret(self, key: str, value: str) -> None:
         """
         Write a secret to the vault.
 
         :param key: The key to write to. Typically a hierarchical path such as `/galaxy/user/1/preferences/editor`
         :param value: The value to write, such as 'vscode'
         :return:
         """
-        pass
 
     @abc.abstractmethod
     def list_secrets(self, key: str) -> List[str]:
         """
         Lists secrets at a given path.
 
         :param key: The key prefix to list. e.g. `/galaxy/user/1/preferences`. A trailing slash is optional.
         :return: The list of subkeys at path. e.g.
                  ['/galaxy/user/1/preferences/editor`, '/galaxy/user/1/preferences/storage`]
                  Note that only immediate subkeys are returned.
         """
-        pass
 
 
 class NullVault(Vault):
-
     def read_secret(self, key: str) -> Optional[str]:
-        raise InvalidVaultConfigException("No vault configured. Make sure the vault_config_file setting is defined in galaxy.yml")
+        raise InvalidVaultConfigException(
+            "No vault configured. Make sure the vault_config_file setting is defined in galaxy.yml"
+        )
 
     def write_secret(self, key: str, value: str) -> None:
-        raise InvalidVaultConfigException("No vault configured. Make sure the vault_config_file setting is defined in galaxy.yml")
+        raise InvalidVaultConfigException(
+            "No vault configured. Make sure the vault_config_file setting is defined in galaxy.yml"
+        )
 
     def list_secrets(self, key: str) -> List[str]:
         raise NotImplementedError()
 
 
 class HashicorpVault(Vault):
-
     def __init__(self, config):
         if not hvac:
-            raise InvalidVaultConfigException("Hashicorp vault library 'hvac' is not available. Make sure hvac is installed.")
-        self.vault_address = config.get('vault_address')
-        self.vault_token = config.get('vault_token')
+            raise InvalidVaultConfigException(
+                "Hashicorp vault library 'hvac' is not available. Make sure hvac is installed."
+            )
+        self.vault_address = config.get("vault_address")
+        self.vault_token = config.get("vault_token")
         self.client = hvac.Client(url=self.vault_address, token=self.vault_token)
 
     def read_secret(self, key: str) -> Optional[str]:
         try:
             response = self.client.secrets.kv.read_secret_version(path=key)
-            return response['data']['data'].get('value')
+            return response["data"]["data"].get("value")
         except hvac.exceptions.InvalidPath:
             return None
 
     def write_secret(self, key: str, value: str) -> None:
-        self.client.secrets.kv.v2.create_or_update_secret(path=key, secret={'value': value})
+        self.client.secrets.kv.v2.create_or_update_secret(path=key, secret={"value": value})
 
     def list_secrets(self, key: str) -> List[str]:
         raise NotImplementedError()
 
 
 class DatabaseVault(Vault):
-
     def __init__(self, sa_session, config):
         self.sa_session = sa_session
-        self.encryption_keys = config.get('encryption_keys')
-        self.fernet_keys = [Fernet(key.encode('utf-8')) for key in self.encryption_keys]
+        self.encryption_keys = config.get("encryption_keys")
+        self.fernet_keys = [Fernet(key.encode("utf-8")) for key in self.encryption_keys]
 
     def _get_multi_fernet(self) -> MultiFernet:
         return MultiFernet(self.fernet_keys)
 
     def _update_or_create(self, key: str, value: Optional[str]) -> model.Vault:
         vault_entry = self.sa_session.query(model.Vault).filter_by(key=key).first()
         if vault_entry:
@@ -136,59 +145,64 @@
             self.sa_session.flush()
         return vault_entry
 
     def read_secret(self, key: str) -> Optional[str]:
         key_obj = self.sa_session.query(model.Vault).filter_by(key=key).first()
         if key_obj and key_obj.value:
             f = self._get_multi_fernet()
-            return f.decrypt(key_obj.value.encode('utf-8')).decode('utf-8')
+            return f.decrypt(key_obj.value.encode("utf-8")).decode("utf-8")
         return None
 
     def write_secret(self, key: str, value: str) -> None:
         f = self._get_multi_fernet()
-        token = f.encrypt(value.encode('utf-8'))
-        self._update_or_create(key=key, value=token.decode('utf-8'))
+        token = f.encrypt(value.encode("utf-8"))
+        self._update_or_create(key=key, value=token.decode("utf-8"))
 
     def list_secrets(self, key: str) -> List[str]:
         raise NotImplementedError()
 
 
 class CustosVault(Vault):
-
     def __init__(self, config):
         if not custos_sdk_available:
-            raise InvalidVaultConfigException("Custos sdk library 'custos-sdk' is not available. Make sure the custos-sdk is installed.")
-        custos_settings = CustosServerClientSettings(custos_host=config.get('custos_host'),
-                                                     custos_port=config.get('custos_port'),
-                                                     custos_client_id=config.get('custos_client_id'),
-                                                     custos_client_sec=config.get('custos_client_sec'))
+            raise InvalidVaultConfigException(
+                "Custos sdk library 'custos-sdk' is not available. Make sure the custos-sdk is installed."
+            )
+        custos_settings = CustosServerClientSettings(
+            custos_host=config.get("custos_host"),
+            custos_port=config.get("custos_port"),
+            custos_client_id=config.get("custos_client_id"),
+            custos_client_sec=config.get("custos_client_sec"),
+        )
         self.client = ResourceSecretManagementClient(custos_settings)
 
     def read_secret(self, key: str) -> Optional[str]:
         try:
             response = self.client.get_kv_credential(key=key)
-            return response.get('value')
+            return response.get("value")
         except KeyDoesNotExist:
             return None
 
     def write_secret(self, key: str, value: str) -> None:
         self.client.set_kv_credential(key=key, value=value)
 
     def list_secrets(self, key: str) -> List[str]:
         raise NotImplementedError()
 
 
 class UserVaultWrapper(Vault):
-
     def __init__(self, vault: Vault, user):
         self.vault = vault
         self.user = user
 
     def read_secret(self, key: str) -> Optional[str]:
-        return self.vault.read_secret(f"user/{self.user.id}/{key}")
+        if self.user:
+            return self.vault.read_secret(f"user/{self.user.id}/{key}")
+        else:
+            return None
 
     def write_secret(self, key: str, value: str) -> None:
         return self.vault.write_secret(f"user/{self.user.id}/{key}", value)
 
     def list_secrets(self, key: str) -> List[str]:
         raise NotImplementedError()
 
@@ -209,15 +223,16 @@
 
     def normalize_key(self, key):
         # remove leading and trailing slashes
         key = key.strip("/")
         if not self.validate_key(key):
             raise InvalidVaultKeyException(
                 f"Vault key: {key} is invalid. Make sure that it is not empty, contains double slashes or contains"
-                "whitespace before or after the separator.")
+                "whitespace before or after the separator."
+            )
         return key
 
     def read_secret(self, key: str) -> Optional[str]:
         key = self.normalize_key(key)
         return self.vault.read_secret(key)
 
     def write_secret(self, key: str, value: str) -> None:
@@ -243,16 +258,15 @@
     def write_secret(self, key: str, value: str) -> None:
         return self.vault.write_secret(f"/{self.prefix}/{key}", value)
 
     def list_secrets(self, key: str) -> List[str]:
         raise NotImplementedError()
 
 
-class VaultFactory(object):
-
+class VaultFactory:
     @staticmethod
     def load_vault_config(vault_conf_yml: str) -> Optional[dict]:
         if os.path.exists(vault_conf_yml):
             with open(vault_conf_yml) as f:
                 return yaml.safe_load(f)
         return None
 
@@ -263,17 +277,17 @@
             vault = HashicorpVault(cfg)
         elif vault_type == "database":
             vault = DatabaseVault(app.model.context, cfg)
         elif vault_type == "custos":
             vault = CustosVault(cfg)
         else:
             raise InvalidVaultConfigException(f"Unknown vault type: {vault_type}")
-        vault_prefix = cfg.get('path_prefix') or "/galaxy"
+        vault_prefix = cfg.get("path_prefix") or "/galaxy"
         return VaultKeyValidationWrapper(VaultKeyPrefixWrapper(vault, prefix=vault_prefix))
 
     @staticmethod
     def from_app(app) -> Vault:
         vault_config = VaultFactory.load_vault_config(app.config.vault_config_file)
         if vault_config:
-            return VaultFactory.from_vault_type(app, vault_config.get('type', None), vault_config)
+            return VaultFactory.from_vault_type(app, vault_config.get("type", None), vault_config)
         log.warning("No vault configured. We recommend defining the vault_config_file setting in galaxy.yml")
         return NullVault()
```

