# Comparing `tmp/ibm_generative_ai-0.1.14-py3-none-any.whl.zip` & `tmp/ibm_generative_ai-0.1.15-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,42 @@
-Zip file size: 37763 bytes, number of entries: 35
--rw-r--r--  2.0 unx      360 b- defN 23-May-05 17:41 genai/__init__.py
--rw-r--r--  2.0 unx      168 b- defN 23-May-07 02:04 genai/_version.py
--rw-r--r--  2.0 unx      466 b- defN 23-May-05 17:41 genai/credentials.py
--rw-r--r--  2.0 unx     2224 b- defN 23-May-05 17:41 genai/metadata.py
--rw-r--r--  2.0 unx    11047 b- defN 23-May-05 17:41 genai/model.py
--rw-r--r--  2.0 unx    22054 b- defN 23-May-05 17:41 genai/prompt_pattern.py
--rw-r--r--  2.0 unx       90 b- defN 23-May-05 17:41 genai/exceptions/__init__.py
--rw-r--r--  2.0 unx      773 b- defN 23-May-05 17:41 genai/exceptions/genai_exception.py
--rw-r--r--  2.0 unx      129 b- defN 23-May-05 17:41 genai/extensions/huggingface/__init__.py
--rw-r--r--  2.0 unx     1263 b- defN 23-May-05 17:41 genai/extensions/huggingface/save_huggingface.py
--rw-r--r--  2.0 unx      177 b- defN 23-May-05 17:41 genai/extensions/langchain/__init__.py
--rw-r--r--  2.0 unx     2600 b- defN 23-May-05 17:41 genai/extensions/langchain/llm.py
--rw-r--r--  2.0 unx     1608 b- defN 23-May-05 17:41 genai/extensions/langchain/prompt.py
--rw-r--r--  2.0 unx       94 b- defN 23-May-05 17:41 genai/extensions/pandas/__init__.py
--rw-r--r--  2.0 unx     5874 b- defN 23-May-05 17:41 genai/extensions/pandas/prompt_sub.py
--rw-r--r--  2.0 unx      487 b- defN 23-May-05 17:41 genai/schemas/__init__.py
--rw-r--r--  2.0 unx     3686 b- defN 23-May-05 17:41 genai/schemas/descriptions.py
--rw-r--r--  2.0 unx     3228 b- defN 23-May-05 17:41 genai/schemas/generate_params.py
--rw-r--r--  2.0 unx      601 b- defN 23-May-05 17:41 genai/schemas/history_params.py
--rw-r--r--  2.0 unx      591 b- defN 23-May-05 17:41 genai/schemas/models.py
--rw-r--r--  2.0 unx     3466 b- defN 23-May-05 17:41 genai/schemas/responses.py
--rw-r--r--  2.0 unx      368 b- defN 23-May-05 17:41 genai/schemas/token_params.py
--rw-r--r--  2.0 unx      262 b- defN 23-May-05 17:41 genai/services/__init__.py
--rw-r--r--  2.0 unx     7336 b- defN 23-May-05 17:41 genai/services/async_generator.py
--rw-r--r--  2.0 unx     2384 b- defN 23-May-05 17:41 genai/services/connection_manager.py
--rw-r--r--  2.0 unx     8048 b- defN 23-May-05 17:41 genai/services/request_handler.py
--rw-r--r--  2.0 unx     6243 b- defN 23-May-05 17:41 genai/services/service_interface.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-05 17:41 genai/utils/__init__.py
--rw-r--r--  2.0 unx     1122 b- defN 23-May-05 17:41 genai/utils/extensions.py
--rw-r--r--  2.0 unx     1731 b- defN 23-May-05 17:41 genai/utils/json_utils.py
--rw-r--r--  2.0 unx    11357 b- defN 23-May-07 02:04 ibm_generative_ai-0.1.14.dist-info/LICENSE
--rw-r--r--  2.0 unx    11194 b- defN 23-May-07 02:04 ibm_generative_ai-0.1.14.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-07 02:04 ibm_generative_ai-0.1.14.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 23-May-07 02:04 ibm_generative_ai-0.1.14.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3005 b- defN 23-May-07 02:04 ibm_generative_ai-0.1.14.dist-info/RECORD
-35 files, 114134 bytes uncompressed, 32915 bytes compressed:  71.2%
+Zip file size: 39010 bytes, number of entries: 40
+-rw-r--r--  2.0 unx      360 b- defN 23-Jun-08 15:15 genai/__init__.py
+-rw-r--r--  2.0 unx      162 b- defN 23-Jun-08 15:16 genai/_version.py
+-rw-r--r--  2.0 unx      466 b- defN 23-Jun-08 15:15 genai/credentials.py
+-rw-r--r--  2.0 unx     2220 b- defN 23-Jun-08 15:15 genai/metadata.py
+-rw-r--r--  2.0 unx    11465 b- defN 23-Jun-08 15:15 genai/model.py
+-rw-r--r--  2.0 unx    22054 b- defN 23-Jun-08 15:15 genai/prompt_pattern.py
+-rw-r--r--  2.0 unx       90 b- defN 23-Jun-08 15:15 genai/exceptions/__init__.py
+-rw-r--r--  2.0 unx      773 b- defN 23-Jun-08 15:15 genai/exceptions/genai_exception.py
+-rw-r--r--  2.0 unx      129 b- defN 23-Jun-08 15:15 genai/extensions/huggingface/__init__.py
+-rw-r--r--  2.0 unx     1274 b- defN 23-Jun-08 15:15 genai/extensions/huggingface/save_huggingface.py
+-rw-r--r--  2.0 unx      177 b- defN 23-Jun-08 15:15 genai/extensions/langchain/__init__.py
+-rw-r--r--  2.0 unx     2608 b- defN 23-Jun-08 15:15 genai/extensions/langchain/llm.py
+-rw-r--r--  2.0 unx     1616 b- defN 23-Jun-08 15:15 genai/extensions/langchain/prompt.py
+-rw-r--r--  2.0 unx      194 b- defN 23-Jun-08 15:15 genai/extensions/localserver/__init__.py
+-rw-r--r--  2.0 unx     1593 b- defN 23-Jun-08 15:15 genai/extensions/localserver/custom_model_interface.py
+-rw-r--r--  2.0 unx     5454 b- defN 23-Jun-08 15:15 genai/extensions/localserver/local_api_server.py
+-rw-r--r--  2.0 unx      385 b- defN 23-Jun-08 15:15 genai/extensions/localserver/schemas.py
+-rw-r--r--  2.0 unx       94 b- defN 23-Jun-08 15:15 genai/extensions/pandas/__init__.py
+-rw-r--r--  2.0 unx     5882 b- defN 23-Jun-08 15:15 genai/extensions/pandas/prompt_sub.py
+-rw-r--r--  2.0 unx      598 b- defN 23-Jun-08 15:15 genai/schemas/__init__.py
+-rw-r--r--  2.0 unx     3686 b- defN 23-Jun-08 15:15 genai/schemas/descriptions.py
+-rw-r--r--  2.0 unx     3228 b- defN 23-Jun-08 15:15 genai/schemas/generate_params.py
+-rw-r--r--  2.0 unx      601 b- defN 23-Jun-08 15:15 genai/schemas/history_params.py
+-rw-r--r--  2.0 unx      591 b- defN 23-Jun-08 15:15 genai/schemas/models.py
+-rw-r--r--  2.0 unx     3650 b- defN 23-Jun-08 15:15 genai/schemas/responses.py
+-rw-r--r--  2.0 unx      368 b- defN 23-Jun-08 15:15 genai/schemas/token_params.py
+-rw-r--r--  2.0 unx      262 b- defN 23-Jun-08 15:15 genai/services/__init__.py
+-rw-r--r--  2.0 unx     7336 b- defN 23-Jun-08 15:15 genai/services/async_generator.py
+-rw-r--r--  2.0 unx     2384 b- defN 23-Jun-08 15:15 genai/services/connection_manager.py
+-rw-r--r--  2.0 unx     9919 b- defN 23-Jun-08 15:15 genai/services/request_handler.py
+-rw-r--r--  2.0 unx     7015 b- defN 23-Jun-08 15:15 genai/services/service_interface.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-08 15:15 genai/utils/__init__.py
+-rw-r--r--  2.0 unx     1122 b- defN 23-Jun-08 15:15 genai/utils/extensions.py
+-rw-r--r--  2.0 unx     1731 b- defN 23-Jun-08 15:15 genai/utils/json_utils.py
+-rw-r--r--  2.0 unx      619 b- defN 23-Jun-08 15:15 genai/utils/search_space_params.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Jun-08 15:16 ibm_generative_ai-0.1.15.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1934 b- defN 23-Jun-08 15:16 ibm_generative_ai-0.1.15.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-08 15:16 ibm_generative_ai-0.1.15.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 23-Jun-08 15:16 ibm_generative_ai-0.1.15.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3501 b- defN 23-Jun-08 15:16 ibm_generative_ai-0.1.15.dist-info/RECORD
+40 files, 116996 bytes uncompressed, 33352 bytes compressed:  71.5%
```

## zipnote {}

```diff
@@ -33,14 +33,26 @@
 
 Filename: genai/extensions/langchain/llm.py
 Comment: 
 
 Filename: genai/extensions/langchain/prompt.py
 Comment: 
 
+Filename: genai/extensions/localserver/__init__.py
+Comment: 
+
+Filename: genai/extensions/localserver/custom_model_interface.py
+Comment: 
+
+Filename: genai/extensions/localserver/local_api_server.py
+Comment: 
+
+Filename: genai/extensions/localserver/schemas.py
+Comment: 
+
 Filename: genai/extensions/pandas/__init__.py
 Comment: 
 
 Filename: genai/extensions/pandas/prompt_sub.py
 Comment: 
 
 Filename: genai/schemas/__init__.py
@@ -84,23 +96,26 @@
 
 Filename: genai/utils/extensions.py
 Comment: 
 
 Filename: genai/utils/json_utils.py
 Comment: 
 
-Filename: ibm_generative_ai-0.1.14.dist-info/LICENSE
+Filename: genai/utils/search_space_params.py
+Comment: 
+
+Filename: ibm_generative_ai-0.1.15.dist-info/LICENSE
 Comment: 
 
-Filename: ibm_generative_ai-0.1.14.dist-info/METADATA
+Filename: ibm_generative_ai-0.1.15.dist-info/METADATA
 Comment: 
 
-Filename: ibm_generative_ai-0.1.14.dist-info/WHEEL
+Filename: ibm_generative_ai-0.1.15.dist-info/WHEEL
 Comment: 
 
-Filename: ibm_generative_ai-0.1.14.dist-info/top_level.txt
+Filename: ibm_generative_ai-0.1.15.dist-info/top_level.txt
 Comment: 
 
-Filename: ibm_generative_ai-0.1.14.dist-info/RECORD
+Filename: ibm_generative_ai-0.1.15.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## genai/_version.py

```diff
@@ -1,4 +1,4 @@
 # file generated by setuptools_scm
 # don't change, don't track in version control
-__version__ = version = '0.1.dev1'
-__version_tuple__ = version_tuple = (0, 1, 'dev1')
+__version__ = version = '0.1.15'
+__version_tuple__ = version_tuple = (0, 1, 15)
```

## genai/metadata.py

```diff
@@ -26,18 +26,18 @@
         """Accepts the terms of use on GENAI.
 
         Returns:
             TermsOfUse: Terms of Use Data
         """
 
         try:
-            tou_response = self.service.terms_of_use()
+            tou_response = self.service.terms_of_use(True)
 
             if tou_response.status_code == 200:
-                tou_data = TermsOfUse(**tou_response.json()["data"])
+                tou_data = TermsOfUse(**tou_response.json())
                 return tou_data
             else:
                 raise GenAiException(tou_response)
         except GenAiException as me:
             raise me
         except Exception as ex:
             raise GenAiException(ex)
```

## genai/model.py

```diff
@@ -1,12 +1,14 @@
 import ast
 import logging
 from collections.abc import Generator
 from typing import Any, Callable, Union
 
+from tqdm import tqdm
+
 from genai.credentials import Credentials
 from genai.exceptions import GenAiException
 from genai.metadata import Metadata
 from genai.prompt_pattern import PromptPattern
 from genai.schemas import GenerateParams, ModelType, TokenParams
 from genai.schemas.responses import (
     GenerateResponse,
@@ -17,14 +19,16 @@
 )
 from genai.services import AsyncResponseGenerator, ServiceInterface
 
 logger = logging.getLogger(__name__)
 
 
 class Model:
+    _accessors = set()
+
     def __init__(self, model: ModelType, params: Union[GenerateParams, TokenParams], credentials: Credentials):
         """Instantiates the Model Interface
 
         Args:
             model (ModelType): The type of model to use
             params (Union[GenerateParams, TokenParams]): Parameters to use during generate requests
             credentials (Credentials): The API Credentials
@@ -117,41 +121,50 @@
         return list(self.generate_as_completed(prompts))
 
     def generate_async(
         self,
         prompts: Union[list[str], list[PromptPattern]],
         ordered: bool = False,
         callback: Callable[[GenerateResult], Any] = None,
+        hide_progressbar: bool = False,
     ) -> Generator[Union[GenerateResult, None]]:
         """The generate endpoint is the centerpiece of the GENAI alpha.
         It provides a simplified and flexible, yet powerful interface to the supported
         models as a service. Given a text prompt as inputs, and required parameters
         the selected model (model_id) will generate a completion text as generated_text.
         This python method generates responses utilizing async capabilities and returns
         responses as they arrive.
 
         Args:
             prompts (list[str]): The list of one or more prompt strings.
             ordered (bool): Whether the responses should be returned in-order.
             callback (Callable[[GenerateResult], Any]): Optional callback
                 to be called after generating result for a prompt.
+            hide_progressbar: boolean flag to hide or show a progress bar.
+                By defaul bar will be always shown.
 
         Returns:
             Generator[Union[GenerateResult, None]]: A list of results
         """
         if len(prompts) > 0 and isinstance(prompts[0], PromptPattern):
             prompts = PromptPattern.list_str(prompts)
 
         logger.debug(f"Calling Generate Async. Prompts: {prompts}, params: {self.params}")
 
         try:
             with AsyncResponseGenerator(
                 self.model, prompts, self.params, self.service, ordered=ordered, callback=callback
             ) as asynchelper:
-                for response in asynchelper.generate_response():
+                for response in tqdm(
+                    asynchelper.generate_response(),
+                    total=len(prompts),
+                    desc="Progress",
+                    unit=" inputs",
+                    disable=hide_progressbar,
+                ):
                     yield response
         except GenAiException as me:
             raise me
         except Exception as ex:
             raise GenAiException(ex)
 
     def tokenize_as_completed(
```

## genai/extensions/huggingface/save_huggingface.py

```diff
@@ -1,11 +1,11 @@
 try:
     from datasets import Dataset
 except ImportError:
-    raise ImportError("Could not import HuggingFace Datasets: Please install ibm-genai[datasets] extension.")
+    raise ImportError("Could not import HuggingFace Datasets: Please install ibm-generative-ai[huggingface] extension.")
 
 from genai.prompt_pattern import PromptPattern
 from genai.utils.extensions import register_promptpattern_accessor
 
 __all__ = ["HuggingFaceDatasetExtension"]
```

## genai/extensions/langchain/llm.py

```diff
@@ -4,15 +4,15 @@
 
 from pydantic import BaseModel, Extra
 
 try:
     from langchain.llms.base import LLM
     from langchain.llms.utils import enforce_stop_tokens
 except ImportError:
-    raise ImportError("Could not import langchain: Please install ibm-genai[langchain] extension.")
+    raise ImportError("Could not import langchain: Please install ibm-generative-ai[langchain] extension.")
 
 from genai import Credentials, Model
 from genai.schemas import GenerateParams, ModelType
 
 logger = logging.getLogger(__name__)
 
 __all__ = ["LangChainInterface"]
```

## genai/extensions/langchain/prompt.py

```diff
@@ -1,15 +1,15 @@
 """Wrapper around IBM GENAI APIs for use in langchain"""
 import logging
 import re
 
 try:
     from langchain import PromptTemplate
 except ImportError:
-    raise ImportError("Could not import langchain: Please install ibm-genai[langchain] extension.")
+    raise ImportError("Could not import langchain: Please install ibm-generative-ai[langchain] extension.")
 
 from genai.prompt_pattern import PromptPattern
 from genai.utils.extensions import register_promptpattern_accessor
 
 logger = logging.getLogger(__name__)
 
 __all__ = ["PromptExtension"]
```

## genai/extensions/pandas/prompt_sub.py

```diff
@@ -1,13 +1,13 @@
 from typing import Literal, Union
 
 try:
     from pandas import DataFrame
 except ImportError:
-    raise ImportError("Could not import pandas: Please install ibm-genai[pandas] extension.")
+    raise ImportError("Could not import pandas: Please install ibm-generative-ai[pandas] extension.")
 
 from genai.exceptions import GenAiException
 from genai.prompt_pattern import PromptPattern
 from genai.utils.extensions import register_promptpattern_accessor
 
 __all__ = ["PandasExtension"]
```

## genai/schemas/__init__.py

```diff
@@ -3,19 +3,22 @@
     GenerateParams,
     LengthPenalty,
     Return,
     ReturnOptions,
 )
 from genai.schemas.history_params import HistoryParams
 from genai.schemas.models import ModelType
+from genai.schemas.responses import GenerateResult, TokenizeResult
 from genai.schemas.token_params import TokenParams
 
 __all__ = [
     "Descriptions",
     "GenerateParams",
     "LengthPenalty",
     "Return",
     "ReturnOptions",
     "TokenParams",
     "HistoryParams",
     "ModelType",
+    "GenerateResult",
+    "TokenizeResult",
 ]
```

## genai/schemas/responses.py

```diff
@@ -43,17 +43,25 @@
 
 
 class GenAiResponseModel(BaseModel, extra=Extra.allow):
     # Validators
     _extra_fields_warning = root_validator(allow_reuse=True)(alert_extra_fields_validator)
 
 
-class TermsOfUse(GenAiResponseModel):
+class TermsOfUseResult(GenAiResponseModel):
     tou_accepted: bool
     tou_accepted_at: datetime
+    firstName: str
+    lastName: str
+    data_usage_consent: bool = False
+    generate_default: dict = None
+
+
+class TermsOfUse(GenAiResponseModel):
+    results: TermsOfUseResult
 
 
 class GeneratedToken(GenAiResponseModel):
     logprob: Optional[float]
     text: Optional[str]
```

## genai/services/request_handler.py

```diff
@@ -1,118 +1,173 @@
 import logging
 
 import httpx
+from httpx import Response
 
+from genai._version import version
 from genai.services.connection_manager import ConnectionManager
 
 logger = logging.getLogger(__name__)
 
 __all__ = ["RequestHandler"]
 
 
 class RequestHandler:
     @staticmethod
-    def _metadata(method: str, key: str, model_id: str = None, inputs: list = None, parameters: dict = None):
+    def _metadata(
+        method: str,
+        key: str,
+        model_id: str = None,
+        inputs: list = None,
+        parameters: dict = None,
+    ) -> tuple[dict, dict]:
         """General function to build header and/or json_data for /post and /get requests.
 
         Args:
             method (str): Request type. Currently accepts GET or POST
             key (str): API key for authorization.
             model_id (str, optional): The id of the language model to be queried. Defaults to None.
             inputs (list, optional): List of inputs to be queried. Defaults to None.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            dict,dict: Headers, json_data for request
+            tuple[dict,dict]: Headers, json_data for request
         """
 
-        if method == "GET":
-            return {"Authorization": f"Bearer {key}"}
-
-        elif method == "POST":
-            headers = {
-                "Content-Type": "application/json",
-                "Authorization": f"Bearer {key}",
-            }
+        headers = {
+            "Authorization": f"Bearer {key}",
+            "x-request-origin": f"python-sdk/{version}",
+        }
+        json_data = {}
 
-            json_data = {}
+        if method == "POST":
+            headers["Content-Type"] = "application/json"
 
             if model_id is not None:
                 json_data["model_id"] = model_id
 
             if inputs is not None:
                 json_data["inputs"] = inputs
 
             if parameters is not None:
                 json_data["parameters"] = parameters
 
-            return headers, json_data
+        if method == "PATCH":
+            headers["Content-Type"] = "application/json"
+
+        return headers, json_data
 
     @staticmethod
-    async def async_post(endpoint: str, key: str, model_id: str = None, inputs: list = None, parameters: dict = None):
+    async def async_post(
+        endpoint: str,
+        key: str,
+        model_id: str = None,
+        inputs: list = None,
+        parameters: dict = None,
+    ):
         """Low level API for async /post request to REST API.
 
         Args:
             endpoint (str): Remote endpoint to be queried.
             key (str): API key for authorization.
             model_id (str, optional): The id of the language model to be queried. Defaults to None.
             inputs (list, optional): List of inputs to be queried. Defaults to None.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            requests.models.Response: Response from the REST API.
+            httpx.Response: Response from the REST API.
         """
         headers, json_data = RequestHandler._metadata(
-            method="POST", key=key, model_id=model_id, inputs=inputs, parameters=parameters
+            method="POST",
+            key=key,
+            model_id=model_id,
+            inputs=inputs,
+            parameters=parameters,
         )
         response = None
         async with httpx.AsyncClient(timeout=ConnectionManager.TIMEOUT) as client:
             response = await client.post(endpoint, headers=headers, json=json_data)
         return response
 
     @staticmethod
+    async def async_patch(endpoint: str, key: str, json_data: dict = None) -> Response:
+        """Low level API for /patch request to REST API.
+
+        Currently only used for TOU endpoint.
+
+        Args:
+            endpoint (str):
+            key (str)
+            payload: (dict, optional)
+
+        Returns:
+            httpx.Response: Response from the REST API.
+        """
+        headers, json_data = RequestHandler._metadata(method="PATCH", key=key)
+        response = None
+        async with httpx.AsyncClient(timeout=ConnectionManager.TIMEOUT) as client:
+            response = await client.patch(endpoint, headers=headers, json=json_data)
+        return response
+
+    @staticmethod
     async def async_generate(
-        endpoint: str, key: str, model_id: str = None, inputs: list = None, parameters: dict = None
+        endpoint: str,
+        key: str,
+        model_id: str = None,
+        inputs: list = None,
+        parameters: dict = None,
     ):
         """Low level API for async /generate request to REST API.
 
         Args:
             endpoint (str): Remote endpoint to be queried.
             key (str): API key for authorization.
             model_id (str, optional): The id of the language model to be queried. Defaults to None.
             inputs (list, optional): List of inputs to be queried. Defaults to None.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            requests.models.Response: Response from the REST API.
+            httpx.Response: Response from the REST API.
         """
         headers, json_data = RequestHandler._metadata(
-            method="POST", key=key, model_id=model_id, inputs=inputs, parameters=parameters
+            method="POST",
+            key=key,
+            model_id=model_id,
+            inputs=inputs,
+            parameters=parameters,
         )
         response = await ConnectionManager.async_generate_client.post(endpoint, headers=headers, json=json_data)
         return response
 
     @staticmethod
     async def async_tokenize(
-        endpoint: str, key: str, model_id: str = None, inputs: list = None, parameters: dict = None
+        endpoint: str,
+        key: str,
+        model_id: str = None,
+        inputs: list = None,
+        parameters: dict = None,
     ):
         """Low level API for async /tokenize request to REST API.
 
         Args:
             endpoint (str): Remote endpoint to be queried.
             key (str): API key for authorization.
             model_id (str, optional): The id of the language model to be queried. Defaults to None.
             inputs (list, optional): List of inputs to be queried. Defaults to None.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            requests.models.Response: Response from the REST API.
+            httpx.Response: Response from the REST API.
         """
         headers, json_data = RequestHandler._metadata(
-            method="POST", key=key, model_id=model_id, inputs=inputs, parameters=parameters
+            method="POST",
+            key=key,
+            model_id=model_id,
+            inputs=inputs,
+            parameters=parameters,
         )
         response = None
         for _ in range(0, ConnectionManager.MAX_RETRIES_TOKENIZE):
             # NOTE: We don't retry-fail with httpx since that'd not
             # not respect the ratelimiting below (5 requests per second).
             # Instead, we do the ratelimiting here with the help of limiter.
             async with ConnectionManager.async_tokenize_limiter:
@@ -127,17 +182,17 @@
 
         Args:
             endpoint (str): Remote endpoint to be queried.
             key (str): API key for authorization.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            requests.models.Response: Response from the REST API.
+            httpx.Response: Response from the REST API.
         """
-        headers = RequestHandler._metadata(method="GET", key=key)
+        headers, _ = RequestHandler._metadata(method="GET", key=key)
 
         async with httpx.AsyncClient(timeout=ConnectionManager.TIMEOUT) as client:
             response = await client.get(url=endpoint, headers=headers, params=parameters)
         return response
 
     @staticmethod
     def post(
@@ -154,43 +209,69 @@
             endpoint (str): Remote endpoint to be queried.
             key (str): API key for authorization.
             model_id (str, optional): The id of the language model to be queried. Defaults to None.
             inputs (list, optional): List of inputs to be queried. Defaults to None.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            requests.models.Response: Response from the REST API.
+            httpx.Response: Response from the REST API.
+            or
+            Generator of streamed response payloads from the REST API.
         """
         headers, json_data = RequestHandler._metadata(
-            method="POST", key=key, model_id=model_id, inputs=inputs, parameters=parameters
+            method="POST",
+            key=key,
+            model_id=model_id,
+            inputs=inputs,
+            parameters=parameters,
         )
 
         if streaming:
             return RequestHandler.post_stream(endpoint=endpoint, headers=headers, json_data=json_data)
         else:
             with httpx.Client(timeout=ConnectionManager.TIMEOUT) as s:
                 response = s.post(url=endpoint, headers=headers, json=json_data)
                 return response
 
     @staticmethod
+    def patch(endpoint: str, key: str, json_data: dict = None) -> Response:
+        """Low level API for /patch request to REST API.
+
+        Currently only used for TOU endpoint.
+
+        Args:
+            endpoint (str):
+            key (str)
+            payload: (dict, optional)
+
+        Returns:
+            httpx.Response: Response from the REST API.
+        """
+        headers, json_data = RequestHandler._metadata(method="PATCH", key=key)
+
+        with httpx.Client(timeout=ConnectionManager.TIMEOUT) as s:
+            response = s.patch(url=endpoint, headers=headers, json=json_data)
+            return response
+
+    @staticmethod
     def post_stream(endpoint, headers, json_data):
         with httpx.Client(timeout=ConnectionManager.TIMEOUT) as s:
             with s.stream(method="POST", url=endpoint, headers=headers, json=json_data) as r:
                 for chunk in r.iter_text():
                     yield chunk
 
     @staticmethod
-    def get(endpoint: str, key: str, parameters: dict = None):
+    def get(endpoint: str, key: str, parameters: dict = None) -> Response:
         """Low level API for /get request to REST API.
 
         Args:
             endpoint (str): Remote endpoint to be queried.
             key (str): API key for authorization.
             parameters (dict, optional): Key-value pairs for model parameters. Defaults to None.
 
         Returns:
-            requests.models.Response: Response from the REST API.
+            httpx.Response: Response from the REST API.
         """
-        headers = RequestHandler._metadata(method="GET", key=key)
+        headers, _ = RequestHandler._metadata(method="GET", key=key)
         with httpx.Client(timeout=ConnectionManager.TIMEOUT) as s:
             response = s.get(url=endpoint, headers=headers, params=parameters)
             return response
```

## genai/services/service_interface.py

```diff
@@ -1,7 +1,9 @@
+from httpx import Response
+
 from genai.exceptions import GenAiException
 from genai.schemas import GenerateParams, HistoryParams, TokenParams
 from genai.services import RequestHandler
 
 
 class ServiceInterface:
     GENERATE = "/generate"
@@ -16,30 +18,41 @@
             service_url (str): Base URL for querying.
             api_key (str): User API key for authorization.
             use_async (bool): Use async version of methods
         """
         self.service_url = service_url.rstrip("/")
         self.key = api_key
 
-    def generate(self, model: str, inputs: list, params: GenerateParams = None, streaming: bool = False):
+    def generate(
+        self,
+        model: str,
+        inputs: list,
+        params: GenerateParams = None,
+        streaming: bool = False,
+    ):
         """Generate a completion text for the given model, inputs, and params.
 
         Args:
             model (str): Model id.
             inputs (list): List of inputs.
             params (GenerateParams, optional): Parameters for generation. Defaults to None.
 
         Returns:
             Any: json from querying for text completion.
         """
         try:
             params = ServiceInterface._sanitize_params(params)
             endpoint = self.service_url + ServiceInterface.GENERATE
             return RequestHandler.post(
-                endpoint, key=self.key, model_id=model, inputs=inputs, parameters=params, streaming=streaming
+                endpoint,
+                key=self.key,
+                model_id=model,
+                inputs=inputs,
+                parameters=params,
+                streaming=streaming,
             )
         except Exception as e:
             raise GenAiException(e)
 
     def tokenize(self, model: str, inputs: list, params: TokenParams = None):
         """Do the conversion of provided inputs to tokens for a given model.
 
@@ -70,23 +83,32 @@
         try:
             params = ServiceInterface._sanitize_params(params)
             endpoint = self.service_url + ServiceInterface.HISTORY
             return RequestHandler.get(endpoint, key=self.key, parameters=params)
         except Exception as e:
             raise GenAiException(e)
 
-    def terms_of_use(self):
-        """Get terms of use.
+    def terms_of_use(self, accept: bool) -> Response:
+        """Accept the API Terms of Use
+
+        Args:
+            accept (bool): If the user accepts the TOU
+
+        Raises:
+            GenAiException: A general GenAI exception if there is an issue
+                with the request
 
         Returns:
-            Any: json for terms of use.
+            httpx.Response: Response from REST API
         """
+        tou_payload = {"tou_accepted": accept}
+
         try:
             endpoint = self.service_url + ServiceInterface.TOU
-            return RequestHandler.post(endpoint, key=self.key)
+            return RequestHandler.patch(endpoint, key=self.key, json_data=tou_payload)
         except Exception as e:
             raise GenAiException(e)
 
     # * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
     #   ASYNC
     # * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *
     async def async_generate(self, model, inputs, params: GenerateParams = None):
@@ -142,23 +164,32 @@
         try:
             params = ServiceInterface._sanitize_params(params)
             endpoint = self.service_url + ServiceInterface.HISTORY
             return await RequestHandler.async_get(endpoint, key=self.key, parameters=params)
         except Exception as e:
             raise GenAiException(e)
 
-    async def async_terms_of_use(self):
-        """Get terms of use.
+    def async_terms_of_use(self, accept: bool) -> Response:
+        """Accept the API Terms of Use
+
+        Args:
+            accept (bool): If the user accepts the TOU
+
+        Raises:
+            GenAiException: A general GenAI exception if there is an issue
+                with the request
 
         Returns:
-            Any: json for terms of use.
+            httpx.Response: Response from REST API
         """
+        tou_payload = {"tou_accepted": accept}
+
         try:
             endpoint = self.service_url + ServiceInterface.TOU
-            return await RequestHandler.async_post(endpoint, key=self.key)
+            return RequestHandler.async_patch(endpoint, key=self.key, json_data=tou_payload)
         except Exception as e:
             raise GenAiException(e)
 
     @staticmethod
     def _sanitize_params(params):
         if params is not None:
             if type(params) is not dict:
```

## Comparing `ibm_generative_ai-0.1.14.dist-info/LICENSE` & `ibm_generative_ai-0.1.15.dist-info/LICENSE`

 * *Files identical despite different names*

