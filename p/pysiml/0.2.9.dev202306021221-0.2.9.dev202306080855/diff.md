# Comparing `tmp/pysiml-0.2.9.dev202306021221-py3-none-any.whl.zip` & `tmp/pysiml-0.2.9.dev202306080855-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,8 +1,8 @@
-Zip file size: 171530 bytes, number of entries: 137
+Zip file size: 171644 bytes, number of entries: 137
 -rw-r--r--  2.0 unx     1520 b- defN 80-Jan-01 00:00 pyproject.toml
 -rw-r--r--  2.0 unx      638 b- defN 80-Jan-01 00:00 siml/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 siml/__main__/__init__.py
 -rw-r--r--  2.0 unx      900 b- defN 80-Jan-01 00:00 siml/__main__/convert_interim_data.py
 -rw-r--r--  2.0 unx     1400 b- defN 80-Jan-01 00:00 siml/__main__/convert_raw_data.py
 -rw-r--r--  2.0 unx     1742 b- defN 80-Jan-01 00:00 siml/__main__/optimize.py
 -rw-r--r--  2.0 unx     8663 b- defN 80-Jan-01 00:00 siml/__main__/plot_losses.py
@@ -11,16 +11,16 @@
 -rw-r--r--  2.0 unx     1376 b- defN 80-Jan-01 00:00 siml/__main__/train.py
 -rw-r--r--  2.0 unx     3961 b- defN 80-Jan-01 00:00 siml/__main__/visualize_graph.py
 -rw-r--r--  2.0 unx      139 b- defN 80-Jan-01 00:00 siml/base/siml_const.py
 -rw-r--r--  2.0 unx      579 b- defN 80-Jan-01 00:00 siml/base/siml_enums.py
 -rw-r--r--  2.0 unx      228 b- defN 80-Jan-01 00:00 siml/base/siml_typing.py
 -rw-r--r--  2.0 unx       57 b- defN 80-Jan-01 00:00 siml/config.py
 -rw-r--r--  2.0 unx     3520 b- defN 80-Jan-01 00:00 siml/data_parallel.py
--rw-r--r--  2.0 unx    25525 b- defN 80-Jan-01 00:00 siml/datasets.py
--rw-r--r--  2.0 unx    26838 b- defN 80-Jan-01 00:00 siml/inferer.py
+-rw-r--r--  2.0 unx    26513 b- defN 80-Jan-01 00:00 siml/datasets.py
+-rw-r--r--  2.0 unx    28921 b- defN 80-Jan-01 00:00 siml/inferer.py
 -rw-r--r--  2.0 unx       84 b- defN 80-Jan-01 00:00 siml/loss_operations/__init__.py
 -rw-r--r--  2.0 unx     1463 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_assignment.py
 -rw-r--r--  2.0 unx     4057 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_calculator.py
 -rw-r--r--  2.0 unx     1960 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_calculator_builder.py
 -rw-r--r--  2.0 unx     2045 b- defN 80-Jan-01 00:00 siml/loss_operations/loss_selector.py
 -rw-r--r--  2.0 unx     1364 b- defN 80-Jan-01 00:00 siml/mains.py
 -rw-r--r--  2.0 unx     2724 b- defN 80-Jan-01 00:00 siml/networks/__init__.py
@@ -78,15 +78,15 @@
 -rw-r--r--  2.0 unx     3245 b- defN 80-Jan-01 00:00 siml/path_like_objects/siml_files/checkpoint_file.py
 -rw-r--r--  2.0 unx     2268 b- defN 80-Jan-01 00:00 siml/path_like_objects/siml_files/interface.py
 -rw-r--r--  2.0 unx     3776 b- defN 80-Jan-01 00:00 siml/path_like_objects/siml_files/numpy_file.py
 -rw-r--r--  2.0 unx     2733 b- defN 80-Jan-01 00:00 siml/path_like_objects/siml_files/pickle_file.py
 -rw-r--r--  2.0 unx     2850 b- defN 80-Jan-01 00:00 siml/path_like_objects/siml_files/yaml_file.py
 -rw-r--r--  2.0 unx    12361 b- defN 80-Jan-01 00:00 siml/prepost.py
 -rw-r--r--  2.0 unx      160 b- defN 80-Jan-01 00:00 siml/preprocessing/__init__.py
--rw-r--r--  2.0 unx    21509 b- defN 80-Jan-01 00:00 siml/preprocessing/converter.py
+-rw-r--r--  2.0 unx    21587 b- defN 80-Jan-01 00:00 siml/preprocessing/converter.py
 -rw-r--r--  2.0 unx     9465 b- defN 80-Jan-01 00:00 siml/preprocessing/scalers_composition.py
 -rw-r--r--  2.0 unx    11050 b- defN 80-Jan-01 00:00 siml/preprocessing/scaling_converter.py
 -rw-r--r--  2.0 unx      176 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/__init__.py
 -rw-r--r--  2.0 unx     1405 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/__init__.py
 -rw-r--r--  2.0 unx      592 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/identity_scaler.py
 -rw-r--r--  2.0 unx      738 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/interface_scaler.py
 -rw-r--r--  2.0 unx     2038 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/isoam_scaler.py
@@ -95,25 +95,25 @@
 -rw-r--r--  2.0 unx     1926 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/sparse_standard_scaler.py
 -rw-r--r--  2.0 unx      570 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/standard_scaler.py
 -rw-r--r--  2.0 unx      865 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scale_functions/user_defined_scaler.py
 -rw-r--r--  2.0 unx     1012 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scaler_result_save.py
 -rw-r--r--  2.0 unx     4367 b- defN 80-Jan-01 00:00 siml/preprocessing/siml_scalers/scaler_wrapper.py
 -rw-r--r--  2.0 unx      154 b- defN 80-Jan-01 00:00 siml/services/__init__.py
 -rw-r--r--  2.0 unx     2342 b- defN 80-Jan-01 00:00 siml/services/environment.py
--rw-r--r--  2.0 unx      258 b- defN 80-Jan-01 00:00 siml/services/inference/__init__.py
+-rw-r--r--  2.0 unx      279 b- defN 80-Jan-01 00:00 siml/services/inference/__init__.py
 -rw-r--r--  2.0 unx     2587 b- defN 80-Jan-01 00:00 siml/services/inference/core_inferer.py
--rw-r--r--  2.0 unx     1709 b- defN 80-Jan-01 00:00 siml/services/inference/data_loader_builder.py
--rw-r--r--  2.0 unx     2613 b- defN 80-Jan-01 00:00 siml/services/inference/engine_bulider.py
--rw-r--r--  2.0 unx     4660 b- defN 80-Jan-01 00:00 siml/services/inference/inner_setting.py
--rw-r--r--  2.0 unx     4716 b- defN 80-Jan-01 00:00 siml/services/inference/metrics_builder.py
+-rw-r--r--  2.0 unx     1818 b- defN 80-Jan-01 00:00 siml/services/inference/data_loader_builder.py
+-rw-r--r--  2.0 unx     2619 b- defN 80-Jan-01 00:00 siml/services/inference/engine_bulider.py
+-rw-r--r--  2.0 unx     5263 b- defN 80-Jan-01 00:00 siml/services/inference/inner_setting.py
+-rw-r--r--  2.0 unx     4710 b- defN 80-Jan-01 00:00 siml/services/inference/metrics_builder.py
 -rw-r--r--  2.0 unx      203 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/__init__.py
--rw-r--r--  2.0 unx     5159 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/post_fem_data.py
--rw-r--r--  2.0 unx     6246 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/postprocessor.py
--rw-r--r--  2.0 unx     8875 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/save_processor.py
--rw-r--r--  2.0 unx      682 b- defN 80-Jan-01 00:00 siml/services/inference/record_object.py
+-rw-r--r--  2.0 unx     5603 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/post_fem_data.py
+-rw-r--r--  2.0 unx     4979 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/postprocessor.py
+-rw-r--r--  2.0 unx     7052 b- defN 80-Jan-01 00:00 siml/services/inference/postprocessing/save_processor.py
+-rw-r--r--  2.0 unx     1082 b- defN 80-Jan-01 00:00 siml/services/inference/record_object.py
 -rw-r--r--  2.0 unx     2574 b- defN 80-Jan-01 00:00 siml/services/model_builder.py
 -rw-r--r--  2.0 unx     4978 b- defN 80-Jan-01 00:00 siml/services/model_selector.py
 -rw-r--r--  2.0 unx     7124 b- defN 80-Jan-01 00:00 siml/services/path_rules.py
 -rw-r--r--  2.0 unx    50226 b- defN 80-Jan-01 00:00 siml/setting.py
 -rw-r--r--  2.0 unx    11028 b- defN 80-Jan-01 00:00 siml/siml_manager.py
 -rw-r--r--  2.0 unx      163 b- defN 80-Jan-01 00:00 siml/siml_variables/__init__.py
 -rw-r--r--  2.0 unx      594 b- defN 80-Jan-01 00:00 siml/siml_variables/array_variables/__init__.py
@@ -127,13 +127,13 @@
 -rw-r--r--  2.0 unx     1774 b- defN 80-Jan-01 00:00 siml/update_functions/element_batch_update.py
 -rw-r--r--  2.0 unx     3623 b- defN 80-Jan-01 00:00 siml/update_functions/pseudo_batch_update.py
 -rw-r--r--  2.0 unx     2702 b- defN 80-Jan-01 00:00 siml/update_functions/standard_update.py
 -rw-r--r--  2.0 unx      429 b- defN 80-Jan-01 00:00 siml/update_functions/update_interface.py
 -rw-r--r--  2.0 unx    23027 b- defN 80-Jan-01 00:00 siml/util.py
 -rw-r--r--  2.0 unx       37 b- defN 80-Jan-01 00:00 siml/utils/__init__.py
 -rw-r--r--  2.0 unx     7682 b- defN 80-Jan-01 00:00 siml/utils/fem_data_utils.py
--rw-r--r--  2.0 unx    11431 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306021221.dist-info/LICENSE
--rw-r--r--  2.0 unx     1638 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306021221.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306021221.dist-info/WHEEL
--rw-r--r--  2.0 unx      388 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306021221.dist-info/entry_points.txt
-?rw-r--r--  2.0 unx    12471 b- defN 16-Jan-01 00:00 pysiml-0.2.9.dev202306021221.dist-info/RECORD
-137 files, 605771 bytes uncompressed, 151608 bytes compressed:  75.0%
+-rw-r--r--  2.0 unx    11431 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306080855.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1638 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306080855.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306080855.dist-info/WHEEL
+-rw-r--r--  2.0 unx      388 b- defN 80-Jan-01 00:00 pysiml-0.2.9.dev202306080855.dist-info/entry_points.txt
+?rw-r--r--  2.0 unx    12472 b- defN 16-Jan-01 00:00 pysiml-0.2.9.dev202306080855.dist-info/RECORD
+137 files, 607408 bytes uncompressed, 151722 bytes compressed:  75.0%
```

## zipnote {}

```diff
@@ -390,23 +390,23 @@
 
 Filename: siml/utils/__init__.py
 Comment: 
 
 Filename: siml/utils/fem_data_utils.py
 Comment: 
 
-Filename: pysiml-0.2.9.dev202306021221.dist-info/LICENSE
+Filename: pysiml-0.2.9.dev202306080855.dist-info/LICENSE
 Comment: 
 
-Filename: pysiml-0.2.9.dev202306021221.dist-info/METADATA
+Filename: pysiml-0.2.9.dev202306080855.dist-info/METADATA
 Comment: 
 
-Filename: pysiml-0.2.9.dev202306021221.dist-info/WHEEL
+Filename: pysiml-0.2.9.dev202306080855.dist-info/WHEEL
 Comment: 
 
-Filename: pysiml-0.2.9.dev202306021221.dist-info/entry_points.txt
+Filename: pysiml-0.2.9.dev202306080855.dist-info/entry_points.txt
 Comment: 
 
-Filename: pysiml-0.2.9.dev202306021221.dist-info/RECORD
+Filename: pysiml-0.2.9.dev202306080855.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pyproject.toml

```diff
@@ -1,10 +1,10 @@
 [tool.poetry]
 name = "pysiml"
-version = "0.2.9.dev202306021221"
+version = "0.2.9.dev202306080855"
 description = "SiML - a Simulation ML library"
 license = "Apache-2.0"
 authors = ["RICOS Co. Ltd."]
 readme = "README.md"
 repository = "https://github.com/ricosjp/pysiml"
 documentation = "https://ricosjp.github.io/pysiml/"
 packages = [
```

## siml/datasets.py

```diff
@@ -1,9 +1,10 @@
 import multiprocessing as multi
 import pathlib
+from typing import Union
 
 import numpy as np
 import torch
 from ignite.utils import convert_tensor
 from tqdm import tqdm
 
 from siml import util
@@ -188,15 +189,15 @@
         return self._preprocess_data(data_directory)
 
     def _preprocess_data(self, raw_data_directory: pathlib.Path):
         dict_data = self.raw_converter.convert_single_data(
             raw_path=raw_data_directory,
             return_results=True
         )
-        dict_data = dict_data[str(raw_data_directory)]
+        dict_data = dict_data[str(raw_data_directory)][0]
         converted_dict_data = self.scalers.transform_dict(
             dict_data
         )
         x_data = self._merge_data(
             converted_dict_data, self.x_variable_names)
 
         if np.all([
@@ -262,49 +263,76 @@
     def __getitem__(self, i):
         return self.data[i]
 
 
 class SimplifiedDataset(BaseDataset):
 
     def __init__(
-            self, x_variable_names, y_variable_names, raw_dict_x,
-            *, answer_raw_dict_y=None, num_workers=0, **kwargs):
+        self, x_variable_names, y_variable_names, raw_dict_x,
+        supports: list[str] = None,
+        *,
+        answer_raw_dict_y=None,
+        num_workers: int = 0,
+        directories: list[pathlib.Path] = None,
+        **kwargs
+    ) -> None:
         self.x_variable_names = x_variable_names
         self.y_variable_names = y_variable_names
         self.raw_dict_x = raw_dict_x
         self.answer_raw_dict_y = answer_raw_dict_y
         self.num_workers = num_workers
+        self.supports = supports
+        self.data_directories = directories if directories is not None else []
+        if len(self.data_directories) > 1:
+            raise ValueError(
+                "Simplified Dataset allows one data only."
+            )
 
         self.x_dict_mode = isinstance(self.x_variable_names, dict)
         self.y_dict_mode = isinstance(self.y_variable_names, dict)
 
         if self.x_dict_mode:
             self.first_variable_name = list(
                 self.x_variable_names.values())[0][0]
         else:
             self.first_variable_name = self.x_variable_names[0]
         return
 
     def __len__(self):
         return 1
 
+    def _get_data_directory(self) -> Union[pathlib.Path, None]:
+        if len(self.data_directories) == 0:
+            return None
+        return self.data_directories[0]
+
     def __getitem__(self, i):
         dict_data = self.raw_dict_x
         x_data = self._merge_data(
             dict_data,
             self.x_variable_names
         )
         if self.answer_raw_dict_y is not None:
             dict_data.update(self.answer_raw_dict_y)
             y_data = self._merge_data(dict_data, self.y_variable_names)
         else:
             y_data = None
 
-        return {
-            'x': x_data, 't': y_data, 'data_directory': None}
+        if self.supports is None:
+            return {
+                'x': x_data, 't': y_data,
+                'data_directory': self._get_data_directory()
+            }
+        else:
+            support_data = [
+                dict_data[support_input_name].astype(np.float32)
+                for support_input_name in self.supports]
+            return {
+                'x': x_data, 't': y_data, 'supports': support_data,
+                'data_directory': self._get_data_directory()}
 
 
 class DataDict(dict):
 
     @property
     def device(self):
         devices = [v.device for v in self.values()]
```

## siml/inferer.py

```diff
@@ -1,26 +1,28 @@
 from __future__ import annotations
 import pathlib
 from typing import Callable, Optional, Union
 
 import numpy as np
 from torch import Tensor
 from torch.utils.data import DataLoader
+import femio
+from ignite.engine import State
 
 from siml import datasets, setting
 from siml.base.siml_const import SimlConstItems
 from siml.loss_operations import LossCalculatorBuilder
 from siml.path_like_objects import SimlDirectory, SimlFileBuilder
 from siml.preprocessing import ScalersComposition
 from siml.preprocessing.converter import (
     ILoadFunction, IConvertFunction, RawConverter)
 from siml.services import ModelEnvironmentSetting, ModelSelectorBuilder
 from siml.services.inference import (
     CoreInferer, InferenceDataLoaderBuilder, InnerInfererSetting,
-    PostPredictionRecord
+    PredictionRecord, PostPredictionRecord
 )
 from siml.services.inference.postprocessing import (
     IInfererSaveFunction, SaveProcessor, PostProcessor,
     PostFEMDataConverter, IFEMDataAdditionFunction
 )
 
 
@@ -333,26 +335,25 @@
         self.data_addition_function = data_addition_function
         self.save_function = save_function
         if scalers is None:
             self._scalers = self._inner_setting.load_scalers()
         else:
             self._scalers = scalers
 
-        fem_data_creator = PostFEMDataConverter(
+        self._fem_data_converter = PostFEMDataConverter(
             inferer_setting=self._inner_setting.inferer_setting,
             conversion_setting=self._inner_setting.conversion_setting,
             load_function=load_function,
             data_addition_function=data_addition_function
         )
 
         self._model_env = self._create_model_env_setting()
         self._collate_fn = self._create_collate_fn()
         self._dataloader_builder = self._create_data_loader_builder()
         self._core_inferer = self._create_core_inferer(
-            fem_data_creator,
             user_loss_function_dic
         )
         self._save_processor = SaveProcessor(
             inner_setting=self._inner_setting,
             user_save_function=save_function
         )
 
@@ -393,20 +394,18 @@
             input_time_slices=input_time_slices,
             output_time_slices=output_time_slices
         )
         return collate_fn
 
     def _create_core_inferer(
         self,
-        fem_data_creator: PostFEMDataConverter,
         user_loss_function_dic: dict = None
     ) -> CoreInferer:
         post_processor = PostProcessor(
             inner_setting=self._inner_setting,
-            fem_data_converter=fem_data_creator,
             scalers=self._scalers
         )
         loss_function = LossCalculatorBuilder.create(
             trainer_setting=self._inner_setting.trainer_setting,
             allow_no_answer=True,
             user_loss_function_dic=user_loss_function_dic
         )
@@ -481,39 +480,40 @@
                 = output_directory_base
 
         inference_loader = self._dataloader_builder.create(
             data_directories=data_directories
         )
         inference_state = self._core_inferer.run(inference_loader)
 
+        records = self._create_post_records(inference_state)
         if self._inner_setting.inferer_setting.save:
             self._save_processor.run(
-                inference_state, save_summary=save_summary
+                records, save_summary=save_summary
             )
 
         if output_all:
             return inference_state
         else:
-            return self._format_results(inference_state.metrics)
+            return self._format_results(records)
 
     def infer_dataset(
         self,
         preprocess_dataset: datasets.PreprocessDataset,
         output_directory_base: Optional[pathlib.Path] = None,
         save_summary: Optional[bool] = True
     ) -> list[dict]:
-        """_summary_
+        """Perform inference for datasets
 
         Parameters
         ----------
         preprocess_dataset : datasets.PreprocessDataset
             dataset of preprocessed data
         output_directory_base : Optional[pathlib.Path], optional
             base output directory, by default None
-        save : Optional[bool], optional
+        save_summary : Optional[bool], optional
             If fed, overwrite save option. by default None
 
         Returns
         -------
         inference_results: list[Dict]
             Inference results contains:
                 - dict_x: input and variables
@@ -538,63 +538,84 @@
             collate_fn=self._collate_fn,
             batch_size=1,
             shuffle=False,
             num_workers=0
         )
         inference_state = self._core_inferer.run(inference_loader)
 
+        records = self._create_post_records(inference_state)
         if self._inner_setting.inferer_setting.save:
             self._save_processor.run(
-                inference_state, save_summary=save_summary
+                records, save_summary=save_summary
             )
 
-        return self._format_results(inference_state.metrics)
+        return self._format_results(records)
 
     def infer_dict_data(
         self,
         scaled_dict_x: dict,
         *,
+        data_directory: pathlib.Path = None,
         scaled_dict_answer: Optional[dict] = None,
-        save_summary: Optional[bool] = False
+        save_summary: Optional[bool] = True,
+        base_fem_data: Optional[femio.FEMData] = None
     ):
         """
-        Infer with simplified model.
+        Infer with dictionary data.
 
         Parameters
         ----------
         scaled_dict_x: dict
             Dict of scaled x data.
+        data_directory: pathlib.Path, optional
+            path to directory of simulation files
         scaled_dict_answer: dict, optional
             Dict of answer scaled y data.
+        save_summary: bool, default True
+            If True, save summary information of inference
+        base_fem_data: femio.FEMData, optional
+            If fed, inference results are registered to base_fem_data and
+             saved as a file.
+
 
         Returns
         -------
         inference_result: Dict
             Inference results contains:
                 - dict_x: input and answer variables
                 - dict_y: inferred variables
                 - loss: Loss value (scaled)
                 - raw_loss: Loss in a raw scale
                 - fem_data: FEMData object
                 - output_directory: Output directory path
                 - data_directory: Input directory path
                 - inference_time: Inference time
         """
+        if data_directory is not None:
+            data_directories = [data_directory]
+        else:
+            data_directories = None
+
         inference_loader = self._dataloader_builder.create(
             raw_dict_x=scaled_dict_x,
-            answer_raw_dict_y=scaled_dict_answer
+            answer_raw_dict_y=scaled_dict_answer,
+            data_directories=data_directories
         )
         inference_state = self._core_inferer.run(inference_loader)
 
+        records = self._create_post_records(
+            inference_state,
+            base_fem_data=base_fem_data
+        )
         if self._inner_setting.inferer_setting.save:
             self._save_processor.run(
-                inference_state, save_summary=save_summary
+                records, save_summary=save_summary
             )
 
-        return self._format_results(inference_state.metrics)
+        return self._format_results(records)
 
     def infer_parameter_study(
             self, model, data_directories, *, n_interpolation=100,
             converter_parameters_pkl=None):
         """
         Infer with performing parameter study. Parameter study is done with the
         data generated by interpolating the input data_directories.
@@ -705,26 +726,71 @@
         setting.write_yaml(
             main_setting,
             output_setting_yaml.file_path,
             key=encrypt_key
         )
         return
 
-    def _format_results(self, metrics: dict) -> list[dict]:
-        results = []
-        records: list[PostPredictionRecord] = metrics.pop("post_results")
-        for i, val in enumerate(records):
-            _data = {
-                name: getattr(val, name)
+    def _format_results(
+        self,
+        records: list[PostPredictionRecord]
+    ) -> list[dict]:
+        results = [
+            {
+                name: getattr(record, name)
                 for name in PostPredictionRecord._fields
             }
-            for name, item in metrics.items():
-                _data.update({name: item[i]})
+            for record in records
+        ]
+        return results
 
-            each_output_directory = \
+    def _create_post_records(
+        self,
+        state: State,
+        base_fem_data: femio.FEMData = None
+    ) -> list[PostPredictionRecord]:
+        records: list[PredictionRecord] = state.metrics["post_results"]
+
+        new_records: list[PostPredictionRecord] = []
+        for i, _record in enumerate(records):
+            fem_data = self._create_fem_data(
+                _record, base_fem_data=base_fem_data
+            )
+            output_directory = \
                 self._inner_setting.get_output_directory(
-                    val.inference_start_datetime,
-                    data_directory=val.data_directory,
+                    _record.inference_start_datetime,
+                    data_directory=_record.data_directory,
                 )
-            _data.update({"output_directory": each_output_directory})
-            results.append(_data)
-        return results
+            _new_record = PostPredictionRecord(
+                *_record,
+                loss=state.metrics["loss"][i],
+                raw_loss=state.metrics["raw_loss"][i],
+                output_directory=output_directory,
+                fem_data=fem_data
+            )
+            new_records.append(_new_record)
+        return new_records
+
+    def _create_fem_data(
+        self,
+        record: PredictionRecord,
+        base_fem_data: femio.FEMData = None
+    ) -> Union[femio.FEMData, None]:
+
+        if self._inner_setting.skip_fem_data_creation(
+            record.data_directory
+        ):
+            return None
+
+        write_simulation_case_dir = \
+            self._inner_setting.get_write_simulation_case_dir(
+                record.data_directory
+            )
+
+        fem_data = self._fem_data_converter.create(
+            dict_data_x=record.dict_x,
+            dict_data_y=record.dict_y,
+            dict_data_answer=record.dict_answer,
+            write_simulation_case_dir=write_simulation_case_dir,
+            base_fem_data=base_fem_data
+        )
+        return fem_data
```

## siml/preprocessing/converter.py

```diff
@@ -100,26 +100,26 @@
             rules = SimlPathRules()
             return rules.determine_output_directory(
                 self.raw_path,
                 self.setting.output_base_directory,
                 allowed_type=DirectoryType.RAW
             )
 
-    def run(self) -> Union[None, dict]:
+    def run(self) -> Union[tuple[dict, femio.FEMData], None]:
 
         values = self._convert()
         if values is None:
             print(
                 f"Conversion process for {self.raw_path} has failed"
             )
             return None
 
         dict_data, fem_data = values
         if self.return_results:
-            return dict_data
+            return (dict_data, fem_data)
 
         self.save_function(
             fem_data,
             dict_data,
             self.output_directory,
             self.force_renew
         )
@@ -308,15 +308,15 @@
             )
 
     def convert(
         self,
         raw_directory: pathlib.Path = None,
         *,
         return_results: bool = False
-    ) -> dict[str, Union[dict, None]]:
+    ) -> dict[str, Union[tuple[dict, femio.FEMData], None]]:
         """Perform conversion.
 
         Parameters
         ----------
         raw_directory: pathlib.Path, optional
             Raw data directory name. If not fed, self.setting.data.raw is used
             instead.
@@ -352,15 +352,15 @@
     def convert_single_data(
         self,
         raw_path: pathlib.Path,
         *,
         output_directory: pathlib.Path = None,
         raise_when_overwrite: bool = False,
         return_results: bool = False
-    ) -> dict[str, Union[dict, None]]:
+    ) -> dict[str, Union[tuple[dict, femio.FEMData], None]]:
         """Convert single directory.
 
         Parameters
         ----------
         raw_path: pathlib.Path
             Input data path of raw data.
         output_directory: pathlib.Path, optional
```

## siml/services/inference/__init__.py

```diff
@@ -1,5 +1,5 @@
 # flake8: noqa
 from .inner_setting import InnerInfererSetting  # NOQA
 from .core_inferer import CoreInferer  # NOQA
 from .data_loader_builder import InferenceDataLoaderBuilder  # NOQA
-from .record_object import PostPredictionRecord, PredictionRecord  # NOQA
+from .record_object import PredictionRecord, RawPredictionRecord, PostPredictionRecord  # NOQA
```

## siml/services/inference/data_loader_builder.py

```diff
@@ -28,15 +28,17 @@
         if raw_dict_x is not None:
             inference_dataset = datasets.SimplifiedDataset(
                 self._trainer_setting.input_names,
                 self._trainer_setting.output_names,
                 raw_dict_x=raw_dict_x,
                 answer_raw_dict_y=answer_raw_dict_y,
                 allow_no_data=allow_no_data,
-                num_workers=0
+                num_workers=0,
+                supports=self._trainer_setting.support_inputs,
+                directories=data_directories
             )
         else:
             inference_dataset = datasets.LazyDataset(
                 self._trainer_setting.input_names,
                 self._trainer_setting.output_names,
                 data_directories,
                 supports=self._trainer_setting.support_inputs,
```

## siml/services/inference/engine_bulider.py

```diff
@@ -4,15 +4,15 @@
 import torch
 from ignite.engine import Engine
 
 from siml import networks, util
 from siml.siml_variables import siml_tensor_variables
 from siml.inferer import ModelEnvironmentSetting
 
-from .record_object import PredictionRecord
+from .record_object import RawPredictionRecord
 from .postprocessing import PostProcessor
 
 
 class InferenceEngineBuilder:
     def __init__(
         self,
         env_setting: ModelEnvironmentSetting,
@@ -60,15 +60,15 @@
 
             data_directory = batch['data_directories'][0]
             print('--')
             print(f"              Data: {data_directory}")
             print(f"Inference time [s]: {elapsed_time:.5e}")
             print('--')
 
-            result = PredictionRecord(
+            result = RawPredictionRecord(
                 y_pred=siml_tensor_variables(y_pred),
                 y=siml_tensor_variables(y),
                 x=siml_tensor_variables(x['x']),
                 original_shapes=x['original_shapes'],
                 data_directory=data_directory,
                 inference_time=elapsed_time
             )
```

## siml/services/inference/inner_setting.py

```diff
@@ -136,7 +136,30 @@
         model_name = self.get_model_name()
         return f"{model_name}_{date_string}"
 
     def get_model_name(self) -> str:
         snapshot_file = self.get_snapshot_file_path()
         model_name = snapshot_file.parent.name
         return model_name
+
+    def skip_fem_data_creation(
+        self,
+        data_directory: Optional[pathlib.Path] = None
+    ) -> bool:
+        write_simulation_base = self.get_write_simulation_case_dir(
+            data_directory
+        )
+
+        if self.inferer_setting.skip_fem_data_creation:
+            return True
+
+        if write_simulation_base is None:
+            return True
+
+        if not write_simulation_base.exists():
+            print(
+                f"{write_simulation_base} does not exist."
+                "Thus, skip creating fem data."
+            )
+            return True
+
+        return False
```

## siml/services/inference/metrics_builder.py

```diff
@@ -4,15 +4,15 @@
 import numpy as np
 import torch
 from ignite.metrics.metric import Metric, reinit__is_reduced
 
 from siml import setting
 from siml.loss_operations import ILossCalculator
 
-from .record_object import PredictionRecord, PostPredictionRecord
+from .record_object import RawPredictionRecord, PredictionRecord
 
 
 class MetricsBuilder():
     def __init__(
         self,
         trainer_setting: setting.TrainerSetting,
         loss_function: ILossCalculator
@@ -44,15 +44,15 @@
     @reinit__is_reduced
     def reset(self):
         self._results = []
         return
 
     @reinit__is_reduced
     def update(self, output):
-        record: PostPredictionRecord = output[2]["post_result"]
+        record: PredictionRecord = output[2]["post_result"]
         self._results.append(record)
         return
 
     def compute(self):
         return self._results
 
 
@@ -69,15 +69,15 @@
         self._results = []
         return
 
     @reinit__is_reduced
     def update(self, output):
         y_pred = output[0]
         y = output[1]
-        record: PredictionRecord = output[2]["result"]
+        record: RawPredictionRecord = output[2]["result"]
         loss = self.loss_function(
             y_pred,
             y,
             original_shapes=record.original_shapes
         )
         if loss is not None:
             print(f"              Loss: {loss}")
@@ -104,15 +104,15 @@
     @reinit__is_reduced
     def reset(self):
         self._results = []
         return
 
     @reinit__is_reduced
     def update(self, output):
-        post_result: PostPredictionRecord = output[2]["post_result"]
+        post_result: PredictionRecord = output[2]["post_result"]
         raw_loss = self._compute_raw_loss(
             post_result.dict_answer,
             post_result.dict_y,
             post_result.original_shapes
         )
 
         if raw_loss is not None:
```

## siml/services/inference/postprocessing/post_fem_data.py

```diff
@@ -45,45 +45,60 @@
 
     def create(
         self,
         dict_data_x: dict[str, np.ndarray],
         dict_data_y: dict[str, np.ndarray],
         dict_data_answer: Optional[dict[str, np.ndarray]] = None,
         write_simulation_case_dir: Union[pathlib.Path, None] = None,
+        base_fem_data: Optional[femio.FEMData] = None
+    ) -> Union[femio.FEMData, None]:
+
+        if base_fem_data is not None:
+            fem_data = base_fem_data
+        else:
+            fem_data = self._create_fem_data(write_simulation_case_dir)
+
+        if fem_data is None:
+            return None
+
+        if self._inferer_setting.convert_to_order1:
+            fem_data = fem_data.to_first_order()
+
+        self._add_fem_data(
+            fem_data,
+            dict_data_x=dict_data_x,
+            dict_data_y=dict_data_y,
+            dict_data_answer=dict_data_answer,
+            write_simulation_case_dir=write_simulation_case_dir
+        )
+
+        return fem_data
+
+    def _create_fem_data(
+        self,
+        write_simulation_case_dir: Union[pathlib.Path, None] = None
     ) -> Union[femio.FEMData, None]:
 
         try:
             fem_data = self._create_simulation_fem_data(
                 write_simulation_case_dir
             )
+            return fem_data
         except ValueError as e:
             write_simulation_stem = self._inferer_setting.write_simulation_stem
             read_simulation_type = self._inferer_setting.read_simulation_type
             print(
                 f"{e}\n"
                 'Could not read FEMData object, set None\n'
                 f"write_simulation_case_dir: {write_simulation_case_dir}\n"
                 f"write_simulation_stem: {write_simulation_stem}\n"
                 f"read_simulation_type: {read_simulation_type}\n"
             )
             return None
 
-        if self._inferer_setting.convert_to_order1:
-            fem_data = fem_data.to_first_order()
-
-        self._add_fem_data(
-            fem_data,
-            dict_data_x=dict_data_x,
-            dict_data_y=dict_data_y,
-            dict_data_answer=dict_data_answer,
-            write_simulation_case_dir=write_simulation_case_dir
-        )
-
-        return fem_data
-
     def _create_simulation_fem_data(
         self,
         write_simulation_case_dir: pathlib.Path
     ) -> femio.FEMData:
 
         write_simulation_stem = self._inferer_setting.write_simulation_stem
         read_simulation_type = self._inferer_setting.read_simulation_type
```

## siml/services/inference/postprocessing/postprocessor.py

```diff
@@ -1,53 +1,47 @@
 from __future__ import annotations
-
-import pathlib
 from typing import Optional, Union
 
 import numpy as np
 
 from siml.preprocessing import ScalersComposition
 from siml.services.inference import InnerInfererSetting
 from siml.services.inference.record_object import (
-    PostPredictionRecord,
-    PredictionRecord
+    PredictionRecord,
+    RawPredictionRecord
 )
 from siml.base.siml_typing import ArrayDataType
 from siml.services.path_rules import SimlPathRules
 from siml.setting import CollectionVariableSetting
 
-from .post_fem_data import PostFEMDataConverter
-
 
 class PostProcessor:
     def __init__(
         self,
         inner_setting: InnerInfererSetting,
         *,
-        fem_data_converter: Optional[PostFEMDataConverter] = None,
         scalers: Optional[ScalersComposition] = None,
     ) -> None:
         self._inner_setting = inner_setting
         self._trainer_setting = inner_setting.trainer_setting
         self._perform_inverse = inner_setting.perform_inverse
-        self._fem_data_converter = fem_data_converter
         self._path_rules = SimlPathRules()
         if self._perform_inverse:
             if scalers is None:
                 raise ValueError(
                     "scalers is None. When perform inverse, "
                     "scalers must be set."
                 )
             self.scalers = scalers
 
     def convert(
         self,
-        record: PredictionRecord,
+        record: RawPredictionRecord,
         start_datetime: str
-    ) -> PostPredictionRecord:
+    ) -> PredictionRecord:
         dict_var_x = self._separate_data(
             record.x.to_numpy(),
             self._trainer_setting.inputs
         )
         dict_var_y = self._separate_data(
             record.y.to_numpy(),
             self._trainer_setting.outputs
@@ -58,36 +52,22 @@
         )
 
         converted_dict_x, converted_dict_y, converted_dict_answer = \
             self._convert_dict(
                 dict_var_x, dict_var_y_pred, dict_var_y
             )
 
-        write_simulation_case_dir = \
-            self._inner_setting.get_write_simulation_case_dir(
-                record.data_directory
-            )
-        fem_data = None
-        if not self._is_skip_fem_data(write_simulation_case_dir):
-            fem_data = self._fem_data_converter.create(
-                dict_data_x=converted_dict_x,
-                dict_data_y=converted_dict_y,
-                dict_data_answer=converted_dict_answer,
-                write_simulation_case_dir=write_simulation_case_dir
-            )
-
-        converted_record = PostPredictionRecord(
+        converted_record = PredictionRecord(
             dict_x=converted_dict_x,
             dict_y=converted_dict_y,
             dict_answer=converted_dict_answer,
             original_shapes=record.original_shapes,
             data_directory=record.data_directory,
             inference_time=record.inference_time,
-            inference_start_datetime=start_datetime,
-            fem_data=fem_data
+            inference_start_datetime=start_datetime
         )
         return converted_record
 
     def _convert_dict(
         self,
         dict_var_x: dict,
         dict_var_y_pred: dict,
@@ -138,33 +118,14 @@
             dim = description.dim
             data_dict.update({
                 description.name:
                 np.swapaxes(data[index:index+dim], 0, axis)})
             index += dim
         return data_dict
 
-    def _is_skip_fem_data(
-        self,
-        write_simulation_base: Optional[pathlib.Path] = None
-    ) -> bool:
-        if self._inner_setting.inferer_setting.skip_fem_data_creation:
-            return True
-
-        if write_simulation_base is None:
-            return True
-
-        if not write_simulation_base.exists():
-            print(
-                f"{write_simulation_base} does not exist."
-                "Thus, skip creating fem data."
-            )
-            return True
-
-        return False
-
     def _format_dict_shape(
         self,
         dict_data: Union[dict, None]
     ) -> Union[dict[str, ArrayDataType], None]:
         if dict_data is None:
             return None
```

## siml/services/inference/postprocessing/save_processor.py

```diff
@@ -1,15 +1,14 @@
 import abc
 import pathlib
 from typing import Optional
 
 import femio
 import numpy as np
 import pandas as pd
-from ignite.engine import State
 
 from siml import setting
 from siml.services.inference.inner_setting import InnerInfererSetting
 from siml.services.inference.record_object import PostPredictionRecord
 
 
 class IInfererSaveFunction(metaclass=abc.ABCMeta):
@@ -20,135 +19,77 @@
         overwrite: bool,
         write_simulation_type: str,
         **kwards
     ) -> None:
         raise NotImplementedError()
 
 
-class WrapperResultItems:
-    def __init__(
-        self,
-        inner_setting: InnerInfererSetting,
-        result_state:  State
-    ) -> None:
-        self._state = result_state
-        self._inner_setting = inner_setting
-
-        self._output_dirs = []
-        self._collect()
-
-    def __len__(self):
-        return len(self._state.metrics["post_results"])
-
-    def _collect(self):
-        for idx in range(len(self)):
-            record = self.get_post_record(idx)
-            each_output_directory = \
-                self._inner_setting.get_output_directory(
-                    record.inference_start_datetime,
-                    data_directory=record.data_directory,
-                )
-            self._output_dirs.append(each_output_directory)
-
-    def get_output_directory(self, idx: int) -> pathlib.Path:
-        return self._output_dirs[idx]
-
-    def get_post_record(self, idx: int) -> PostPredictionRecord:
-        return self._state.metrics["post_results"][idx]
-
-    def get_metrics(self, name: str) -> list[float]:
-        return self._state.metrics[name]
-
-    def get_item(self, idx: int, name: str):
-        if name in PostPredictionRecord._fields:
-            record = self.get_post_record(idx)
-            return getattr(record, name)
-
-        if name == "output_directory":
-            return self._output_dirs[idx]
-
-        if name in self._state.metrics:
-            return self._state.metrics[name][idx]
-
-        raise KeyError(f"{name}")
-
-
 class SaveProcessor():
     def __init__(
         self,
         inner_setting: InnerInfererSetting,
-        user_save_function: Optional[IInfererSaveFunction] = None
+        user_save_function: Optional[IInfererSaveFunction] = None,
     ) -> None:
         self._inner_setting = inner_setting
         self._inferer_setting = inner_setting.inferer_setting
         self._conversion_setting = inner_setting.conversion_setting
         self._user_save_function = user_save_function
 
     def run(
         self,
-        result_state: State,
+        records: list[PostPredictionRecord],
         *,
         save_summary: bool = True
     ) -> None:
 
-        results = WrapperResultItems(
-            self._inner_setting,
-            result_state
-        )
         # Save each results
-        self.save_each_results(results)
+        self._save_each_results(
+            records,
+            save_x=save_summary
+        )
 
-        if not save_summary:
-            return
+        if save_summary:
+            self._save_summary(records)
 
+    def _save_summary(self, records: list[PostPredictionRecord]):
         # Save overall settings
         output_directory = self._inner_setting.get_output_directory(
-            date_string=results.get_item(0, "inference_start_datetime")
+            date_string=records[0].inference_start_datetime
         )
         self._save_settings(output_directory)
 
         self._save_logs(
+            records=records,
             output_directory=output_directory,
-            results=results
         )
 
-    def save_each_results(
+    def _save_each_results(
         self,
-        results: WrapperResultItems
-    ):
-        for idx in range(len(results)):
-            record = results.get_post_record(idx)
-            output_directory = results.get_output_directory(idx)
-
-            self.save_npy_variables(
-                dict_data_x=record.dict_x,
-                dict_data_y=record.dict_y,
-                output_directory=output_directory
+        records: list[PostPredictionRecord],
+        save_x: bool = False
+    ) -> None:
+        for record in records:
+            output_directory = self._inner_setting.get_output_directory(
+                record.inference_start_datetime,
+                data_directory=record.data_directory,
             )
 
-            fem_data = record.fem_data
-            if fem_data is None:
+            if save_x:
+                self._save_npy_data(record.dict_x, output_directory)
+            self._save_npy_data(record.dict_y, output_directory)
+
+            if record.fem_data is None:
                 continue
 
-            self.save_fem_data(
+            self._save_fem_data(
                 output_directory=output_directory,
-                fem_data=fem_data
+                fem_data=record.fem_data
             )
 
-    def save_npy_variables(
-        self,
-        dict_data_x: dict,
-        dict_data_y: dict,
-        output_directory: pathlib.Path
-    ) -> None:
-        self._save_npy_data(dict_data_x, output_directory)
-        self._save_npy_data(dict_data_y, output_directory)
-        return
-
-    def save_fem_data(
+    def _save_fem_data(
         self,
         output_directory: pathlib.Path,
         fem_data: femio.FEMData
     ):
         default_save_func = FEMDataSaveFunction()
         overwrite = self._inferer_setting.overwrite
         write_simulation_type = self._inferer_setting.write_simulation_type
@@ -196,32 +137,27 @@
             output_directory / 'settings.yml',
             key=self._inner_setting.main_setting.get_crypt_key()
         )
         return
 
     def _save_logs(
         self,
-        results: WrapperResultItems,
+        records: list[PostPredictionRecord],
         output_directory: pathlib.Path
     ) -> None:
         column_names = [
             'loss', 'raw_loss', 'output_directory', 'data_directory',
             'inference_time'
         ]
 
-        log_dict = {}
-        for column_name in column_names:
-            log_dict.update(
-                {
-                    column_name: [
-                        results.get_item(idx, column_name)
-                        for idx in range(len(results))
-                    ]
-                }
-            )
+        log_dict = {
+            column_name: [
+                getattr(rec, column_name) for rec in records
+            ] for column_name in column_names
+        }
 
         pd.DataFrame(log_dict).to_csv(
             output_directory / 'infer.csv', index=None)
         return
 
 
 class FEMDataSaveFunction(IInfererSaveFunction):
```

## siml/services/inference/record_object.py

```diff
@@ -3,25 +3,39 @@
 
 import femio
 import numpy as np
 
 from siml.siml_variables import ISimlVariables
 
 
-class PredictionRecord(NamedTuple):
+class RawPredictionRecord(NamedTuple):
     y_pred: ISimlVariables
     y: ISimlVariables
     x: ISimlVariables
     original_shapes: tuple
     inference_time: float
     data_directory: Union[pathlib.Path, None]
 
 
-class PostPredictionRecord(NamedTuple):
+class PredictionRecord(NamedTuple):
     dict_x: dict[str, np.ndarray]
     dict_y: dict[str, np.ndarray]
     original_shapes: tuple
     data_directory: pathlib.Path
     inference_time: float
     inference_start_datetime: str
     dict_answer: Optional[dict[str, np.ndarray]] = None
+
+
+class PostPredictionRecord(NamedTuple):
+    dict_x: dict[str, np.ndarray]
+    dict_y: dict[str, np.ndarray]
+    original_shapes: tuple
+    data_directory: pathlib.Path
+    inference_time: float
+    inference_start_datetime: str
+    dict_answer: dict[str, np.ndarray]
+    # above items must be the same order of PredictionRecord
+    loss: float
+    raw_loss: float
+    output_directory: pathlib.Path
     fem_data: Optional[femio.FEMData] = None
```

## Comparing `pysiml-0.2.9.dev202306021221.dist-info/LICENSE` & `pysiml-0.2.9.dev202306080855.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pysiml-0.2.9.dev202306021221.dist-info/METADATA` & `pysiml-0.2.9.dev202306080855.dist-info/METADATA`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pysiml
-Version: 0.2.9.dev202306021221
+Version: 0.2.9.dev202306080855
 Summary: SiML - a Simulation ML library
 Home-page: https://github.com/ricosjp/pysiml
 License: Apache-2.0
 Author: RICOS Co. Ltd.
 Requires-Python: >=3.9,<3.10
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3
```

## Comparing `pysiml-0.2.9.dev202306021221.dist-info/RECORD` & `pysiml-0.2.9.dev202306080855.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-pyproject.toml,sha256=k1mLickEYzoy8edtRe1Y9coRjnZ0u3KD7fwmjuCJ-Xs,1520
+pyproject.toml,sha256=TUMmGEpgH-Rlqv6v8Rm6_GWw_cAVIj1ocGQgV7fMl8c,1520
 siml/__init__.py,sha256=mC61oSLHO5F6zXHhOrzH51R-vkH0mL4MShhSBiRRe6s,638
 siml/__main__/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 siml/__main__/convert_interim_data.py,sha256=jA6GD7BlnsEjARcln3UBwXlFj64BeFALAmgD1ohBkqo,900
 siml/__main__/convert_raw_data.py,sha256=VNVOyx9lChopF2Q-9OXtN_-MtfBsyjS_JvFwCjdkv0c,1400
 siml/__main__/optimize.py,sha256=6HALn60Mxh4GUiPbSXjvrdpkoQ4NgYSzCHCBRbli4W0,1742
 siml/__main__/plot_losses.py,sha256=0uMsoPN-xR0JhPg0WzSQL5GbD80ovuFO12eVpc0x6Xc,8663
 siml/__main__/prepare_preprocess_converters.py,sha256=w_rzic--RUyISagQOJw_5-oEkOFEKXUN7G1TkTOxd8g,927
@@ -10,16 +10,16 @@
 siml/__main__/train.py,sha256=7cLknw6yx4Ehm_K9UkUGIbg3QPAXJpr_HpNWP60MvPs,1376
 siml/__main__/visualize_graph.py,sha256=xyPqVZHA0p3xnB41w4PsKCfy7feicBRMXZveQdo_lpM,3961
 siml/base/siml_const.py,sha256=XHUALl1xhy7YVorWj_weZ0FTfsoitJdFcfKGm-9C1L8,139
 siml/base/siml_enums.py,sha256=aR3INV-U399qmFQBrjnYvB2JRyQmkaqA9cETbsx4A2I,579
 siml/base/siml_typing.py,sha256=A0pQGeRPNQmS3Dd6qNq3umZkbHRrn1YwDb0Owlurweo,228
 siml/config.py,sha256=_Cc2w65g05Y5QDuh8b8UIY_XFZbwKmXx_Z40ATxVw_I,57
 siml/data_parallel.py,sha256=b4DU0QCVVbFWB02QdvtMLuLuTIz57npYSlbmKfJVjVo,3520
-siml/datasets.py,sha256=J8YrA0jXzac_QjrlLig-ViUSUpF9jUgAiJI7GQjJy0M,25525
-siml/inferer.py,sha256=4FuLdfou7uALlEPZZF_mCnjA5PIYAi2bX6FQKgFSIVw,26838
+siml/datasets.py,sha256=v1eVzPCWgCtsR-l5xqzL1ozlGOInYXgef6FoGXk218w,26513
+siml/inferer.py,sha256=jxAlHnajD2utJnbCk7VQlN2yYP2LFtNYCPkKr4uPIhI,28921
 siml/loss_operations/__init__.py,sha256=N1Hz-fJTL3e1MCeJt2QP99TDvbO9s6AlP_XzpdiBOpA,84
 siml/loss_operations/loss_assignment.py,sha256=Eyf7riWlLm4fMWFuqpmKJIAbd-fwIWdIK0sDgTCtcLQ,1463
 siml/loss_operations/loss_calculator.py,sha256=mac4Bo4Kb2B6pGDUBamTKEPk_cxo8xPtkG9ydwE4gqM,4057
 siml/loss_operations/loss_calculator_builder.py,sha256=VOXTFCPqIOAIbq37UNsc4N6OlWWs6IECXlLxl-hsCYU,1960
 siml/loss_operations/loss_selector.py,sha256=d-sTYDF6yIyNbwQwuwpEOoUaX45Wk-7f1vs4x_q7tH0,2045
 siml/mains.py,sha256=2Bah9LWn8YPmmXfianE-_nhl9581nUlgwDtA0xUy30o,1364
 siml/networks/__init__.py,sha256=8lF58NgkEeR1cl_h5fELLyIOMeYZUS-L4wWChGSMCH8,2724
@@ -77,15 +77,15 @@
 siml/path_like_objects/siml_files/checkpoint_file.py,sha256=SmGKbhNn1SbnZwM5b2HN-UIVR_Uz5aW4e2yBimA7A5w,3245
 siml/path_like_objects/siml_files/interface.py,sha256=wrc3u6DnmZy2Tqc2toJhQx-Nt1Vz2D-ISltjAKC4d9A,2268
 siml/path_like_objects/siml_files/numpy_file.py,sha256=g3lWeK9qkcESbokyCfe1UflgEy_v6fRooosJlQF7_4o,3776
 siml/path_like_objects/siml_files/pickle_file.py,sha256=NF_jQWHFaB4478eQf3jrMHRr3mOQvXajc3KIT7zjofg,2733
 siml/path_like_objects/siml_files/yaml_file.py,sha256=uoXrH5Di1MUOBwr-2z2g6cPVGdnnAqoLyfK76bfygbo,2850
 siml/prepost.py,sha256=QXS0J07tKqhc7B8ZctWgzVBxfy7gPDFL2dZ40L9_-LQ,12361
 siml/preprocessing/__init__.py,sha256=Y1Ij4vUo0l8TVKWN3EUI0eB_tDcogmovR7FMNO5ppjs,160
-siml/preprocessing/converter.py,sha256=a7iUdMsnU0icFrbNQAJG3BeDmonhG63rj6IXLGGLK58,21509
+siml/preprocessing/converter.py,sha256=PypyZxjiCK8aP8jxqqQQRpchgTA63iBvl7iA15SwIlc,21587
 siml/preprocessing/scalers_composition.py,sha256=eKEkxgEpXXx1xe_WIHSlkNcKvezTQe6QlePbsmWvcU0,9465
 siml/preprocessing/scaling_converter.py,sha256=eiI10BjSlztPeTUZwPyHrXSF1N-3AJ9kMl7pWec1jjU,11050
 siml/preprocessing/siml_scalers/__init__.py,sha256=s1f6V4Z56KY1klO29hFYszuQLNiyTTMx36jsLRtb9PA,176
 siml/preprocessing/siml_scalers/scale_functions/__init__.py,sha256=U0OYAqKY6HRAVhztbmQgDEc7u7sqULBYYnCaYxRhZjQ,1405
 siml/preprocessing/siml_scalers/scale_functions/identity_scaler.py,sha256=W3GyGt6BzJrg6jV_SDc59cDkXuNyAxgdSEqm0JpXvlg,592
 siml/preprocessing/siml_scalers/scale_functions/interface_scaler.py,sha256=Om7j_rA9pI0eeHK15IFQInHd-edIL2jIJEOur-xbI6o,738
 siml/preprocessing/siml_scalers/scale_functions/isoam_scaler.py,sha256=KlPbXA0Ysotk5uYW6DHGvaedAZlOkU8QzUE4h5F0OQ0,2038
@@ -94,25 +94,25 @@
 siml/preprocessing/siml_scalers/scale_functions/sparse_standard_scaler.py,sha256=wdk1e2Cil-OyIo5SiG1PYKY5K60zHUEIPEYPmND4hQA,1926
 siml/preprocessing/siml_scalers/scale_functions/standard_scaler.py,sha256=qvyUGXXw4j60xGw0RwXUaHaYlnVZHR9XB_DFkayySBg,570
 siml/preprocessing/siml_scalers/scale_functions/user_defined_scaler.py,sha256=GFwX9UyJ6z9iTy8XiiJgvlv65W6dBRZ8bDLV2sECdTs,865
 siml/preprocessing/siml_scalers/scaler_result_save.py,sha256=9ZD7dibMlteAZvpePfsK5sEAhf4VMadAMclKJpypz54,1012
 siml/preprocessing/siml_scalers/scaler_wrapper.py,sha256=_YoFh6KXCS093l-CjMw8hxvy3pO0GxdMcwCj9uZwJ0o,4367
 siml/services/__init__.py,sha256=UfdYF0fGhc34bu7gPjTSCQgkD0Hnlaavx4AI5yc9iaA,154
 siml/services/environment.py,sha256=oi0GhoCxvI6HtIYPYa0D6JAh2RdD9Ao0jeeV2pVUykQ,2342
-siml/services/inference/__init__.py,sha256=W5aK2ZB7h3B2jqdCFxePon8kL5DZLab8Ew0jYm-s7Bs,258
+siml/services/inference/__init__.py,sha256=bCltATxDFTamdw_d4jpVP68ZfKUpguExJsDqLUbzVgk,279
 siml/services/inference/core_inferer.py,sha256=NUx1mGojvBlIr0ukDRMzv9hG57hC4Gv2Zso1FPBsyyI,2587
-siml/services/inference/data_loader_builder.py,sha256=-c-jWuhFP-A-T9OrGi62Rcm2yq-X1f9zcdUcywwjixw,1709
-siml/services/inference/engine_bulider.py,sha256=vkYUghBd487pSnn6IWm7Y_AUJWOFCGvjaRPsWkHIb28,2613
-siml/services/inference/inner_setting.py,sha256=I92HE_tqPtBNGWO-XctKaf-KXUMi1rACrY70CDy9HSQ,4660
-siml/services/inference/metrics_builder.py,sha256=FbyVzgQ5J4AMBETG3LTy8g5pixnb9E9Hx0U9iyoSFy0,4716
+siml/services/inference/data_loader_builder.py,sha256=5FhUabZ39P1yEODpM3pPduFLAyriyexB_qUHmPuJ7NU,1818
+siml/services/inference/engine_bulider.py,sha256=y6K7J_nhK1-3f2Cj3vNQiNh4yzda79ury12RaCFBovE,2619
+siml/services/inference/inner_setting.py,sha256=NdIYoZucJheYDYNRK2Sa3AyW4ehCxgyCEZqzccqtAdg,5263
+siml/services/inference/metrics_builder.py,sha256=I8ORAOZO4Mf5b0AaPCfpdu0ieqx1peuKkW9Z9H8RaAY,4710
 siml/services/inference/postprocessing/__init__.py,sha256=txubVxvNDftzgw3M05LP2GPmwvvJZCZKJcU02q4iiqs,203
-siml/services/inference/postprocessing/post_fem_data.py,sha256=5bpZQjCUUCNJQikJWnQHecDt_XvAM4GefF0nG59kwxY,5159
-siml/services/inference/postprocessing/postprocessor.py,sha256=qFwbpoHCXDcMXb0wjqAsA4a4a_79KkpK0CX8a5eNCw8,6246
-siml/services/inference/postprocessing/save_processor.py,sha256=r_twKIzuwcY-H7yX6vES9yfYULipnLuxTrVwighiWVs,8875
-siml/services/inference/record_object.py,sha256=pqtKnLnSvqbTBoRCcDoWPf0FjImFUditpEdcYdvwFOg,682
+siml/services/inference/postprocessing/post_fem_data.py,sha256=8apB2-cUyssum_6BOx9uVUhmNNvBjoXRnSWn4w4aJ-A,5603
+siml/services/inference/postprocessing/postprocessor.py,sha256=lnHod9-Wa87QLb4ZMq7cOOeRxFjKQYq4NeuPaTHD6Nk,4979
+siml/services/inference/postprocessing/save_processor.py,sha256=oJVxJJplyH8dLiYIpG1GJWD_Y58RwrnAwlqNt_dLuA0,7052
+siml/services/inference/record_object.py,sha256=9cSmtHyNuVwtQbAxkMoUOhCg7TQ6fgyFFwWqyBJ3Gw4,1082
 siml/services/model_builder.py,sha256=ybQorVluaFo-7gBFftUp4h5GaKCUhUDYfqG2PLl6o8M,2574
 siml/services/model_selector.py,sha256=q2oXdJcujaOa6Y_HFHZ1emxkNaRN00BAGi_kc0zXmvE,4978
 siml/services/path_rules.py,sha256=fPNdpXcf8GZtI6ZfMrEkKcdo0JkxxPNmnvohbY1evI8,7124
 siml/setting.py,sha256=brzjJydqLTzNngj-Lc6BsMGXkk3UGgqzzL_v_oJJWmQ,50226
 siml/siml_manager.py,sha256=HVDMjvFPq81UsKBlbwv7SrBnYE8USqQHfxZ8tb_iD84,11028
 siml/siml_variables/__init__.py,sha256=Nt8iJ0yLPBJpg1o6K_b5Qr-go5iurvC9FnHxI4lD6eg,163
 siml/siml_variables/array_variables/__init__.py,sha256=Y2syJXtd98L-Z3kRTCOlCEVZffc-xBbuMIS1uv3ekQU,594
@@ -126,12 +126,12 @@
 siml/update_functions/element_batch_update.py,sha256=93_L9jl4D9lFQXmEX1Lo8l4n3rDf1wdZljPJIQd9rUs,1774
 siml/update_functions/pseudo_batch_update.py,sha256=7PS6SEXOMB57uaJQhZ8VrtEiIkDF59JE1c6yyrKL3YY,3623
 siml/update_functions/standard_update.py,sha256=jIgeQ_c9R1dgcxDQWzx-w5TkJ7naDKmNi_WUFFchxc4,2702
 siml/update_functions/update_interface.py,sha256=O1fGrE21iXckz7NTOTDNlyQ4rll1SQDehdc_Kk5we0Q,429
 siml/util.py,sha256=AzvnYN1svE28ioHGPgMjkpiK35hCA5m5qwFEDb3agHA,23027
 siml/utils/__init__.py,sha256=DVdxWEBXfapWAWNM0jUouKJzru4RyK2ru2QheBmrm_s,37
 siml/utils/fem_data_utils.py,sha256=LDB_MxYPg_WELl6D_az9ikMZxAF5Ml5qKUXR2CnuxvE,7682
-pysiml-0.2.9.dev202306021221.dist-info/LICENSE,sha256=Ok4O8jxCIpLhGZjTNzgBW8V8xITF_SUVEp1juLijZpA,11431
-pysiml-0.2.9.dev202306021221.dist-info/METADATA,sha256=kpB-qDeW3b8A0WK50svzGsPIwNsm_-TFDa5Op73SjAY,1638
-pysiml-0.2.9.dev202306021221.dist-info/WHEEL,sha256=7Z8_27uaHI_UZAc4Uox4PpBhQ9Y5_modZXWMxtUi4NU,88
-pysiml-0.2.9.dev202306021221.dist-info/entry_points.txt,sha256=zXU0Gd56XeUfgaeG-CGXcYaTpORSU6kVeQ_vVvjIFPs,388
-pysiml-0.2.9.dev202306021221.dist-info/RECORD,,
+pysiml-0.2.9.dev202306080855.dist-info/LICENSE,sha256=Ok4O8jxCIpLhGZjTNzgBW8V8xITF_SUVEp1juLijZpA,11431
+pysiml-0.2.9.dev202306080855.dist-info/METADATA,sha256=EUbOhQ4SL1BedhhOPJ4Dn2au9QDJx7oBHashtESWmPI,1638
+pysiml-0.2.9.dev202306080855.dist-info/WHEEL,sha256=7Z8_27uaHI_UZAc4Uox4PpBhQ9Y5_modZXWMxtUi4NU,88
+pysiml-0.2.9.dev202306080855.dist-info/entry_points.txt,sha256=zXU0Gd56XeUfgaeG-CGXcYaTpORSU6kVeQ_vVvjIFPs,388
+pysiml-0.2.9.dev202306080855.dist-info/RECORD,,
```

